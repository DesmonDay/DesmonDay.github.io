<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="python," />





  <link rel="alternate" href="/atom.xml" title="DesmonDay's Blog" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="为什么要研究实例：Why look at case studies?就像我们看别人的代码来学习编程一样，通过研究别人构建有效组件的实例也有利于我们的进步。实际上，在计算机视觉任务中表现良好的神经网络框架，往往也适用于其他任务。 本周框架： 尽管我的方向是NLP而不是CV，但我觉得学习这些知识也可能给我带来一些启发。 经典网络：Classic NetworkLeNet-5论文名称：1998, Gra">
<meta name="keywords" content="python">
<meta property="og:type" content="article">
<meta property="og:title" content="第11周-深度卷积神经网络">
<meta property="og:url" content="https://github.com/DesmonDay/2019/04/22/deep-learningw11/index.html">
<meta property="og:site_name" content="DesmonDay&#39;s Blog">
<meta property="og:description" content="为什么要研究实例：Why look at case studies?就像我们看别人的代码来学习编程一样，通过研究别人构建有效组件的实例也有利于我们的进步。实际上，在计算机视觉任务中表现良好的神经网络框架，往往也适用于其他任务。 本周框架： 尽管我的方向是NLP而不是CV，但我觉得学习这些知识也可能给我带来一些启发。 经典网络：Classic NetworkLeNet-5论文名称：1998, Gra">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-3687089c68bac73c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-4e910b02d6352a29.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-79f60a2ca25b8384.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-13a8a275d97047b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-d9c1196cdbfa4e2c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-1be201d72a334145.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-1ed377ce8f44da79.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-63e9024edcaf03ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-35fcbb0c593197eb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-9578fd8c334562e7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-6a549d4dbec3f148.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-aac996651ccbf765.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-fbc5aefdd70c01d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-ee93a77f7c64cfd1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-f7157e63d390b856.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-e46e27fa822065a1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-85df96549d033170.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-82ee668d12ee5fb7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-933d7f4a64642dbd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-59c8869b6c28ee2c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-85a4a092cabb2646.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-266e0c1258429a9f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-ddb921702cff581f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-60d4ae8655494447.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-08325eec9599fb15.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-621c4a01351e88a1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-5fa4d3bb2f7e35d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-14646dba1095e556.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-62695a5a31fe9f1d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-9bdf1d2dddfaa040.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-82542b2f348fda8a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-b53bd750e48c3398.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-a27eadbcae8ea2db.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-02b0b5e0ea98a2c3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-24b9c326de7a32ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-bfed90c554f6fd81.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-6a0e5278100d6218.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-72cfe8a4efaeeada.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:updated_time" content="2019-04-23T08:47:41.584Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="第11周-深度卷积神经网络">
<meta name="twitter:description" content="为什么要研究实例：Why look at case studies?就像我们看别人的代码来学习编程一样，通过研究别人构建有效组件的实例也有利于我们的进步。实际上，在计算机视觉任务中表现良好的神经网络框架，往往也适用于其他任务。 本周框架： 尽管我的方向是NLP而不是CV，但我觉得学习这些知识也可能给我带来一些启发。 经典网络：Classic NetworkLeNet-5论文名称：1998, Gra">
<meta name="twitter:image" content="https://upload-images.jianshu.io/upload_images/8636110-3687089c68bac73c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://github.com/DesmonDay/2019/04/22/deep-learningw11/"/>





  <title>第11周-深度卷积神经网络 | DesmonDay's Blog</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">DesmonDay's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">一只小辣鸡的自我拯救之路</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/DesmonDay/2019/04/22/deep-learningw11/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="DesmonDay">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DesmonDay's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">第11周-深度卷积神经网络</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-22T19:56:53+08:00">
                2019-04-22
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="为什么要研究实例：Why-look-at-case-studies"><a href="#为什么要研究实例：Why-look-at-case-studies" class="headerlink" title="为什么要研究实例：Why look at case studies?"></a>为什么要研究实例：Why look at case studies?</h1><p>就像我们看别人的代码来学习编程一样，通过研究别人构建有效组件的实例也有利于我们的进步。实际上，在计算机视觉任务中表现良好的神经网络框架，往往也适用于其他任务。</p>
<p>本周框架：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-3687089c68bac73c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>尽管我的方向是NLP而不是CV，但我觉得学习这些知识也可能给我带来一些启发。</p>
<h1 id="经典网络：Classic-Network"><a href="#经典网络：Classic-Network" class="headerlink" title="经典网络：Classic Network"></a>经典网络：Classic Network</h1><h2 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h2><p>论文名称：1998, Gradient-based learning applied to document recognition</p>
<p>LeNet-5针对的是灰度图像，因此图片的大小为32x32x1。实际上，LeNet-5的结构与我们上一篇博客所讲的很像。由于这篇论文是在1998年写成的，当时人们更经常用平均池化，并且不使用padding。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-4e910b02d6352a29.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>随着网络层次的加深，可以发现信道数量一直在增加，而n_H和n_W不断减小。因此在现代的卷积神经网络中，我们会添加padding层。</p>
<h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>论文名称：2012, ImageNet classification with deep convolutional neural network</p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-79f60a2ca25b8384.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>另外，AlexNet有以下几个特点：</p>
<ol>
<li>与LeNet-5相似，但是网络很大</li>
<li>AlexNet使用的是ReLu激活函数，而LeNet-5当时用的是sigmoid和tanh</li>
<li>由于当时的计算能力不强，因此AlexNet是在多个GPU上运行的，即将多个层的运算放置到不同的GPU上</li>
<li>AlexNet还使用了Local Response Normalization(LRN)，但现在基本不使用了，因此不讲解。</li>
</ol>
<p>另外，AlexNet非常大，它具有着一共6000,000个参数。</p>
<h2 id="VGG-16"><a href="#VGG-16" class="headerlink" title="VGG-16"></a>VGG-16</h2><p>VGG的参数比较少，它是只需要专注于构建卷积层的简单网络。其最大的优点就是简化了网络结构。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-13a8a275d97047b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>可以看到，所有的卷积核参数、池化层参数都是相同的。注意到图中的”x2”,”x3”表示连续做了几次卷积操作。尽管网络结构看起来很深，并且它的参数达到了1.38亿（很大），但由于其理解起来比较简单，因此受到了很多人的青睐。另外，可以注意到图像的高和宽很有规律地减小(224-&gt;112-&gt;56-&gt;28-&gt;14-&gt;7)，而卷积的通道数也有规律地增长(64-&gt;128-&gt;512)，因此也受到了吴老师的称赞。</p>
<p>尽管现在也有另一个VGG-19，这个网络要更大。但是由于VGG-16的表现与其差不多，因此更多人使用的是VGG-16。</p>
<p>阅读论文推荐顺序：AlexNet-&gt;VGG-&gt;LeNet-5</p>
<h1 id="残差网络：Residual-Networks"><a href="#残差网络：Residual-Networks" class="headerlink" title="残差网络：Residual Networks"></a>残差网络：Residual Networks</h1><p>论文名称：2015，Deep residual networks for image recognition</p>
<p>ResNet是由残差块(residual block)构建的，下面介绍一下残差块。</p>
<p>在我们一般的神经网络计算中，假设我们从a[l]计算到a[l+2]，一般经过下面的几个阶段：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d9c1196cdbfa4e2c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>可以看到，在得到输入a[l]后，需要经过LINEAR-&gt;ReLU得到下一层输入a[l+1]，再继续同样的操作得到a[l+2]。而这一条主路径在残差网络中有所变化。</p>
<p>我们会将a[l]复制到神经网络的深层，在ReLu非线性激活前加上a[l]，我们把这条路称为”short cut/skip connection”。因此a[l]插入的时机是在LINEAR之后，ReLU之前。图示表示如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-1be201d72a334145.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>如图，右下角的新的a[l+2]的式子就代表了一个残差块。在实际操作中，a[l]可以跳过一层或者好几层，从而将信息传递到神经网络的更深层。ResNet的发明者发现使用残差块能够训练更深的神经网络。所以构建一个ResNet网络就是通过将很多这样的残差块堆积在一起，形成一个深层神经网络。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-1ed377ce8f44da79.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>如上图所示，该神经网络是由5个残差块连接在一起构成的残差网络。</p>
<p>如果我们使用标准优化算法训练一个普通的神经网络(plain network)，没有残差块，凭借经验，我们会发现随着网络深度的加深，训练错误会减小，然后再增大。而理论上，随着网络深度的加深，训练错误应该越小。因此对于普通神经网络来说，深度越深，用优化算法越难训练，因此训练错误越多。在使用ResNet，随着网络加深，训练错误一直在减少。<strong>这种方式有助于解决梯度消失和梯度爆炸问题，让我们在训练更深网络的同时，又能保持良好的性能。</strong></p>
<h2 id="为什么ResNet表现好"><a href="#为什么ResNet表现好" class="headerlink" title="为什么ResNet表现好"></a>为什么ResNet表现好</h2><p>下面给出一个例子来解释ResNet表现好的原因，至少可以说明，如何在构建更深层次的ResNet网络的同时，还不降低他们在训练集上的效率。通常来讲，网络在训练集上表现好，才能在hold-out交叉验证集上或dev/test set有好的表现。</p>
<p>在训练普通的神经网络时，我们发现网络越深，它在训练集上的表现越差，因此我们往往不使用太深的网络。但这一原则在训练ResNet的时候并不适用。下面看例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-63e9024edcaf03ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>如图，我们给Big NN添加了两层，并且增加了一个残差块。此时的输出a[l+2]为残差块的输出。我们注意到，如果我们使用L2正则化或者权重衰减，它会压缩W[l+2]的值。如果X[l+2]=0, b=0，那么这几项就没有了，因此g(a[l]) = a[l]。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-35fcbb0c593197eb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>因此这意味着，即使给神经网络增加了这两层，它的效率也并不逊色于更简单的神经网络，因为学习identity function对该网络来说并不难。不论是把残差块添加到神经网络的中间还是末端位置，都不会影响网络的表现。吴老师认为，残差网络表现好的原因是<strong>这些残差块能够很容易地学习恒等函数</strong>(相比之下，普通深层网络是难以学习的），我们能确定网络性能不会受到影响，甚至可以提高我们的网络性能。</p>
<p>除此之外，关于残差网络的另一个值得探讨的细节是<strong>假设z[l+2]和a[l]具有相同的维度</strong>。因此在ResNet中使用了很多的卷积，使得a[l]的维度等于z[l+2]的维度。</p>
<p>接下来看一下网络对比的例子。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-9578fd8c334562e7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>可以看到，ResNet的卷积核基本都是3x3相同的，保证了z[l+2]和a[l]的维度相同。</p>
<h1 id="Network-in-Network-and-1x1-convolutions"><a href="#Network-in-Network-and-1x1-convolutions" class="headerlink" title="Network in Network and 1x1 convolutions"></a>Network in Network and 1x1 convolutions</h1><p>论文名称：Network in Network, 2013</p>
<p>在架构内容设计(designing content architectures)方面，其中一个比较有帮助的想法是使用1x1卷积。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-6a549d4dbec3f148.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>我们看到，对于6x6x1平面而言，1x1卷积只是将原图像的每个数字乘以卷积核内的数字而已，因此没有起到作用。而对于一张x6x32的图片来说，使用1x1过滤器进行卷积效果更好。具体来说，1x1卷积核的作用是遍历这36个单元格，计算左图中32个数字和过滤器中32个数字的乘积，然后应用ReLU非线性函数，图例如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-aac996651ccbf765.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>即将数字对应相乘后再相加，再应用到ReLU上。</p>
<p>所以1x1卷积从根本上理解，这32个单元都应用了一个全连接神经网络。而全连接层的作用是输入32个数字和过滤器数量，标记为n_C[l+1]，在36个单元上重复此过程，因此输出结果为6x6x#filters。这个方法通常被称为1x1卷积，有时也称为Network in Network。</p>
<p>下面给一个Network in Network的应用。假设我们有一个28x28x192的输入层，我们可以使用池化层压缩它的高度和宽度，而如果信道数量很大，我们可以用1x1卷积来缩小信道数量的大小。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-fbc5aefdd70c01d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>如果我们想保持信道数量不变，也是可行的：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ee93a77f7c64cfd1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>当然，想要增加信道数量也可以。</p>
<p>因此，通过1x1卷积的简单操作，我们可以压缩或者保持输入层中的信道数量，甚至是增加信道数量。在下一节，我们将讲述1x1卷积是如何帮助构建Inception网络的。</p>
<h1 id="谷歌Inception网络"><a href="#谷歌Inception网络" class="headerlink" title="谷歌Inception网络"></a>谷歌Inception网络</h1><p>论文名称：Going deeper with convolutions, 2014</p>
<p>构建卷积层时，我们要决定卷积核的大小究竟是1x3/3x3/5↓，或者要不要添加池化层。而Inception网络的作用是代替你来做决定，尽管这样做使得网络结构变得复杂，但表现却非常好。</p>
<h2 id="Inception核心模块"><a href="#Inception核心模块" class="headerlink" title="Inception核心模块"></a>Inception核心模块</h2><p>Inception网络的核心模块如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f7157e63d390b856.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>其基本思想是，Inception网络不需要人为决定使用哪个卷积核或是否需要池化，而是由网络自行确定这些参数，你可以给网络添加这些参数的所有可能值，然后把这些输出连接起来，让网络自己学习它需要什么样的参数、采用哪些卷积核组合。</p>
<h2 id="The-problem-of-computational-cost"><a href="#The-problem-of-computational-cost" class="headerlink" title="The problem of computational cost"></a>The problem of computational cost</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-e46e27fa822065a1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>乘法运算的总次数为每个输出值所需的乘法运算次数(5x5x192)乘以输出值个数(28x28x32)，结果等于1.2亿。即使在现代，用计算机来进行1.2亿次乘法运算，其成本也相当高。</p>
<h2 id="Using-1x1-convolution"><a href="#Using-1x1-convolution" class="headerlink" title="Using 1x1 convolution"></a>Using 1x1 convolution</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-85df96549d033170.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-82ee668d12ee5fb7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>我们要做的是把左边这个大的输入层压缩成较小的中间层(bottleneck layer)，再用卷积核恢复原来的输出大小。接下来看看计算成本。</p>
<p>首先，第一个卷积层的乘法次数为输出值个数(28x28x16)乘以每个输出值所需乘法运算次数(192)，相乘结果为240万；对于第二个卷积层的乘法次数为输出值个数(28x28x32)乘以每个输出值所需乘法运算次数(5x5x16)，相乘结果为1千万。因此总的计算成本为原来的十分之一，即1240万。所需的加法次数与乘法次数类似，因此只统计了乘法运算的次数。</p>
<p>总结：如果我们在构建神经网络层的时候，不想决定池化层是使用1x1、3x3、还是5x5的过滤器，那么inception模块可以让我们应用各种类型的过滤器，再将它们连接起来。事实证明，只要合理构建瓶颈层，我们既可以显著缩小表示层规模，又不会降低网络性能，从而节省了大量计算。</p>
<h2 id="完整结构"><a href="#完整结构" class="headerlink" title="完整结构"></a>完整结构</h2><p>在前面的几节，我们已经知道了Inception网络的基础模块。在本视频中，我们将学习如何将这些模块组合起来，构建我们的Inception网络。</p>
<p>一个Inception模块：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-933d7f4a64642dbd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>Inception网络所做的就是将多个Inception模块组合起来：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-59c8869b6c28ee2c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>我们可以发现其中有很多Inception模块，另外网络中有一些额外的最大化池来改变维度中的高和宽。因此我们实际上就是用多个Inception模块在不同位置进行的组合。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-85a4a092cabb2646.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>Inception网络还有一些额外的分支。而这些分支所做的就是通过隐藏层来做输出，即通过一些全连接层，然后使用一个softmax来预测输出结果的标签。它确保了即使是隐藏单元和中间层，它们也参与了特征计算，可以预测图片的分类。这个特点在Inception网络中起到一种调整的作用，也能防止网络发生过拟合。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-266e0c1258429a9f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>最后总结，如果我们理解了Inception模块，就能理解Inception网络，其实质就是将Inception模块一环接一环，最后组成网络。自从Inception模块诞生以来，经过研究者们的不断发展，衍生出许多新的版本。所以当我们在看一些比较新的Inception算法的论文时，发现人们使用这些新版本的算法效果也一样很好，比如Inception V2、V3以及V4，还有版本引入了跳跃连接(ResNet)的方法，也有特别好的效果。但所有的变体都建立在同一种基础的思想上，就是把许多Inception模块通过某种方式连接在一起。</p>
<p>接下来，我们会讲解如何真正使用这些算法来构建自己的计算机视觉系统。</p>
<h1 id="使用开源的实现方案-Using-open-source-implementations"><a href="#使用开源的实现方案-Using-open-source-implementations" class="headerlink" title="使用开源的实现方案: Using open-source implementations"></a>使用开源的实现方案: Using open-source implementations</h1><p>到目前我们已经学习了几个非常有效的神经网络和ConvNet架构。接下来会分享几条如何使用它们的实用性建议，首先从使用开放源码的实现开始。</p>
<p>事实证明很多神经网络复杂细致，因而难以复制，因为一些参数调整的细节问题，例如学习率衰减，会影响性能。幸运的是，很多深度学习者会将自己的成果开源，放在<strong>Github</strong>上。因此如果我们看到一篇研究论文想应用其成果，我们通常会在网络上寻找一个开源的实现，这比自己实现要好得多。</p>
<p>因此一个常用的工作流程是，选择一个喜欢的架构，接着寻找一个开源实现，从Github下载，再进行调整。</p>
<h1 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h1><p>如果你要做一个计算机视觉的应用，相比于从头训练权重，或者说从随机初始化权重开始，如果我们能下载别人已经训练好的网络结构的权重，我们通常能够进展比较快。用这个预训练模型转移到我们感兴趣的任务上，也就是用迁移学习把公共数据集的知识迁移到我们自己的问题上。</p>
<h2 id="数据集很小"><a href="#数据集很小" class="headerlink" title="数据集很小"></a>数据集很小</h2><p>举例子。假设我们要对猫图片分类，类别有Tiger、Misty和Neither。但我们没有那么多的猫图片，也就是我们的数据集很小。吴老师建议我们从网上下载一些神经网络开源的实现，不仅下载代码，同时也把权重下载下来。</p>
<p>比如对于ImageNet数据集，一共有1000个类别，因此在大多数网络结构最后都有一个Softmax分类器要预测1000种类别。这时候我们可以把最后一层去掉，而创建自己的Softmax单元。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ddb921702cff581f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>如图，我们通常将前面已经预训练好的权重保持不变(freeze)，而只训练Softmax层的权重。通过使用其他人预训练的权重，我们可能得到很好的性能，即使我们的数据集很小。幸运的是，很多深度学习框架都支持这种操作，我们可以设置<strong>trainableParameter=0</strong>或者<strong>freeze=1</strong>的参数，保证前面预训练好的权重不参与训练，即允许我们指定是否训练特顶层的权重。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-60d4ae8655494447.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>另一个加速训练的技巧是我们可以先计算出Softmax前一层的输出或激活值，将它们保存在硬盘中（因为前面部分不变），再运用Softmax函数进行预测。</p>
<h2 id="数据集更大"><a href="#数据集更大" class="headerlink" title="数据集更大"></a>数据集更大</h2><p>根据经验，如果我们有一个更大的标记好的数据集，在这种情况下，我们可以冻结更少的层，然后训练后面的层。即我们的规律是，如果我们的数据集越大，那么需要冻结的层数越少，能够训练的层数也越多。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-08325eec9599fb15.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>当然，如果我们的数据集很大，那么可以用开源的网络和它的权重作为初始化，然后训练整个网络（当然，输出层可能要根据自己的需要进行改变）。</p>
<h1 id="数据增强-Data-Augmentation"><a href="#数据增强-Data-Augmentation" class="headerlink" title="数据增强: Data Augmentation"></a>数据增强: Data Augmentation</h1><p>在实践中，更多的数据对大多数计算机视觉任务都有帮助。而不像其他领域，有时候得到充足的数据，但是效果却不怎么样。在现代，计算机视觉的主要问题就是没有办法得到充足的数据，因此这就意味着我们在训练计算机视觉模型时，数据增强可能会起到作用。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-621c4a01351e88a1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-5fa4d3bb2f7e35d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>（color shifting主要针对光线照明之类的情况）</p>
<h2 id="Implementing-distortion-during-training"><a href="#Implementing-distortion-during-training" class="headerlink" title="Implementing distortion during training"></a>Implementing distortion during training</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-14646dba1095e556.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h1 id="计算机视觉现状"><a href="#计算机视觉现状" class="headerlink" title="计算机视觉现状"></a>计算机视觉现状</h1><p><img src="https://upload-images.jianshu.io/upload_images/8636110-62695a5a31fe9f1d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>通常不用于实际生产，而只用于竞赛或者baseline测试：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-9bdf1d2dddfaa040.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-82542b2f348fda8a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h1 id="本周作业"><a href="#本周作业" class="headerlink" title="本周作业"></a>本周作业</h1><h2 id="Keras-Tutorial-the-Happy-House"><a href="#Keras-Tutorial-the-Happy-House" class="headerlink" title="Keras Tutorial - the Happy House"></a>Keras Tutorial - the Happy House</h2><p>Keras is more restrictive than the lower-level frameworks, so there are some very complex models that you can implement in TensorFlow but not (without more difficulty) in Keras. That being said, Keras will work fine for many common models.</p>
<p>导入包：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#import tensorflow as tf</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> image</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> layer_utils</span><br><span class="line"><span class="keyword">from</span> keras.utils.data_utils <span class="keyword">import</span> get_file</span><br><span class="line"><span class="keyword">from</span> keras.applications.imagenet_utils <span class="keyword">import</span> preprocess_input</span><br><span class="line"><span class="keyword">import</span> pydot</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> SVG</span><br><span class="line"><span class="keyword">from</span> keras.utils.vis_utils <span class="keyword">import</span> model_to_dot</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> plot_model</span><br><span class="line"><span class="keyword">from</span> kt_utils <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</span><br><span class="line">K.set_image_data_format(<span class="string">'channels_last'</span>)</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.pyplot <span class="keyword">import</span> imshow</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure></p>
<h3 id="1-The-Happy-House"><a href="#1-The-Happy-House" class="headerlink" title="1- The Happy House"></a>1- The Happy House</h3><p>For your next vacation, you decided to spend a week with five of your friends from school. It is a very convenient house with many things to do nearby. But the most important benefit is that everybody has commited to be happy when they are in the house. So anyone wanting to enter the house must prove their current state of happiness.</p>
<p>As a deep learning expert, to make sure the “Happy” rule is strictly applied, you are going to build an algorithm which that uses pictures from the front door camera to check if the person is happy or not. The door should open only if the person is happy.</p>
<p>You have gathered pictures of your friends and yourself, taken by the front-door camera. The dataset is labbeled.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b53bd750e48c3398.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>Run the following code to normalize the dataset and learn about its shapes.<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize image vectors</span></span><br><span class="line">X_train = X_train_orig/<span class="number">255.</span></span><br><span class="line">X_test = X_test_orig/<span class="number">255.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Reshape</span></span><br><span class="line">Y_train = Y_train_orig.T</span><br><span class="line">Y_test = Y_test_orig.T</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"number of training examples = "</span> + str(X_train.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"number of test examples = "</span> + str(X_test.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X_train shape: "</span> + str(X_train.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Y_train shape: "</span> + str(Y_train.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X_test shape: "</span> + str(X_test.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Y_test shape: "</span> + str(Y_test.shape))</span><br></pre></td></tr></table></figure></p>
<p><strong>Details of the “Happy” dataset</strong>:</p>
<ul>
<li>Images are of shape (64,64,3)</li>
<li>Training: 600 pictures</li>
<li>Test: 150 pictures</li>
</ul>
<h3 id="2-Building-a-model-in-Keras"><a href="#2-Building-a-model-in-Keras" class="headerlink" title="2- Building a model in Keras"></a>2- Building a model in Keras</h3><p>Keras is very good for rapid prototyping. In just a short time you will be able to build a model that achieves outstanding results.</p>
<p>Here is an example of a model in Keras:<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(input_shape)</span>:</span></span><br><span class="line">    <span class="comment"># Define the input placeholder as a tensor with shape input_shape.</span></span><br><span class="line">    X_input = Input(input_shape)</span><br><span class="line">    <span class="comment"># Zero-Padding: pads the border of X_input with zeroes</span></span><br><span class="line">    X = ZeroPadding2D((<span class="number">3</span>,<span class="number">3</span>))(X_input)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># CONV -&gt; BN -&gt; RELU Block applied to X</span></span><br><span class="line">    X = Conv2D(<span class="number">32</span>,(<span class="number">7</span>,<span class="number">7</span>),strides=(<span class="number">1</span>,<span class="number">1</span>),name=<span class="string">'conv0'</span>)(X)</span><br><span class="line">    X = BatchNormalization(axis=<span class="number">3</span>, name=<span class="string">'bn0'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># MAXPOOL</span></span><br><span class="line">    X = MaxPooling2D((<span class="number">2</span>,<span class="number">2</span>),name=<span class="string">'max_pool'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># FLATTEN X (means convert it to a vector) + FULLYCONNECTED</span></span><br><span class="line">    X = Flatten()(X)</span><br><span class="line">    X = Dense(<span class="number">1</span>,activation=<span class="string">'sigmoid'</span>,name=<span class="string">'fc'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create model. This creates your Keras model instance, you'll use this instance to train/test the model.</span></span><br><span class="line">    model = Model(inputs=X_input, outputs=X, name=<span class="string">'HappyModel'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure></p>
<p>Note that Keras uses a different convention with variable names than we’ve previously used with numpy and TensorFlow. In particular, rather than creating and assigning a new variable on each step of forward propagation such as <code>X</code>, <code>Z1</code>, <code>A1</code>, <code>Z2</code>, <code>A2</code>, etc. for the computations for the different layers, in Keras code each line above just reassigns <code>X</code> to a new value using <code>X = ...</code>. In other words, during each step of forward propagation, we are just writing the latest value in the commputation into the same variable <code>X</code>. The only exception was <code>X_input</code>, which we kept separate and did not overwrite, since we needed it at the end to create the Keras model instance (<code>model = Model(inputs = X_input, ...)</code> above). </p>
<p><strong>Exercise</strong>: Implement a <code>HappyModel()</code>. This assignment is more open-ended than most. We suggest that you start by implementing a model using the architecture we suggest, and run through the rest of this assignment using that as your initial model. But after that, come back and take initiative to try out other model architectures. For example, you might take inspiration from the model above, but then vary the network architecture and hyperparameters however you wish. You can also use other functions such as <code>AveragePooling2D()</code>, <code>GlobalMaxPooling2D()</code>, <code>Dropout()</code>. </p>
<p><strong>Note</strong>: You have to be careful with your data’s shapes. Use what you’ve learned in the videos to make sure your convolutional, pooling and fully-connected layers are adapted to the volumes you’re applying it to.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">HappyModel</span><span class="params">(input_shape)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the HappyModel.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    input_shape -- shape of the images of the dataset</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a Model() instance in Keras</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Feel free to use the suggested outline in the text above to get started, and run through the whole</span></span><br><span class="line">    <span class="comment"># exercise (including the later portions of this notebook) once. The come back also try out other</span></span><br><span class="line">    <span class="comment"># network architectures as well. </span></span><br><span class="line">    X_input = Input(shape=input_shape)</span><br><span class="line">    X = ZeroPadding2D(padding=(<span class="number">1</span>, <span class="number">1</span>))(X_input)</span><br><span class="line">    X = Conv2D(<span class="number">8</span>, kernel_size=(<span class="number">3</span>,<span class="number">3</span>), strides=(<span class="number">1</span>,<span class="number">1</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis=<span class="number">3</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    X = MaxPooling2D(pool_size=(<span class="number">2</span>,<span class="number">2</span>), strides=(<span class="number">2</span>,<span class="number">2</span>), padding=<span class="string">'valid'</span>)(X)</span><br><span class="line"></span><br><span class="line">    X = ZeroPadding2D(padding=(<span class="number">1</span>, <span class="number">1</span>))(X)</span><br><span class="line">    X = Conv2D(<span class="number">16</span>, kernel_size=(<span class="number">3</span>,<span class="number">3</span>), strides=(<span class="number">1</span>,<span class="number">1</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis=<span class="number">3</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    X = MaxPooling2D(pool_size=(<span class="number">2</span>,<span class="number">2</span>), strides=(<span class="number">2</span>,<span class="number">2</span>), padding=<span class="string">'valid'</span>)(X)</span><br><span class="line"></span><br><span class="line">    X = ZeroPadding2D(padding=(<span class="number">1</span>, <span class="number">1</span>))(X)</span><br><span class="line">    X = Conv2D(<span class="number">32</span>, kernel_size=(<span class="number">3</span>,<span class="number">3</span>), strides=(<span class="number">1</span>,<span class="number">1</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis=<span class="number">3</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    X = MaxPooling2D(pool_size=(<span class="number">2</span>,<span class="number">2</span>), strides=(<span class="number">2</span>,<span class="number">2</span>), padding=<span class="string">'valid'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># FC</span></span><br><span class="line">    X = Flatten()(X)</span><br><span class="line">    Y = Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)(X)</span><br><span class="line">    </span><br><span class="line">    model = Model(inputs = X_input, outputs = Y, name=<span class="string">'HappyModel'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<p>You have now built a function to describe your model. To train and test this model, there are four steps in Keras:</p>
<ol>
<li>Create the model by calling the function above</li>
<li>Compile the model by calling <code>model.compile(optimizer = &quot;...&quot;, loss = &quot;...&quot;, metrics = [&quot;accuracy&quot;])</code></li>
<li>Train the model on train data by calling <code>model.fit(x = ..., y = ..., epochs = ..., batch_size = ...)</code></li>
<li>Test the model on test data by calling <code>model.evaluate(x = ..., y = ...)</code></li>
</ol>
<p>If you want to know more about <code>model.compile()</code>, <code>model.fit()</code>, <code>model.evaluate()</code> and their arguments, refer to the official <a href="https://keras.io/models/model/" target="_blank" rel="noopener">Keras documentation</a>.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: create the model.</span></span><br><span class="line">happyModel = HappyModel((<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: compile the model to configure the learning process. </span></span><br><span class="line">happyModel.compile(optimizer=keras.optimizers.Adam(lr=<span class="number">0.001</span>,beta_1=<span class="number">0.9</span>,beta_2=<span class="number">0.999</span>,epsilon=<span class="number">1e-08</span>,decay=<span class="number">0.0</span>),loss=<span class="string">'binary_crossentropy'</span>,metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3: train the model</span></span><br><span class="line">happyModel.fit(x=X_train, y=Y_train, batch_size=<span class="number">16</span>, epochs=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 4: test/evaluate the model.</span></span><br><span class="line">preds = happyModel.evaluate(x=X_test,y=Y_test)</span><br><span class="line"></span><br><span class="line">print(preds)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Loss = "</span> + str(preds[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Test Accuracy = "</span> + str(preds[<span class="number">1</span>]))</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure>
<p>测试结果输出为：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-a27eadbcae8ea2db.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h3 id="3-Conclusion"><a href="#3-Conclusion" class="headerlink" title="3- Conclusion"></a>3- Conclusion</h3><p><strong>What we would like you to remember from this assignment:</strong></p>
<ul>
<li>Keras is a tool we recommend for rapid prototyping. It allows you to quickly try out different model architectures. Are there any applications of deep learning to your daily life that you’d like to implement using Keras?</li>
<li>Remember how to code a model in Keras and the four steps leading to the evaluation of your model on the test set. Create-&gt;Compile-&gt;Fit/Train-&gt;Evaluate/Test.</li>
</ul>
<h3 id="Test-with-your-own-image-Optional"><a href="#Test-with-your-own-image-Optional" class="headerlink" title="Test with your own image (Optional)"></a>Test with your own image (Optional)</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ###</span></span><br><span class="line">img_path = <span class="string">'images/my_image.jpg'</span></span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line">img = image.load_img(img_path, target_size=(<span class="number">64</span>, <span class="number">64</span>))</span><br><span class="line">imshow(img)</span><br><span class="line"></span><br><span class="line">x = image.img_to_array(img)</span><br><span class="line">x = np.expand_dims(x, axis=<span class="number">0</span>)</span><br><span class="line">x = preprocess_input(x)</span><br><span class="line"></span><br><span class="line">print(happyModel.predict(x))</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line"><span class="comment">### 5- Other useful functions in Keras (Optional)</span></span><br><span class="line">Two other basic features of Keras that yo<span class="string">u'll find useful are:</span></span><br><span class="line"><span class="string">- `model.summary()`: prints the details of your layers in a table with the sizes of its inputs/outputs</span></span><br><span class="line"><span class="string">- `plot_model()`: plots your graph in a nice layout. You can even save it as ".png" using SVG() if you'</span>d like to share it on social media ;). It <span class="keyword">is</span> saved <span class="keyword">in</span> <span class="string">"File"</span> then <span class="string">"Open..."</span> <span class="keyword">in</span> the upper bar of the notebook.</span><br><span class="line">![image.png](https://upload-images.jianshu.io/upload_images/8636110-7783b4f931ab61f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)</span><br><span class="line">![image.png](https://upload-images.jianshu.io/upload_images/8636110-041ce06a21b93545.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)</span><br><span class="line"></span><br><span class="line">可以通过这个过程来巩固自己的卷积维数计算。</span><br><span class="line"></span><br><span class="line"><span class="comment">## Residual Network</span></span><br><span class="line"></span><br><span class="line">**In this assignment, you will:**</span><br><span class="line">- Implement the basic building blocks of ResNets. </span><br><span class="line">- Put together these building blocks to implement <span class="keyword">and</span> train a state-of-the-art neural network <span class="keyword">for</span> image classification.</span><br><span class="line"></span><br><span class="line">导入包:</span><br><span class="line">``` py</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model, load_model</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> image</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> layer_utils</span><br><span class="line"><span class="keyword">from</span> keras.utils.data_utils <span class="keyword">import</span> get_file</span><br><span class="line"><span class="keyword">from</span> keras.applications.imagenet_utils <span class="keyword">import</span> preprocess_input</span><br><span class="line"><span class="keyword">import</span> pydot</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> SVG</span><br><span class="line"><span class="keyword">from</span> keras.utils.vis_utils <span class="keyword">import</span> model_to_dot</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> plot_model</span><br><span class="line"><span class="keyword">from</span> resnets_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> keras.initializers <span class="keyword">import</span> glorot_uniform</span><br><span class="line"><span class="keyword">import</span> scipy.misc</span><br><span class="line"><span class="keyword">from</span> matplotlib.pyplot <span class="keyword">import</span> imshow</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</span><br><span class="line">K.set_image_data_format(<span class="string">'channels_last'</span>)</span><br><span class="line">K.set_learning_phase(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="1-The-problem-of-very-deep-neural-networks"><a href="#1-The-problem-of-very-deep-neural-networks" class="headerlink" title="1- The problem of very deep neural networks"></a>1- The problem of very deep neural networks</h3><p>Last week, you built your first convolutional neural network. In recent years, neural networks have become deeper, with state-of-the-art networks going from just a few layers (e.g., AlexNet) to over a hundred layers.</p>
<p>The main benefit of a very deep network is that it can represent very complex functions. It can also learn features at many different levels of abstraction, from edges (at the lower layers) to very complex features (at the deeper layers). However, using a deeper network doesn’t always help. A huge barrier to training them is <strong>vanishing gradients</strong>: very deep networks often have a gradient signal that goes to zero quickly, thus making gradient descent unbearably slow. More specifically, during gradient descent, as you backprop from the final layer back to the first layer, you are multiplying by the weight matrix on each step, and thus the gradient can decrease exponentially quickly to zero (or, in rare cases, grow exponentially quickly and “explode” to take very large values).</p>
<p>During training, you might therefore see the magnitude (or norm) of the gradient for the earlier layers descrease to zero very rapidly as training proceeds:</p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-02b0b5e0ea98a2c3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>You are now going to solve this problem by building a Residual Network!</p>
<h3 id="2-Building-a-Residual-Network"><a href="#2-Building-a-Residual-Network" class="headerlink" title="2- Building a Residual Network"></a>2- Building a Residual Network</h3><p>In ResNets, a “shortcut” or a “skip connection” allows the gradient to be directly backpropagated to earlier layers:</p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-24b9c326de7a32ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>The image on the left shows the “main path” through the network. The image on the right adds a shortcut to the main path. By stacking these ResNet blocks on top of each other, you can form a very deep network.</p>
<p>We also saw in lecture that having ResNet blocks with the shortcut also makes it very easy for one of the blocks to learn an identity function. This means that you can stack on additional ResNet blocks with little risk of harming training set performance. (There is also some evidence that the ease of learning an identity function—even more than skip connections helping with vanishing gradients—accounts for ResNets’ remarkable performance.)</p>
<p>Two main types of blocks are used in a ResNet, depending mainly on whether the input/output dimensions are same or different. You are going to implement both of them.</p>
<h4 id="2-1-The-identity-block"><a href="#2-1-The-identity-block" class="headerlink" title="2.1- The identity block"></a>2.1- The identity block</h4><p>The identity block is the standard block used in ResNets, and corresponds to the case where the input activation (say a[l]) has the same dimension as the output activation (say a[l+2]).<br><img src="https://upload-images.jianshu.io/upload_images/8636110-bfed90c554f6fd81.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>In this exercise, you’ll actually implement a slightly more powerful version of this identity block, in which the skip connection “skips over” 3 hidden layers rather than 2 layers. It looks like this:</p>
<p>Here’re the individual steps.</p>
<p>First component of main path:</p>
<ul>
<li>The first CONV2D has $F_1$ filters of shape (1,1) and a stride of (1,1). Its padding is “valid” and its name should be <code>conv_name_base + &#39;2a&#39;</code>. Use 0 as the seed for the random initialization. </li>
<li>The first BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2a&#39;</code>.</li>
<li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li>
</ul>
<p>Second component of main path:</p>
<ul>
<li>The second CONV2D has $F_2$ filters of shape $(f,f)$ and a stride of (1,1). Its padding is “same” and its name should be <code>conv_name_base + &#39;2b&#39;</code>. Use 0 as the seed for the random initialization. </li>
<li>The second BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2b&#39;</code>.</li>
<li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li>
</ul>
<p>Third component of main path:</p>
<ul>
<li>The third CONV2D has $F_3$ filters of shape (1,1) and a stride of (1,1). Its padding is “valid” and its name should be <code>conv_name_base + &#39;2c&#39;</code>. Use 0 as the seed for the random initialization. </li>
<li>The third BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2c&#39;</code>. Note that there is no ReLU activation function in this component. </li>
</ul>
<p>Final step: </p>
<ul>
<li>The shortcut and the input are added together.</li>
<li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">identity_block</span><span class="params">(X, f, filters, stage, block)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the identity block as defined in Figure 4</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    f -- integer, specifying the shape of the middle CONV's window for the main path</span></span><br><span class="line"><span class="string">    filters -- python list of integers, defining the number of filters in the CONV layers of the main path</span></span><br><span class="line"><span class="string">    stage -- integer, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    block -- string/character, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># defining name basis</span></span><br><span class="line">    conv_name_base = <span class="string">'res'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    bn_name_base = <span class="string">'bn'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve Filters</span></span><br><span class="line">    F1, F2, F3 = filters</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Save the input value. You'll need this later to add back to the main path. </span></span><br><span class="line">    X_shortcut = X</span><br><span class="line"></span><br><span class="line">    <span class="comment"># First component of main path</span></span><br><span class="line">    <span class="comment"># 这个的参数filters代表卷积核的数量。</span></span><br><span class="line">    X = Conv2D(filters=F1, kernel_size=(<span class="number">1</span>,<span class="number">1</span>), strides=(<span class="number">1</span>,<span class="number">1</span>), padding=<span class="string">'valid'</span>, name=conv_name_base+<span class="string">'2a'</span>, kernel_initializer=glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis=<span class="number">3</span>, name=bn_name_base+<span class="string">'2a'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Second component of main path (≈3 lines)</span></span><br><span class="line">    X = Conv2D(filters=F2, kernel_size=(f,f), strides=(<span class="number">1</span>,<span class="number">1</span>), padding=<span class="string">'same'</span>, name=conv_name_base+<span class="string">'2b'</span>, kernel_initializer=glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis=<span class="number">3</span>, name=bn_name_base+<span class="string">'2b'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Third component of main path (≈2 lines)</span></span><br><span class="line">    X = Conv2D(filters=F3, kernel_size=(<span class="number">1</span>,<span class="number">1</span>), strides=(<span class="number">1</span>,<span class="number">1</span>), padding=<span class="string">'valid'</span>, name=conv_name_base+<span class="string">'2c'</span>, kernel_initializer=glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis=<span class="number">3</span>, name=bn_name_base+<span class="string">'2c'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)</span></span><br><span class="line">    X = Add()([X, X_shortcut])</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br></pre></td></tr></table></figure>
<p>测试：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.reset_default_graph()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    A_prev = tf.placeholder(<span class="string">"float"</span>,[<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">6</span>])</span><br><span class="line">    X = np.random.randn(<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">6</span>)</span><br><span class="line">    A = identity_block(A_prev, f = <span class="number">2</span>, filters=[<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>], stage=<span class="number">1</span>, block=<span class="string">'a'</span>)</span><br><span class="line">    sess.run(sess.global_variables_initializer())</span><br><span class="line">    out = sess.run([A], feed_dict=&#123;A_prev:X, K.learning_phase():<span class="number">0</span>&#125;)</span><br><span class="line">    print(<span class="string">"out="</span> + str(out[<span class="number">0</span>][<span class="number">1</span>][<span class="number">1</span>][<span class="number">0</span>]))</span><br></pre></td></tr></table></figure></p>
<h3 id="2-2-The-convolutional-block"><a href="#2-2-The-convolutional-block" class="headerlink" title="2.2- The convolutional block"></a>2.2- The convolutional block</h3><p>You’ve implemented the ResNet identity block. Next, the ResNet “convolutional block” is the other type of block. You can use this type of block when the input and output dimensions don’t match up. The difference with the identity block is that there is a CONV2D layer in the shortcut path:</p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-6a0e5278100d6218.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>The CONV2D layer in the shortcut path is used to resize the input  xx  to a different dimension, so that the dimensions match up in the final addition needed to add the shortcut value back to the main path. (This plays a similar role as the matrix  WsWs  discussed in lecture.) For example, to reduce the activation dimensions’s height and width by a factor of 2, you can use a 1x1 convolution with a stride of 2. The CONV2D layer on the shortcut path does not use any non-linear activation function. Its main role is to just apply a (learned) linear function that reduces the dimension of the input, so that the dimensions match up for the later addition step.</p>
<p>The details of the convolutional block are as follows.</p>
<p>First component of main path:</p>
<ul>
<li>The first CONV2D has $F_1$ filters of shape (1,1) and a stride of (s,s). Its padding is “valid” and its name should be <code>conv_name_base + &#39;2a&#39;</code>. </li>
<li>The first BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2a&#39;</code>.</li>
<li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li>
</ul>
<p>Second component of main path:</p>
<ul>
<li>The second CONV2D has $F_2$ filters of (f,f) and a stride of (1,1). Its padding is “same” and it’s name should be <code>conv_name_base + &#39;2b&#39;</code>.</li>
<li>The second BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2b&#39;</code>.</li>
<li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li>
</ul>
<p>Third component of main path:</p>
<ul>
<li>The third CONV2D has $F_3$ filters of (1,1) and a stride of (1,1). Its padding is “valid” and it’s name should be <code>conv_name_base + &#39;2c&#39;</code>.</li>
<li>The third BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2c&#39;</code>. Note that there is no ReLU activation function in this component. </li>
</ul>
<p>Shortcut path:</p>
<ul>
<li>The CONV2D has $F_3$ filters of shape (1,1) and a stride of (s,s). Its padding is “valid” and its name should be <code>conv_name_base + &#39;1&#39;</code>.</li>
<li>The BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;1&#39;</code>. </li>
</ul>
<p>Final step: </p>
<ul>
<li>The shortcut and the main path values are added together.</li>
<li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li>
</ul>
<p><strong>Exercise</strong>: Implement the convolutional block. We have implemented the first component of the main path; you should implement the rest. As before, always use 0 as the seed for the random initialization, to ensure consistency with our grader.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convolutional_block</span><span class="params">(X, f, filters, stage, block, s=<span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the identity block as defined in Figure 4</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    f -- integer, specifying the shape of the middle CONV's window for the main path</span></span><br><span class="line"><span class="string">    filters -- python list of integers, defining the number of filters in the CONV layers of the main path</span></span><br><span class="line"><span class="string">    stage -- integer, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    block -- string/character, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># defining name basis</span></span><br><span class="line">    conv_name_base = <span class="string">'res'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    bn_name_base = <span class="string">'bn'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve Filters</span></span><br><span class="line">    F1, F2, F3 = filters</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Save the input value. You'll need this later to add back to the main path. </span></span><br><span class="line">    X_shortcut = X</span><br><span class="line"></span><br><span class="line">    <span class="comment">##### MAIN PATH #####</span></span><br><span class="line">    <span class="comment"># First component of main path</span></span><br><span class="line">    X = Conv2D(filters = F1, kernel_size = (<span class="number">1</span>, <span class="number">1</span>), strides = (s,s), padding = <span class="string">'valid'</span>, name = conv_name_base + <span class="string">'2a'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2a'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Second component of main path (≈3 lines)</span></span><br><span class="line">    X = Conv2D(filters=F2, kernel_size=(f,f), strides=(<span class="number">1</span>,<span class="number">1</span>), padding=<span class="string">'same'</span>, name=conv_name_base+<span class="string">'2b'</span>, kernel_initializer=glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis=<span class="number">3</span>,name=bn_name_base+<span class="string">'2b'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Third component of main path (≈2 lines)</span></span><br><span class="line">    X = Conv2D(filters=F3, kernel_size=(<span class="number">1</span>,<span class="number">1</span>), strides=(<span class="number">1</span>,<span class="number">1</span>), padding=<span class="string">'valid'</span>, name=conv_name_base+<span class="string">'2c'</span>, kernel_initializer=glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis=<span class="number">3</span>, name=bn_name_base+<span class="string">'2c'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment">##### SHORTCUT PATH #### (≈2 lines)</span></span><br><span class="line">    X_shortcut = Conv2D(filters=F3, kernel_size=(<span class="number">1</span>,<span class="number">1</span>), strides=(s,s), padding=<span class="string">'valid'</span>, name=conv_name_base+<span class="string">'1'</span>, kernel_initializer=glorot_uniform(seed=<span class="number">0</span>))(X_shortcut)</span><br><span class="line">    X_shortcut = BatchNormalization(axis=<span class="number">3</span>, name=bn_name_base+<span class="string">'1'</span>)(X_shortcut)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)</span></span><br><span class="line">    X = Add()([X, X_shortcut])</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure>
<h3 id="3-Building-your-first-ResNet-model-50-layers"><a href="#3-Building-your-first-ResNet-model-50-layers" class="headerlink" title="3- Building your first ResNet model(50 layers)"></a>3- Building your first ResNet model(50 layers)</h3><p>You now have the necessary blocks to build a very deep ResNet. The following figure describes in detail the architecture of this neural network. “ID BLOCK” in the diagram stands for “Identity block,” and “ID BLOCK x3” means you should stack 3 identity blocks together.</p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-72cfe8a4efaeeada.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>The details of this ResNet-50 model are:</p>
<ul>
<li>Zero-padding pads the input with a pad of (3,3)</li>
<li>Stage 1:<ul>
<li>The 2D Convolution has 64 filters of shape (7,7) and uses a stride of (2,2). Its name is “conv1”.</li>
<li>BatchNorm is applied to the channels axis of the input.</li>
<li>MaxPooling uses a (3,3) window and a (2,2) stride.</li>
</ul>
</li>
<li>Stage 2:<ul>
<li>The convolutional block uses three set of filters of size [64,64,256], “f” is 3, “s” is 1 and the block is “a”.</li>
<li>The 2 identity blocks use three set of filters of size [64,64,256], “f” is 3 and the blocks are “b” and “c”.</li>
</ul>
</li>
<li>Stage 3:<ul>
<li>The convolutional block uses three set of filters of size [128,128,512], “f” is 3, “s” is 2 and the block is “a”.</li>
<li>The 3 identity blocks use three set of filters of size [128,128,512], “f” is 3 and the blocks are “b”, “c” and “d”.</li>
</ul>
</li>
<li>Stage 4:<ul>
<li>The convolutional block uses three set of filters of size [256, 256, 1024], “f” is 3, “s” is 2 and the block is “a”.</li>
<li>The 5 identity blocks use three set of filters of size [256, 256, 1024], “f” is 3 and the blocks are “b”, “c”, “d”, “e” and “f”.</li>
</ul>
</li>
<li>Stage 5:<ul>
<li>The convolutional block uses three set of filters of size [512, 512, 2048], “f” is 3, “s” is 2 and the block is “a”.</li>
<li>The 2 identity blocks use three set of filters of size [256, 256, 2048], “f” is 3 and the blocks are “b” and “c”.</li>
</ul>
</li>
<li>The 2D Average Pooling uses a window of shape (2,2) and its name is “avg_pool”.</li>
<li>The flatten doesn’t have any hyperparameters or name.</li>
<li>The Fully Connected (Dense) layer reduces its input to the number of classes using a softmax activation. Its name should be <code>&#39;fc&#39; + str(classes)</code>.</li>
</ul>
<p><strong>Exercise</strong>: Implement the ResNet with 50 layers described in the figure above. We have implemented Stages 1 and 2. Please implement the rest. (The syntax for implementing Stages 3-5 should be quite similar to that of Stage 2.) Make sure you follow the naming convention in the text above. </p>
<p>You’ll need to use this function: </p>
<ul>
<li>Average pooling <a href="https://keras.io/layers/pooling/#averagepooling2d" target="_blank" rel="noopener">see reference</a></li>
</ul>
<p>Here’re some other functions we used in the code below:</p>
<ul>
<li>Conv2D: <a href="https://keras.io/layers/convolutional/#conv2d" target="_blank" rel="noopener">See reference</a></li>
<li>BatchNorm: <a href="https://keras.io/layers/normalization/#batchnormalization" target="_blank" rel="noopener">See reference</a> (axis: Integer, the axis that should be normalized (typically the features axis))</li>
<li>Zero padding: <a href="https://keras.io/layers/convolutional/#zeropadding2d" target="_blank" rel="noopener">See reference</a></li>
<li>Max pooling: <a href="https://keras.io/layers/pooling/#maxpooling2d" target="_blank" rel="noopener">See reference</a></li>
<li>Fully conected layer: <a href="https://keras.io/layers/core/#dense" target="_blank" rel="noopener">See reference</a></li>
<li>Addition: <a href="https://keras.io/layers/merge/#add" target="_blank" rel="noopener">See reference</a></li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ResNet50</span><span class="params">(input_shape=<span class="params">(<span class="number">64</span>,<span class="number">64</span>,<span class="number">3</span>)</span>, classes=<span class="number">6</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the popular ResNet50 the following architecture:</span></span><br><span class="line"><span class="string">    CONV2D -&gt; BATCHNORM -&gt; RELU -&gt; MAXPOOL -&gt; CONVBLOCK -&gt; IDBLOCK*2 -&gt; CONVBLOCK -&gt; IDBLOCK*3</span></span><br><span class="line"><span class="string">    -&gt; CONVBLOCK -&gt; IDBLOCK*5 -&gt; CONVBLOCK -&gt; IDBLOCK*2 -&gt; AVGPOOL -&gt; TOPLAYER</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    input_shape -- shape of the images of the dataset</span></span><br><span class="line"><span class="string">    classes -- integer, number of classes</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a Model() instance in Keras</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Define the input as a tensor with shape input_shape</span></span><br><span class="line">    X_input = Input(input_shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero-Padding</span></span><br><span class="line">    X = ZeroPadding2D((<span class="number">3</span>, <span class="number">3</span>))(X_input)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 1</span></span><br><span class="line">    X = Conv2D(<span class="number">64</span>,kernel_size=(<span class="number">7</span>,<span class="number">7</span>),strides=(<span class="number">2</span>,<span class="number">2</span>),name=<span class="string">"conv1"</span>,kernel_initializer=glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis3, name=<span class="string">"bn_conv1"</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    X = MaxPooling2D((<span class="number">3</span>,<span class="number">3</span>), strides=(<span class="number">2</span>,<span class="number">2</span>))(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 2</span></span><br><span class="line">    X = convolutional_block(X, f=<span class="number">3</span>, filters=[<span class="number">64</span>,<span class="number">64</span>,<span class="number">256</span>], stage=<span class="number">2</span>, block=<span class="string">'a'</span>, s=<span class="number">1</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">64</span>,<span class="number">64</span>,<span class="number">256</span>], stage=<span class="number">2</span>, block=<span class="string">'b'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">64</span>,<span class="number">64</span>,<span class="number">256</span>], stage=<span class="number">2</span>, block=<span class="string">'c'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 3</span></span><br><span class="line">    X = convolutional_block(X, f=<span class="number">3</span>, filters=[<span class="number">128</span>,<span class="number">128</span>,<span class="number">512</span>], stage=<span class="number">3</span>, block=<span class="string">'a'</span>, s=<span class="number">2</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">128</span>,<span class="number">128</span>,<span class="number">512</span>], block=<span class="string">'b'</span>, stage=<span class="number">3</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">128</span>,<span class="number">128</span>,<span class="number">512</span>], block=<span class="string">'c'</span>, stage=<span class="number">3</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">128</span>,<span class="number">128</span>,<span class="number">512</span>], block=<span class="string">'d'</span>, stage=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 4</span></span><br><span class="line">    X = convolutional_block(X, f=<span class="number">3</span>, filters=[<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], s=<span class="number">2</span>, block=<span class="string">'a'</span>, stage=<span class="number">4</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], block=<span class="string">'b'</span>, stage=<span class="number">4</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], block=<span class="string">'c'</span>, stage=<span class="number">4</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], block=<span class="string">'d'</span>, stage=<span class="number">4</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], block=<span class="string">'e'</span>, stage=<span class="number">4</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], block=<span class="string">'f'</span>, stage=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 5</span></span><br><span class="line">    X = convolutional_block(X, f=<span class="number">3</span>, filters=[<span class="number">512</span>,<span class="number">512</span>,<span class="number">2048</span>], s=<span class="number">2</span>, block=<span class="string">'a'</span>, stage=<span class="number">5</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">2048</span>], block=<span class="string">'b'</span>, stage=<span class="number">5</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">2048</span>], block=<span class="string">'c'</span>, stage=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># AVGPOOL</span></span><br><span class="line">    X = AveragePooling2D(pool_size=(<span class="number">2</span>,<span class="number">2</span>),name=<span class="string">'avg_pool'</span>)(X)</span><br></pre></td></tr></table></figure>
<p>接下来是之前一样的四个步骤：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = ResNet50(input_shape = (<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>), classes = <span class="number">6</span>)</span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'categorical_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(X_train, Y_train, epochs = <span class="number">20</span>, batch_size = <span class="number">32</span>)</span><br><span class="line">preds = model.evaluate(X_test, Y_test)</span><br></pre></td></tr></table></figure></p>
<p>ResNet50 is a powerful model for image classification when it is trained for an adequate number of iterations. We hope you can use what you’ve learnt and apply it to your own classification problem to perform state-of-the-art accuracy.</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/python/" rel="tag"><i class="fa fa-tag"></i> python</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/21/deep-learningw10/" rel="next" title="第10周-卷积神经网络">
                <i class="fa fa-chevron-left"></i> 第10周-卷积神经网络
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/04/23/deep-learningwx/" rel="prev" title="占坑-目标检测">
                占坑-目标检测 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="DesmonDay" />
          <p class="site-author-name" itemprop="name">DesmonDay</p>
           
              <p class="site-description motion-element" itemprop="description">主攻方向：NLP</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">112</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">14</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">11</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/DesmonDay" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#为什么要研究实例：Why-look-at-case-studies"><span class="nav-number">1.</span> <span class="nav-text">为什么要研究实例：Why look at case studies?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#经典网络：Classic-Network"><span class="nav-number">2.</span> <span class="nav-text">经典网络：Classic Network</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#LeNet-5"><span class="nav-number">2.1.</span> <span class="nav-text">LeNet-5</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#AlexNet"><span class="nav-number">2.2.</span> <span class="nav-text">AlexNet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VGG-16"><span class="nav-number">2.3.</span> <span class="nav-text">VGG-16</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#残差网络：Residual-Networks"><span class="nav-number">3.</span> <span class="nav-text">残差网络：Residual Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#为什么ResNet表现好"><span class="nav-number">3.1.</span> <span class="nav-text">为什么ResNet表现好</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Network-in-Network-and-1x1-convolutions"><span class="nav-number">4.</span> <span class="nav-text">Network in Network and 1x1 convolutions</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#谷歌Inception网络"><span class="nav-number">5.</span> <span class="nav-text">谷歌Inception网络</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Inception核心模块"><span class="nav-number">5.1.</span> <span class="nav-text">Inception核心模块</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-problem-of-computational-cost"><span class="nav-number">5.2.</span> <span class="nav-text">The problem of computational cost</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Using-1x1-convolution"><span class="nav-number">5.3.</span> <span class="nav-text">Using 1x1 convolution</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#完整结构"><span class="nav-number">5.4.</span> <span class="nav-text">完整结构</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#使用开源的实现方案-Using-open-source-implementations"><span class="nav-number">6.</span> <span class="nav-text">使用开源的实现方案: Using open-source implementations</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#迁移学习"><span class="nav-number">7.</span> <span class="nav-text">迁移学习</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#数据集很小"><span class="nav-number">7.1.</span> <span class="nav-text">数据集很小</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据集更大"><span class="nav-number">7.2.</span> <span class="nav-text">数据集更大</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#数据增强-Data-Augmentation"><span class="nav-number">8.</span> <span class="nav-text">数据增强: Data Augmentation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Implementing-distortion-during-training"><span class="nav-number">8.1.</span> <span class="nav-text">Implementing distortion during training</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#计算机视觉现状"><span class="nav-number">9.</span> <span class="nav-text">计算机视觉现状</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#本周作业"><span class="nav-number">10.</span> <span class="nav-text">本周作业</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Keras-Tutorial-the-Happy-House"><span class="nav-number">10.1.</span> <span class="nav-text">Keras Tutorial - the Happy House</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-The-Happy-House"><span class="nav-number">10.1.1.</span> <span class="nav-text">1- The Happy House</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Building-a-model-in-Keras"><span class="nav-number">10.1.2.</span> <span class="nav-text">2- Building a model in Keras</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Conclusion"><span class="nav-number">10.1.3.</span> <span class="nav-text">3- Conclusion</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Test-with-your-own-image-Optional"><span class="nav-number">10.1.4.</span> <span class="nav-text">Test with your own image (Optional)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-The-problem-of-very-deep-neural-networks"><span class="nav-number">10.1.5.</span> <span class="nav-text">1- The problem of very deep neural networks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Building-a-Residual-Network"><span class="nav-number">10.1.6.</span> <span class="nav-text">2- Building a Residual Network</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-The-identity-block"><span class="nav-number">10.1.6.1.</span> <span class="nav-text">2.1- The identity block</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-The-convolutional-block"><span class="nav-number">10.1.7.</span> <span class="nav-text">2.2- The convolutional block</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Building-your-first-ResNet-model-50-layers"><span class="nav-number">10.1.8.</span> <span class="nav-text">3- Building your first ResNet model(50 layers)</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">DesmonDay</span>
</div>



<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv"></span>
</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->



  


  




	





  





  






  





  

  

  

  

  

  

</body>
</html>


