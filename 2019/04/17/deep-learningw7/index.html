<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="python," />





  <link rel="alternate" href="/atom.xml" title="DesmonDay's Blog" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="调试处理-Tuning process在深度学习中，我们需要调整的超参数有学习率alpha、Momentum的参数beta、Adam优化算法的参数beta1/beta2/epsilon、神经网络层数、每一层隐藏单元个数、衰退率learning rate decay、mini-batch的大小等等。在这些超参数中，一些超参数比其他的要重要，其中学习率是最重要的超参数。图中，红色为1，橙色为2，紫色为">
<meta name="keywords" content="python">
<meta property="og:type" content="article">
<meta property="og:title" content="第7周-超参数调试、Batch正则化和程序框架">
<meta property="og:url" content="https://github.com/DesmonDay/2019/04/17/deep-learningw7/index.html">
<meta property="og:site_name" content="DesmonDay&#39;s Blog">
<meta property="og:description" content="调试处理-Tuning process在深度学习中，我们需要调整的超参数有学习率alpha、Momentum的参数beta、Adam优化算法的参数beta1/beta2/epsilon、神经网络层数、每一层隐藏单元个数、衰退率learning rate decay、mini-batch的大小等等。在这些超参数中，一些超参数比其他的要重要，其中学习率是最重要的超参数。图中，红色为1，橙色为2，紫色为">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-614ac5dedfb62fba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-3c3aa61662ca95ca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-dd45dffc58d6b230.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-d0364e61dc42e640.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-e71d62ea9e02eaa7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-537d7048bfd60f4c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-d9a7d88b3ab44cd3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-8994a5d740f75b29.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-e7de36e5dad0a21c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-775b2e5b1e351d96.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-fb4bb4712854d8ca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-24152afe1d99a49e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-e520559dfb8398ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-05624bba3926d5d2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-5cecdac3b73ab75f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-7e8c3f45eb2fc31c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-98d15e5d90f5b2e3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-bbb70262d7bbfa50.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-50c507c273d8ca17.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-09b94965cfeb2353.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-2973d044b8e02939.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-92f9e7e0c5bf6fca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-0106142b36f0b484.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-80661870a8174346.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-7deaa194aa1f6222.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-1c1ce0c9393e4a41.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-dd2f0508b990f685.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-d7f70c6b0f4c858f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-9e6d230b9de67024.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-e97f4875e64ea430.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-6fe151d952572db2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-408e2209b83d8c8f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-e94150daadfbe46c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-def65b969b402de8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-5022cfcfc1a94396.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:updated_time" content="2019-04-18T10:12:18.429Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="第7周-超参数调试、Batch正则化和程序框架">
<meta name="twitter:description" content="调试处理-Tuning process在深度学习中，我们需要调整的超参数有学习率alpha、Momentum的参数beta、Adam优化算法的参数beta1/beta2/epsilon、神经网络层数、每一层隐藏单元个数、衰退率learning rate decay、mini-batch的大小等等。在这些超参数中，一些超参数比其他的要重要，其中学习率是最重要的超参数。图中，红色为1，橙色为2，紫色为">
<meta name="twitter:image" content="https://upload-images.jianshu.io/upload_images/8636110-614ac5dedfb62fba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://github.com/DesmonDay/2019/04/17/deep-learningw7/"/>





  <title>第7周-超参数调试、Batch正则化和程序框架 | DesmonDay's Blog</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">DesmonDay's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">一只小辣鸡的自我拯救之路</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/DesmonDay/2019/04/17/deep-learningw7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="DesmonDay">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DesmonDay's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">第7周-超参数调试、Batch正则化和程序框架</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-17T14:48:08+08:00">
                2019-04-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="调试处理-Tuning-process"><a href="#调试处理-Tuning-process" class="headerlink" title="调试处理-Tuning process"></a>调试处理-Tuning process</h1><p>在深度学习中，我们需要调整的超参数有学习率alpha、Momentum的参数beta、Adam优化算法的参数beta1/beta2/epsilon、神经网络层数、每一层隐藏单元个数、衰退率learning rate decay、mini-batch的大小等等。在这些超参数中，一些超参数比其他的要重要，其中学习率是最重要的超参数。图中，红色为1，橙色为2，紫色为3，Adam的参数则通常为默认值。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-614ac5dedfb62fba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p><strong>策略一： Try random values, but don’t use a grid.</strong> 通常，我们可能会使用网格(grid)搜索，但这种方法仅适用于超参数较少的情况。当训练深度神经网络时，我们不使用网格搜索，而是设置随机值。有时我们能难预知哪些超参数更重要，因为我们搜索的超参数可能有很多个，因此采取随机取值而不是网格取值表明你探究了更多重要超参数的潜在值。</p>
<p><strong>策略二： Coarse to fine(从粗糙到惊喜)</strong> 形象化例子如下，现在粗糙的网格中随机搜索，再在结果比较优的几个取值周围进行更精细地随机选取。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-3c3aa61662ca95ca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h1 id="为超参数选择合适的范围"><a href="#为超参数选择合适的范围" class="headerlink" title="为超参数选择合适的范围"></a>为超参数选择合适的范围</h1><p>在上一节中我们知道了在超参数范围中随机取值可以提高我们的搜索效率。但随机取值并不是在有效值范围内的随机均匀取值，而是选择合适的标尺用于探究这些超参数。</p>
<p>对于可以随机均匀取值的超参数，如隐藏层单元个数，神经网络层数等：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-dd45dffc58d6b230.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>而对于有些超参数则不适合使用随机均匀取值。比如学习率，我们觉得最小取值为0.0001，最大取值为1。显然，90%的搜索会集中在0.0001到0.1之间，但在0.1到1却只有10%的可能。因此，我们可以采取另一种搜索策略。如图，设置几个固定点为0.0001,0.001，0.01,0.1和1，在这些范围内再进行随机均匀取值。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d0364e61dc42e640.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>用python表示为：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">r = <span class="number">-4</span> * np.random.rand() <span class="comment"># 那么r属于[-4,0]</span></span><br><span class="line">alpha = pow(<span class="number">10</span>, r) <span class="comment"># 那么alpha属于[10^(-4), 1]</span></span><br></pre></td></tr></table></figure></p>
<p>另一个比较棘手的超参数调参例子是beta，其是用来计算指数的加权平均值。假设我们认定beta是0.9到0.999中的某个值。我们需要注意的是，beta取值0.9类似于与计算10天的温度平均值，取值0.999相当于在1000个值中取平均。因此我们在0.9到0.999中取值，就不适合用线性搜索，即不可在此区间随机均匀取值。</p>
<p>因此最好的方法是<strong>考虑1-beta</strong>，其取值为0.1到0.001。然后再应用学习率的取值方法，有r取值在[-3,-1]，再设置1-beta=10^r，从而得到beta。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e71d62ea9e02eaa7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>为什么不可以使用线性取值呢？这是因为，当beta越接近1时，其所得结果的灵敏度会变化，即使beta只有微小的变化。因此当beta在0.9到0.9005之间取值，我们的结果几乎不会变化；但beta在0.999(1000个温度数据)到0.9995(2000个温度数据)之间取值，则会对我们的算法产生巨大影响。</p>
<p>因此，我们需要在超参数选择中做出正确的scale decision。</p>
<h1 id="超参数训练的实践：Panda-vs-Caviar"><a href="#超参数训练的实践：Panda-vs-Caviar" class="headerlink" title="超参数训练的实践：Panda vs Caviar"></a>超参数训练的实践：Panda vs Caviar</h1><p>到目前为止，我们已经听了许多关于如何搜索最优超参数的内容，在结束该讨论之前，我们讲讲如何组织超参数搜索过程。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-537d7048bfd60f4c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>如今的深度学习已经应用到许多不同的领域，某个应用领域的超参数设定，有可能通用与另一领域，不同的应用领域出现相互交融。比如，吴老师说，他曾经看到过计算机视觉领域中涌现的巧妙方法，比如Confonets或ResNets，它们还成功应用于语音识别。</p>
<p>深度学习领域中，发展很好的一点是不同应用领域的人们会阅读越来多其它研究深度学习领域的文章，跨领域寻找灵感。</p>
<p>就超参数设定而言，即使我们只研究一个问题，比如逻辑学，如果我们已经找到一组很好的参数设置，并继续发展算法。或许在几个月的过程中，观察到数据会逐渐改变，而这些改变使得我们原来的超参数设定不再好用。因此我们需要重新测试或评估我们的超参数(Re-test hyperparameters occasionally)，至少每隔几个月一次，以确保对数值依然满意。</p>
<p>最后，关于如何搜索超参数的问题，有两种重要的思路。一个是babysitting one model，即每天根据模型的表，对该模型进行不同参数的调整（如学习率），这通常是因为我们没有足够的计算能力；一个是Training many models in parallel，同时训练多种模型，从中选择表现最优的模型，用这种方式我们可以试验许多不同的参数设置，从中选择最好的。</p>
<p>上面两种方法就好像熊猫和鱼卵的对比，而这主要是由于我们的计算资源来决定的。</p>
<h1 id="Batch-Norm-——-感觉还不太懂，需要回看"><a href="#Batch-Norm-——-感觉还不太懂，需要回看" class="headerlink" title="Batch Norm —— 感觉还不太懂，需要回看"></a>Batch Norm —— 感觉还不太懂，需要回看</h1><p>机器学习领域有个很重要的假设：IID独立同分布假设，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。那Batch Norm的作用是什么呢？Batch Norm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。</p>
<h2 id="正则化网络的激活函数-——-Normalizing-activations-in-a-network"><a href="#正则化网络的激活函数-——-Normalizing-activations-in-a-network" class="headerlink" title="正则化网络的激活函数 —— Normalizing activations in a network"></a>正则化网络的激活函数 —— Normalizing activations in a network</h2><p>在之前的课程中我们学到过归一化输入特征对于训练神经网络参数W和b的速度提升有很大帮助，如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d9a7d88b3ab44cd3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>那么这就产生了对于每一层隐藏层的输入是否要归一化的问题。对于有些学者而言，有着是归一化Z还是A的讨论，这里吴老师默认第一选择是归一化Z。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-8994a5d740f75b29.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p><strong>Implementing Batch Norm</strong> 假设我们有隐藏单元值Z[1]到Z[m]，这里简化了原有的符号表示。Batch Norm使得归一化不仅适用于训练的输入，也能适用于隐藏层的输入。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e7de36e5dad0a21c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>在图中，iteration内我们首先计算了平均值mu和方差，并且计算了我们原有归一化后的Z值。但是由于我们并不希望每一个隐藏层都具有相同的平均值和方差，因此添加了两个超参数gamma和beta来调整对应的平均值和方差。计算得到结果后，我们使用新的Z值而不是原来的Z值来参与训练。</p>
<h2 id="将Batch-Norm拟合进神经网络"><a href="#将Batch-Norm拟合进神经网络" class="headerlink" title="将Batch Norm拟合进神经网络"></a>将Batch Norm拟合进神经网络</h2><p>接下来我们将Batch Norm拟合进神经网络中，简单的图示过程如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-775b2e5b1e351d96.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>因此我们可以得到整个神经网络的参数为：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-fb4bb4712854d8ca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>对于beta[l]和gamma[l]，我们也可以使用梯度下降的方法来对其进行更新，注意这里的beta与优化算法中的beta是两个完全不同的参数。</p>
<p>在实际应用深度学习框架时，我们往往不需要实现Batch Norm的细节，比如Tensorflow中，可以直接使用tf.nn.batch_normalization来实现BN。</p>
<p>实际中，Batch Norm经常与Mini-batch一同使用，简单的图示过程如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-24152afe1d99a49e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>这里有一个需要注意的细节。我们在Batch Norm中的参数为W[l],b[l],beta[l]和gamma[l]。在原先的实现中，我们计算Z[l]=W[l]a[l-1]+b[l]，但在实施了Batch Norm之后，b[l]都会被减去，因为我们在减去平均值时就相当于将b[l]消去了。因此在使用Batch归一化时，我们可以将b[l]简单地设置为常数0，而不需要对其进行更新。另外注意参数的维度即可。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e520559dfb8398ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>接下来讲解整个过程：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-05624bba3926d5d2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>注意到，db实际上不用再计算了。另外，Batch Norm也适用于其他的优化算法，如Adam等。</p>
<h2 id="为什么Batch-Norm奏效？"><a href="#为什么Batch-Norm奏效？" class="headerlink" title="为什么Batch Norm奏效？"></a>为什么Batch Norm奏效？</h2><p>一个原因就是我们之前在归一化输入特征时讲到的，通过归一化所有的输入特征值，以获得类似范围的值，可以加快学习速度。</p>
<p>另一个原因就是考虑到covariate shift的问题，这个问题是指如果我们有一个从X到Y的映射函数，当X的分布发生改变时，那么这个函数也要变化。<br>对于深度学习这种包含很多隐层的网络结构，在训练过程中，因为各层参数不停在变化，所以每个隐层都会面临covariate shift的问题，也就是在训练过程中，隐层的输入分布老是变来变去，这就是所谓的“Internal Covariate Shift”，Internal指的是深层网络的隐层，是发生在网络内部的事情，而不是covariate shift问题只发生在输入层。因此Batch Norm可以确保，<strong>无论输入的数据如何变化，输入的均值和方差保持不变</strong>。</p>
<p>Batch Norm减少了输入值改变带来的问题，它使得这些值变得更稳定，即使输入分布改变了一些，那么归一化后它改变的程度也刽很多。它所做的是当前层的输入改变时，使得后层需要适应的程度减少了。这就意味着减弱了前层参数的作用与后层参数的作用之间的联系，使得网络每一层都可以自己学习，而稍稍独立于其它层，也有利于加速整个网络的学习。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-5cecdac3b73ab75f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>Batch Norm还有一个作用，它有<strong>轻微的正则化效果</strong>，将Batch Norm应用于Mini-batch上，因为是在mini-batch上计算均值和方差，而不是在整个数据集上，因此可以存在一点噪声，而这些噪声的作用和dropout类似，dropout是在每个隐藏层的激活值上增加了噪音，通过一定的概率使得隐藏单元激活或者失活；另一个轻微但非直观的效果是，如果我们应用了较大的mini-batch，如512而不是64，我们减少了噪音，因此减少了正则化效果。这也是dropout的一个奇怪的性质，就是应用较大的mini-batch可以减少正则化效果。</p>
<p>一般来说，我们不会把Batch Norm当做正则化方式，而是把它当做将归一化隐藏层并且加速学习的一种方式。</p>
<h2 id="Batch-Norm-at-test-time"><a href="#Batch-Norm-at-test-time" class="headerlink" title="Batch Norm at test time"></a>Batch Norm at test time</h2><p>Batch Nrom将数据以mini-batch的形式进行处理，但在测试时，我们可能需要对每个样本逐一处理（预测）。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7e8c3f45eb2fc31c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>回想最开始，我们是通过以上等式执行Batch Norm。在一个mini-batch中，将所有的Z(i)值求和计算均值，计算方差后再计算z_norm(i)，最后再次调整z_norm得到z_tilda。注意，用于计算的均值和方差是在整个mini-batch上计算的，但在测试时，我们不可能将一个mini-batch的样本同时处理，因此需要用其他方式得到均值和方差，并且假设我们只有一个样本的话，一个样本的均值和方差没有意义。因此实际上，为了将我们的神经网络运用于测试，需要单独估算均值和方差。在典型的Batch Norm运用中，我们需要用一个<strong>指数加权平均</strong>来估算，这个平均值涵盖了所有的mini-batch。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-98d15e5d90f5b2e3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>假设我们在<strong>训练集</strong>上有多个mini-batch，通过在每个mini-batch上计算当前隐藏层的均值mu和方差，我们得到了每一层的均值和方差的不同数值（以mini-batch来变化），因此我们可以像之前计算温度一样计算得到均值和方差的指数加权平均值。最后在测试时，使用均值和方差的指数加权平均来求z_norm，再使用我们在神经网络训练过程中得到的beta和gamma参数来计算我们的测试样本的z_tilda值。</p>
<p>关于Batch Norm更详细的知识解释可看：<a href="https://www.cnblogs.com/guoyaohua/p/8724433.html" target="_blank" rel="noopener">https://www.cnblogs.com/guoyaohua/p/8724433.html</a></p>
<h1 id="Softmax回归"><a href="#Softmax回归" class="headerlink" title="Softmax回归"></a>Softmax回归</h1><p>在之前我们所讲到的分类都是二元分类，接下来讲解与多元分类相关的Softmax回归。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-bbb70262d7bbfa50.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>我们用大写字母C来表示输入会被分入的类别总个数，如上图一共有4类，即0,1,2,3。我们要用神经网络来进行多元分类，希望有输出层的神经元个数来告知我们这4种类型中每一个的概率有多大。（为什么这里输出层单元可以有这样的对应关系呢？我不明白）<br><img src="https://upload-images.jianshu.io/upload_images/8636110-50c507c273d8ca17.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>要做到多个概率的输入，需要用到Softmax函数。与sigmoid和relu激活函数的输入和输出不同（这两者的输入输出都是一个实值），Softmax的输入可以是一个向量。由下图可知，我们的Z[L]的维度为(4,1)，而得到的输出a[L]的维度也是(4,1)。并且，计算时首先算出Z[L]每个元素的指数幂，随后再进行整体归一化，得到对应的概率值，而这个概率值也就是我们想要的结果。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-09b94965cfeb2353.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>在图中右边也给了一个简单的计算例子，即算出来Z[L] = [5 2 -1 3].T，通过计算指数幂得到t = [148.4, 7.4, 0.4, 20.1].T，总和为176.3，从而计算得到概率输出为a[L] = [0.842, 0.042, 0.002, 0.114].T。</p>
<p>接下来举了没有隐藏层的神经网络结合Softmax的例子方便理解。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-2973d044b8e02939.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>可以看到，尽管没有输出层，Softmax还是可以学习到线性分界，那么结合隐藏层的话，就可以得到更复杂的非线性分界了。</p>
<h1 id="训练一个Softmax分类器"><a href="#训练一个Softmax分类器" class="headerlink" title="训练一个Softmax分类器"></a>训练一个Softmax分类器</h1><h2 id="Understanding-softmax"><a href="#Understanding-softmax" class="headerlink" title="Understanding softmax"></a>Understanding softmax</h2><p>用临时变量t进行归一化，之后计算得到对应的概率。hard max会观察Z的值，然后直接在最大的元素上设置输出为1，其他的为0；而Softmax则使得Z到概率之间的映射更为温和。而Softmax回归实际上是Logistic回归的扩展。当C=2，我们可以得到输出层的两个概率，而由于我们实际上不需要两个概率，只要得到其中一个值就可以知道另一个，因此Logistic的输出实际上只有一个。因此我们可以说softmax回归将logistic回归推广到了两种分类以上。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-92f9e7e0c5bf6fca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h2 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h2><p>假设我们的ground truth label是cat，即y = [0 1 0 0].T，而我们训练得到的a[L] = y_hat = [0.3 0.2 0.1 0.4].T，这实际上不是好的结果。那么我们需要一个loss function来衡量误差。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-0106142b36f0b484.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>概括来说，损失函数所做的就是找到训练集中的真实类别，然后试图使该类别相应的概率尽可能地高。左边显示的是单个样本的loss function，而右边以W,b为参数的则是整个数据集的loss function。</p>
<h2 id="Gradient-descent-with-softmax"><a href="#Gradient-descent-with-softmax" class="headerlink" title="Gradient descent with softmax"></a>Gradient descent with softmax</h2><p>由于之后可以用深度学习框架来做作业了，吴恩达老师没有讲具体的求导过程…</p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-80661870a8174346.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>其实，最关键的就是求得a关于z的导数，这里分为了两种情况：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7deaa194aa1f6222.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-1c1ce0c9393e4a41.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>得到了上述之后，再求Loss函数关于a的导数，相乘即可，从而就有了Loss函数关于z的导数。</p>
<h1 id="深度学习框架"><a href="#深度学习框架" class="headerlink" title="深度学习框架"></a>深度学习框架</h1><p><img src="https://upload-images.jianshu.io/upload_images/8636110-dd2f0508b990f685.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>还需补上Pytorch，这个也是必须掌握的。所以我要掌握的有Tensorflow和Pytorch。</p>
<h1 id="Tensorflow"><a href="#Tensorflow" class="headerlink" title="Tensorflow"></a>Tensorflow</h1><p>这里吴恩达老师给了一些Tensorflow基本结构的例子。</p>
<p>首先，我们设置cost function为J(w) = w^2 - l0*w + 25，而我们希望求得使得J(w)最小化的w（显然w=5时J最小），简单的tensorflow程序如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d7f70c6b0f4c858f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>注意到cost函数可以由注释部分写为下方比较简单的形式。而tf.train.GradientDescentOptimizer中的参数为学习率，只有当run学习函数的时候，w才会变化。再经过1000次迭代后，注意到输出的w为4.9999886，这与w=5非常接近。</p>
<p>如果我们希望加入训练数据，比如在这个二次方程中，希望将方程的系数作为输入的数据，可以用tf.placeholder来完成。示例代码如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-9e6d230b9de67024.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>注意，我们使用feed_dict参数传入我们的训练数据。</p>
<p>另外，有个可以注意的地方：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e97f4875e64ea430.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>通常我们写程序的时候，采取右边的with方式来写，这种写法有利于在执行内循环出错时的内存释放。</p>
<p>Tensorflow程序的核心是计算损失函数，然后Tensorflow会自动求出导数，以及如何最小化损失。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-6fe151d952572db2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>这个损失函数的作用就是让TensorFlow建立计算图，计算图所做的事情如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-408e2209b83d8c8f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>而Tensorflow的优点是，通过用这个计算图基本实现前向传播，而且内置了所有必要的反向函数，因此我们在使用内置函数计算前向传播时，它可以自动地计算反向传播。（Tensorflow计算图用的是运算符作为结点）</p>
<h1 id="本周作业"><a href="#本周作业" class="headerlink" title="本周作业"></a>本周作业</h1><h2 id="1-Exploring-the-Tensorflow-Library"><a href="#1-Exploring-the-Tensorflow-Library" class="headerlink" title="1- Exploring the Tensorflow Library"></a>1- Exploring the Tensorflow Library</h2><p>Import packages:<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.framework <span class="keyword">import</span> ops</span><br><span class="line"><span class="keyword">from</span> tf_utils <span class="keyword">import</span> load_dataset, random_mini_batches, convert_to_one_hot, predict</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p>
<p>首先给出一个简单的Loss function的例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e94150daadfbe46c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">y_hat = tf.constant(<span class="number">36</span>, name=<span class="string">"y_hat"</span>)</span><br><span class="line">y = tf.constant(<span class="number">39</span>, name=<span class="string">"y"</span>)</span><br><span class="line"></span><br><span class="line">loss = tf.Variable((y-y_hat)**<span class="number">2</span>, name=<span class="string">"loss"</span>)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(loss))</span><br></pre></td></tr></table></figure>
<p>Writing and running programs in TensorFlow has the following steps:</p>
<ol>
<li>Create Tensors (variables) that are not yet executed/evaluated.</li>
<li>Write operations between those Tensors.</li>
<li>Initialize your Tensors.</li>
<li>Create a Session.</li>
<li>Run the Session. This will run the operations you’d written above.</li>
</ol>
<p>接下来看一个例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-def65b969b402de8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>我们没有得到20的结果，而是得到了一个tensor的介绍：You got a tensor saying that the result is a tensor that does not have the shape attribute, and is of type “int32”. 我们所做的只是将其放入了计算图，但并没有开始运算。为了能够使这两个数字相乘，我们需要创建会话并且运行它。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-5022cfcfc1a94396.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/python/" rel="tag"><i class="fa fa-tag"></i> python</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/17/deep-learningw8/" rel="next" title="第8周-机器学习ML策略(1)">
                <i class="fa fa-chevron-left"></i> 第8周-机器学习ML策略(1)
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="DesmonDay" />
          <p class="site-author-name" itemprop="name">DesmonDay</p>
           
              <p class="site-description motion-element" itemprop="description">主攻方向：NLP</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">102</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">14</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">10</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/DesmonDay" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#调试处理-Tuning-process"><span class="nav-number">1.</span> <span class="nav-text">调试处理-Tuning process</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#为超参数选择合适的范围"><span class="nav-number">2.</span> <span class="nav-text">为超参数选择合适的范围</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#超参数训练的实践：Panda-vs-Caviar"><span class="nav-number">3.</span> <span class="nav-text">超参数训练的实践：Panda vs Caviar</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Batch-Norm-——-感觉还不太懂，需要回看"><span class="nav-number">4.</span> <span class="nav-text">Batch Norm —— 感觉还不太懂，需要回看</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#正则化网络的激活函数-——-Normalizing-activations-in-a-network"><span class="nav-number">4.1.</span> <span class="nav-text">正则化网络的激活函数 —— Normalizing activations in a network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#将Batch-Norm拟合进神经网络"><span class="nav-number">4.2.</span> <span class="nav-text">将Batch Norm拟合进神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#为什么Batch-Norm奏效？"><span class="nav-number">4.3.</span> <span class="nav-text">为什么Batch Norm奏效？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Batch-Norm-at-test-time"><span class="nav-number">4.4.</span> <span class="nav-text">Batch Norm at test time</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Softmax回归"><span class="nav-number">5.</span> <span class="nav-text">Softmax回归</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#训练一个Softmax分类器"><span class="nav-number">6.</span> <span class="nav-text">训练一个Softmax分类器</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Understanding-softmax"><span class="nav-number">6.1.</span> <span class="nav-text">Understanding softmax</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Loss-function"><span class="nav-number">6.2.</span> <span class="nav-text">Loss function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient-descent-with-softmax"><span class="nav-number">6.3.</span> <span class="nav-text">Gradient descent with softmax</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#深度学习框架"><span class="nav-number">7.</span> <span class="nav-text">深度学习框架</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Tensorflow"><span class="nav-number">8.</span> <span class="nav-text">Tensorflow</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#本周作业"><span class="nav-number">9.</span> <span class="nav-text">本周作业</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Exploring-the-Tensorflow-Library"><span class="nav-number">9.1.</span> <span class="nav-text">1- Exploring the Tensorflow Library</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">DesmonDay</span>
</div>



<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv"></span>
</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->



  


  




	





  





  






  





  

  

  

  

  

  

</body>
</html>


