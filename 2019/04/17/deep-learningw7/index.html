<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="python," />





  <link rel="alternate" href="/atom.xml" title="DesmonDay's Blog" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="调试处理-Tuning process在深度学习中，我们需要调整的超参数有学习率alpha、Momentum的参数beta、Adam优化算法的参数beta1/beta2/epsilon、神经网络层数、每一层隐藏单元个数、衰退率learning rate decay、mini-batch的大小等等。在这些超参数中，一些超参数比其他的要重要，其中学习率是最重要的超参数。图中，红色为1，橙色为2，紫色为">
<meta name="keywords" content="python">
<meta property="og:type" content="article">
<meta property="og:title" content="第7周-超参数调试、Batch正则化和程序框架">
<meta property="og:url" content="https://github.com/DesmonDay/2019/04/17/deep-learningw7/index.html">
<meta property="og:site_name" content="DesmonDay&#39;s Blog">
<meta property="og:description" content="调试处理-Tuning process在深度学习中，我们需要调整的超参数有学习率alpha、Momentum的参数beta、Adam优化算法的参数beta1/beta2/epsilon、神经网络层数、每一层隐藏单元个数、衰退率learning rate decay、mini-batch的大小等等。在这些超参数中，一些超参数比其他的要重要，其中学习率是最重要的超参数。图中，红色为1，橙色为2，紫色为">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-614ac5dedfb62fba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-3c3aa61662ca95ca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-dd45dffc58d6b230.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-d0364e61dc42e640.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-e71d62ea9e02eaa7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-537d7048bfd60f4c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-d9a7d88b3ab44cd3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-8994a5d740f75b29.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-e7de36e5dad0a21c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-775b2e5b1e351d96.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-fb4bb4712854d8ca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-24152afe1d99a49e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-e520559dfb8398ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-05624bba3926d5d2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-5cecdac3b73ab75f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-7e8c3f45eb2fc31c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-98d15e5d90f5b2e3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-bbb70262d7bbfa50.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-50c507c273d8ca17.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-09b94965cfeb2353.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-2973d044b8e02939.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-92f9e7e0c5bf6fca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-0106142b36f0b484.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-80661870a8174346.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-7deaa194aa1f6222.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-1c1ce0c9393e4a41.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-dd2f0508b990f685.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-d7f70c6b0f4c858f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-9e6d230b9de67024.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-e97f4875e64ea430.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-6fe151d952572db2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-408e2209b83d8c8f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-e94150daadfbe46c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-def65b969b402de8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-5022cfcfc1a94396.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-0b3589e4d39947fe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-0db81609e5d5461e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-26496561ada5c4f3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-926f06281619b2fb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-e6fde280ea117e2f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:updated_time" content="2019-04-18T15:15:30.062Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="第7周-超参数调试、Batch正则化和程序框架">
<meta name="twitter:description" content="调试处理-Tuning process在深度学习中，我们需要调整的超参数有学习率alpha、Momentum的参数beta、Adam优化算法的参数beta1/beta2/epsilon、神经网络层数、每一层隐藏单元个数、衰退率learning rate decay、mini-batch的大小等等。在这些超参数中，一些超参数比其他的要重要，其中学习率是最重要的超参数。图中，红色为1，橙色为2，紫色为">
<meta name="twitter:image" content="https://upload-images.jianshu.io/upload_images/8636110-614ac5dedfb62fba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://github.com/DesmonDay/2019/04/17/deep-learningw7/"/>





  <title>第7周-超参数调试、Batch正则化和程序框架 | DesmonDay's Blog</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">DesmonDay's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">一只小辣鸡的自我拯救之路</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/DesmonDay/2019/04/17/deep-learningw7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="DesmonDay">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DesmonDay's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">第7周-超参数调试、Batch正则化和程序框架</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-17T14:48:08+08:00">
                2019-04-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="调试处理-Tuning-process"><a href="#调试处理-Tuning-process" class="headerlink" title="调试处理-Tuning process"></a>调试处理-Tuning process</h1><p>在深度学习中，我们需要调整的超参数有学习率alpha、Momentum的参数beta、Adam优化算法的参数beta1/beta2/epsilon、神经网络层数、每一层隐藏单元个数、衰退率learning rate decay、mini-batch的大小等等。在这些超参数中，一些超参数比其他的要重要，其中学习率是最重要的超参数。图中，红色为1，橙色为2，紫色为3，Adam的参数则通常为默认值。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-614ac5dedfb62fba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p><strong>策略一： Try random values, but don’t use a grid.</strong> 通常，我们可能会使用网格(grid)搜索，但这种方法仅适用于超参数较少的情况。当训练深度神经网络时，我们不使用网格搜索，而是设置随机值。有时我们能难预知哪些超参数更重要，因为我们搜索的超参数可能有很多个，因此采取随机取值而不是网格取值表明你探究了更多重要超参数的潜在值。</p>
<p><strong>策略二： Coarse to fine(从粗糙到惊喜)</strong> 形象化例子如下，现在粗糙的网格中随机搜索，再在结果比较优的几个取值周围进行更精细地随机选取。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-3c3aa61662ca95ca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h1 id="为超参数选择合适的范围"><a href="#为超参数选择合适的范围" class="headerlink" title="为超参数选择合适的范围"></a>为超参数选择合适的范围</h1><p>在上一节中我们知道了在超参数范围中随机取值可以提高我们的搜索效率。但随机取值并不是在有效值范围内的随机均匀取值，而是选择合适的标尺用于探究这些超参数。</p>
<p>对于可以随机均匀取值的超参数，如隐藏层单元个数，神经网络层数等：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-dd45dffc58d6b230.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>而对于有些超参数则不适合使用随机均匀取值。比如学习率，我们觉得最小取值为0.0001，最大取值为1。显然，90%的搜索会集中在0.0001到0.1之间，但在0.1到1却只有10%的可能。因此，我们可以采取另一种搜索策略。如图，设置几个固定点为0.0001,0.001，0.01,0.1和1，在这些范围内再进行随机均匀取值。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d0364e61dc42e640.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>用python表示为：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">r = <span class="number">-4</span> * np.random.rand() <span class="comment"># 那么r属于[-4,0]</span></span><br><span class="line">alpha = pow(<span class="number">10</span>, r) <span class="comment"># 那么alpha属于[10^(-4), 1]</span></span><br></pre></td></tr></table></figure></p>
<p>另一个比较棘手的超参数调参例子是beta，其是用来计算指数的加权平均值。假设我们认定beta是0.9到0.999中的某个值。我们需要注意的是，beta取值0.9类似于与计算10天的温度平均值，取值0.999相当于在1000个值中取平均。因此我们在0.9到0.999中取值，就不适合用线性搜索，即不可在此区间随机均匀取值。</p>
<p>因此最好的方法是<strong>考虑1-beta</strong>，其取值为0.1到0.001。然后再应用学习率的取值方法，有r取值在[-3,-1]，再设置1-beta=10^r，从而得到beta。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e71d62ea9e02eaa7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>为什么不可以使用线性取值呢？这是因为，当beta越接近1时，其所得结果的灵敏度会变化，即使beta只有微小的变化。因此当beta在0.9到0.9005之间取值，我们的结果几乎不会变化；但beta在0.999(1000个温度数据)到0.9995(2000个温度数据)之间取值，则会对我们的算法产生巨大影响。</p>
<p>因此，我们需要在超参数选择中做出正确的scale decision。</p>
<h1 id="超参数训练的实践：Panda-vs-Caviar"><a href="#超参数训练的实践：Panda-vs-Caviar" class="headerlink" title="超参数训练的实践：Panda vs Caviar"></a>超参数训练的实践：Panda vs Caviar</h1><p>到目前为止，我们已经听了许多关于如何搜索最优超参数的内容，在结束该讨论之前，我们讲讲如何组织超参数搜索过程。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-537d7048bfd60f4c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>如今的深度学习已经应用到许多不同的领域，某个应用领域的超参数设定，有可能通用与另一领域，不同的应用领域出现相互交融。比如，吴老师说，他曾经看到过计算机视觉领域中涌现的巧妙方法，比如Confonets或ResNets，它们还成功应用于语音识别。</p>
<p>深度学习领域中，发展很好的一点是不同应用领域的人们会阅读越来多其它研究深度学习领域的文章，跨领域寻找灵感。</p>
<p>就超参数设定而言，即使我们只研究一个问题，比如逻辑学，如果我们已经找到一组很好的参数设置，并继续发展算法。或许在几个月的过程中，观察到数据会逐渐改变，而这些改变使得我们原来的超参数设定不再好用。因此我们需要重新测试或评估我们的超参数(Re-test hyperparameters occasionally)，至少每隔几个月一次，以确保对数值依然满意。</p>
<p>最后，关于如何搜索超参数的问题，有两种重要的思路。一个是babysitting one model，即每天根据模型的表，对该模型进行不同参数的调整（如学习率），这通常是因为我们没有足够的计算能力；一个是Training many models in parallel，同时训练多种模型，从中选择表现最优的模型，用这种方式我们可以试验许多不同的参数设置，从中选择最好的。</p>
<p>上面两种方法就好像熊猫和鱼卵的对比，而这主要是由于我们的计算资源来决定的。</p>
<h1 id="Batch-Norm-——-感觉还不太懂，需要回看"><a href="#Batch-Norm-——-感觉还不太懂，需要回看" class="headerlink" title="Batch Norm —— 感觉还不太懂，需要回看"></a>Batch Norm —— 感觉还不太懂，需要回看</h1><p>机器学习领域有个很重要的假设：IID独立同分布假设，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。那Batch Norm的作用是什么呢？Batch Norm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。</p>
<h2 id="正则化网络的激活函数-——-Normalizing-activations-in-a-network"><a href="#正则化网络的激活函数-——-Normalizing-activations-in-a-network" class="headerlink" title="正则化网络的激活函数 —— Normalizing activations in a network"></a>正则化网络的激活函数 —— Normalizing activations in a network</h2><p>在之前的课程中我们学到过归一化输入特征对于训练神经网络参数W和b的速度提升有很大帮助，如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d9a7d88b3ab44cd3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>那么这就产生了对于每一层隐藏层的输入是否要归一化的问题。对于有些学者而言，有着是归一化Z还是A的讨论，这里吴老师默认第一选择是归一化Z。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-8994a5d740f75b29.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p><strong>Implementing Batch Norm</strong> 假设我们有隐藏单元值Z[1]到Z[m]，这里简化了原有的符号表示。Batch Norm使得归一化不仅适用于训练的输入，也能适用于隐藏层的输入。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e7de36e5dad0a21c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>在图中，iteration内我们首先计算了平均值mu和方差，并且计算了我们原有归一化后的Z值。但是由于我们并不希望每一个隐藏层都具有相同的平均值和方差，因此添加了两个超参数gamma和beta来调整对应的平均值和方差。计算得到结果后，我们使用新的Z值而不是原来的Z值来参与训练。</p>
<h2 id="将Batch-Norm拟合进神经网络"><a href="#将Batch-Norm拟合进神经网络" class="headerlink" title="将Batch Norm拟合进神经网络"></a>将Batch Norm拟合进神经网络</h2><p>接下来我们将Batch Norm拟合进神经网络中，简单的图示过程如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-775b2e5b1e351d96.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>因此我们可以得到整个神经网络的参数为：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-fb4bb4712854d8ca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>对于beta[l]和gamma[l]，我们也可以使用梯度下降的方法来对其进行更新，注意这里的beta与优化算法中的beta是两个完全不同的参数。</p>
<p>在实际应用深度学习框架时，我们往往不需要实现Batch Norm的细节，比如Tensorflow中，可以直接使用tf.nn.batch_normalization来实现BN。</p>
<p>实际中，Batch Norm经常与Mini-batch一同使用，简单的图示过程如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-24152afe1d99a49e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>这里有一个需要注意的细节。我们在Batch Norm中的参数为W[l],b[l],beta[l]和gamma[l]。在原先的实现中，我们计算Z[l]=W[l]a[l-1]+b[l]，但在实施了Batch Norm之后，b[l]都会被减去，因为我们在减去平均值时就相当于将b[l]消去了。因此在使用Batch归一化时，我们可以将b[l]简单地设置为常数0，而不需要对其进行更新。另外注意参数的维度即可。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e520559dfb8398ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>接下来讲解整个过程：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-05624bba3926d5d2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>注意到，db实际上不用再计算了。另外，Batch Norm也适用于其他的优化算法，如Adam等。</p>
<h2 id="为什么Batch-Norm奏效？"><a href="#为什么Batch-Norm奏效？" class="headerlink" title="为什么Batch Norm奏效？"></a>为什么Batch Norm奏效？</h2><p>一个原因就是我们之前在归一化输入特征时讲到的，通过归一化所有的输入特征值，以获得类似范围的值，可以加快学习速度。</p>
<p>另一个原因就是考虑到covariate shift的问题，这个问题是指如果我们有一个从X到Y的映射函数，当X的分布发生改变时，那么这个函数也要变化。<br>对于深度学习这种包含很多隐层的网络结构，在训练过程中，因为各层参数不停在变化，所以每个隐层都会面临covariate shift的问题，也就是在训练过程中，隐层的输入分布老是变来变去，这就是所谓的“Internal Covariate Shift”，Internal指的是深层网络的隐层，是发生在网络内部的事情，而不是covariate shift问题只发生在输入层。因此Batch Norm可以确保，<strong>无论输入的数据如何变化，输入的均值和方差保持不变</strong>。</p>
<p>Batch Norm减少了输入值改变带来的问题，它使得这些值变得更稳定，即使输入分布改变了一些，那么归一化后它改变的程度也刽很多。它所做的是当前层的输入改变时，使得后层需要适应的程度减少了。这就意味着减弱了前层参数的作用与后层参数的作用之间的联系，使得网络每一层都可以自己学习，而稍稍独立于其它层，也有利于加速整个网络的学习。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-5cecdac3b73ab75f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>Batch Norm还有一个作用，它有<strong>轻微的正则化效果</strong>，将Batch Norm应用于Mini-batch上，因为是在mini-batch上计算均值和方差，而不是在整个数据集上，因此可以存在一点噪声，而这些噪声的作用和dropout类似，dropout是在每个隐藏层的激活值上增加了噪音，通过一定的概率使得隐藏单元激活或者失活；另一个轻微但非直观的效果是，如果我们应用了较大的mini-batch，如512而不是64，我们减少了噪音，因此减少了正则化效果。这也是dropout的一个奇怪的性质，就是应用较大的mini-batch可以减少正则化效果。</p>
<p>一般来说，我们不会把Batch Norm当做正则化方式，而是把它当做将归一化隐藏层并且加速学习的一种方式。</p>
<h2 id="Batch-Norm-at-test-time"><a href="#Batch-Norm-at-test-time" class="headerlink" title="Batch Norm at test time"></a>Batch Norm at test time</h2><p>Batch Nrom将数据以mini-batch的形式进行处理，但在测试时，我们可能需要对每个样本逐一处理（预测）。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7e8c3f45eb2fc31c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>回想最开始，我们是通过以上等式执行Batch Norm。在一个mini-batch中，将所有的Z(i)值求和计算均值，计算方差后再计算z_norm(i)，最后再次调整z_norm得到z_tilda。注意，用于计算的均值和方差是在整个mini-batch上计算的，但在测试时，我们不可能将一个mini-batch的样本同时处理，因此需要用其他方式得到均值和方差，并且假设我们只有一个样本的话，一个样本的均值和方差没有意义。因此实际上，为了将我们的神经网络运用于测试，需要单独估算均值和方差。在典型的Batch Norm运用中，我们需要用一个<strong>指数加权平均</strong>来估算，这个平均值涵盖了所有的mini-batch。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-98d15e5d90f5b2e3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>假设我们在<strong>训练集</strong>上有多个mini-batch，通过在每个mini-batch上计算当前隐藏层的均值mu和方差，我们得到了每一层的均值和方差的不同数值（以mini-batch来变化），因此我们可以像之前计算温度一样计算得到均值和方差的指数加权平均值。最后在测试时，使用均值和方差的指数加权平均来求z_norm，再使用我们在神经网络训练过程中得到的beta和gamma参数来计算我们的测试样本的z_tilda值。</p>
<p>关于Batch Norm更详细的知识解释可看：<a href="https://www.cnblogs.com/guoyaohua/p/8724433.html" target="_blank" rel="noopener">https://www.cnblogs.com/guoyaohua/p/8724433.html</a></p>
<h1 id="Softmax回归"><a href="#Softmax回归" class="headerlink" title="Softmax回归"></a>Softmax回归</h1><p>在之前我们所讲到的分类都是二元分类，接下来讲解与多元分类相关的Softmax回归。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-bbb70262d7bbfa50.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>我们用大写字母C来表示输入会被分入的类别总个数，如上图一共有4类，即0,1,2,3。我们要用神经网络来进行多元分类，希望有输出层的神经元个数来告知我们这4种类型中每一个的概率有多大。（为什么这里输出层单元可以有这样的对应关系呢？我不明白）<br><img src="https://upload-images.jianshu.io/upload_images/8636110-50c507c273d8ca17.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>要做到多个概率的输入，需要用到Softmax函数。与sigmoid和relu激活函数的输入和输出不同（这两者的输入输出都是一个实值），Softmax的输入可以是一个向量。由下图可知，我们的Z[L]的维度为(4,1)，而得到的输出a[L]的维度也是(4,1)。并且，计算时首先算出Z[L]每个元素的指数幂，随后再进行整体归一化，得到对应的概率值，而这个概率值也就是我们想要的结果。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-09b94965cfeb2353.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>在图中右边也给了一个简单的计算例子，即算出来Z[L] = [5 2 -1 3].T，通过计算指数幂得到t = [148.4, 7.4, 0.4, 20.1].T，总和为176.3，从而计算得到概率输出为a[L] = [0.842, 0.042, 0.002, 0.114].T。</p>
<p>接下来举了没有隐藏层的神经网络结合Softmax的例子方便理解。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-2973d044b8e02939.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>可以看到，尽管没有输出层，Softmax还是可以学习到线性分界，那么结合隐藏层的话，就可以得到更复杂的非线性分界了。</p>
<h1 id="训练一个Softmax分类器"><a href="#训练一个Softmax分类器" class="headerlink" title="训练一个Softmax分类器"></a>训练一个Softmax分类器</h1><h2 id="Understanding-softmax"><a href="#Understanding-softmax" class="headerlink" title="Understanding softmax"></a>Understanding softmax</h2><p>用临时变量t进行归一化，之后计算得到对应的概率。hard max会观察Z的值，然后直接在最大的元素上设置输出为1，其他的为0；而Softmax则使得Z到概率之间的映射更为温和。而Softmax回归实际上是Logistic回归的扩展。当C=2，我们可以得到输出层的两个概率，而由于我们实际上不需要两个概率，只要得到其中一个值就可以知道另一个，因此Logistic的输出实际上只有一个。因此我们可以说softmax回归将logistic回归推广到了两种分类以上。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-92f9e7e0c5bf6fca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h2 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h2><p>假设我们的ground truth label是cat，即y = [0 1 0 0].T，而我们训练得到的a[L] = y_hat = [0.3 0.2 0.1 0.4].T，这实际上不是好的结果。那么我们需要一个loss function来衡量误差。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-0106142b36f0b484.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>概括来说，损失函数所做的就是找到训练集中的真实类别，然后试图使该类别相应的概率尽可能地高。左边显示的是单个样本的loss function，而右边以W,b为参数的则是整个数据集的loss function。</p>
<h2 id="Gradient-descent-with-softmax"><a href="#Gradient-descent-with-softmax" class="headerlink" title="Gradient descent with softmax"></a>Gradient descent with softmax</h2><p>由于之后可以用深度学习框架来做作业了，吴恩达老师没有讲具体的求导过程…</p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-80661870a8174346.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>其实，最关键的就是求得a关于z的导数，这里分为了两种情况：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7deaa194aa1f6222.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-1c1ce0c9393e4a41.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>得到了上述之后，再求Loss函数关于a的导数，相乘即可，从而就有了Loss函数关于z的导数。</p>
<h1 id="深度学习框架"><a href="#深度学习框架" class="headerlink" title="深度学习框架"></a>深度学习框架</h1><p><img src="https://upload-images.jianshu.io/upload_images/8636110-dd2f0508b990f685.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>还需补上Pytorch，这个也是必须掌握的。所以我要掌握的有Tensorflow和Pytorch。</p>
<h1 id="Tensorflow"><a href="#Tensorflow" class="headerlink" title="Tensorflow"></a>Tensorflow</h1><p>这里吴恩达老师给了一些Tensorflow基本结构的例子。</p>
<p>首先，我们设置cost function为J(w) = w^2 - l0*w + 25，而我们希望求得使得J(w)最小化的w（显然w=5时J最小），简单的tensorflow程序如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d7f70c6b0f4c858f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>注意到cost函数可以由注释部分写为下方比较简单的形式。而tf.train.GradientDescentOptimizer中的参数为学习率，只有当run学习函数的时候，w才会变化。再经过1000次迭代后，注意到输出的w为4.9999886，这与w=5非常接近。</p>
<p>如果我们希望加入训练数据，比如在这个二次方程中，希望将方程的系数作为输入的数据，可以用tf.placeholder来完成。示例代码如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-9e6d230b9de67024.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>注意，我们使用feed_dict参数传入我们的训练数据。</p>
<p>另外，有个可以注意的地方：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e97f4875e64ea430.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>通常我们写程序的时候，采取右边的with方式来写，这种写法有利于在执行内循环出错时的内存释放。</p>
<p>Tensorflow程序的核心是计算损失函数，然后Tensorflow会自动求出导数，以及如何最小化损失。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-6fe151d952572db2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>这个损失函数的作用就是让TensorFlow建立计算图，计算图所做的事情如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-408e2209b83d8c8f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>而Tensorflow的优点是，通过用这个计算图基本实现前向传播，而且内置了所有必要的反向函数，因此我们在使用内置函数计算前向传播时，它可以自动地计算反向传播。（Tensorflow计算图用的是运算符作为结点）</p>
<h1 id="本周作业"><a href="#本周作业" class="headerlink" title="本周作业"></a>本周作业</h1><h2 id="1-Exploring-the-Tensorflow-Library"><a href="#1-Exploring-the-Tensorflow-Library" class="headerlink" title="1- Exploring the Tensorflow Library"></a>1- Exploring the Tensorflow Library</h2><p>Import packages:<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.framework <span class="keyword">import</span> ops</span><br><span class="line"><span class="keyword">from</span> tf_utils <span class="keyword">import</span> load_dataset, random_mini_batches, convert_to_one_hot, predict</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p>
<p>首先给出一个简单的Loss function的例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e94150daadfbe46c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">y_hat = tf.constant(<span class="number">36</span>, name=<span class="string">"y_hat"</span>)</span><br><span class="line">y = tf.constant(<span class="number">39</span>, name=<span class="string">"y"</span>)</span><br><span class="line"></span><br><span class="line">loss = tf.Variable((y-y_hat)**<span class="number">2</span>, name=<span class="string">"loss"</span>)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(loss))</span><br></pre></td></tr></table></figure>
<p>Writing and running programs in TensorFlow has the following steps:</p>
<ol>
<li>Create Tensors (variables) that are not yet executed/evaluated.</li>
<li>Write operations between those Tensors.</li>
<li>Initialize your Tensors.</li>
<li>Create a Session.</li>
<li>Run the Session. This will run the operations you’d written above.</li>
</ol>
<p>接下来看一个例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-def65b969b402de8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>我们没有得到20的结果，而是得到了一个tensor的介绍：You got a tensor saying that the result is a tensor that does not have the shape attribute, and is of type “int32”. 我们所做的只是将其放入了计算图，但并没有开始运算。为了能够使这两个数字相乘，我们需要创建会话并且运行它。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-5022cfcfc1a94396.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p><strong>Summarize: remember to initialize your variables, create a session and run the operations inside the session.</strong></p>
<p>接下来，我们需要知道placeholders。A placeholder is an object whose value you can specify only later. To specify values for a placeholder, you can pass in values by using a “feed dictionary” (feed_dict variable). Below, we created a placeholder for x. This allows us to pass in a number later when we run the session.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-0b3589e4d39947fe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>Here’s what’s happening: When you specify the operations needed for a computation, you are telling TensorFlow how to construct a computation graph. The computation graph can have some placeholders whose values you will specify only later. Finally, when you run the session, you are telling TensorFlow to execute the computation graph.</p>
<h3 id="1-1-Linear-function"><a href="#1-1-Linear-function" class="headerlink" title="1.1- Linear function"></a>1.1- Linear function</h3><p>Lets start this programming exercise by computing the following equation: Y=WX+bY, where W and  X are random matrices and b is a random vector.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_function</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a linear function: </span></span><br><span class="line"><span class="string">            Initializes W to be a random tensor of shape (4,3)</span></span><br><span class="line"><span class="string">            Initializes X to be a random tensor of shape (3,1)</span></span><br><span class="line"><span class="string">            Initializes b to be a random tensor of shape (4,1)</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    result -- runs the session for Y = WX + b </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">### START CODE HERE ### (4 lines of code)</span></span><br><span class="line">    X = tf.constant(np.random.randn(<span class="number">3</span>,<span class="number">1</span>), name=<span class="string">"X"</span>)</span><br><span class="line">    W = tf.constant(np.random.randn(<span class="number">4</span>,<span class="number">3</span>), name=<span class="string">"W"</span>)</span><br><span class="line">    b = tf.constant(np.random.randn(<span class="number">4</span>,<span class="number">1</span>), name=<span class="string">"b"</span>)</span><br><span class="line">    Y = tf.add(tf.matmul(W,X),b)</span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create the session using tf.Session() and run it with sess.run(...) on the variable you want to calculate</span></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    result = sess.run(Y)</span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># close the session </span></span><br><span class="line">    sess.close()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<h3 id="1-2-Computing-the-sigmoid"><a href="#1-2-Computing-the-sigmoid" class="headerlink" title="1.2- Computing the sigmoid"></a>1.2- Computing the sigmoid</h3><p>tf.placeholder的参数意义：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.placeholder(</span><br><span class="line">    dtype,</span><br><span class="line">    shape=None,</span><br><span class="line">    name=None</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<p>Tensorflow offers a variety of commonly used neural network functions like tf.sigmoid and tf.softmax. For this exercise lets compute the sigmoid function of an input. You will do this exercise using a placeholder variable x. When running the session, you should use the feed dictionary to pass in the input z. In this exercise, you will have to (i) create a placeholder x, (ii) define the operations needed to compute the sigmoid using tf.sigmoid, and then (iii) run the session.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the sigmoid of z</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    z -- input value, scalar or vector</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    results -- the sigmoid of z</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### ( approx. 4 lines of code)</span></span><br><span class="line">    <span class="comment"># Create a placeholder for x. Name it 'x'.</span></span><br><span class="line">    x = tf.placeholder(tf.float32, name=<span class="string">"x"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute sigmoid(x)</span></span><br><span class="line">    sigmoid = tf.sigmoid(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create a session, and run it. Please use the method 2 explained above. </span></span><br><span class="line">    <span class="comment"># You should use a feed_dict to pass z's value to x. </span></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        <span class="comment"># Run session and call the output "result"</span></span><br><span class="line">        result = sess.run(sigmoid, feed_dict=&#123;x:z&#125;)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>
<p><strong>Summarize</strong>:</p>
<ol>
<li>Create placeholders.</li>
<li>Specify the computation graph corresponding to operations you want to compute.</li>
<li>Create the session.</li>
<li>Run the session, using a feed dictionary if necessary to specify placeholder variables’ values.</li>
</ol>
<h3 id="1-3-Computing-the-Cost"><a href="#1-3-Computing-the-Cost" class="headerlink" title="1.3- Computing the Cost"></a>1.3- Computing the Cost</h3><p>You can also use a built-in function to compute the cost of your neural network. So instead of needing to write code to compute this as a function of a[2]_i and y(i) for i=1…m:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-0db81609e5d5461e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>这里用的一个函数是tf.nn.sigmoid_cross_entropy_with_logits，其中tf.nn.sigmoid_cross_entropy_with_logits(logits = …,  labels = …)，另外可以看到下面注释的notes中写着：What we’ve been calling “z” and “y” in this class are respectively called “logits” and “labels” in the TensorFlow documentation. So logits will feed into z, and labels into y. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(logits, labels)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the cost using the sigmoid cross entropy</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    logits -- vector containing z, output of the last linear unit (before the final sigmoid activation)</span></span><br><span class="line"><span class="string">    labels -- vector of labels y (1 or 0) </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Note: What we've been calling "z" and "y" in this class are respectively called "logits" and "labels" </span></span><br><span class="line"><span class="string">    in the TensorFlow documentation. So logits will feed into z, and labels into y. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost -- runs the session of the cost (formula (2))</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create the placeholders for "logits" (z) and "labels" (y) (approx. 2 lines)</span></span><br><span class="line">    z = tf.placeholder(tf.float32, name=<span class="string">"z"</span>)</span><br><span class="line">    y = tf.placeholder(tf.float32, name=<span class="string">"y"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Use the loss function (approx. 1 line)</span></span><br><span class="line">    cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=z,labels=y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create a session (approx. 1 line). See method 1 above.</span></span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Run the session (approx. 1 line).</span></span><br><span class="line">    cost = sess.run(cost, feed_dict=&#123;z:logits, y:labels&#125;)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Close the session (approx. 1 line). See method 1 above.</span></span><br><span class="line">    sess.close()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure>
<h3 id="1-4-Using-One-Hot-encodings"><a href="#1-4-Using-One-Hot-encodings" class="headerlink" title="1.4- Using One Hot encodings"></a>1.4- Using One Hot encodings</h3><p>Many times in deep learning you will have a y vector with numbers ranging from 0 to C-1, where C is the number of classes. If C is for example 4, then you might have the following y vector which you will need to convert as follows:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-26496561ada5c4f3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>This is called a “one hot” encoding, because in the converted representation exactly one element of each column is “hot” (meaning set to 1). To do this conversion in numpy, you might have to write a few lines of code. In tensorflow, you can use one line of code:</p>
<ul>
<li>tf.one_hot(indices, depth, axis)</li>
</ul>
<p>注意，axis=0是按列编码，即与上图中类似，而axis=1则按照行编码。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_hot_matrix</span><span class="params">(labels, C)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates a matrix where the i-th row corresponds to the ith class number and the jth column</span></span><br><span class="line"><span class="string">                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) </span></span><br><span class="line"><span class="string">                     will be 1. </span></span><br><span class="line"><span class="string">                     </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    labels -- vector containing the labels </span></span><br><span class="line"><span class="string">    C -- number of classes, the depth of the one hot dimension</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    one_hot -- one hot matrix</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line)</span></span><br><span class="line">    C = tf.constant(C,name=<span class="string">"C"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Use tf.one_hot, be careful with the axis (approx. 1 line)</span></span><br><span class="line">    one_hot_matrix = tf.one_hot(indices=labels,depth=C,axis=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create the session (approx. 1 line)</span></span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Run the session (approx. 1 line)</span></span><br><span class="line">    one_hot = sess.run(one_hot_matrix)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Close the session (approx. 1 line). See method 1 above.</span></span><br><span class="line">    sess.close()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> one_hot</span><br></pre></td></tr></table></figure>
<h3 id="1-5-Initialize-with-zeros-and-ones"><a href="#1-5-Initialize-with-zeros-and-ones" class="headerlink" title="1.5- Initialize with zeros and ones"></a>1.5- Initialize with zeros and ones</h3><p>Now you will learn how to initialize a vector of zeros and ones. The function you will be calling is tf.ones(). To initialize with zeros you could use tf.zeros() instead. These functions take in a shape and return an array of dimension shape full of zeros and ones respectively.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ones</span><span class="params">(shape)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates an array of ones of dimension shape</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    shape -- shape of the array you want to create</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    ones -- array containing only ones</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create "ones" tensor using tf.ones(...). (approx. 1 line)</span></span><br><span class="line">    ones = tf.ones(shape)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create the session (approx. 1 line)</span></span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Run the session to compute 'ones' (approx. 1 line)</span></span><br><span class="line">    ones = sess.run(ones)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Close the session (approx. 1 line). See method 1 above.</span></span><br><span class="line">    sess.close()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> ones</span><br></pre></td></tr></table></figure>
<h2 id="2-Building-your-first-neural-network-in-tensorflow"><a href="#2-Building-your-first-neural-network-in-tensorflow" class="headerlink" title="2- Building your first neural network in tensorflow"></a>2- Building your first neural network in tensorflow</h2><p>In this part of the assignment you will build a neural network using tensorflow. Remember that there are two parts to implement a tensorflow model:</p>
<ul>
<li>Create the computation graph</li>
<li>Run the graph</li>
</ul>
<h3 id="2-0-Problem-statement-SIGNS-Dataset"><a href="#2-0-Problem-statement-SIGNS-Dataset" class="headerlink" title="2.0- Problem statement: SIGNS Dataset"></a>2.0- Problem statement: SIGNS Dataset</h3><p>One afternoon, with some friends we decided to teach our computers to decipher sign language. We spent a few hours taking pictures in front of a white wall and came up with the following dataset. It’s now your job to build an algorithm that would facilitate communications from a speech-impaired person to someone who doesn’t understand sign language.</p>
<ul>
<li><strong>Training set</strong>: 1080 pictures (64 by 64 pixels) of signs representing numbers from 0 to 5 (180 pictures per number).</li>
<li><strong>Test set</strong>: 120 pictures (64 by 64 pixels) of signs representing numbers from 0 to 5 (20 pictures per number).</li>
</ul>
<p>Note that this is a subset of the SIGNS dataset. The complete dataset contains many more signs. Here are examples for each number, and how an explanation of how we represent the labels. These are the original pictures, before we lowered the image resolutoion to 64 by 64 pixels.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-926f06281619b2fb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>Run the following code to load the dataset.<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()</span><br></pre></td></tr></table></figure></p>
<p>Change the index below and run the cell to visualize some examples in the dataset.<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of a picture</span></span><br><span class="line">index = <span class="number">0</span></span><br><span class="line">plt.imshow(X_train_orig[index])</span><br><span class="line">print(<span class="string">"y = "</span> + str(np.squeeze(Y_train_orig[:,index])))</span><br></pre></td></tr></table></figure></p>
<p>As usual you flatten the image dataset, then normalize it by dividing by 255. On top of that, you will convert each label to a one-hot vector as shown in Figure 1. Run the cell below to do so.<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Flatten the traing and test images</span></span><br><span class="line">X_train_flatten = X_train_orig.reshape(X_train_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line">X_test_flatten = X_test_orig.reshape(X_test_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line"><span class="comment"># Normalize the image vectors</span></span><br><span class="line">X_train = X_train_flatten / <span class="number">255.</span></span><br><span class="line">X_test = X_test_flatten / <span class="number">255.</span></span><br><span class="line"><span class="comment"># Convert training and test labels to one hot matrixs</span></span><br><span class="line">Y_train = convert_to_one_hot(Y_train_orig, <span class="number">6</span>)</span><br><span class="line">Y_test = convert_to_one_hot(Y_test_orig, <span class="number">6</span>)</span><br></pre></td></tr></table></figure></p>
<p>Your goal is to build an algorithm capable of recognizing a sign with high accuracy. To do so, you are going to build a tensorflow model that is almost the same as one you have previously built in numpy for cat recognition (but now using a softmax output). It is a great occasion to compare your numpy implementation to the tensorflow one.</p>
<p>The model is LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SOFTMAX. The SIGMOID output layer has been converted to a SOFTMAX. A SOFTMAX layer generalizes SIGMOID to when there are more than two classes.</p>
<h3 id="2-1-Create-placeholders"><a href="#2-1-Create-placeholders" class="headerlink" title="2.1- Create placeholders"></a>2.1- Create placeholders</h3><p>Your first task is to create placeholders for X and Y. This will allow you to later pass your training data in when you run your session.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_placeholders</span><span class="params">(n_x, n_y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates the placeholders for the tensorflow session.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)</span></span><br><span class="line"><span class="string">    n_y -- scalar, number of classes (from 0 to 5, so -&gt; 6)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X -- placeholder for the data input, of shape [n_x, None] and dtype "float"</span></span><br><span class="line"><span class="string">    Y -- placeholder for the input labels, of shape [n_y, None] and dtype "float"</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Tips:</span></span><br><span class="line"><span class="string">    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.</span></span><br><span class="line"><span class="string">      In fact, the number of examples during test/train is different.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">    X = tf.placeholder(dtype=<span class="string">"float"</span>,shape=(n_x,<span class="keyword">None</span>))</span><br><span class="line">    Y = tf.placeholder(dtype=<span class="string">"float"</span>,shape=(n_y,<span class="keyword">None</span>))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X, Y</span><br></pre></td></tr></table></figure>
<h3 id="2-2-Initializing-the-parameters"><a href="#2-2-Initializing-the-parameters" class="headerlink" title="2.2- Initializing the parameters"></a>2.2- Initializing the parameters</h3><p>Your second task is to initialize the parameters in tensorflow. </p>
<p><strong>Exercise</strong>: Implement the function below to initialize the parameters in tensorflow. You are going use Xavier Initialization for weights and Zero Initialization for biases. The shapes are given below. As an example, to help you, for W1 and b1 you could use:</p>
<ul>
<li>W1 = tf.get_variable(“W1”, [25,12288], initializer = tf.contrib.layers.xavier_initializer(seed = 1)) (tf.contrib这一方法在Tensorflow2.0会被取消)</li>
<li>b1 = tf.get_variable(“b1”, [25,1], initializer = tf.zeros_initializer())</li>
</ul>
<p>先要写一下tf.get_variable的各个参数名和含义：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">tf.get_variable(</span><br><span class="line">    name, <span class="comment"># 新变量或现有变量的名称。</span></span><br><span class="line">    shape=<span class="keyword">None</span>, <span class="comment"># 新变量或现有变量的形状。</span></span><br><span class="line">    dtype=<span class="keyword">None</span>, <span class="comment"># 新变量或现有变量的类型（默认为DT_FLOAT）。</span></span><br><span class="line">    initializer=<span class="keyword">None</span>, <span class="comment"># 如果创建了则用它来初始化变量。</span></span><br><span class="line">    regularizer=<span class="keyword">None</span>, <span class="comment"># </span></span><br><span class="line">    trainable=<span class="keyword">True</span>, <span class="comment"># 如果为True，还将变量添加到图形集合GraphKeys.TRAINABLE_VARIABLES</span></span><br><span class="line">    collections=<span class="keyword">None</span>, <span class="comment"># 要将变量添加到的图表集合列表</span></span><br><span class="line">    caching_device=<span class="keyword">None</span>, <span class="comment"># 可选的设备字符串或函数，描述变量应被缓存以供读取的位置。</span></span><br><span class="line">    partitioner=<span class="keyword">None</span>, <span class="comment"># 可选callable，接受完全定义的TensorShape和要创建的Variable的dtype，并返回每个轴的分区列表</span></span><br><span class="line">    validate_shape=<span class="keyword">True</span>, <span class="comment"># 如果为False，则允许使用未知形状的值初始化变量。</span></span><br><span class="line">    use_resource=<span class="keyword">None</span>, <span class="comment"># 如果为False，则创建常规变量。如果为true，则使用定义良好的语义创建实验性ResourceVariable。</span></span><br><span class="line">    custom_getter=<span class="keyword">None</span>,</span><br><span class="line">    constraint=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Initializes parameters to build a neural network with tensorflow. The shapes are:</span></span><br><span class="line"><span class="string">                        W1 : [25, 12288]</span></span><br><span class="line"><span class="string">                        b1 : [25, 1]</span></span><br><span class="line"><span class="string">                        W2 : [12, 25]</span></span><br><span class="line"><span class="string">                        b2 : [12, 1]</span></span><br><span class="line"><span class="string">                        W3 : [6, 12]</span></span><br><span class="line"><span class="string">                        b3 : [6, 1]</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    tf.set_random_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 6 lines of code)</span></span><br><span class="line">    W1 = tf.get_variable(<span class="string">"W1"</span>, shape=(<span class="number">25</span>,<span class="number">12288</span>), initializer=tf.contrib.layers.xavier_initializer(seed=<span class="number">1</span>))</span><br><span class="line">    b1 = tf.get_variable(<span class="string">"b1"</span>, shape=(<span class="number">25</span>,<span class="number">1</span>), initializer=tf.zeros_initializer())</span><br><span class="line">    W2 = tf.get_variable(<span class="string">"W2"</span>, shape=(<span class="number">12</span>,<span class="number">25</span>), initializer=tf.contrib.layers.xavier_initializer(seed=<span class="number">1</span>))</span><br><span class="line">    b2 = tf.get_variable(<span class="string">"b2"</span>, shape=(<span class="number">12</span>,<span class="number">1</span>), initializer=tf.zeros_initializer())</span><br><span class="line">    W3 = tf.get_variable(<span class="string">"W3"</span>, shape=(<span class="number">6</span>,<span class="number">12</span>), initializer=tf.contrib.layers.xavier_initializer(seed=<span class="number">1</span>))</span><br><span class="line">    b3 = tf.get_variable(<span class="string">"b3"</span>, shape=(<span class="number">6</span>,<span class="number">1</span>), initializer=tf.zeros_initializer())</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2,</span><br><span class="line">                  <span class="string">"W3"</span>: W3,</span><br><span class="line">                  <span class="string">"b3"</span>: b3&#125;</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure>
<p>At this moment, the parameters haven’t been evaluated yet.</p>
<h3 id="2-3-Forward-propagation-in-tensorflow"><a href="#2-3-Forward-propagation-in-tensorflow" class="headerlink" title="2.3- Forward propagation in tensorflow"></a>2.3- Forward propagation in tensorflow</h3><p>You will now implement the forward propagation module in tensorflow. The function will take in a dictionary of parameters and it will complete the forward pass. The functions you will be using are:</p>
<ul>
<li>tf.add(…,…) to do an addition</li>
<li>tf.matmul(…,…) to do a matrix multiplication</li>
<li>tf.nn.relu(…) to apply the ReLU activation</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the forward propagation for the model: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SOFTMAX</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset placeholder, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3"</span></span><br><span class="line"><span class="string">                  the shapes are given in initialize_parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z3 -- the output of the last LINEAR unit</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve the parameters from the dictionary "parameters" </span></span><br><span class="line">    W1 = parameters[<span class="string">'W1'</span>]</span><br><span class="line">    b1 = parameters[<span class="string">'b1'</span>]</span><br><span class="line">    W2 = parameters[<span class="string">'W2'</span>]</span><br><span class="line">    b2 = parameters[<span class="string">'b2'</span>]</span><br><span class="line">    W3 = parameters[<span class="string">'W3'</span>]</span><br><span class="line">    b3 = parameters[<span class="string">'b3'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:</span></span><br><span class="line">    Z1 = tf.add(tf.matmul(W1,X),b1)                                          <span class="comment"># Z1 = np.dot(W1, X) + b1</span></span><br><span class="line">    A1 = tf.nn.relu(Z1)                                              <span class="comment"># A1 = relu(Z1)</span></span><br><span class="line">    Z2 = tf.add(tf.matmul(W2,A1),b2)                                              <span class="comment"># Z2 = np.dot(W2, a1) + b2</span></span><br><span class="line">    A2 = tf.nn.relu(Z2)                                             <span class="comment"># A2 = relu(Z2)</span></span><br><span class="line">    Z3 = tf.add(tf.matmul(W3,A2),b3)                                              <span class="comment"># Z3 = np.dot(W3,Z2) + b3</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z3</span><br></pre></td></tr></table></figure>
<p>You may have noticed that the forward propagation doesn’t output any cache. You will understand why below, when we get to brackpropagation.</p>
<h3 id="2-4-Compute-cost"><a href="#2-4-Compute-cost" class="headerlink" title="2.4- Compute cost"></a>2.4- Compute cost</h3><p>As seen before, it is very easy to compute the cost using:</p>
<pre><code>- tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = ..., labels = ...))
</code></pre><p><strong>Question</strong>: Implement the cost function below.</p>
<ul>
<li>It is important to know that the “logits” and “labels” inputs of tf.nn.softmax_cross_entropy_with_logits are expected to be of shape (number of examples, num_classes). We have thus transposed Z3 and Y for you.</li>
<li>Besides, tf.reduce_mean basically does the summation over the examples. 注意！</li>
</ul>
<p>这里要注意tf.nn.softmax_cross_entropy_with_logits，其参数含义如下：</p>
<ul>
<li>logits: 神经网络最后一层的输出，如果有batch的话，它的大小就是[batchsize，num_classes]，单样本的话，大小就是num_classes.</li>
<li>labels: 实际的标签，大小同上。</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(Z3, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the cost</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector placeholder, same shape as Z3</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost - Tensor of the cost function</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)</span></span><br><span class="line">    logits = tf.transpose(Z3)</span><br><span class="line">    labels = tf.transpose(Y)</span><br><span class="line"></span><br><span class="line">    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure>
<h3 id="2-5-Backward-propagation-amp-parameter-update"><a href="#2-5-Backward-propagation-amp-parameter-update" class="headerlink" title="2.5- Backward propagation &amp; parameter update"></a>2.5- Backward propagation &amp; parameter update</h3><p>This is where you become grateful to programming frameworks. All the backpropagation and the parameters update is taken care of in 1 line of code. It is very easy to incorporate this line in the model.</p>
<p>After you compute the cost function. You will create an “optimizer” object. You have to call this object along with the cost when running the tf.session. When called, it will perform an optimization on the given cost with the chosen method and learning rate.</p>
<p>For instance, for gradient descent the optimizer would be:</p>
<ul>
<li>optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)</li>
</ul>
<p>To make the optimization you would do:</p>
<ul>
<li>_, c = sess.run([optimizer, cost], feed_dict={X:minibatch_X, Y:minibatch_Y})</li>
</ul>
<p>This computes the backpropagation by passing through the tensorflow graph in the reverse order. From cost to inputs.</p>
<p><strong>Note</strong> When coding, we often use _ as a “throwaway” variable to store values that we won’t need to use later. Here, _ takes on the evaluated value of optimizer, which we don’t need (and c takes the value of the cost variable).</p>
<h3 id="2-6-Building-the-model"><a href="#2-6-Building-the-model" class="headerlink" title="2.6- Building the model"></a>2.6- Building the model</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_test, Y_test, learning_rate=<span class="number">0.0001</span>, num_epoches=<span class="number">1500</span>, minibatch_size=<span class="number">32</span>, print_cost=True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a three-layer tensorflow neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SOFTMAX.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)</span></span><br><span class="line"><span class="string">    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)</span></span><br><span class="line"><span class="string">    X_test -- training set, of shape (input size = 12288, number of training examples = 120)</span></span><br><span class="line"><span class="string">    Y_test -- test set, of shape (output size = 6, number of test examples = 120)</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the optimization</span></span><br><span class="line"><span class="string">    num_epochs -- number of epochs of the optimization loop</span></span><br><span class="line"><span class="string">    minibatch_size -- size of a minibatch</span></span><br><span class="line"><span class="string">    print_cost -- True to print the cost every 100 epochs</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    ops.reset_default_graph() <span class="comment"># tensorflow在生产环境下，需要将default graph 重新初始化，以保证内存中没有其他的Graph，或者说我们需要在每个session之后清理相应的Graph。</span></span><br><span class="line">    tf.set_random_seed(<span class="number">1</span>)</span><br><span class="line">    seed = <span class="number">3</span></span><br><span class="line">    (n_x, m) = X_train.shape</span><br><span class="line">    n_y = Y_train.shape[<span class="number">0</span>]</span><br><span class="line">    costs = []</span><br><span class="line"></span><br><span class="line">    X, Y = create_placeholders(n_x,, n_y)</span><br><span class="line"></span><br><span class="line">    parameters = initialize_parameters()</span><br><span class="line"></span><br><span class="line">    Z3 = forward_propagation(X, parameters)</span><br><span class="line"></span><br><span class="line">    cost = compute_cost(Z3, Y)</span><br><span class="line"></span><br><span class="line">    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)</span><br><span class="line"></span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        sess.run(init)</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">            epoch_cost = <span class="number">0</span></span><br><span class="line">            num_minibatches = int(m / minibatch_size)</span><br><span class="line">            seed += <span class="number">1</span></span><br><span class="line">            minibatches = random_mini_batches(X_train, Y_train, minibatch_size)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> minibatch <span class="keyword">in</span> minibatches:</span><br><span class="line">                (minibatch_X, minibatch_Y) = minibatch</span><br><span class="line">                _, minibatch_cost = sess.run([optimizer, cost], feed_dict=&#123;X:minibatch_X,Y:minibatch_Y&#125;) <span class="comment"># 列表表示optimizer和cost同时计算</span></span><br><span class="line">                epoch_cost += minibatch_cost / num_minibatches</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> print_cost == <span class="keyword">True</span> <span class="keyword">and</span> epoch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">print</span> (<span class="string">"Cost after epoch %i: %f"</span> % (epoch, epoch_cost))</span><br><span class="line">            <span class="keyword">if</span> print_cost == <span class="keyword">True</span> <span class="keyword">and</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">                costs.append(epoch_cost)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># plot the cost</span></span><br><span class="line">        plt.plot(np.squeeze(costs))</span><br><span class="line">        plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">        plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">        plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># lets save the parameters in a variable</span></span><br><span class="line">        parameters = sess.run(parameters)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"Parameters have been trained!"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the correct predictions</span></span><br><span class="line">        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate accuracy on the test set</span></span><br><span class="line">        accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>)) <span class="comment"># tf.cast转变数据格式</span></span><br><span class="line"></span><br><span class="line">        print(<span class="string">"Train Accuracy:"</span>, accuracy.eval(&#123;X:X_train, Y:Y_train&#125;))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line"></span><br><span class="line">parameters = model(X_train, Y_train, X_test, Y_test)</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">![image.png](https://upload-images.jianshu.io/upload_images/8636110-52fb43d5d3768581.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)</span><br><span class="line"></span><br><span class="line">可以看到，结果有些过拟合了。</span><br><span class="line"></span><br><span class="line">补充函数细节：</span><br><span class="line"><span class="number">1.</span> tf.equal(A, B)是对比这两个矩阵或者向量的相等的元素，如果是相等的那就返回<span class="keyword">True</span>，反正返回<span class="keyword">False</span>，返回的值的矩阵维度和A是一样的</span><br><span class="line">。</span><br><span class="line"><span class="number">2.</span> tf.cast转换数据格式，如tf.cast(correct_prediction, <span class="string">"float"</span>)，其中correct_prediction本为bool格式，现在转换为float格式。</span><br><span class="line"><span class="number">3.</span> tf.reduce_mean(A,axis=<span class="number">0</span>) <span class="comment">#求平均，其中axis=0是按列求平均，axis=1按行求</span></span><br><span class="line"><span class="number">4.</span> accuracy.eval(&#123;X:X_train, Y:Y_train&#125;的写法类似于：</span><br><span class="line">``` py</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(accuracy, feed_dict=&#123;X:X_train,Y:Y_train&#125;)</span><br></pre></td></tr></table></figure>
<p><strong>Insights</strong>:</p>
<ul>
<li>Your model seems big enough to fit the training set well. However, given the difference between train and test accuracy, you could try to add L2 or dropout regularization to reduce overfitting.</li>
<li>Think about the session as a block of code to train the model. Each time you run the session on a minibatch, it trains the parameters. In total you have run the session a large number of times (1500 epochs) until you obtained well trained parameters.</li>
</ul>
<h3 id="2-7-Reduct-overfitting"><a href="#2-7-Reduct-overfitting" class="headerlink" title="2.7- Reduct overfitting"></a>2.7- Reduct overfitting</h3><p>Add dropout, keep_prob=0.9, Result:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e6fde280ea117e2f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>发现效果很差，可能的原因是本身神经网络层数就少，而且隐藏层结点个数也比较少，因此不适合使用dropout。</p>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p><strong>What you should remember</strong>:</p>
<ul>
<li>Tensorflow is a programming framework used in deep learning</li>
<li>The two main object classes in tensorflow are Tensors and Operators.</li>
<li>When you code in tensorflow you have to take the following steps:<ul>
<li>Create a graph containing Tensors (Variables, Placeholders …) and Operations (tf.matmul, tf.add, …)</li>
<li>Create a session</li>
<li>Initialize the session</li>
<li>Run the session to execute the graph</li>
</ul>
</li>
<li>You can execute the graph multiple times as you’ve seen in model()</li>
<li>The backpropagation and optimization is automatically done when running the session on the “optimizer” object.</li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/python/" rel="tag"><i class="fa fa-tag"></i> python</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/16/deep-learningw6/" rel="next" title="第6周-优化算法">
                <i class="fa fa-chevron-left"></i> 第6周-优化算法
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/04/18/deep-learningw8/" rel="prev" title="第8周-机器学习ML策略(1)">
                第8周-机器学习ML策略(1) <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="DesmonDay" />
          <p class="site-author-name" itemprop="name">DesmonDay</p>
           
              <p class="site-description motion-element" itemprop="description">主攻方向：NLP</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">102</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">14</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">10</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/DesmonDay" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#调试处理-Tuning-process"><span class="nav-number">1.</span> <span class="nav-text">调试处理-Tuning process</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#为超参数选择合适的范围"><span class="nav-number">2.</span> <span class="nav-text">为超参数选择合适的范围</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#超参数训练的实践：Panda-vs-Caviar"><span class="nav-number">3.</span> <span class="nav-text">超参数训练的实践：Panda vs Caviar</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Batch-Norm-——-感觉还不太懂，需要回看"><span class="nav-number">4.</span> <span class="nav-text">Batch Norm —— 感觉还不太懂，需要回看</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#正则化网络的激活函数-——-Normalizing-activations-in-a-network"><span class="nav-number">4.1.</span> <span class="nav-text">正则化网络的激活函数 —— Normalizing activations in a network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#将Batch-Norm拟合进神经网络"><span class="nav-number">4.2.</span> <span class="nav-text">将Batch Norm拟合进神经网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#为什么Batch-Norm奏效？"><span class="nav-number">4.3.</span> <span class="nav-text">为什么Batch Norm奏效？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Batch-Norm-at-test-time"><span class="nav-number">4.4.</span> <span class="nav-text">Batch Norm at test time</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Softmax回归"><span class="nav-number">5.</span> <span class="nav-text">Softmax回归</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#训练一个Softmax分类器"><span class="nav-number">6.</span> <span class="nav-text">训练一个Softmax分类器</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Understanding-softmax"><span class="nav-number">6.1.</span> <span class="nav-text">Understanding softmax</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Loss-function"><span class="nav-number">6.2.</span> <span class="nav-text">Loss function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient-descent-with-softmax"><span class="nav-number">6.3.</span> <span class="nav-text">Gradient descent with softmax</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#深度学习框架"><span class="nav-number">7.</span> <span class="nav-text">深度学习框架</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Tensorflow"><span class="nav-number">8.</span> <span class="nav-text">Tensorflow</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#本周作业"><span class="nav-number">9.</span> <span class="nav-text">本周作业</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Exploring-the-Tensorflow-Library"><span class="nav-number">9.1.</span> <span class="nav-text">1- Exploring the Tensorflow Library</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-Linear-function"><span class="nav-number">9.1.1.</span> <span class="nav-text">1.1- Linear function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-Computing-the-sigmoid"><span class="nav-number">9.1.2.</span> <span class="nav-text">1.2- Computing the sigmoid</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-Computing-the-Cost"><span class="nav-number">9.1.3.</span> <span class="nav-text">1.3- Computing the Cost</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-Using-One-Hot-encodings"><span class="nav-number">9.1.4.</span> <span class="nav-text">1.4- Using One Hot encodings</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-Initialize-with-zeros-and-ones"><span class="nav-number">9.1.5.</span> <span class="nav-text">1.5- Initialize with zeros and ones</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Building-your-first-neural-network-in-tensorflow"><span class="nav-number">9.2.</span> <span class="nav-text">2- Building your first neural network in tensorflow</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-0-Problem-statement-SIGNS-Dataset"><span class="nav-number">9.2.1.</span> <span class="nav-text">2.0- Problem statement: SIGNS Dataset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-Create-placeholders"><span class="nav-number">9.2.2.</span> <span class="nav-text">2.1- Create placeholders</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Initializing-the-parameters"><span class="nav-number">9.2.3.</span> <span class="nav-text">2.2- Initializing the parameters</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-Forward-propagation-in-tensorflow"><span class="nav-number">9.2.4.</span> <span class="nav-text">2.3- Forward propagation in tensorflow</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-Compute-cost"><span class="nav-number">9.2.5.</span> <span class="nav-text">2.4- Compute cost</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-Backward-propagation-amp-parameter-update"><span class="nav-number">9.2.6.</span> <span class="nav-text">2.5- Backward propagation &amp; parameter update</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-Building-the-model"><span class="nav-number">9.2.7.</span> <span class="nav-text">2.6- Building the model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-7-Reduct-overfitting"><span class="nav-number">9.2.8.</span> <span class="nav-text">2.7- Reduct overfitting</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Summary"><span class="nav-number">9.2.9.</span> <span class="nav-text">Summary</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">DesmonDay</span>
</div>



<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv"></span>
</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->



  


  




	





  





  






  





  

  

  

  

  

  

</body>
</html>


