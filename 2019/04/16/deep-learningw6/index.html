<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="python," />





  <link rel="alternate" href="/atom.xml" title="DesmonDay's Blog" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="Mini-batch梯度下降如果针对整个数据集进行梯度下降，如果数据集很大，那么速度就会很慢。因此，我们可以对训练集中的一部分进行梯度下降。因此可以将训练集分为多个mini-batch，针对每个mini-batch进行梯度下降。从图中，我们看到Batch vs Mini-Batch，这里对比的就是我们原本的针对整个训练集进行梯度下降，以及选取mini-batch进行梯度下降的两种算法的对比。另外，">
<meta name="keywords" content="python">
<meta property="og:type" content="article">
<meta property="og:title" content="第6周-优化算法">
<meta property="og:url" content="https://github.com/DesmonDay/2019/04/16/deep-learningw6/index.html">
<meta property="og:site_name" content="DesmonDay&#39;s Blog">
<meta property="og:description" content="Mini-batch梯度下降如果针对整个数据集进行梯度下降，如果数据集很大，那么速度就会很慢。因此，我们可以对训练集中的一部分进行梯度下降。因此可以将训练集分为多个mini-batch，针对每个mini-batch进行梯度下降。从图中，我们看到Batch vs Mini-Batch，这里对比的就是我们原本的针对整个训练集进行梯度下降，以及选取mini-batch进行梯度下降的两种算法的对比。另外，">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-b90b350915d8f58d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-48eeaa69726e431c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-2fb5165da85d56d2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-7a03ffbba9277896.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-551f71d8ab370f62.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-09c7eea411372cee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-21e01bea913eb11f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-4d85231e7f47c235.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-c0c6d59de3f109ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-4a5ff2b6a6d30d45.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-d4bb770fc95e9dc4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-859ebd4c5aa72ba2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-befc97b423e2f524.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-985686082610c848.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-61f572480ad43f6a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-dbd732b1528e2151.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-172f808eac7f2cb0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-2ff21025939023a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-431dfd3e84bd8190.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-630ba6bad3fbc27b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-89256312e13395ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-0f0fa5c4ca3507ed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-31c17b1d12db0402.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-6217f59e9c3c3ea4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-651b92262bcf2019.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-5a0ca18edc26ef32.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-b63d6a4c12dc670f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-3605ee1a34e68d5f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-7ddff38de2eb2b55.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-904369f041ec7b05.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-5fa4deb2997c6ad9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-550ee71c4d175b2c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-438631276f2ef3a1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-5118dd90e09c52ea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-8444151dd32c3ed0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-60376c25400e0caf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-6be70187abe0d1f2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-659ccbba10be0431.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-af530967c5a62d74.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-d3d841aee105446b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-2292db4d01a01927.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:updated_time" content="2019-04-17T04:55:12.575Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="第6周-优化算法">
<meta name="twitter:description" content="Mini-batch梯度下降如果针对整个数据集进行梯度下降，如果数据集很大，那么速度就会很慢。因此，我们可以对训练集中的一部分进行梯度下降。因此可以将训练集分为多个mini-batch，针对每个mini-batch进行梯度下降。从图中，我们看到Batch vs Mini-Batch，这里对比的就是我们原本的针对整个训练集进行梯度下降，以及选取mini-batch进行梯度下降的两种算法的对比。另外，">
<meta name="twitter:image" content="https://upload-images.jianshu.io/upload_images/8636110-b90b350915d8f58d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://github.com/DesmonDay/2019/04/16/deep-learningw6/"/>





  <title>第6周-优化算法 | DesmonDay's Blog</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">DesmonDay's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">一只小辣鸡的自我拯救之路</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/DesmonDay/2019/04/16/deep-learningw6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="DesmonDay">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DesmonDay's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">第6周-优化算法</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-16T16:34:21+08:00">
                2019-04-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Mini-batch梯度下降"><a href="#Mini-batch梯度下降" class="headerlink" title="Mini-batch梯度下降"></a>Mini-batch梯度下降</h1><p>如果针对整个数据集进行梯度下降，如果数据集很大，那么速度就会很慢。因此，我们可以对训练集中的一部分进行梯度下降。因此可以将训练集分为多个mini-batch，针对每个mini-batch进行梯度下降。从图中，我们看到Batch vs Mini-Batch，这里对比的就是我们原本的针对整个训练集进行梯度下降，以及选取mini-batch进行梯度下降的两种算法的对比。另外，针对每个mini-batch，其标记符号为X^{1}…X^{t}.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b90b350915d8f58d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>Mini-batch梯度下降的实现过程：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-48eeaa69726e431c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>从图中可知，我们在一次for循环中遍历所有的mini-batch样本，每次的计算仅针对X^{t}和Y^{t}来进行，整个的for循环称为一次epoch。如果我们要进行多次epoch，可以再添加一个针对epoch的for循环。</p>
<h1 id="理解Mini-batch梯度下降"><a href="#理解Mini-batch梯度下降" class="headerlink" title="理解Mini-batch梯度下降"></a>理解Mini-batch梯度下降</h1><p><img src="https://upload-images.jianshu.io/upload_images/8636110-2fb5165da85d56d2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>因此我们需要决定mini-batch的大小，一般有如下三种情况：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7a03ffbba9277896.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>三种选择下的不同梯度下降情况如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-551f71d8ab370f62.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-09c7eea411372cee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<ol>
<li>随机梯度下降：失去了向量化的速度优势，且梯度来回震荡剧烈；</li>
<li>Mini-Batch: 学习速度最快，并且不需要处理整个数据集；</li>
<li>Batch: 每一次迭代时间过长，需要处理整个数据集。</li>
</ol>
<p>那么如何选择合适的位于1与m之间的mini-batch大小呢？</p>
<ol>
<li>当数据集较小，如m&lt;=2000，则使用Batch梯度下降</li>
<li>当数据集较大，通常选择：64,128,256,512，1024等等（2的次数）（通常这里需要多加尝试，选择合适的值）</li>
<li>确保mini batch的大小在可用内存大小的范围内。</li>
</ol>
<h1 id="指数加权平均"><a href="#指数加权平均" class="headerlink" title="指数加权平均"></a>指数加权平均</h1><p>首先，我们需要知道平均数求法，即比如我们现在有100天的温度值，要求这100天的平均温度值，那么我们可以直接利用公式进行计算：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-21e01bea913eb11f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>何为指数加权平均，如何计算指数加权平均，下面给了一个关于温度的例子，可以看到得到的红线相比原来的蓝点要平滑许多。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-4d85231e7f47c235.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>改变$\beta$的作用，导致我们得到的指数加权平均数的不同：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c0c6d59de3f109ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>可以看到，当$\beta$越大，得到的曲线越平缓，但是却不能很好反映温度的变化；$\beta$越小，则曲线越接近温度的变化，但这样子得到的噪声或异常值也会有很多。因此，合适的$\beta$值，也会影响着算法的表现。</p>
<h1 id="理解指数加权平均"><a href="#理解指数加权平均" class="headerlink" title="理解指数加权平均"></a>理解指数加权平均</h1><p>指数加权平均是几种优化算法的关键步骤，而其实质上是将每日温度与指数衰减函数相乘，然后求和。形象化的图形解释如下图：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-4a5ff2b6a6d30d45.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>另外，按照粗略的计算，当$\beta=0.9$时，相当于计算了10天的平均温度，当$\beta=0.98$时，相当于计算了50天的平均温度。</p>
<p>实际的实现：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d4bb770fc95e9dc4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>可以看到实现很简单，并且内存占用也很少，因此其效率也很高。尽管这不是最精确的计算，因为最精确的计算时直接计算温度的平均值，这往往会得到更好的预测，但这种做法的缺点是如果保存所有最近的温度数据和过去的天数温度，这会占用很多内存。因此在深度学习中往往使用指数加权平均数来计算。</p>
<h1 id="指数加权平均的偏差修正-Bias-correction-in-exponentially-weighted-average"><a href="#指数加权平均的偏差修正-Bias-correction-in-exponentially-weighted-average" class="headerlink" title="指数加权平均的偏差修正-Bias correction in exponentially weighted average"></a>指数加权平均的偏差修正-Bias correction in exponentially weighted average</h1><p>有一种偏差修正的技巧，能够使我们通过指数加权平均计算的平均数更加准确。如下图，当$\beta=0.9$时，我们得到红线；当$\beta=0.98$时，如果我们直接使用上面所写的原公式计算，即$v_t = \beta v_{t-1} + (1-\beta)\theta_t$，我们实际上得到的是紫线，而其实更合理的是绿色线。可以看到紫色曲线的起点较低，因此我们需要处理这种情况。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-859ebd4c5aa72ba2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>举原公式的一个例子，我们设置$v_0$初值为0，所以计算$v_1$时，由于$v_0=0$，而且我们第一天的温度为40的话，那么计算出来的$v_1$实际上很小，不符合实际情况，继续计算下去的结果也均会偏小。因此我们可以有方法来让估测更好，特别是在估测初期。我们将估计的值不再表示为$v_t$，而是表示为$\frac{v_t}{1-\beta ^t}$。因此当t比较小时，分母较小，我们可以消除偏差；当t比较大时，分母接近1，则还原为原式。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-befc97b423e2f524.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h1 id="动量梯度下降-Gradient-descent-with-momentum"><a href="#动量梯度下降-Gradient-descent-with-momentum" class="headerlink" title="动量梯度下降-Gradient descent with momentum"></a>动量梯度下降-Gradient descent with momentum</h1><p>有一种叫做Momentum的优化算法，它的运行速度总是比标准的梯度下降算法要快。其简单的思想就是，计算梯度的指数加权平均数，并利用该梯度来更新权重。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-985686082610c848.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>假设我们要对上图的成本函数J做梯度下降，其中红点为最优点，蓝点为我们的起点。如果我们采用梯度下降法，那么整个过程就如下图的蓝线，其频繁的上下波动明显减慢了梯度下降法的速度。为了避免大幅度的波动（紫线），我们也需要采用一个较小的学习率。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-61f572480ad43f6a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>因此，我们可以采用动量梯度下降的方法，其对应的步骤如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-dbd732b1528e2151.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>我们可以形象地把整个过程当做一个从山上滚下来的球的加速运动，其中，以$V_{db}$的计算式为例，$d_b$起到了加速度的作用，而原来的$V_{db}$则表示速度，$\beta$则表现为摩擦力，保证了球不会无限加速下去。在这个例子中，球获得了动量，因此也称为动量梯度下降。</p>
<p>由上图算法步骤，我们可知超参数包括学习率$\alpha$和指数加权参数$\beta$，而$\beta$最常用的值为0.9（在梯度下降中则表示平均了前十次迭代的梯度，另外0.9是很棒的鲁棒数）。在实际中，我们在使用梯度下降或M<br>omentum的时候，通常不需要考虑偏差修正。另外，$db$和$dW$最开始初始化为零向量。另外，$v_{dW}=\beta v_{dw}+(1-\beta)dW$可以写成另一种算法$v_{dW}=\beta v_{dw}+dW$，即去掉了$1-\beta$。但大多数情况下，原有的表达形式更方便，效果也更好一点。</p>
<h1 id="RMSprop-Root-Mean-Square-Prop"><a href="#RMSprop-Root-Mean-Square-Prop" class="headerlink" title="RMSprop - Root Mean Square Prop"></a>RMSprop - Root Mean Square Prop</h1><p>RMSprop也是可以加速梯度下降的一种优化算法。我们在上一节就看到了，普通的梯度下降在横轴方向推进，但在纵轴方向上则来回摆动。因此，我们会希望梯度的下降在纵轴的速度比较慢，而横轴则比较快，如绿线所示。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-172f808eac7f2cb0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>这一部分与动量梯度下降有所不同。具体如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-2ff21025939023a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>可以看到，在梯度更新部分，后半个式子乘的是$(dW)^2$（这也是叫做Square的原因）。随后，更新参数时减去的是学习率乘以$\frac{dW}{\sqrt(S_{dW})}$，因此有Root Mean Square的说法。使用RMSprop进行更新，我们得到的梯度下降速度在纵轴方向上摆动较小，在横轴方向则持续推进。另外，我们也可以选取较大的学习率，并且也不会影响到纵轴的速度。</p>
<p>需要注意的是，由于在下一节会将Momentum和RMSprop结合起来，所以为了区分两个相同的超参数$\beta$，因此我们将RMSprop的超参数写作$\beta_2$。同时，为了避免分母为0，即$\sqrt(S_{dW})$接近0，因此我们会加一个小小的$\epsilon$来使得数据稳定。通常取$\epsilon$为10^{-8}.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-431dfd3e84bd8190.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>总结：RMSprop与Momentum相似的一点是可以消除梯度下降中的摆动，同时也包括mini-batch梯度下降，并且RMSprop允许我们使用一个更大的学习率$\alpha$。从而加快算法的学习速度。</p>
<p>一个趣事：RMSprop最开始不是在学术论文上提出的，而是Jeff在Coursera课程上提出的。</p>
<h1 id="Adam优化算法"><a href="#Adam优化算法" class="headerlink" title="Adam优化算法"></a>Adam优化算法</h1><p>RMSprop和Adam优化算法可以使用于不同的神经网络结构，而Adam算法是将Momentum和RMSprop结合起来的算法。算法过程如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-630ba6bad3fbc27b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>接下来对上述算法流程的一个解释：首先初始化四个变量。在每一次的迭代中，计算当前mini-batch的梯度dW和db，然后计算Momentum和RMSprop各自需要的量（注意指数加权平均的\beta表示不一样）。之后，再对两者将进行偏差矫正，之后使用更新公式将Momentum和RMSprop结合起来，从而得到了Adam优化算法。</p>
<p>在此算法中有许多超参数：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-89256312e13395ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>对于后面三个，我们通常直接使用缺省值，而真正需要调整的是学习率$\alpha$。</p>
<p>为什么叫做Adam，其全称为Adaptive Moment Estimation.因此$\beta_1$用来计算dW微分，称为first moment，而$\beta_2$用来计算平方数的指数加权平均数，称为second moment。</p>
<h1 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h1><p>加快梯度下降的一个方法是随时间增长而慢慢地减少学习率，我们称为学习率衰减(decay)。对于下图，蓝线指我们使用mini-batch但是固定的学习率，因此最后我们可能不断地再最优点迭代但很难达到最优区域；而针对绿线，我们首先可以采取较大的学习率，到了接近收敛的时候，则采用较小的学习率，以便于接近最优点。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-0f0fa5c4ca3507ed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>接下来讲讲如何实现学习率衰减。</p>
<p>方法：首先理解epoch的含义，一次epoch是指遍历一次训练集全集，形象化表示如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-31c17b1d12db0402.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>而学习率衰减的公式及一个简单的例子如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-6217f59e9c3c3ea4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>可以看到，这里包括了两个需要调整的超参数：$\alpha_0$和衰减率decay_rate，从而实现了学习率随着epoch的增大而不断较小的目的。</p>
<p>其他方法：如指数衰减。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-651b92262bcf2019.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>或者其他三种形式的衰减：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-5a0ca18edc26ef32.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>另外，也有一些人采用手动调整学习率的方法。在下周，我们会学习如何对大量的超参数进行高效搜索的方法。</p>
<h1 id="局部最优的问题"><a href="#局部最优的问题" class="headerlink" title="局部最优的问题"></a>局部最优的问题</h1><p>在深度学习研究早期，人们总是担心优化算法会困在极差的局部最优中。但随着深度学习理论的不断发展，我们对局部最优有了更深的理解。</p>
<p>当我们创建一个神经网络，通常梯度为0的点并不是这个图中的局部最优点。实际上，成本函数的零梯度点通常是鞍点(saddle points)。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b63d6a4c12dc670f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>但一个具有高维空间的成本函数，如果梯度为0，那么在每个方向它可能是凸函数，也可能是凹函数。比如，对于一个20000维的空间，如果要得到局部最优，这个可能性是比较小的，而我们更可能是遇到鞍点，如下图右图。因此我们从深度学习的历史可以学习到，我们在低维空间学习到的经验，并不适用于高维空间的情况。（低维空间容易遇到局部最优点，但高维空间更多的是鞍点）</p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-3605ee1a34e68d5f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>Problems of plateaus: 平稳段：导数长期接近0。对于水平平面，我们需要很长的一段时间才能达到平稳段的点，再由于左边或右边的随机扰动，最后才可以走出平稳段。因此我们可以知道，我们在高维空间里是不太可能困在局部最优中的，但另一个问题是我们可能需要较长的时间才能走出平稳段。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7ddff38de2eb2b55.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h1 id="本周作业"><a href="#本周作业" class="headerlink" title="本周作业"></a>本周作业</h1><p>代入包：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> opt_utils <span class="keyword">import</span> load_params_and_grads, initialize_parameters, forward_propagation, backward_propagation</span><br><span class="line"><span class="keyword">from</span> opt_utils <span class="keyword">import</span> compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">7.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br></pre></td></tr></table></figure></p>
<h2 id="1-Gradient-Descent"><a href="#1-Gradient-Descent" class="headerlink" title="1- Gradient Descent"></a>1- Gradient Descent</h2><p>一个简单的exercise:<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_with_gd</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using one step of gradient descent</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters to be updated:</span></span><br><span class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients to update each parameters:</span></span><br><span class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></span><br><span class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural networks</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update rule for each parameter</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L): <span class="comment"># l从0开始记起，因此for循环中记为l+1</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">'dW'</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">'db'</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure></p>
<p>接下来，展示一个区分Batch和SGD的代码样例：</p>
<ul>
<li><p><strong>(Batch) Gradient Descent</strong>:</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X = data_input</span><br><span class="line">Y = labels</span><br><span class="line">parameters = initialize_parameters(layers_dims)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">    <span class="comment"># Forward propagation</span></span><br><span class="line">    a, caches = forward_propagation(X, parameters)</span><br><span class="line">    <span class="comment"># Compute cost.</span></span><br><span class="line">    cost = compute_cost(a, Y)</span><br><span class="line">    <span class="comment"># Backward propagation.</span></span><br><span class="line">    grads = backward_propagation(a, caches, parameters)</span><br><span class="line">    <span class="comment"># Update parameters.</span></span><br><span class="line">    parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>Stochastic Gradient Descent</strong>:</p>
  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">X = data_input</span><br><span class="line">Y = labels</span><br><span class="line">parameters = initialize_parameters(layers_dims)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, m):</span><br><span class="line">        <span class="comment"># Forward propagation</span></span><br><span class="line">        a, caches = forward_propagation(X[:,j], parameters)</span><br><span class="line">        <span class="comment"># Compute cost</span></span><br><span class="line">        cost = compute_cost(a, Y[:,j])</span><br><span class="line">        <span class="comment"># Backward propagation</span></span><br><span class="line">        grads = backward_propagation(a, caches, parameters)</span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>可以看到实现随机梯度下降需要三个for循环：</p>
<ul>
<li>Over the number of iterations</li>
<li>Over the m training examples</li>
<li>Over the layers (to update all parameters, from (W[1],b[1]) to (W[L],b[L]))</li>
</ul>
<p><strong>What we should remember</strong>:</p>
<ul>
<li>The difference between gradient descent, mini-batch gradient descent and stochastic gradient descent is the number of examples you use to perform one update step.</li>
<li>You have to tune a learning rate hyperparameter α.</li>
<li>With a well-turned mini-batch size, usually it outperforms either gradient descent or stochastic gradient descent (particularly when the training set is large).</li>
</ul>
<h2 id="2-Mini-Batch-Gradient-Descent"><a href="#2-Mini-Batch-Gradient-Descent" class="headerlink" title="2- Mini-Batch Gradient Descent"></a>2- Mini-Batch Gradient Descent</h2><p>实现Mini-Batch梯度下降包括两步，一是对训练样本进行随机打乱，二是划分mini-batches。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-904369f041ec7b05.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-5fa4deb2997c6ad9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_mini_batches</span><span class="params">(X, Y, mini_batch_size=<span class="number">64</span>, seed=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates a list of random minibatches from (X, Y)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    mini_batch_size -- size of the mini-batches, integer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    mini_batches = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 1: Shuffle (X,Y)</span></span><br><span class="line">    permutation = list(np.random.permutation(m)) <span class="comment"># 将[0,m)序列打乱</span></span><br><span class="line">    shuffled_X = X[:, permutation]</span><br><span class="line">    shuffled_Y = Y[:, permutation].reshape((<span class="number">1</span>,m))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 2: Partition (Shuffled_X, shuffled_Y) Minus the end case.</span></span><br><span class="line">    num_complete_minibatches = math.floor(m / mini_batch_size) <span class="comment">#number of mini batches of size mini_batch_size in your partitionning</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>, num_complete_minibatches):</span><br><span class="line">        mini_batch_X = shuffled_X[:, mini_batch_size*k : mini_batch_size*(k+<span class="number">1</span>)]</span><br><span class="line">        mini_batch_Y = shuffle_Y[:, mini_batch_size*k : mini_batch_size*(k+<span class="number">1</span>)].reshape((<span class="number">1</span>,<span class="number">-1</span>))</span><br><span class="line">        mini_batch = (mini_batdch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Handling the end case</span></span><br><span class="line">    <span class="keyword">if</span> m % mini_batch_size != <span class="number">0</span>:</span><br><span class="line">        mini_batch_X = shuffled_X[:, mini_batch_size*num_complete_minibatches : m]</span><br><span class="line">        mini_batch_Y = shuffled_Y[:, mini_batch_size*num_complete_minibatches : m].reshape((<span class="number">1</span>,<span class="number">-1</span>))</span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> mini_batches</span><br></pre></td></tr></table></figure>
<p><strong>What we should remember</strong>:</p>
<ul>
<li>Shuffling and Partitioning are the two steps required to build mini-batches</li>
<li>Powers of two are often chosen to be the mini-batch size, e.g., 16,32,64,128.</li>
</ul>
<h2 id="3-Momentum"><a href="#3-Momentum" class="headerlink" title="3- Momentum"></a>3- Momentum</h2><p>Because mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will “oscillate” toward convergence. Using momentum can reduce these oscillations.</p>
<p>Momentum takes into account the past gradients to smooth out the update. We will store the ‘direction’ of the previous gradients in the variable v. Formally, this will be the exponentially weighted average of the gradient on previous steps. </p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_velocity</span><span class="params">(parameters)</span>:</span> <span class="comment"># 只是初始化</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Initializes the velocity as a python dictionary with:</span></span><br><span class="line"><span class="string">                - keys: "dW1", "db1", ..., "dWL", "dbL" </span></span><br><span class="line"><span class="string">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters.</span></span><br><span class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    v -- python dictionary containing the current velocity.</span></span><br><span class="line"><span class="string">                    v['dW' + str(l)] = velocity of dWl</span></span><br><span class="line"><span class="string">                    v['db' + str(l)] = velocity of dbl</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // <span class="number">2</span></span><br><span class="line">    v = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)].shape[<span class="number">0</span>], parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)].shape[<span class="number">1</span>]))</span><br><span class="line">        v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">"b"</span>+str(l+<span class="number">1</span>)].shape[<span class="number">0</span>], parameters[<span class="string">'b'</span>+str(l+<span class="number">1</span>)].shape[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">return</span> v</span><br></pre></td></tr></table></figure>
<p>接下来，实现具体的Momentum算法：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-550ee71c4d175b2c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_with_momentum</span><span class="params">(parameters, grads, v, beta, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using Momentum</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients for each parameters:</span></span><br><span class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></span><br><span class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></span><br><span class="line"><span class="string">    v -- python dictionary containing the current velocity:</span></span><br><span class="line"><span class="string">                    v['dW' + str(l)] = ...</span></span><br><span class="line"><span class="string">                    v['db' + str(l)] = ...</span></span><br><span class="line"><span class="string">    beta -- the momentum hyperparameter, scalar</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    v -- python dictionary containing your updated velocities</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    L = len(parameters) // <span class="number">2</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = beta * v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] + (<span class="number">1</span>-beta) * grads[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = beta * v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] + (<span class="number">1</span>-beta) * grads[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l+<span class="number">1</span>)] =  parameters[<span class="string">'W'</span> + str(l+<span class="number">1</span>)] - learning_rate * v[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">'b'</span> + str(l+<span class="number">1</span>)] - learning_rate * v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">    <span class="keyword">return</span> parameters, v</span><br></pre></td></tr></table></figure></p>
<p><strong>Note</strong>:</p>
<ul>
<li>The velocity is initialized with zeros. So the algorithm will take a few iterations to “build up” velocity and start to take bigger steps.</li>
<li>If β = 0, then this just becomes standard gradient descent without momentum.</li>
</ul>
<p><strong>How do we choose β</strong>?</p>
<ul>
<li>The larger the momentum β is, the smoother the update because the more we take the past gradients into account. But if β is too big, it could also smooth out the updates too much.</li>
<li>Common values for β range from 0.8 to 0.999. If you don’t feel inclined to tune this,  β=0.9 is often a reasonable default.</li>
<li>Tuning the optimal β for your model might need trying several values to see what works best in term of reducing the value of the cost function J.</li>
</ul>
<p>**What we should remember”:</p>
<ul>
<li>Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent.</li>
<li>You have to tune a momentum hyperparameter β and a learning rate α.</li>
</ul>
<h2 id="4-Adam"><a href="#4-Adam" class="headerlink" title="4- Adam"></a>4- Adam</h2><p>Adam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp (described in lecture) and Momentum.</p>
<p><strong>How does Adam word?</strong></p>
<ol>
<li>It calculates an exponentially weighted average of past gradients, and stores it in variables v (before bias correction) and  v_corrected  (with bias correction).</li>
<li>It calculates an exponentially weighted average of the squares of the past gradients, and stores it in variables s (before bias correction) and s_corrected(with bias correction).</li>
<li>It updates parameters in a direction based on combining information from “1” and “2”.</li>
</ol>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-438631276f2ef3a1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_adam</span><span class="params">(parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Initializes v and s as two python dictionaries with:</span></span><br><span class="line"><span class="string">                - keys: "dW1", "db1", ..., "dWL", "dbL" </span></span><br><span class="line"><span class="string">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters.</span></span><br><span class="line"><span class="string">                    parameters["W" + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters["b" + str(l)] = bl</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    v -- python dictionary that will contain the exponentially weighted average of the gradient.</span></span><br><span class="line"><span class="string">                    v["dW" + str(l)] = ...</span></span><br><span class="line"><span class="string">                    v["db" + str(l)] = ...</span></span><br><span class="line"><span class="string">    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.</span></span><br><span class="line"><span class="string">                    s["dW" + str(l)] = ...</span></span><br><span class="line"><span class="string">                    s["db" + str(l)] = ...</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural networks</span></span><br><span class="line">    v = &#123;&#125;</span><br><span class="line">    s = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        v[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)].shape[<span class="number">0</span>],parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)].shape[<span class="number">1</span>]))</span><br><span class="line">        v[<span class="string">"db"</span>+str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">"b"</span>+str(l+<span class="number">1</span>)].shape[<span class="number">0</span>],parameters[<span class="string">"b"</span>+str(l+<span class="number">1</span>)].shape[<span class="number">1</span>]))</span><br><span class="line">        s[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)].shape[<span class="number">0</span>],parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)].shape[<span class="number">1</span>]))</span><br><span class="line">        s[<span class="string">"db"</span>+str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">"b"</span>+str(l+<span class="number">1</span>)].shape[<span class="number">0</span>],parameters[<span class="string">"b"</span>+str(l+<span class="number">1</span>)].shape[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">return</span> v, s</span><br></pre></td></tr></table></figure>
<p>接下来，更新参数：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_with_adam</span><span class="params">(parameters, grads, v, s, t, learning_rate=<span class="number">0.01</span>, beta1=<span class="number">0.9</span>, beta2=<span class="number">0.999</span>, epsilon=<span class="number">1e-8</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using Adam</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients for each parameters:</span></span><br><span class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></span><br><span class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></span><br><span class="line"><span class="string">    v -- Adam variable, moving average of the first gradient, python dictionary</span></span><br><span class="line"><span class="string">    s -- Adam variable, moving average of the squared gradient, python dictionary</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar.</span></span><br><span class="line"><span class="string">    beta1 -- Exponential decay hyperparameter for the first moment estimates </span></span><br><span class="line"><span class="string">    beta2 -- Exponential decay hyperparameter for the second moment estimates </span></span><br><span class="line"><span class="string">    epsilon -- hyperparameter preventing division by zero in Adam updates</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    v -- Adam variable, moving average of the first gradient, python dictionary</span></span><br><span class="line"><span class="string">    s -- Adam variable, moving average of the squared gradient, python dictionary</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(parameters) // <span class="number">2</span></span><br><span class="line">    v_corrected = &#123;&#125;</span><br><span class="line">    s_corrected = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        v[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)] = beta1 * v[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)] + (<span class="number">1</span>-beta1) * grads[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        v[<span class="string">"db"</span>+str(l+<span class="number">1</span>)] = beta1 * v[<span class="string">"db"</span>+str(l+<span class="number">1</span>)] + (<span class="number">1</span>-beta1) * grads[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># bias-correction</span></span><br><span class="line">        v_corrected[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] / (<span class="number">1</span>-pow(beta1, t))</span><br><span class="line">        v_corrected[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] / (<span class="number">1</span>-pow(beta1, t))</span><br><span class="line"></span><br><span class="line">        s[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)] = beta2 * s[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)] + (<span class="number">1</span>-beta2) * pow(grads[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)],<span class="number">2</span>)</span><br><span class="line">        s[<span class="string">"db"</span>+str(l+<span class="number">1</span>)] = beta2 * s[<span class="string">"db"</span>+str(l+<span class="number">1</span>)] + (<span class="number">1</span>-beta2) * pow(grads[<span class="string">"db"</span> + str(l+<span class="number">1</span>)],<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># bias-correction</span></span><br><span class="line">        s_corrected[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = s[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] / (<span class="number">1</span>-pow(beta2, t))</span><br><span class="line">        s_corrected[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = s[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] / (<span class="number">1</span>-pow(beta2, t))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># update W and b</span></span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] - learning_rate * v_corrected[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)] / np.sqrt(s_corrected[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)] + epsilon)</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] - learning_rate * v_corrected[<span class="string">"db"</span>+str(l+<span class="number">1</span>)] / np.sqrt(s_corrected[<span class="string">"db"</span>+str(l+<span class="number">1</span>)] + epsilon)</span><br><span class="line">    <span class="keyword">return</span> parameters, v, s</span><br></pre></td></tr></table></figure></p>
<h2 id="5-Model-with-different-optimization-algorithm"><a href="#5-Model-with-different-optimization-algorithm" class="headerlink" title="5- Model with different optimization algorithm"></a>5- Model with different optimization algorithm</h2><p>Lets use the following “moons” dataset to test the different optimization methods. (The dataset is named “moons” because the data from each of the two classes looks a bit like a crescent-shaped moon.)<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_X, train_Y = load_dataset()</span><br></pre></td></tr></table></figure></p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-5118dd90e09c52ea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, Y, layers_dims, optimizer, learning_rate=<span class="number">0.007</span>, mini_batch_size=<span class="number">64</span>, beta=<span class="number">0.9</span>, beta1=<span class="number">0.9</span>, beta2=<span class="number">0.999</span>, epsilon=<span class="number">1e-8</span>, num_epochs = <span class="number">10000</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    3-layer neural network model which can be run in different optimizer modes.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    layers_dims -- python list, containing the size of each layer</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar.</span></span><br><span class="line"><span class="string">    mini_batch_size -- the size of a mini batch</span></span><br><span class="line"><span class="string">    beta -- Momentum hyperparameter</span></span><br><span class="line"><span class="string">    beta1 -- Exponential decay hyperparameter for the past gradients estimates </span></span><br><span class="line"><span class="string">    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates </span></span><br><span class="line"><span class="string">    epsilon -- hyperparameter preventing division by zero in Adam updates</span></span><br><span class="line"><span class="string">    num_epochs -- number of epochs</span></span><br><span class="line"><span class="string">    print_cost -- True to print the cost every 1000 epochs</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(layers_dims) <span class="comment"># number of layers in the neural networks</span></span><br><span class="line">    costs = []</span><br><span class="line">    t = <span class="number">0</span></span><br><span class="line">    seed = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize parameters</span></span><br><span class="line">    parameters = initialize_parameters(layers_dims)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize the optimizer</span></span><br><span class="line">    <span class="keyword">if</span> optimizer == <span class="string">"gd"</span>:</span><br><span class="line">        <span class="keyword">pass</span> <span class="comment"># no initialization required for gradient descent</span></span><br><span class="line">    <span class="keyword">elif</span> optimizer == <span class="string">"momentum"</span>:</span><br><span class="line">        v = initialize_velocity(parameters)</span><br><span class="line">    <span class="keyword">elif</span> optimizer == <span class="string">"adam"</span>:</span><br><span class="line">        v, s = initialize_adam(parameters)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Optimization loop</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="comment"># Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch</span></span><br><span class="line">        seed = seed + <span class="number">1</span></span><br><span class="line">        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> minibatch <span class="keyword">in</span> minibatches:</span><br><span class="line">            <span class="comment"># Select a mini_batch</span></span><br><span class="line">            (minibatch_X, minibatch_Y) = minibatch</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Forward propagation</span></span><br><span class="line">            a3, caches = forward_propagation(mini_batch_X, parameters)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute cost</span></span><br><span class="line">            cost = compute_cost(a3, mini_batch_Y)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Backward propagation</span></span><br><span class="line">            grads = backward_propagation(minibatch_X, mini_batch_Y, caches)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Update parameters</span></span><br><span class="line">            <span class="keyword">if</span> optimizer == <span class="string">"gd"</span>：</span><br><span class="line">                parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">            <span class="keyword">elif</span> optimizer == <span class="string">"momentum"</span>:</span><br><span class="line">                parameters = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)</span><br><span class="line">            <span class="keyword">elif</span> optimizer == <span class="string">"adam"</span>:</span><br><span class="line">                t = t + <span class="number">1</span> <span class="comment"># Adam counter</span></span><br><span class="line">                parameters = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2, epsilon)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print the cost every 1000 epoch</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after epoch %i: %f"</span> %(i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'epochs (per 100)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate = "</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure></p>
<h3 id="5-1-Mini-batch-Gradient-descent"><a href="#5-1-Mini-batch-Gradient-descent" class="headerlink" title="5.1- Mini-batch Gradient descent"></a>5.1- Mini-batch Gradient descent</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train 3-layer model</span></span><br><span class="line">layers_dims = [train_X.shape[<span class="number">0</span>], <span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">parameters = model(train_X, train_Y, layers_dims, optimizer = <span class="string">"gd"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = predict(train_X, train_Y, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot decision boundary</span></span><br><span class="line">plt.title(<span class="string">"Model with Gradient Descent optimization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">2.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure>
<p>结果：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-8444151dd32c3ed0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-60376c25400e0caf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h3 id="5-2-Mini-batch-gradient-descent-with-momentum"><a href="#5-2-Mini-batch-gradient-descent-with-momentum" class="headerlink" title="5.2- Mini-batch gradient descent with momentum"></a>5.2- Mini-batch gradient descent with momentum</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train 3-layer model</span></span><br><span class="line">layers_dims = [train_X.shape[<span class="number">0</span>], <span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">parameters = model(train_X, train_Y, layers_dims, beta = <span class="number">0.9</span>, optimizer = <span class="string">"momentum"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = predict(train_X, train_Y, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot decision boundary</span></span><br><span class="line">plt.title(<span class="string">"Model with Momentum optimization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">2.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure>
<p>结果：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-6be70187abe0d1f2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-659ccbba10be0431.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>Because this example is relatively simple, the gains from using momemtum are small; but for more complex problems you might see bigger gains.</p>
<h3 id="5-3-Mini-batch-with-Adam-model"><a href="#5-3-Mini-batch-with-Adam-model" class="headerlink" title="5.3- Mini-batch with Adam model"></a>5.3- Mini-batch with Adam model</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train 3-layer model</span></span><br><span class="line">layers_dims = [train_X.shape[<span class="number">0</span>], <span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">parameters = model(train_X, train_Y, layers_dims, optimizer = <span class="string">"adam"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = predict(train_X, train_Y, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot decision boundary</span></span><br><span class="line">plt.title(<span class="string">"Model with Adam optimization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">2.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure>
<p>结果：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-af530967c5a62d74.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-d3d841aee105446b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h3 id="5-4-Summary"><a href="#5-4-Summary" class="headerlink" title="5.4- Summary"></a>5.4- Summary</h3><p><img src="https://upload-images.jianshu.io/upload_images/8636110-2292db4d01a01927.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>Momentum usually helps, but given the small learning rate and the simplistic dataset, its impact is almost negligeable.Also, the huge oscillations you see in the cost come from the fact that some minibatches are more difficult than others for the optimization algorithm.</p>
<p>Adam on the other hand, clearly outperforms mini-batch gradient descent and Momentum. If you run the model for more epochs on this simple dataset, all three methods will lead to very good results. However, you’ve seen that Adam converges a lot faster.</p>
<p>Some advantages of Adam include:</p>
<ul>
<li>Relatively low memory requirements (though higher than gradient descent and gradient descent with momentum)</li>
<li>Usually works well even with little tuning of hyperparameters(except α)</li>
</ul>
<p>Adam论文链接：<a href="https://arxiv.org/pdf/1412.6980.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1412.6980.pdf</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/python/" rel="tag"><i class="fa fa-tag"></i> python</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/15/NLP学习1/" rel="next" title="Word Embedding教程">
                <i class="fa fa-chevron-left"></i> Word Embedding教程
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/04/17/deep-learningw7/" rel="prev" title="第7周-超参数调试、Batch正则化和程序框架">
                第7周-超参数调试、Batch正则化和程序框架 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="DesmonDay" />
          <p class="site-author-name" itemprop="name">DesmonDay</p>
           
              <p class="site-description motion-element" itemprop="description">主攻方向：NLP</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">112</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">14</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">11</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/DesmonDay" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Mini-batch梯度下降"><span class="nav-number">1.</span> <span class="nav-text">Mini-batch梯度下降</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#理解Mini-batch梯度下降"><span class="nav-number">2.</span> <span class="nav-text">理解Mini-batch梯度下降</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#指数加权平均"><span class="nav-number">3.</span> <span class="nav-text">指数加权平均</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#理解指数加权平均"><span class="nav-number">4.</span> <span class="nav-text">理解指数加权平均</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#指数加权平均的偏差修正-Bias-correction-in-exponentially-weighted-average"><span class="nav-number">5.</span> <span class="nav-text">指数加权平均的偏差修正-Bias correction in exponentially weighted average</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#动量梯度下降-Gradient-descent-with-momentum"><span class="nav-number">6.</span> <span class="nav-text">动量梯度下降-Gradient descent with momentum</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#RMSprop-Root-Mean-Square-Prop"><span class="nav-number">7.</span> <span class="nav-text">RMSprop - Root Mean Square Prop</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Adam优化算法"><span class="nav-number">8.</span> <span class="nav-text">Adam优化算法</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#学习率衰减"><span class="nav-number">9.</span> <span class="nav-text">学习率衰减</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#局部最优的问题"><span class="nav-number">10.</span> <span class="nav-text">局部最优的问题</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#本周作业"><span class="nav-number">11.</span> <span class="nav-text">本周作业</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Gradient-Descent"><span class="nav-number">11.1.</span> <span class="nav-text">1- Gradient Descent</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Mini-Batch-Gradient-Descent"><span class="nav-number">11.2.</span> <span class="nav-text">2- Mini-Batch Gradient Descent</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Momentum"><span class="nav-number">11.3.</span> <span class="nav-text">3- Momentum</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-Adam"><span class="nav-number">11.4.</span> <span class="nav-text">4- Adam</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-Model-with-different-optimization-algorithm"><span class="nav-number">11.5.</span> <span class="nav-text">5- Model with different optimization algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-Mini-batch-Gradient-descent"><span class="nav-number">11.5.1.</span> <span class="nav-text">5.1- Mini-batch Gradient descent</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-Mini-batch-gradient-descent-with-momentum"><span class="nav-number">11.5.2.</span> <span class="nav-text">5.2- Mini-batch gradient descent with momentum</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-Mini-batch-with-Adam-model"><span class="nav-number">11.5.3.</span> <span class="nav-text">5.3- Mini-batch with Adam model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-Summary"><span class="nav-number">11.5.4.</span> <span class="nav-text">5.4- Summary</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">DesmonDay</span>
</div>



<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv"></span>
</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->



  


  




	





  





  






  





  

  

  

  

  

  

</body>
</html>


