<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="python," />





  <link rel="alternate" href="/atom.xml" title="DesmonDay's Blog" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="下面进入神经网络基础的学习，这部分大多已经学过了，这里就当作是一次复习。 二元分类-Binary Classification图片用红绿蓝三个通道，通过这些输入预测是否是猫。 将这张猫的图片表示成输入的特征向量： 对应的记号： 需要注意的是X是每个样本列向量的堆叠。 Logistic回归-Logistic Regressionlogistic回归用于解决二元分类问题。这里要注意，P(y=1|x)指">
<meta name="keywords" content="python">
<meta property="og:type" content="article">
<meta property="og:title" content="第2周-神经网络基础">
<meta property="og:url" content="https://github.com/DesmonDay/2019/04/01/deep-learningw2/index.html">
<meta property="og:site_name" content="DesmonDay&#39;s Blog">
<meta property="og:description" content="下面进入神经网络基础的学习，这部分大多已经学过了，这里就当作是一次复习。 二元分类-Binary Classification图片用红绿蓝三个通道，通过这些输入预测是否是猫。 将这张猫的图片表示成输入的特征向量： 对应的记号： 需要注意的是X是每个样本列向量的堆叠。 Logistic回归-Logistic Regressionlogistic回归用于解决二元分类问题。这里要注意，P(y=1|x)指">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-58570aa3a16bf715.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-d344493c07cb5f2d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-075f444ce29f4d83.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-32b12b2fb0f2047d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-afc4b3675c063183.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-f81af276c1811d76.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-fc767b037a75dcde.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-f210c7aa4f359e44.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-68afab65388cfbed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-7825abe0a5eee6b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-b2ae3c7b8c474f00.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-7b061c83f3839af5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-d070f51fd777450f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-2bf7423664b0b8c6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-75dfcee80a8ec6bd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-075ab455f7acdfd3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-2eb987492d9501b3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-0794a79f9ba514d3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-11142b52a693d999.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-23875c59a63e8893.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-7569387a10a5a5db.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-d3a9af1f9958495a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-ffe6d306dac83a4f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-76c074e494865c1c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-066827da288afd80.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-7740e442d0adffd5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-e79ce24e4500f7ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-7ce97fae643e8ca0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-73f51176fb37e694.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-20e5400369b44374.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-7ac0d47a6a9ba9c9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-f065864226e11d7e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:updated_time" content="2019-04-12T07:24:16.486Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="第2周-神经网络基础">
<meta name="twitter:description" content="下面进入神经网络基础的学习，这部分大多已经学过了，这里就当作是一次复习。 二元分类-Binary Classification图片用红绿蓝三个通道，通过这些输入预测是否是猫。 将这张猫的图片表示成输入的特征向量： 对应的记号： 需要注意的是X是每个样本列向量的堆叠。 Logistic回归-Logistic Regressionlogistic回归用于解决二元分类问题。这里要注意，P(y=1|x)指">
<meta name="twitter:image" content="https://upload-images.jianshu.io/upload_images/8636110-58570aa3a16bf715.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://github.com/DesmonDay/2019/04/01/deep-learningw2/"/>





  <title>第2周-神经网络基础 | DesmonDay's Blog</title>
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">DesmonDay's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">一只小辣鸡的自我拯救之路</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/DesmonDay/2019/04/01/deep-learningw2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="DesmonDay">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DesmonDay's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">第2周-神经网络基础</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-01T17:26:58+08:00">
                2019-04-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>下面进入神经网络基础的学习，这部分大多已经学过了，这里就当作是一次复习。</p>
<h1 id="二元分类-Binary-Classification"><a href="#二元分类-Binary-Classification" class="headerlink" title="二元分类-Binary Classification"></a>二元分类-Binary Classification</h1><p>图片用红绿蓝三个通道，通过这些输入预测是否是猫。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-58570aa3a16bf715.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>将这张猫的图片表示成输入的特征向量：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d344493c07cb5f2d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>对应的记号：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-075f444ce29f4d83.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>需要注意的是X是每个样本列向量的堆叠。</p>
<h1 id="Logistic回归-Logistic-Regression"><a href="#Logistic回归-Logistic-Regression" class="headerlink" title="Logistic回归-Logistic Regression"></a>Logistic回归-Logistic Regression</h1><p>logistic回归用于解决二元分类问题。这里要注意，P(y=1|x)指训练样本为x时，y为1的概率。在下图写的Output那里，最初我们是直接用一个线性分类器，但是因为我们希望输出的是概率，而w^T+b可能大于1，可能小于0，因此要添加sigmoid函数将其值域压缩到(0,1)。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-32b12b2fb0f2047d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>另外，红色部分那里是另一种表示形式，将b作为其中一个参数theta_0。在实现神经网络时，最好用蓝色部分的符号表示。</p>
<p>更清晰的表示如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-afc4b3675c063183.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h1 id="logistic回归损失函数-Logistic-Regression-cost-function"><a href="#logistic回归损失函数-Logistic-Regression-cost-function" class="headerlink" title="logistic回归损失函数-Logistic Regression cost function"></a>logistic回归损失函数-Logistic Regression cost function</h1><p><img src="https://upload-images.jianshu.io/upload_images/8636110-f81af276c1811d76.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>具体公式推导可以见上图。我们可以发现，最开始我们写的损失函数L是误差平方公式，然而这个函数的优化形式是非凸函数，如果求解会有多个局部最优解。使用梯度下降可能就找不到全局最优解。因此需要进一步地对损失函数进行研究。</p>
<ol>
<li>损失函数(loss function)定义为 L(y’, y) = -(ylogy’+(1-y)log(1-y’))，并且当y=1时，希望y’尽量大；当y=0时，希望y’尽量小。损失函数仅适用于单个的训练样本。</li>
<li>代价函数(cost function)如上图中所写，是整个训练集的平均损失函数。我们的目标就是找到合适的参数w和b来最小化cost function的值。</li>
</ol>
<h1 id="梯度下降法-Gradient-Descent"><a href="#梯度下降法-Gradient-Descent" class="headerlink" title="梯度下降法-Gradient Descent"></a>梯度下降法-Gradient Descent</h1><p>梯度下降法，初始化参数时可以随机初始化，因为对应的成本函数是凸函数，因此总会到达全局最优解。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-fc767b037a75dcde.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>更新公式：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f210c7aa4f359e44.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>这里要注意导数的符号表示，当变量为两个或多于两个时，使用花体符号；如果只有一个变量，可以用d表示。当我们要在代码中实现导数时，可以直接写成dw,db。</p>
<h1 id="计算图-Computation-Graph"><a href="#计算图-Computation-Graph" class="headerlink" title="计算图-Computation Graph"></a>计算图-Computation Graph</h1><p>一个简单的计算图例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-68afab65388cfbed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>其中，计算图是指用蓝色箭头画出来的从左到右的计算。</p>
<h1 id="计算图的导数计算-Derivatives-with-a-Computation-Graph"><a href="#计算图的导数计算-Derivatives-with-a-Computation-Graph" class="headerlink" title="计算图的导数计算-Derivatives with a Computation Graph"></a>计算图的导数计算-Derivatives with a Computation Graph</h1><p>这一小节，吴老师用一个简单的例子介绍了求导的链式法则以及反向传播的概念，并且也告诉我们在编程中如果需要求关于某个变量的导数，可以直接写作da, dvar之类的。 我们先求出了dv，然后利用dv求出da, du，接着再利用所求可以继续求出da, db, dc，即反向传播。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7825abe0a5eee6b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-b2ae3c7b8c474f00.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>因此，一个计算图就是从左到右的计算成本函数J，再从反向计算导数。</p>
<h1 id="Logistic回归中的梯度下降-Logistic-Regression-Gradient-Descent"><a href="#Logistic回归中的梯度下降-Logistic-Regression-Gradient-Descent" class="headerlink" title="Logistic回归中的梯度下降-Logistic Regression Gradient Descent"></a>Logistic回归中的梯度下降-Logistic Regression Gradient Descent</h1><p>本节介绍的是单个样本的Logistic回归中的梯度下降过程。如图，从右往左计算导数，其中还用到了求导的链式法则。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7b061c83f3839af5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h1 id="M个样本上进行logistic回归的梯度下降过程"><a href="#M个样本上进行logistic回归的梯度下降过程" class="headerlink" title="M个样本上进行logistic回归的梯度下降过程"></a>M个样本上进行logistic回归的梯度下降过程</h1><p>首先，我们需要回忆logistic回归的cost function如下。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d070f51fd777450f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>假设我们要求dw1，对应的公式如下。可以发现，我们需要累加每个样本对应的dw，最后还需要求平均。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-2bf7423664b0b8c6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>因此最终可以得到如下的更新过程：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-75dfcee80a8ec6bd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>然而，这样的更新过程有两个缺点：一是需要遍历所有的样本(i=1,2,…,m)；二是需要遍历所有的特征(dw1,dw2)。也就是说，需要两个for循环，然而在代码中显式地使用for循环会使效果低下，因此我们的解决方法是：Vectorization，向量化。</p>
<h1 id="向量化-Vectorization"><a href="#向量化-Vectorization" class="headerlink" title="向量化-Vectorization"></a>向量化-Vectorization</h1><p>向量化通常是消除我们代码中显示for循环语句的艺术。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-075ab455f7acdfd3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>从图中可以看到，左边是非向量化写法，即用for循环来实现矩阵乘法；而右边则是向量化写法，使用python中的numpy库来实现。下面给出在juypter notebook中实现的两种写法，对比其时间差异：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-2eb987492d9501b3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>显然，向量化的计算快了许多。可扩展深度学习一般是在GPU上运行的，而jupyter notebook是基于CPU的。GPU擅长SIMD指令(Single Instruction Multiple Data，但之灵多数据流)，而CPU的表现也不差。因此，我们看到numpy的向量化可以加速代码运行。因此，我们可以得到一个经验法则：不要显式地使用for循环。</p>
<h2 id="More-exmaples"><a href="#More-exmaples" class="headerlink" title="More exmaples"></a>More exmaples</h2><p>Neural network programming guideline: Whenever possible, avoid explicit for-loops.</p>
<ol>
<li><p>一个矩阵和向量的乘法计算：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-0794a79f9ba514d3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
</li>
<li><p>计算向量中每个元素的指数运算：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-11142b52a693d999.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
</li>
</ol>
<p>因此上述例子也告诉我们：在实现代码时，先看看我们能否用numpy的内置函数，而不是使用for循环。</p>
<p>接着，我们对logistic回归中的一个有关特征的循环进行向量化：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-23875c59a63e8893.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h1 id="向量化Logistic回归"><a href="#向量化Logistic回归" class="headerlink" title="向量化Logistic回归"></a>向量化Logistic回归</h1><h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-7569387a10a5a5db.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>从图中可以看到，我们可以将Z和A用向量化的方法得到对应的矩阵，即一次性计算了所有样本的w.T+b和a，而不需要使用for循环。</p>
<h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>首先，吴老师使用向量化计算出参数b和w的梯度db,dw：注意左边是for循环做法，右边是向量化做法。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d3a9af1f9958495a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>接着是整个Logistic回归的向量化算法过程，将X作为矩阵参与计算：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ffe6d306dac83a4f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>可以发现，最后梯度下降的迭代仍然需要一个for循环来实现，这部分则是无法向量化的。</p>
<h1 id="Python中的广播-Broadcasting-in-Python"><a href="#Python中的广播-Broadcasting-in-Python" class="headerlink" title="Python中的广播-Broadcasting in Python"></a>Python中的广播-Broadcasting in Python</h1><p>首先，周老师给出了一个计算百分比的例子，用来说明广播的作用：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-76c074e494865c1c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>对应的python代码如下，可以发现，我们先使用了sum函数，这里axis=0时表示按列相加，而axis=1则表示按行相加；另外，这里用到广播的地方是percentage的计算。而老师说这里的reshape是可以去掉的，但是这个reshape其实也起到了一个保证我们的矩阵维数不会出错的作用。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-066827da288afd80.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>更多的广播例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7740e442d0adffd5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>广播的一些常用法则：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e79ce24e4500f7ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h1 id="A-note-on-python-numpy-vectors"><a href="#A-note-on-python-numpy-vectors" class="headerlink" title="A note on python/numpy vectors"></a>A note on python/numpy vectors</h1><h2 id="消除代码中秩为1的数组"><a href="#消除代码中秩为1的数组" class="headerlink" title="消除代码中秩为1的数组"></a>消除代码中秩为1的数组</h2><p>写向量时，不要这样写：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.random.randn(<span class="number">5</span>)</span><br><span class="line"><span class="comment"># a.shape = (5,) # 既不是行向量，也不是列向量</span></span><br></pre></td></tr></table></figure></p>
<p>而如果我们要把上述a转换成向量，可以用reshape函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = a.reshape((<span class="number">5</span>,<span class="number">1</span>))</span><br><span class="line">a = a.reshape((<span class="number">1</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure></p>
<p>应该这样写：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = np.random.randn(<span class="number">5</span>,<span class="number">1</span>) <span class="comment"># a.shape=(5,1) column vector</span></span><br><span class="line">a = np.random.randn(<span class="number">1</span>,<span class="number">5</span>) <span class="comment"># a.shape=(1,5) row vector</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(a.shape == (<span class="number">5</span>,<span class="number">1</span>)) <span class="comment"># 确保这是一个向量，而且执行速度很快</span></span><br></pre></td></tr></table></figure></p>
<h2 id="不要害羞，使用reshape或assert来保证维度不出错"><a href="#不要害羞，使用reshape或assert来保证维度不出错" class="headerlink" title="不要害羞，使用reshape或assert来保证维度不出错"></a>不要害羞，使用reshape或assert来保证维度不出错</h2><h1 id="Logistic回归中成本函数的证明"><a href="#Logistic回归中成本函数的证明" class="headerlink" title="Logistic回归中成本函数的证明"></a>Logistic回归中成本函数的证明</h1><p>Lost function函数的来由，我们最小化L(a,y)，实际上就是最大化logP(y|x)<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7ce97fae643e8ca0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>而对于整个训练集的成本函数：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-73f51176fb37e694.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>在假设样本集为独立同分布的前提下，通过最大似然估计，可以知道我们在最小化cost function的同时，也在最大化似然估计。</p>
<h1 id="本周作业"><a href="#本周作业" class="headerlink" title="本周作业"></a>本周作业</h1><p>参考别人记录的作业内容，这里就顺便抄题目了！</p>
<h2 id="Part-1-Python-Basics-with-Numpy-optional-assignment"><a href="#Part-1-Python-Basics-with-Numpy-optional-assignment" class="headerlink" title="Part 1: Python Basics with Numpy (optional assignment)"></a>Part 1: Python Basics with Numpy (optional assignment)</h2><p>What we need to Remember:</p>
<ul>
<li>np.exp(x) works for any np.array x and applies the exponential function to every coordinate</li>
<li><p>the sigmoid function and its gradient</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: sigmoid</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># this means you can access numpy functions by writing np.function() instead of numpy.function()</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the sigmoid of x</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- A scalar or numpy array of any size</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    s -- sigmoid(x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    s = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_derivative</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.</span></span><br><span class="line"><span class="string">    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- A scalar or numpy array</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    ds -- Your computed gradient.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    s = sigmoid(x)</span><br><span class="line">    ds = s*(<span class="number">1</span>-s)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> ds</span><br></pre></td></tr></table></figure>
</li>
<li><p>image2vector is commonly used in deep learning</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: image2vector</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">image2vector</span><span class="params">(image)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    image -- a numpy array of shape (length, height, depth)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    v -- a vector of shape (length*height*depth, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    v = image.reshape(image.shape[<span class="number">0</span>]*image.shape[<span class="number">1</span>]*image.shape[<span class="number">2</span>], <span class="number">1</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> v</span><br></pre></td></tr></table></figure>
</li>
<li><p>np.reshape is widely used. In the future, you’ll see that keeping your matrix/vector dimensions straight will go toward eliminating a lot of bugs.</p>
</li>
<li>numpy has efficient built-in functions</li>
<li>broadcasting is extremely useful</li>
<li>Note that np.dot() performs a matrix-matrix or matrix-vector multiplication. This is different from np.multiply() and the * operator (which is equivalent to .* in Matlab/Octave), which performs an element-wise multiplication.</li>
<li>np.dot(x,x) = 所有对应位置元素相乘之和。</li>
<li>Vectorization is very important in deep learning. It provides computational efficiency and clarity.</li>
<li><p>You have reviewed the L1 and L2 loss.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L1</span><span class="params">(yhat, y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    yhat -- vector of size m (predicted labels)</span></span><br><span class="line"><span class="string">    y -- vector of size m (true labels)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    loss -- the value of the L1 loss function defined above</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    loss = np.sum(abs(y - yhat))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L2</span><span class="params">(yhat, y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    yhat -- vector of size m (predicted labels)</span></span><br><span class="line"><span class="string">    y -- vector of size m (true labels)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    loss -- the value of the L2 loss function defined above</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    loss = np.dot(y-yhat, y-yhat)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>
</li>
<li><p>You are familiar with many numpy functions such as np.sum, np.dot, np.multiply, np.maximum, etc…</p>
</li>
</ul>
<h3 id="np-dot-np-outer-np-multiply"><a href="#np-dot-np-outer-np-multiply" class="headerlink" title="np.dot(),np.outer(),np.multiply(),*"></a>np.dot(),np.outer(),np.multiply(),*</h3><ol>
<li>np.dot()如果碰到的是秩为1的数组，那么执行的是对应位置的元素相乘再相加；如果遇到的是秩不为1的数组，那么执行的是矩阵相乘。需要注意的是矩阵与矩阵相乘是秩为2，矩阵和向量相乘秩为1。</li>
<li>np.multiply()表示的是数据和矩阵相应位置相乘，输出和输出的结果shape一致。</li>
<li>np.outer()表示的是两个向量相乘，拿第一个向量的元素<strong>分别</strong>与第二个向量所有元素相乘得到结果的一行。</li>
<li>*对数组执行的是对应位置相乘（成本函数里的就是这么计算！！！而不是用np.dot），对矩阵执行的是矩阵相乘。</li>
</ol>
<h2 id="Part-2-Logistic-Regression-with-a-Neural-Network-mindset"><a href="#Part-2-Logistic-Regression-with-a-Neural-Network-mindset" class="headerlink" title="Part 2: Logistic Regression with a Neural Network mindset"></a>Part 2: Logistic Regression with a Neural Network mindset</h2><p>这部分作业是完成一个Logistic回归算法，来分辨图片是否为猫。</p>
<h3 id="数据集预处理"><a href="#数据集预处理" class="headerlink" title="数据集预处理"></a>数据集预处理</h3><p>很多时候，我们经常会遇到的bug是有关于矩阵/向量维数的，因此我们必须保证清楚了解自己设置的矩阵维数是否正确。因此在写代码的过程中，时不时使用X.shape查看矩阵/向量的维数。</p>
<p>有关X.reshape()的一个小技巧：A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b ∗ c ∗ d, a) is to use: 这样子之后，矩阵的每一列都是一个样本。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_flatten = X.reshape(X.shape[<span class="number">0</span>], <span class="number">-1</span>).T      <span class="comment"># X.T is the transpose of X</span></span><br></pre></td></tr></table></figure></p>
<p><strong>What we need to remember:</strong><br>Common steps for pre-processing a new dataset are:</p>
<ul>
<li>Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, …)</li>
<li>Reshape the datasets such that each example is now a vector of size(num_px*num_px*3,1)</li>
<li>“Standardize” the data 标准化</li>
</ul>
<h3 id="Gereral-Architecture-of-the-learning-algorithm"><a href="#Gereral-Architecture-of-the-learning-algorithm" class="headerlink" title="Gereral Architecture of the learning algorithm"></a>Gereral Architecture of the learning algorithm</h3><p><img src="https://upload-images.jianshu.io/upload_images/8636110-20e5400369b44374.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-7ac0d47a6a9ba9c9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>关键步骤：</p>
<ul>
<li>Initialize the parameters of the model</li>
<li>Learn the parameters for the model by minimizing the cost  </li>
<li>Use the learned parameters to make predictions (on the test set)</li>
<li>Analyse the results and conclude</li>
</ul>
<h3 id="算法的各个模块"><a href="#算法的各个模块" class="headerlink" title="算法的各个模块"></a>算法的各个模块</h3><p>主要步骤：</p>
<ol>
<li>Define the model structure (such as number of input features)</li>
<li>Initialize the model’s parameters</li>
<li>Loop:<ul>
<li>Calculate current loss (forward propagation)</li>
<li>Calculate current gradient (backward propagation)</li>
<li>Update parameters (gradient descent)</li>
</ul>
</li>
</ol>
<h4 id="Helper-functions-Sigmoid"><a href="#Helper-functions-Sigmoid" class="headerlink" title="Helper functions-Sigmoid"></a>Helper functions-Sigmoid</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the sigmoid of z</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    z -- A scalar or numpy array of any size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    s -- sigmoid(z)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    s = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure>
<h4 id="初始化参数-Initializing-parameters"><a href="#初始化参数-Initializing-parameters" class="headerlink" title="初始化参数-Initializing parameters"></a>初始化参数-Initializing parameters</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_with_zeros</span><span class="params">(dim)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    dim -- size of the w vector we want (or number of parameters in this case)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    w -- initialized vector of shape (dim, 1)</span></span><br><span class="line"><span class="string">    b -- initialized scalar (corresponds to the bias)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    w = np.zeros((dim, <span class="number">1</span>))</span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(w.shape == (dim, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(isinstance(b, float) <span class="keyword">or</span> isinstance(b, int))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> w, b</span><br></pre></td></tr></table></figure>
<h4 id="Forward-and-Backward-propagation"><a href="#Forward-and-Backward-propagation" class="headerlink" title="Forward and Backward propagation"></a>Forward and Backward propagation</h4><p>要用到的公式：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f065864226e11d7e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>这里有个地方我以前经常搞混，就是不知道什么时候用np.dot，np.multiply和*，现在清楚了：普通的矩阵乘法就是用np.dot，而对应位置相乘，如上图里面cost function的计算，则要用到np.multiply或者*。弄清楚了就不会错了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">propagate</span><span class="params">(w, b, X, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the cost function and its gradient for the propagation explained above</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of size (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    cost -- negative log-likelihood cost for logistic regression</span></span><br><span class="line"><span class="string">    dw -- gradient of the loss with respect to w, thus same shape as w</span></span><br><span class="line"><span class="string">    db -- gradient of the loss with respect to b, thus same shape as b</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Tips:</span></span><br><span class="line"><span class="string">    - Write your code step by step for the propagation. np.log(), np.dot()</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>] <span class="comment"># 样本数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># FORWARD PROPAGATION (FROM X TO COST)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    A = sigmoid(np.dot(w.T, X) + b)</span><br><span class="line">    cost = -(<span class="number">1.0</span>/m) * np.sum(Y*np.log(A) + (<span class="number">1</span>-Y)*np.log(<span class="number">1</span>-A))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># BACKWARD PROPAGATION (TO FIND GRAD)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    dw = np.dot(X, (A-Y).T) / m</span><br><span class="line">    db = np.sum(A-Y) / m</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">assert</span>(dw.shape == w.shape)</span><br><span class="line">    <span class="keyword">assert</span>(db.dtype == float)</span><br><span class="line">    cost = np.squeeze(cost)</span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,</span><br><span class="line">             <span class="string">"db"</span>: db&#125;</span><br><span class="line">    <span class="keyword">return</span> grads, cost</span><br></pre></td></tr></table></figure>
<h4 id="Optimization-更新"><a href="#Optimization-更新" class="headerlink" title="Optimization-更新"></a>Optimization-更新</h4><p>利用更新公式: theta = theta - alpha * dtheta<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(w, b, X, Y, num_iterations, learning_rate, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    This function optimizes w and b by running a gradient descent algorithm</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of shape (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">    print_cost -- True to print the loss every 100 steps</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    params -- dictionary containing the weights w and bias b</span></span><br><span class="line"><span class="string">    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function</span></span><br><span class="line"><span class="string">    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Tips:</span></span><br><span class="line"><span class="string">    You basically need to write down two steps and iterate through them:</span></span><br><span class="line"><span class="string">        1) Calculate the cost and the gradient for the current parameters. Use propagate().</span></span><br><span class="line"><span class="string">        2) Update the parameters using gradient descent rule for w and b.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    costs = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Cost and gradient calculation (≈ 1-4 lines of code)</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### </span></span><br><span class="line">        grads, cost = propagate(w, b, X, Y)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Retrieve derivatives from grads</span></span><br><span class="line">        dw = grads[<span class="string">"dw"</span>]</span><br><span class="line">        db = grads[<span class="string">"db"</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># update rule (≈ 2 lines of code)</span></span><br><span class="line">        <span class="comment">### START CODE HERE ###</span></span><br><span class="line">        w = w - learning_rate*dw</span><br><span class="line">        b = b - learning_rate*db</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Record the costs</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the cost every 100 training examples</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">    </span><br><span class="line">    params = &#123;<span class="string">"w"</span>: w,</span><br><span class="line">              <span class="string">"b"</span>: b&#125;</span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,</span><br><span class="line">             <span class="string">"db"</span>: db&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> params, grads, costs</span><br></pre></td></tr></table></figure></p>
<h4 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(w, b, X)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of size (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    Y_prediction = np.zeros((<span class="number">1</span>,m))</span><br><span class="line">    w = w.reshape(X.shape[<span class="number">0</span>], <span class="number">1</span>) <span class="comment"># 保证维数正确</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute vector "A" predicting the probabilities of a cat being present in the picture</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    A = sigmoid(np.dot(w.T,X) + b)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(A.shape[<span class="number">1</span>]):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert probabilities A[0,i] to actual predictions p[0,i]</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">        <span class="keyword">if</span> A[<span class="number">0</span>][i] &gt; <span class="number">0.5</span>: </span><br><span class="line">            Y_prediction[<span class="number">0</span>][i] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>][i] = <span class="number">0</span></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(Y_prediction.shape == (<span class="number">1</span>, m))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Y_prediction</span><br></pre></td></tr></table></figure>
<h3 id="合并模块-Merge-all-functions-into-a-model"><a href="#合并模块-Merge-all-functions-into-a-model" class="headerlink" title="合并模块-Merge all functions into a model"></a>合并模块-Merge all functions into a model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_test, Y_test, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.5</span>, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Builds the logistic regression model by calling the function you've implemented previously</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)</span></span><br><span class="line"><span class="string">    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)</span></span><br><span class="line"><span class="string">    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)</span></span><br><span class="line"><span class="string">    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)</span></span><br><span class="line"><span class="string">    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters</span></span><br><span class="line"><span class="string">    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()</span></span><br><span class="line"><span class="string">    print_cost -- Set to true to print the cost every 100 iterations</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    d -- dictionary containing information about the model.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize parameters with zeros (≈ 1 line of code)</span></span><br><span class="line">    w, b = initialize_with_zeros(X_train.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Gradient descent (≈ 1 line of code)</span></span><br><span class="line">    params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve parameters w and b from dictionary "parameters"</span></span><br><span class="line">    w = params[<span class="string">"w"</span>]</span><br><span class="line">    b = params[<span class="string">"b"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Predict test/train set examples (≈ 2 lines of code)</span></span><br><span class="line">    Y_prediction_train = predict(w, b, X_train)</span><br><span class="line">    Y_prediction_test = predict(w, b, X_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print train/test Errors</span></span><br><span class="line">    print(<span class="string">"train accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_train - Y_train)) * <span class="number">100</span>))</span><br><span class="line">    print(<span class="string">"test accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_test - Y_test)) * <span class="number">100</span>))</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    d = &#123;<span class="string">"costs"</span>: costs,</span><br><span class="line">         <span class="string">"Y_prediction_test"</span>: Y_prediction_test, </span><br><span class="line">         <span class="string">"Y_prediction_train"</span> : Y_prediction_train, </span><br><span class="line">         <span class="string">"w"</span> : w, </span><br><span class="line">         <span class="string">"b"</span> : b,</span><br><span class="line">         <span class="string">"learning_rate"</span> : learning_rate,</span><br><span class="line">         <span class="string">"num_iterations"</span>: num_iterations&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> d</span><br><span class="line"></span><br><span class="line">d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.005</span>, print_cost = <span class="keyword">True</span>) <span class="comment">#训练</span></span><br></pre></td></tr></table></figure>
<h3 id="有关学习率的进一步学习"><a href="#有关学习率的进一步学习" class="headerlink" title="有关学习率的进一步学习"></a>有关学习率的进一步学习</h3><p>If the learning rate is too large we may “overshoot” the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That’s why it is crucial to use a well-tuned(精心调整) learning rate.</p>
<p>In deep learning, 我们建议：</p>
<ol>
<li>Choose the learning rate that better minimizes the cost function. 选择合适的学习率。</li>
<li>If your model overfits, use other techniques to reduce overfitting. (We’ll talk about this in later videos.) 减少过拟合。</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过这次任务，我们学习到：</p>
<ol>
<li>Preprocessing the dataset is important.</li>
<li>You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model().</li>
<li>Tuning the learning rate (which is an example of a “hyperparameter”) can make a big difference to the algorithm. You will see more examples of this later in this course!</li>
</ol>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/python/" rel="tag"><i class="fa fa-tag"></i> python</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/01/deep-learningw1/" rel="next" title="第1周-深度学习概论">
                <i class="fa fa-chevron-left"></i> 第1周-深度学习概论
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/04/04/Seq-embedding/" rel="prev" title="Seq-embedding">
                Seq-embedding <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="DesmonDay" />
          <p class="site-author-name" itemprop="name">DesmonDay</p>
           
              <p class="site-description motion-element" itemprop="description">主攻方向：NLP</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">97</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">14</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">10</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/DesmonDay" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#二元分类-Binary-Classification"><span class="nav-number">1.</span> <span class="nav-text">二元分类-Binary Classification</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Logistic回归-Logistic-Regression"><span class="nav-number">2.</span> <span class="nav-text">Logistic回归-Logistic Regression</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#logistic回归损失函数-Logistic-Regression-cost-function"><span class="nav-number">3.</span> <span class="nav-text">logistic回归损失函数-Logistic Regression cost function</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#梯度下降法-Gradient-Descent"><span class="nav-number">4.</span> <span class="nav-text">梯度下降法-Gradient Descent</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#计算图-Computation-Graph"><span class="nav-number">5.</span> <span class="nav-text">计算图-Computation Graph</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#计算图的导数计算-Derivatives-with-a-Computation-Graph"><span class="nav-number">6.</span> <span class="nav-text">计算图的导数计算-Derivatives with a Computation Graph</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Logistic回归中的梯度下降-Logistic-Regression-Gradient-Descent"><span class="nav-number">7.</span> <span class="nav-text">Logistic回归中的梯度下降-Logistic Regression Gradient Descent</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#M个样本上进行logistic回归的梯度下降过程"><span class="nav-number">8.</span> <span class="nav-text">M个样本上进行logistic回归的梯度下降过程</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#向量化-Vectorization"><span class="nav-number">9.</span> <span class="nav-text">向量化-Vectorization</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#More-exmaples"><span class="nav-number">9.1.</span> <span class="nav-text">More exmaples</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#向量化Logistic回归"><span class="nav-number">10.</span> <span class="nav-text">向量化Logistic回归</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#前向传播"><span class="nav-number">10.1.</span> <span class="nav-text">前向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度下降"><span class="nav-number">10.2.</span> <span class="nav-text">梯度下降</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Python中的广播-Broadcasting-in-Python"><span class="nav-number">11.</span> <span class="nav-text">Python中的广播-Broadcasting in Python</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#A-note-on-python-numpy-vectors"><span class="nav-number">12.</span> <span class="nav-text">A note on python/numpy vectors</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#消除代码中秩为1的数组"><span class="nav-number">12.1.</span> <span class="nav-text">消除代码中秩为1的数组</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#不要害羞，使用reshape或assert来保证维度不出错"><span class="nav-number">12.2.</span> <span class="nav-text">不要害羞，使用reshape或assert来保证维度不出错</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Logistic回归中成本函数的证明"><span class="nav-number">13.</span> <span class="nav-text">Logistic回归中成本函数的证明</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#本周作业"><span class="nav-number">14.</span> <span class="nav-text">本周作业</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Part-1-Python-Basics-with-Numpy-optional-assignment"><span class="nav-number">14.1.</span> <span class="nav-text">Part 1: Python Basics with Numpy (optional assignment)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#np-dot-np-outer-np-multiply"><span class="nav-number">14.1.1.</span> <span class="nav-text">np.dot(),np.outer(),np.multiply(),*</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Part-2-Logistic-Regression-with-a-Neural-Network-mindset"><span class="nav-number">14.2.</span> <span class="nav-text">Part 2: Logistic Regression with a Neural Network mindset</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集预处理"><span class="nav-number">14.2.1.</span> <span class="nav-text">数据集预处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Gereral-Architecture-of-the-learning-algorithm"><span class="nav-number">14.2.2.</span> <span class="nav-text">Gereral Architecture of the learning algorithm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#算法的各个模块"><span class="nav-number">14.2.3.</span> <span class="nav-text">算法的各个模块</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Helper-functions-Sigmoid"><span class="nav-number">14.2.3.1.</span> <span class="nav-text">Helper functions-Sigmoid</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#初始化参数-Initializing-parameters"><span class="nav-number">14.2.3.2.</span> <span class="nav-text">初始化参数-Initializing parameters</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Forward-and-Backward-propagation"><span class="nav-number">14.2.3.3.</span> <span class="nav-text">Forward and Backward propagation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Optimization-更新"><span class="nav-number">14.2.3.4.</span> <span class="nav-text">Optimization-更新</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#预测"><span class="nav-number">14.2.3.5.</span> <span class="nav-text">预测</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#合并模块-Merge-all-functions-into-a-model"><span class="nav-number">14.2.4.</span> <span class="nav-text">合并模块-Merge all functions into a model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#有关学习率的进一步学习"><span class="nav-number">14.2.5.</span> <span class="nav-text">有关学习率的进一步学习</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-number">14.3.</span> <span class="nav-text">总结</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">DesmonDay</span>
</div>



<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv"></span>
</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  






  





  

  

  

  

  

  

</body>
</html>


