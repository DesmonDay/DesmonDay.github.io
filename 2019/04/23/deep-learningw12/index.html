<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="python," />





  <link rel="alternate" href="/atom.xml" title="DesmonDay's Blog" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="为什么用序列模型：Why sequence models?首先，我们看几个序列数据的例子:所有的这些问题都可以作为使用标签数据(X,Y)作为训练集的监督学习。不过从上图，我们也可以知道这里的序列数据是非常不同的。有些输入输出都是序列，但长度不同；有的只有输入或输出才是序列，等等。 数学符号：NotationMotivation Example假设我们想要建立一个能够自动识别句中人名位置的序列模型。">
<meta name="keywords" content="python">
<meta property="og:type" content="article">
<meta property="og:title" content="第12周-循环神经网络(RNN)">
<meta property="og:url" content="https://github.com/DesmonDay/2019/04/23/deep-learningw12/index.html">
<meta property="og:site_name" content="DesmonDay&#39;s Blog">
<meta property="og:description" content="为什么用序列模型：Why sequence models?首先，我们看几个序列数据的例子:所有的这些问题都可以作为使用标签数据(X,Y)作为训练集的监督学习。不过从上图，我们也可以知道这里的序列数据是非常不同的。有些输入输出都是序列，但长度不同；有的只有输入或输出才是序列，等等。 数学符号：NotationMotivation Example假设我们想要建立一个能够自动识别句中人名位置的序列模型。">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-d932f29b3f3fbe30.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-37849034ae083b14.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-df6c867d4977f8d3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-12426e1eb0b70a16.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-53233244b3a9e6c2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-8fe77d39fca4e914.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-2260330da984017d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-aa877c8fdbe61164.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-65dafc9207622144.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-f8a89d0b56d93c18.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-243c146d3b45b0b8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-3628eebbc758f041.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-dbc63f1e76c12e00.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-a0fe2e3720d0fd9e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-b37e5421a68fa78a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-ff02aec2c8ab0538.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-ea9b7b9a8f14b011.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-cd02087594a1250d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-123e68b7a83222d7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-b447ca3ca31d8374.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-8bcd5703f21d5141.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-453e6eac0e9de534.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-62a3311d3bc8e1bc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-d5bcb1c69363f9dc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-f3547621968f6bec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-a55c72fd35631fbe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-9266f437645d63ea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-ae929432c67ac294.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-c191e422a19f7b6c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-54e8c57ca8ba962a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-e7571e716d844f83.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-78b59a7d11dc5caa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-1884db2f163dac1e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-265dd247b2b9312b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-33ab1bcbf06b3cb3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-bfb4d77c79ad247d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-51fc765c5b4d2f63.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-c121835dcc48c5b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-abe4597abfc4bf63.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-b9d520811feecc7e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-b4800a75c12cf672.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-7d87ba39f2e6cd12.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-7be385f8b161c998.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-59fce5507d3623da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-1fc6bc02fac1c769.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-131ec80bce316acc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-f4be19597aa9375b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-f926c844c1a38601.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-25df2c3b3314c65e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-557568c6fe786add.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-fddaead310ac055e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-d09e6eeb0e3993da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-c4272320bb39158d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-b3d7de5f4728a3a6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-bd0c79d5a5cf075d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-0ad7eaf74f28955d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-5bc620d0876deeb7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-94465d05da44841f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-207909d44ae745ae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-920edd50c3d7362e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-ae40aa5b003d9629.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-3639d261d113dffc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-fd40e2ab6147b4a7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-7b6fcea20ef8253a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-afb408d18ac35097.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-7b7f7cdfe6c3e3e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-560316ebb79f05e5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-969c576b3584b1ae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-cc03d74c1fe1ab89.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-ec9cdb2b5803abb7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-72f75c51fdc14580.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-faa16612756bff66.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-d5af764c972ec343.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-58b5ff406da60509.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-5eeabc165db7889c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-3ae9380d4a619dbc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-002ba438ea40c34b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/8636110-53b7e99a279cda10.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:updated_time" content="2019-05-01T15:47:53.333Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="第12周-循环神经网络(RNN)">
<meta name="twitter:description" content="为什么用序列模型：Why sequence models?首先，我们看几个序列数据的例子:所有的这些问题都可以作为使用标签数据(X,Y)作为训练集的监督学习。不过从上图，我们也可以知道这里的序列数据是非常不同的。有些输入输出都是序列，但长度不同；有的只有输入或输出才是序列，等等。 数学符号：NotationMotivation Example假设我们想要建立一个能够自动识别句中人名位置的序列模型。">
<meta name="twitter:image" content="https://upload-images.jianshu.io/upload_images/8636110-d932f29b3f3fbe30.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://github.com/DesmonDay/2019/04/23/deep-learningw12/"/>





  <title>第12周-循环神经网络(RNN) | DesmonDay's Blog</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">DesmonDay's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">一只小辣鸡的自我拯救之路</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://github.com/DesmonDay/2019/04/23/deep-learningw12/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="DesmonDay">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DesmonDay's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">第12周-循环神经网络(RNN)</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-04-23T17:21:00+08:00">
                2019-04-23
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="为什么用序列模型：Why-sequence-models"><a href="#为什么用序列模型：Why-sequence-models" class="headerlink" title="为什么用序列模型：Why sequence models?"></a>为什么用序列模型：Why sequence models?</h1><p>首先，我们看几个序列数据的例子:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d932f29b3f3fbe30.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>所有的这些问题都可以作为使用标签数据(X,Y)作为训练集的监督学习。不过从上图，我们也可以知道这里的序列数据是非常不同的。有些输入输出都是序列，但长度不同；有的只有输入或输出才是序列，等等。</p>
<h1 id="数学符号：Notation"><a href="#数学符号：Notation" class="headerlink" title="数学符号：Notation"></a>数学符号：Notation</h1><h2 id="Motivation-Example"><a href="#Motivation-Example" class="headerlink" title="Motivation Example"></a>Motivation Example</h2><p>假设我们想要建立一个能够自动识别句中人名位置的序列模型。所以这就是一个命名实体识别问题，常用于搜索引擎。比如，索引过去24小时内所有新闻报道提及的人名。命名实体识别系统可以用来查找不同类型的文本中的人名、公司名、时间、地点、国家名、货币名等等。</p>
<p>假设我们的输入输出设置如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-37849034ae083b14.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-df6c867d4977f8d3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>其中，y用一个向量对应句子中的每个单词，如果该单词为人名，那么为1，否则为0。从技术层面角度，这并不是最好的输出形式，还有其他更加复杂的输出形式。它不仅能够表明输入词是否是人名的一部分，还能够告诉你这个人名从句子哪里开始和结束。</p>
<p>我们将用x<1>,x<2>,…,x<9>来索引句子中单词的位置，用x&lt;\t&gt;表示序列的中间位置。这里的t意味着它们是时序序列，输出同样的用y<1>,y<2>,…y<9>来索引位置。另外，我们用T_y表示句子的长度。为了表示训练样本i的序列中第t个单词(元素)，我们用X(i)&lt;\t&gt;来表示，再用Tx(i)表示第i个训练样本的输入序列长度，这种表示方法对输出序列也成立。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-12426e1eb0b70a16.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></9></2></1></9></2></1></p>
<h2 id="Representing-words"><a href="#Representing-words" class="headerlink" title="Representing words"></a>Representing words</h2><p>接下来探讨怎样表示一个句子里单个的词。首先，我们可能会做一张词汇表，将我们要表示的词按字典顺序放入。比如，这里有个词汇量为10000的词汇表，这对自然处理语言应用来说是非常小规模的。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-53233244b3a9e6c2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>接下来，我们可以用<strong>one-hot</strong>表示每个单词，这个每个单词用one-hot向量来表示：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-8fe77d39fca4e914.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>我们的目标是，用这样的表示方式表示X，用序列模型在X和目标输出Y之间学习建立一个映射，我们会把它当做监督学习的问题来做。</p>
<p>另一个需要注意的问题是，如果我们遇到了一个不在单词表中的单词，我们就创建一个新的标记<code>&quot;&lt;UNK&gt;&quot;</code>，来表示这个单词不在词汇表中。</p>
<h1 id="循环神经网络模型"><a href="#循环神经网络模型" class="headerlink" title="循环神经网络模型"></a>循环神经网络模型</h1><h2 id="Why-not-a-standard-network"><a href="#Why-not-a-standard-network" class="headerlink" title="Why not a standard network?"></a>Why not a standard network?</h2><p>A standard network:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-2260330da984017d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>Problems:</p>
<ul>
<li>Inputs, outputs can be different lengths in different examples.</li>
<li>Doesn’t share features learned across different positions of text.类似卷积神经网络中所学到的，我们希望将部分图片里学到的内容快速推广到图片的其他部分，而我们希望对序列数据也有相似的效果。</li>
</ul>
<p>另外，用一个更好的representation能够让我们减少模型中的参数数量。</p>
<h2 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h2><p>RNN的基本结构如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-aa877c8fdbe61164.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>首先顺序读取序列中的第一个单词，并且尝试预测一个输出值；再读取第二个单词的时候，我们不仅仅利用这个单词进行预测，而且会用到前一个隐藏层的输出作为当前层的输入来预测。就这样按照顺序进行下去。另外，我们通常为设置一个伪激活值a<0>作为RNN的最初始输入，通常为0向量。另外在本例中，Tx=Ty，如果不等，则需要对网络结构进行调整。</0></p>
<p>循环神经网络是从左向右扫描数据，同时每个时间步的参数也是共享的。我们用W_ax来表示从x<1>到隐藏层的连接的一系列参数，并且每个时间步所使用的都是相同的参数W_ax。而激活值，也就是水平联系，是由参数W_aa决定的，同时每一个时间步都使用相同的参数W_aa。同样的，输出都由参数W_ya决定。（对这里的参数名称做一个解释，比如W_ax，表示这个参数会乘以X来得到a，类似这样感知的理解就好，看图就能够明白了）<br><img src="https://upload-images.jianshu.io/upload_images/8636110-65dafc9207622144.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>注意：最右侧循环图可以理解为左边图的简化版本，但难以理解。</1></p>
<p>在这种RNN中，当我们预测y<3>时，不仅要使用x<3>的信息，还要使用来自x<1>和x<2>的信息。但这个RNN的一个缺点是它只使用了这个序列中<strong>之前的信息</strong>来做出预测。即当预测y<3>时，它没有用到如x<4>、x<5>、x<6>等等的信息。这就造成了一个问题：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f8a89d0b56d93c18.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>为了判断Teddy是否是人名的一部分，只使用前面的两个单词来预测是远远不够的，即之给定前面的三个单词，不可能确切知道Teddy是否是人名的一部分。</6></5></4></3></2></1></3></3></p>
<p>因此这个特定结构神经网络的缺点是，它在某一时刻的预测仅使用了从序列中之前时刻的输入信息，并没有使用序列之后地信息。这个问题我们会在Bidirectional RNN(双向RNN)中得到解答。</p>
<h2 id="Forward-Propagation"><a href="#Forward-Propagation" class="headerlink" title="Forward Propagation"></a>Forward Propagation</h2><p>我们将网络结构更清晰的表示如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-243c146d3b45b0b8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>用等式表示为：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-3628eebbc758f041.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>我们注意到，式子可以在最后很有规律地表示出其结果。另外，激活函数可以是不同的，比如在RNN中，g1往往选择为tanh，也可以为relu，而g2需要根据我们y的输出值来确定（如果是二分类问题，则选择为sigmoid，如本例的命名实体识别问题；如果是多分类，可以选择softmax等等）。</p>
<h2 id="Simplified-RNN-notation"><a href="#Simplified-RNN-notation" class="headerlink" title="Simplified RNN notation"></a>Simplified RNN notation</h2><p>在上一小节，我们得到了以下公式：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-dbc63f1e76c12e00.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>我们将a&lt;\t&gt;的式子简写作：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-a0fe2e3720d0fd9e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>其中，具体的解释如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b37e5421a68fa78a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>将y&lt;\t&gt;写作：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ff02aec2c8ab0538.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>此时，W_y和b_y的下标只有y，这表示会输出什么类型的量，所以W_y是计算y类型的量的权重矩阵；而上面的W_a和b_a表示这些参数是用来计算a类型输出的。</p>
<h1 id="Back-propagation-through-time"><a href="#Back-propagation-through-time" class="headerlink" title="Back propagation through time"></a>Back propagation through time</h1><p>我们已经学习了循环神经网络的基础结构，在本节，我们将了解反向传播是怎样在循环神经网络中运行的。</p>
<p>我们已知的前向传播过程：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ea9b7b9a8f14b011.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>接下来，为了计算反向传播，我们还需要一个损失函数。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-cd02087594a1250d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>由于类似二元分类，因此我们选取的损失函数为交叉熵损失函数。</p>
<p>将损失函数和反向传播表示到网络中：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-123e68b7a83222d7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>其中最为重要的递归操作为从右往左的梯度计算。</p>
<h1 id="不同类型的RNN"><a href="#不同类型的RNN" class="headerlink" title="不同类型的RNN"></a>不同类型的RNN</h1><p>本文参考Andrej Karpathy的博客: The Unreasonable Effectiveness of Recurrent Neural Networks（对应翻译版：<a href="https://blog.csdn.net/menc15/article/details/78775010" target="_blank" rel="noopener">https://blog.csdn.net/menc15/article/details/78775010</a> ）</p>
<p>在上一节，我们介绍的RNN结构里Tx和Ty是相等的。但是在实际应用中，Tx和Ty不一定相等。本节会进行介绍。</p>
<p>常见序列数据：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b447ca3ca31d8374.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h2 id="Examples-of-RNN-architectures"><a href="#Examples-of-RNN-architectures" class="headerlink" title="Examples of RNN architectures"></a>Examples of RNN architectures</h2><p>many-to-many(输入输出长度相同), many-to-one(例如情感分类), one-to-one(普通神经网络):<br><img src="https://upload-images.jianshu.io/upload_images/8636110-8bcd5703f21d5141.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>one-to-many(音乐生成), many-to-many(输入输出长度不同，如翻译，用的是encoder-decoder模型):<br><img src="https://upload-images.jianshu.io/upload_images/8636110-453e6eac0e9de534.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>另外还有一个attention模型，之后会讲解。</p>
<h2 id="Summary-of-RNN"><a href="#Summary-of-RNN" class="headerlink" title="Summary of RNN"></a>Summary of RNN</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-62a3311d3bc8e1bc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h1 id="语言模型和序列生成"><a href="#语言模型和序列生成" class="headerlink" title="语言模型和序列生成"></a>语言模型和序列生成</h1><p>一个语言模型能够计算出句子的可能性，如语言识别中，举一个简单的例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d5bcb1c69363f9dc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>因此，语言模型会计算出某个特定句子出现的概率是多少。这个模型是两种系统的基本组成部分，即语音识别系统和机器翻译系统，它能正确输出最接近的句子。而语言模型做的基本工作就是输入一个句子，准确地说是一个文本序列，然后会估计该序列中各个单词出现的可能性。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f3547621968f6bec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h2 id="Language-modeling-with-an-RNN"><a href="#Language-modeling-with-an-RNN" class="headerlink" title="Language modeling with an RNN"></a>Language modeling with an RNN</h2><p>首先我们需要一个训练集，这个训练集可能是一个大型的英文语料库，也可能是其他我们需要的语言的语料库。</p>
<ol>
<li>第一件事需要进行tokenize，分解文本流为词，或将其转化为序列（比如用one-hot向量表示每一个词）。</li>
<li>另一件可能要做的事是定义句子的结尾，一般的做法是增加一个额外的标记，叫做<code>&quot;&lt;EOS&gt;&quot;</code>。这样能够让我们清楚一个句子什么时候结束。因此<code>EOS</code>标记可以添加到训练集中每一个句子的结尾。（注意句号可以去掉，或者当做一个单词也计入词典）</li>
<li>另一件是如果我们的训练集里有一些词并不在词典里（这个词典可能是10,000个常见的英文单词），那么将不在里面的单词用<code>&quot;&lt;UNK&gt;&quot;</code>取代，即用<code>&quot;&lt;UNK&gt;&quot;</code>代替未知词。我们只针对<code>&quot;&lt;UNK&gt;&quot;</code>来建立概率模型。</li>
<li>下一步，我们要构建一个RNN来构建这些序列的概率模型。</li>
</ol>
<h2 id="RNN-model"><a href="#RNN-model" class="headerlink" title="RNN model"></a>RNN model</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-a55c72fd35631fbe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-9266f437645d63ea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>由图上的RNN模型可知，我们首先输入x<1>和a<0>均为零向量，然后开始尝试输出第一个词y_hat<1>。随后，我们将y_hat<1>和a<1>作为计算a<2>的输入，接着通过softmax函数计算出第二个预测词y_hat<2>（这时候，我们所求的是使P(?|Cats)最大的单词，这样依次计算下去…</2></2></1></1></1></0></1></p>
<p>我们使用的是Softmax函数作为输出层的激活函数，因此选用的loss function为交叉熵损失函数。其中，y_i&lt;\t&gt;为真实的输出，而y_hat_i&lt;\t&gt;则为预测的单词输出，然后再将所有时刻的loss相加，得到最后总的loss function。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ae929432c67ac294.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>假设现在有一个新句子，为了简单起见，假设其含有是三个单词。那么第一层Softmax会计算P(y<1>)，第二层Softmax计算P(y<2>|y<1>)，第三层Softmax计算P(y<3>|y<1>,y<2>)。从而整个句子的概率为：<br>    <img src="https://upload-images.jianshu.io/upload_images/8636110-c191e422a19f7b6c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></2></1></3></1></2></1></p>
<h1 id="对新序列采样：sampling-novel-sequences"><a href="#对新序列采样：sampling-novel-sequences" class="headerlink" title="对新序列采样：sampling novel sequences"></a>对新序列采样：sampling novel sequences</h1><p>在训练完一个序列模型之后，要想了解这个模型学到了什么，一种非正式的方法就是进行一次新序列采样。</p>
<h2 id="Sampling-a-sequence-from-a-trained-RNN"><a href="#Sampling-a-sequence-from-a-trained-RNN" class="headerlink" title="Sampling a sequence from a trained RNN"></a>Sampling a sequence from a trained RNN</h2><p>我们记得一个序列模型模拟了任意特定单词序列的概率，而我们要做的是对这个概率分布进行采样，来生成一个新的单词序列。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-54e8c57ca8ba962a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>已知我们训练时所用的RNN模型如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e7571e716d844f83.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>然而为了采样，我们会做一些不同的事情。第一步要做的是对我们想要模型生成的第一个词进行采样，于是我们输入x<1>=0,a<0>=0，因此现在我们的第一个时间步得到的是所有可能的输出，即经过Softmax层后得到的概率。然后根据这个Softmax的分布进行随机采样，使用numpy命令如np.random.choice来对第一个词进行采样；接下来我们进入第二个时间步，把刚刚采样的y<1>传递到下一个位置作为输入，接着Softmax层会预测第二个词；这样依次进行…</1></0></1></p>
<p>什么时候一个句子算结束呢？一个方法是如果代表句子结尾的标识<code>&quot;&lt;EOS&gt;&quot;</code>在词典中，那么我们可以一直进行采样直到<strong>得到<code>&quot;&lt;EOS&gt;&quot;</code></strong>，这代表我们已经抵达结尾，可以停止随机采样；另一个方法是词典中没有这个标识，那么我们可以决定从20个或100个或其他词中进行采样，直到达到所<strong>设定的时间步</strong>。</p>
<p>这种方法可能会出现预测出<code>&quot;&lt;UNK&gt;&quot;</code>的情况。如果我们想要避免这种情况，那么可以在<code>&quot;&lt;UNK&gt;&quot;</code>出现时就继续在剩下的词中进行重采样，直到得到一个不是<code>&quot;&lt;UNK&gt;&quot;</code>的单词；当然，如果我们不介意有未知标识的产生，也可以不理会。</p>
<p>以上就是我们从RNN语言模型中生成一个随机选择的句子。</p>
<h2 id="Character-level-language-model"><a href="#Character-level-language-model" class="headerlink" title="Character-level language model"></a>Character-level language model</h2><p>在之前我们所建立的都是基于词汇的RNN模型，也就是说，字典中的词都是英语单词。根据我们的实际应用，我们还可以构建一个基于字符的RNN模型。这时，我们的字典中不再是单词，而是常见字符。</p>
<p>此时输入输出都是单个字符，而不再是单独的词汇。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-78b59a7d11dc5caa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>基于字符的语言模型的优点是我们不必担心会出现未知的标识，比如基于字符的语言模型会把’Mao’视为概率非0的序列，而基于词汇的语言模型，如果’Mao’不在字典中，那么我们只能把它当做未知标识。</p>
<p>然后基于字符的语言模型的一个明显缺点是我们最后会得到太长的序列。所以基于字符的语言模型在捕捉句子中的依赖关系，也就是句子较前部分如何影响较后部分，不如基于词汇的语言模型，并且它的计算成本也会很大。</p>
<p>因此在自然语言领域中，大多数应用是使用基于词汇的语言模型。在随着计算能力的提高，在一些特殊情况下，人们也会开始使用基于字符的模型，但这也需要更昂贵的计算成本。</p>
<p>下面展示一些采样后的结果：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-1884db2f163dac1e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h1 id="Vanishing-gradients-with-RNNs"><a href="#Vanishing-gradients-with-RNNs" class="headerlink" title="Vanishing gradients with RNNs"></a>Vanishing gradients with RNNs</h1><p>在前面的学习我们已经了解了RNN是如何工作的，并且知道如何将RNN应用到具体问题上，比如命名实体识别(name entity recognition)、语言模型等等。然后基本的RNN模型有一个很大的问题，也就是<strong>梯度消失</strong>的问题。</p>
<p>下面解释梯度消失。首先给出两个句子，这两个句子有着长期的依赖，也就是很前面的单词对句子很后面的单词有影响。(cat对应was，cats对应were)</p>
<ul>
<li>The cat, which already ate…, was full.</li>
<li>The cats, which already ate…, were full.<br>但是目前我们见到的<strong>基本RNN模型不擅长捕获这种长期依赖效应</strong>。在之前的讨论，我们知道在训练很深的网络时，我们讨论了梯度消失的问题。如果深度很深，那么从输出y得到的梯度将很难传播回去，很难影响到前面层的权重。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-265dd247b2b9312b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></li>
</ul>
<p>而RNN也有着同样的问题，所以其反向传播也比较困难。因为同样的梯度消失的问题，后面层的输出误差很难影响前面层的计算。这就意味着，实际上能难让一个神经网络能够意识到它要记住看到的是单数名词还是复数名词，然后在序列后面生成依赖单复数形式。也就是说，基本的RNN模型里，一个单词只能被其前面的几个单词所影响。这就是RNN的一个缺点。</p>
<p>如果出现了<strong>梯度爆炸</strong>的情况，一般很容易发现，因为梯度可能会出现如NaN(数值溢出之类的现象。一个解决方法就是进行<strong>梯度修剪</strong>(gradient clipping)，也就是观察我们的梯度向量，如果它大于某个阈值，那么我们缩放梯度向量，保证它不会太大。</p>
<p>但梯度消失是更难解决的。</p>
<p>另外复习:</p>
<ul>
<li>ReLu激活函数的主要贡献是：<ul>
<li>解决了梯度消失、爆炸的问题</li>
<li>计算方便、计算速度快</li>
<li>加速了网络的训练</li>
</ul>
</li>
<li>BatchNorm: 本质上是解决反向传播过程中的梯度问题。反向传播式子中有w的存在，所以w的大小影响了梯度的消失和爆炸。BatchNorm就是通过对每一层的输出规范化为均值和方差一致的方法，消除了w带来的放大缩小的影响，进而解决梯度消失和爆炸的问题。</li>
<li>残差结构：解决了梯度消失过快的问题，因此即使构建的网络很深层也不必担心。</li>
</ul>
<p>参考博客：<a href="https://blog.csdn.net/qq_25737169/article/details/78847691" target="_blank" rel="noopener">https://blog.csdn.net/qq_25737169/article/details/78847691</a></p>
<h1 id="GRU-Gated-Recurrent-Unit"><a href="#GRU-Gated-Recurrent-Unit" class="headerlink" title="GRU: Gated Recurrent Unit"></a>GRU: Gated Recurrent Unit</h1><p>参考论文：</p>
<ol>
<li>On the properties of neural machine translation: Encoder-decoder approaches, 2014.</li>
<li>Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling, 2014.</li>
</ol>
<p>我们已经学习了基础的RNN模型的运行机制，在本节，我们会学习门控循环单元GPU。它改变了RNN的隐藏层，使其更好地捕捉深层连接，并改善了梯度消失问题。</p>
<h2 id="RNN-unit"><a href="#RNN-unit" class="headerlink" title="RNN unit"></a>RNN unit</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-33ab1bcbf06b3cb3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h2 id="GRU-simplified"><a href="#GRU-simplified" class="headerlink" title="GRU(simplified)"></a>GRU(simplified)</h2><p>GRU设置了一个新的变量C，为记忆细胞(memory cell)，记忆细胞的作用是提供了记忆的能力，比如一只猫是单数还是复数。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-bfb4d77c79ad247d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>在GRU中，c&lt;\t&gt;等于激活值a&lt;\t&gt;。（因为在LSTM中，两个值并不相等，因此为避免混淆，这里采用两种不同的名称表示，即便他们的值是一样的。）</p>
<p>接下来写出GPU(简化版本)中的关键公式：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-51fc765c5b4d2f63.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>其中，第一个式子表示记忆细胞的候选值，我们用tanh计算出来。</p>
<p>而GPU的关键思想就是第二个式子，也就是门（用”Γ”代替，另外小写u表示”update”)。为了思考GRU的工作机制，我们思考门的原理。可以看到它是用一个sigmoid函数来计算，对大多数输入来说，sigmoid值在大多数情况下都接近0或者1。</p>
<p>接下来看第三个式子。如果当Γ_u=1时，说明我们将c&lt;\t&gt;设置为计算的候选值c_tilda&lt;\t&gt;。而如果Γ_u=0，说明我们将c&lt;\t&gt;设置为c&lt;\t-1&gt;，这也说明，Γ_u越小，则前一时刻的状态信息带入越多。而针对我们之前所说的猫吃饱没的例子，我们应该将cat和was之间的所有Γ_u都设置为0，即不进行更新，只设置为旧的值。这样到了was的时候，神经网络还能记得cats的信息。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c121835dcc48c5b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>我们将GRU图示化，以便更好理解：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-abe4597abfc4bf63.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>而GRU的优点是，通过门来决定，当我们从左到右扫描一个句子的时候，这个时机是应该更新还是不更新记忆细胞。由于sigmoid值很可能取0，因此我们可以通过门的设定来维持记忆细胞的值，即c&lt;\t&gt;=c&lt;\t-1&gt;，而因为Γ_u很接近0，从而不会有梯度消失的问题了。因为Γ_u很接近0，所以c&lt;\t&gt;几乎等于c&lt;\t-1&gt;，而且c&lt;\t&gt;的值也很好地被维持了，即使经过很多很多的时间步。所以这就是缓解梯度消失问题的关键，因此允许神经网络运行在非常庞大的依赖词上，比如cat和was单词，即便它们被很多单词分隔，也仍然能够运行。</p>
<p>下面补充一点实现的细节。在上面有关记忆细胞候选值的式子里，c&lt;\t&gt;可以是一个向量。如果我们有一个100维的隐藏的激活值，那么c&lt;\t&gt;也是100维的，c_tilda&lt;\t&gt;也是相同的维度，从而Γ_u也是相同的维度。因此说明c&lt;\t&gt;的式子实际上是元素对应的乘积。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b9d520811feecc7e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>而这100维的记忆细胞里，我们只更新需要更新的比特。</p>
<p>当然，在实际应用中Γ_u不会真的等于0或1，有时候它是0到1的中间值，但这对于直观思考是很方便地。而元素对应的成绩做的是告诉GRU单元哪个记忆细胞的向量维度在每个时间步要做更新，因此我们可以选择保持一些比特不变，而去更新其他的比特。</p>
<h2 id="Full-GRU"><a href="#Full-GRU" class="headerlink" title="Full GRU"></a>Full GRU</h2><p>针对完整版本的GRU，我们添加了一个变量Γ_r，这个Γ_r告诉我们计算出的c&lt;\t-1&gt;与c&lt;\t&gt;的候选值的相关性有多大。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b4800a75c12cf672.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>为什么需要Γ_r呢？这是因为多年来研究者们试验过很多不同可能的方法来设计这些单元，尝试让神经网络有更深层的连接，尝试产生更大范围的影响，并且解决梯度消失的问题。</p>
<h2 id="参考其他博客的解释"><a href="#参考其他博客的解释" class="headerlink" title="参考其他博客的解释"></a>参考其他博客的解释</h2><p>为了让自己更加深刻的理解GRU，接下来还参考了以下博文：<a href="https://www.cnblogs.com/jiangxinyang/p/9376021.html" target="_blank" rel="noopener">https://www.cnblogs.com/jiangxinyang/p/9376021.html</a></p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-7d87ba39f2e6cd12.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>图中的zt和rt分别表示更新门和重置门。更新门用于控制前一时刻的状态信息被带入到当前状态中的程度，更新门的值越大说明前一时刻的状态信息带入越多。重置门控制前一状态有多少信息被写入到当前的候选集 h_tilda_t 上，重置门越小，前一状态的信息被写入的越少。</p>
<ol>
<li><p>GRU的前向传播：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7be385f8b161c998.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
</li>
<li><p>GRU的训练过程：从前向传播过程中的公式可以看出要学习的参数有Wr、Wz、Wh、Wo。其中前三个参数都是拼接的（因为后先的向量也是拼接的），所以在训练的过程中需要将他们分割出来：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-59fce5507d3623da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>输出层的输入和输出层的输出分别为（注意到输出的激活函数是sigmoid）：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-1fc6bc02fac1c769.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>损失函数（这里用的是平方损失函数）：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-131ec80bce316acc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>接下来求偏导（我没有进行推导）：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f4be19597aa9375b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>其中各中间参数为：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f926c844c1a38601.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>在算出了对各参数的偏导之后，就可以更新参数，依次迭代直到损失收敛。</p>
</li>
</ol>
<h1 id="长短时记忆网络：LSTM"><a href="#长短时记忆网络：LSTM" class="headerlink" title="长短时记忆网络：LSTM"></a>长短时记忆网络：LSTM</h1><p>论文标题：Long shot-term memory, 1997.</p>
<p>在上一节，我们学习了GRU，它能够让我们在序列中学习非常深的连接(long range connection)。LSTM单元也能做到这一点。</p>
<p>下面是GRU和LSTM的公式对照。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-25df2c3b3314c65e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>我们注意到，LSTM不再有a&lt;\t&gt;=c&lt;\t&gt;。和GRU一样，有一个更新门，而LSTM的新特性是不只有一个更新门Γ_u控制，而新增加了一个遗忘门Γ_f。然后增加了一个输出门来用于输出a&lt;\t&gt;。</p>
<p>接下来将LSTM表示为图形：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-557568c6fe786add.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>接下来我们将多个LSTM单元按顺序连接起来，可以发现一件有趣的事：我们会发现在连接的图中有一条线（即所画的红线），这条线显示了只要我们正确地设置了遗忘门和更新门，LSTM是能够很容易地将c<0>的值一直传递到右边，比如c<3>=c<0>。这就是为什么LSTM和GRU非常擅长于长时间记忆某个值，即使经过很长的时间步，依旧能够保持住存在于记忆细胞中的某个值。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-fddaead310ac055e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></0></3></0></p>
<p>然而，这里介绍的和一般使用的版本不太一样。最常用的版本(LSTM的变体：Recurrent Nets that Time and Count, 2000)是我们的门值不仅取决于a&lt;\t-1&gt;和x&lt;\t&gt;，有时候人们也会偷窥一下c&lt;\t-1&gt;的值，这叫做peephole connection(窥视孔连接)。它的意思是，门值不仅取决于a&lt;\t-1&gt;和x&lt;\t&gt;，也取决于上一个记忆细胞的值。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d09e6eeb0e3993da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>那么我们应该什么时候用GRU，什么时候用LSTM？这里没有统一的准则。尽管我们先介绍了GRU，但在深度学习历史上，LSTM是更早出现的，而GRU是近几年才发明出来的，它可能源于在更加复杂的LSTM模型中做出的简化。研究者们在很多不同问题上尝试了这两种模型，看看在不同问题不同算法中哪个模型更好。</p>
<p>GRU的优点是更加简单，所以更容易创建一个更大的网络，而且它只有两个门，在计算性上也运行得更快，可以扩大模型的规模；而LSTM更加强大和灵活，因为它有三个门而不是两个。如果我们必须选择一个来使用，那么LSTM应该会作为默认选择来尝试。</p>
<h2 id="LSTM反向传播推导-未验证"><a href="#LSTM反向传播推导-未验证" class="headerlink" title="LSTM反向传播推导(未验证)"></a>LSTM反向传播推导(未验证)</h2><p>门求偏导：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c4272320bb39158d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-b3d7de5f4728a3a6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>参数求偏导：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-bd0c79d5a5cf075d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-0ad7eaf74f28955d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>最后，计算隐藏状态、记忆状态和输入的偏导数：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-5bc620d0876deeb7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h1 id="双向RNN：Bidirectional-RNN-BRNN"><a href="#双向RNN：Bidirectional-RNN-BRNN" class="headerlink" title="双向RNN：Bidirectional RNN(BRNN)"></a>双向RNN：Bidirectional RNN(BRNN)</h1><p>到目前为止我们已经学习了RNN模型的关键构件，但还有两个方法能够让我们构建更好的模型。其中一个方法就是双向RNN模型，这个模型能够让我们在序列的某点处不仅可以获取之前的信息，还可以获取之后的信息；第二个方法是深层RNN，我们将在下一节讲解。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-94465d05da44841f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>为了了解RNN的动机，我们先看一下之前在命名实体识别中见过多次的神经网络。这个网络的问题是，在判断Teddy是否是人名的一部分时，光看句子的前面部分是不够的。为了判断y<3>是0还是1，除了前三个单词，我们还需要更多的信息。所以这是一个单向的RNN。无论里面的单元是GRU、LSTM，这个结论总是成立的，即这是一个单向的RNN神经网络。</3></p>
<p>下面我们来解释BRNN的工作原理。为了简单，我们用四个输入或者说只有四个单词的句子，这样输入只有x<1>到x<4>。然后我们有四个前向的循环单元，这四个循环单元都有一个当前输入x&lt;\t&gt;，进而得到预测的y_hat&lt;\t&gt;。这是最初的结构：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-207909d44ae745ae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></4></1></p>
<p>接下来，我们要增加一个反向循环层。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-920edd50c3d7362e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>这样，网络就形成了一个无环图(acyclic graph)。给定一个输入序列x<1>到x<4>，这个序列首先计算前向的a<1>，然后计算前向的a<2>，接着a<3>、a<4>。而反向序列从计算a’<4>开始反向进行，计算反向的a’<3>，直到计算完成。再把所有的激活值都计算完成后，就可以得到预测结果了。（这里我们用a<1>表示前向激活值，a’<1>表示后向激活值）</1></1></3></4></4></3></2></1></4></1></p>
<p>而预测结果的计算公式如下式子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ae40aa5b003d9629.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>通过这种方式，我们的预测不仅利用了过去的信息，也利用了未来的信息。而神经网络里的基础单元不仅仅可以是标准RNN单元，也可以是GRU单元或者LSTM单元。事实上，很多的NLP问题，有LSTM单元的双向RNN模型是用得最多的。所以如果有NLP问题，并且文本句子都是完整的，首先需要标定这些句子，那么一个有LSTM单元的双向RNN模型，既有前向过程也有反向过程，将是不错的首选。</p>
<p>BPNN不仅能用于基本的RNN结构，也能用于GRU和LSTM。通过这些改变，我们可以用一个RNN或GRU或LSTM构建的模型，并且能够预测任意位置，即使在句子的中间位置，因为模型能够考虑整个句子的信息。而这个<strong>双向RNN网络模型的缺点是我们需要完整的数据序列，才能够预测任意位置</strong>。比如我们需要构建一个语音识别系统，那么BRNN需要考虑整个语音表达。也就是说，我们需要等某个人说完，获取整个语音表达才能处理这段语音，并进一步做语音识别。因此，对于实际的语音识别的应用，通常会有更加复杂的模块，而不仅仅用我们见过的标准BRNN模型。当然，对于很多自然语言处理的应用，如果我们总是可以获取整个句子，那么标准的BRNN模型实际上会很有效。</p>
<h1 id="深层循环神经网络-Deep-RNNs"><a href="#深层循环神经网络-Deep-RNNs" class="headerlink" title="深层循环神经网络: Deep RNNs"></a>深层循环神经网络: Deep RNNs</h1><p>目前我们学到的不同RNN的版本，每个都可以独当一面。但是要学习非常复杂的函数时，通常我们会把RNN的多个层堆叠在一起构建更深的模型。</p>
<p>我们知道标准的神经网络结构如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-3639d261d113dffc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>下面给出一个具有三个隐层的RNN网络：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-fd40e2ab6147b4a7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>我们下面来讲解一下a[2]<3>应该怎么计算。激活值a[2]<3>有两个输入，一个从左边传来， 一个从下边传来，那么计算的时候，用激活函数作用于权重矩阵如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7b6fcea20ef8253a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>另外，Wa^[2]和ba^[2]也用于同一层的计算，同理第一层有参数Wa^[1]和ba^[1]参与计算。对于RNN来说，能够有三层已经很多了，一般我们设置为1或者2。（注意，time step为深度，这个为层数num_layers）</3></3></p>
<p>当然，有一种结构也会比较常见。即我们将输出去掉，替换成一些深层，而这些层并没有水平连接，只是一个深层网络，然后用来预测y<1>，同样也加深层网络来预测y<2>，等等。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-afb408d18ac35097.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></2></1></p>
<p>而这些单元既可以是标准的RNN单元，也可以是GRU或者LSTM单元。通常，深层的RNN训练需要很多计算资源，需要很长的时间。</p>
<h1 id="本周作业"><a href="#本周作业" class="headerlink" title="本周作业"></a>本周作业</h1><p>第一个作业里有关基本RNN和LSTM的反向传播的部分，没有写完，就先放着了，以后有机会再重新填坑。</p>
<h2 id="Building-your-Recurrent-Neural-Network-Step-by-Step"><a href="#Building-your-Recurrent-Neural-Network-Step-by-Step" class="headerlink" title="Building your Recurrent Neural Network - Step by Step"></a>Building your Recurrent Neural Network - Step by Step</h2><p>Recurrent Neural Networks (RNN) are very effective for Natural Language Processing and other sequence tasks because they have “memory”. They can read inputs x⟨t⟩ (such as words) one at a time, and remember some information/context through the hidden layer activations that get passed from one time-step to the next. This allows a uni-directional RNN to take information from the past to process later inputs. A bidirection RNN can take context from both the past and the future.</p>
<p>一些符号标记：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7b7f7cdfe6c3e3e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h3 id="1-Forward-propagation-for-the-basic-Recurrent-Neural-Network"><a href="#1-Forward-propagation-for-the-basic-Recurrent-Neural-Network" class="headerlink" title="1-Forward propagation for the basic Recurrent Neural Network"></a>1-Forward propagation for the basic Recurrent Neural Network</h3><p>Later this week, you will generate music using an RNN. The basic RNN that you will implement has the structure below. In this example, Tx = Ty.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-560316ebb79f05e5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>Here’s how you can implement an RNN:</p>
<p><strong>Steps</strong>:</p>
<ol>
<li>Implement the calculations needed for one time-step of the RNN.</li>
<li>Implement a loop over $T_x$ time-steps in order to process all the inputs, one at a time. </li>
</ol>
<h4 id="1-1-RNN-cell"><a href="#1-1-RNN-cell" class="headerlink" title="1.1- RNN cell"></a>1.1- RNN cell</h4><p>A Recurrent neural network can be seen as the repetition of a single cell. You are first going to implement the computations for a single time-step. The following figure describes the operations for a single time-step of an RNN cell.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-969c576b3584b1ae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-cc03d74c1fe1ab89.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>一个RNN cell处理一个词，而下面代码将其向量化，所以一个RNN cell可以同时处理m个样本中的词。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell_forward</span><span class="params">(xt, a_prev, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a single forward step of the RNN-cell as described in Figure (2)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    xt -- your input data at timestep "t", numpy array of shape (n_x, m). n_x为词向量的长度，m为样本个数</span></span><br><span class="line"><span class="string">    a_prev -- Hidden state at timestep "t-1", numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        ba --  Bias, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a_next -- next hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    yt_pred -- prediction at timestep "t", numpy array of shape (n_y, m)</span></span><br><span class="line"><span class="string">    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Retrieve parameters from "parameters"</span></span><br><span class="line">    Wax = parameters[<span class="string">"Wax"</span>]</span><br><span class="line">    Waa = parameters[<span class="string">"Waa"</span>]</span><br><span class="line">    Wya = parameters[<span class="string">"Wya"</span>]</span><br><span class="line">    ba = parameters[<span class="string">"ba"</span>]</span><br><span class="line">    by = parameters[<span class="string">"by"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute next activation state using the formula given above</span></span><br><span class="line">    a_next = np.tanh(np.dot(Waa,a_prev)+np.dot(Wax,xt)+ba)</span><br><span class="line">    <span class="comment"># compute output of the current cell using the formula given above</span></span><br><span class="line">    yt_pred = softmax(np.dot(Wya,a_next)+by)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># store values you need for backward propagation in cache</span></span><br><span class="line">    cache = (a_next, a_prev, xt, parameters)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a_next, yt_pred, cache</span><br></pre></td></tr></table></figure></p>
<p>测试：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">xt = np.random.randn(<span class="number">3</span>,<span class="number">10</span>)</span><br><span class="line">a_prev = np.random.randn(<span class="number">5</span>,<span class="number">10</span>)</span><br><span class="line">Waa = np.random.randn(<span class="number">5</span>,<span class="number">5</span>)</span><br><span class="line">Wax = np.random.randn(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">Wya = np.random.randn(<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line">ba = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">by = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">parameters = &#123;<span class="string">"Waa"</span>:Waa, <span class="string">"Wax"</span>: Wax, <span class="string">"Wya"</span>: Wya, <span class="string">"ba"</span>: ba, <span class="string">"by"</span>: by&#125;</span><br><span class="line"></span><br><span class="line">a_next, yt_pred, cache = rnn_cell_forward(xt, a_prev, parameters)</span><br></pre></td></tr></table></figure></p>
<h4 id="1-2-RNN-forward-pass"><a href="#1-2-RNN-forward-pass" class="headerlink" title="1.2- RNN forward pass"></a>1.2- RNN forward pass</h4><p>You can see an RNN as the repetition of the cell you’ve just built. If your input sequence or data is carried over 10 time steps, then you will copy the RNN cell 10 times. Each cell takes as input the hidden state from the previous cell(a&lt;\t-1&gt;) and the current time-step’s input data(x&lt;\t&gt;). It outputs a hidden state(a&lt;\t&gt;) and a prediciton(y&lt;\t&gt;) for this time-step.</p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-ec9cdb2b5803abb7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p><strong>Exercise</strong>: Code the forward propagation of the RNN described in Figure(3).</p>
<p><strong>Instructions</strong>:</p>
<ol>
<li>Create a vector of zeros (a) that will store all the hidden states computed by the RNN.</li>
<li>Initialize the “next” hidden state as a0 (initial hidden state).</li>
<li>Start looping over each time step, your incremental index is t:<ul>
<li>Update the “next” hidden state and the cache by running rnn_step_forward</li>
<li>Store the “next” hidden state in a(t^th position)</li>
<li>Store the prediction in y</li>
<li>Add the cache to the list of caches</li>
</ul>
</li>
<li>Return a, y and caches<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span><span class="params">(x, a0, parameters)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Implement the forward propagation of the recurrent neural network described in Figure(3).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        x -- Input data for every time-step, of shape(n_x, m, T_x).</span></span><br><span class="line"><span class="string">        a0 -- Initial hidden state, of shape (n_a, m).</span></span><br><span class="line"><span class="string">        parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                ba --  Bias numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- tuple of values needed for the backward pass, contains (list of caches, x)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialze "caches" which will contain the list of all caches</span></span><br><span class="line">    caches = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of x and Wy</span></span><br><span class="line">    n_x, m, T_x = x.shape</span><br><span class="line">    n_y, n_a = parameters[<span class="string">"Wya"</span>].shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># initialize "a" and "y" with zeros</span></span><br><span class="line">    a = np.zeros((n_a, m, T_x))</span><br><span class="line">    y_pred = np.zeros((n_y, m, T_x))</span><br><span class="line"></span><br><span class="line">    a_next = a0</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T_x):</span><br><span class="line">        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t],a_next, parameters)</span><br><span class="line">        a[:,:,t] = a_next</span><br><span class="line">        y_pred[:,:,t] = yt_pred</span><br><span class="line">        caches.append(cache)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    caches = (caches, x)</span><br><span class="line">    <span class="keyword">return</span> a, y_pred, caches</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>You’ve successfully built the forward propagation of a recurrent neural network from scratch. This will work well enough for some applications, but it suffers from vanishing gradient problems. So it works best when each output y&lt;\t&gt; can be estimated using mainly “local” context(meaning information from inputs x&lt;\t’&gt;) where t’ is not too far from t.</p>
<h3 id="2-backward-propagation-for-the-basic-Recurrent-Neural-Network"><a href="#2-backward-propagation-for-the-basic-Recurrent-Neural-Network" class="headerlink" title="2-backward propagation for the basic Recurrent Neural Network"></a>2-backward propagation for the basic Recurrent Neural Network</h3><h4 id="2-1-Basic-RNN-backward-pass"><a href="#2-1-Basic-RNN-backward-pass" class="headerlink" title="2.1- Basic RNN backward pass"></a>2.1- Basic RNN backward pass</h4><p>We start by computing the backward pass for the basic RNN-cell.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-72f75c51fdc14580.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>注意，上图中右边求导数的公式只计算了相对于a_next的梯度，完整的公式需要乘上a_next相对于cost function J的梯度。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell_backward</span><span class="params">(da_next, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the backward pass for the RNN-cell (single time-step).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    da_next -- Gradient of loss with respect to next hidden state</span></span><br><span class="line"><span class="string">    cache -- python dictionary containing useful values (output of rnn_step_forward())</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dx -- Gradients of input data, of shape (n_x, m)</span></span><br><span class="line"><span class="string">                        da_prev -- Gradients of previous hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        dba -- Gradients of bias vector, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Retrieve values from cache</span></span><br><span class="line">    (a_next, a_prev, xt, parameters) = cache</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve values from parameters</span></span><br><span class="line">    Wax = parameters[<span class="string">"Wax"</span>]</span><br><span class="line">    Waa = parameters[<span class="string">"Waa"</span>]</span><br><span class="line">    Wya = parameters[<span class="string">"Wya"</span>]</span><br><span class="line">    ba = parameters[<span class="string">"ba"</span>]</span><br><span class="line">    by = parameters[<span class="string">"by"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the gradient of tanh with respect to a_next</span></span><br><span class="line">    dtanh = (<span class="number">1</span>-a_next*a_next)*da_next</span><br><span class="line">    dxt = np.dot(Wax.T, dtanh)</span><br><span class="line">    dWax = np.dot(dtanh, xt.T)</span><br><span class="line">    da_prev = np.dot(Waa.T, dtanh)</span><br><span class="line">    dWaa = np.dot(dtanh, a_prev.T)</span><br><span class="line">    dba = np.sum(dtanh, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    gradients = &#123;<span class="string">"dxt"</span>: dxt, <span class="string">"da_prev"</span>: da_prev, <span class="string">"dWax"</span>: dWax, <span class="string">"dWaa"</span>: dWaa, <span class="string">"dba"</span>: dba&#125;</span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure>
<p><strong>Backward pass through the RNN</strong></p>
<p>好难，先占坑。</p>
<p>Computing the gradients of the cost with respect to a&lt;\t&gt; at every time-step t is useful because it is what helps the gradient backpropagate to the previous RNN-cell To do so, you need to iterate through all the time steps starting at the end, and at each step, you increment the overall dba, dWaa, dWax and you store dx.</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span><span class="params">(da, caches)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward pass for a RNN over an entire sequence of input data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    da -- Upstream gradients of all hidden states, of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- tuple containing information from the forward pass (rnn_forward)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dx -- Gradient w.r.t. the input data, numpy-array of shape (n_x, m, T_x)</span></span><br><span class="line"><span class="string">                        da0 -- Gradient w.r.t the initial hidden state, numpy-array of shape (n_a, m)</span></span><br><span class="line"><span class="string">                        dWax -- Gradient w.r.t the input's weight matrix, numpy-array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        dWaa -- Gradient w.r.t the hidden state's weight matrix, numpy-arrayof shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        dba -- Gradient w.r.t the bias, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Retrieve values from the first cache (t=1) of caches (≈2 lines)</span></span><br><span class="line">   </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from da's and x1's shapes (≈2 lines)</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize the gradients with the right sizes (≈6 lines)</span></span><br></pre></td></tr></table></figure>
<h3 id="3-Forward-propagation-of-Long-Short-Term-Memory-LSTM-network"><a href="#3-Forward-propagation-of-Long-Short-Term-Memory-LSTM-network" class="headerlink" title="3- Forward propagation of Long Short-Term Memory(LSTM) network"></a>3- Forward propagation of Long Short-Term Memory(LSTM) network</h3><p>This following figure shows the operations of an LSTM-cell.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-faa16612756bff66.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>Similar to the RNN example above, you will start by implementing the LSTM cell for a single time-step. Then you can iteratively call it from inside a for-loop to have it process an input with T_x time-steps.</p>
<p><strong>About the gates</strong></p>
<ul>
<li><p>Forget gate: For the sake of this illustration, lets assume we are reading words in a piece of text, and want use an LSTM to keep track of grammatical structures, such as whether the subject is singular or plural. If the subject changes from a singular word to a plural word, we need to find a way to get rid of our previously stored memory value of the singular/plural state. In an LSTM, the forget gate lets us do this:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d5af764c972ec343.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>Here,  Wf are weights that govern the forget gate’s behavior. We concatenate [a⟨t−1⟩,x⟨t⟩] and multiply by Wf. The equation above results in a vector  Γ⟨t⟩f with values between 0 and 1. This forget gate vector will be multiplied element-wise by the previous cell state c⟨t−1⟩. So if one of the values of  Γ⟨t⟩fΓf⟨t⟩  is 0 (or close to 0) then it means that the LSTM should remove that piece of information (e.g. the singular subject) in the corresponding component of  c⟨t−1⟩. If one of the values is 1, then it will keep the information.</p>
</li>
<li><p>Update gate: Once we forget that the subject being discussed is singular, we need to find a way to update it to reflect that the new subject is now plural. Here is the formula for the update gate:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-58b5ff406da60509.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
</li>
<li><p>Updating the cell: To update the new subject we need to create a new vector of numbers that we can add to our previous cell state. The equation we use is:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-5eeabc165db7889c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
</li>
<li><p>Output gate: To decide which outputs we will use, we will use the following two formulas:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-3ae9380d4a619dbc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
</li>
</ul>
<h4 id="3-1-LSTM-cell"><a href="#3-1-LSTM-cell" class="headerlink" title="3.1- LSTM cell"></a>3.1- LSTM cell</h4><p>Exercise: Implement the LSTM cell described in the Figure (3).</p>
<p>Instructions:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-002ba438ea40c34b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>直接按照公式计算即可，注意concat（矩阵拼接）的写法。可以用np.vstack((a,b))，表示竖直拼接。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_cell_forward</span><span class="params">(xt, a_prev, c_prev, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement a single forward step of the LSTM-cell as described in Figure (4)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    xt -- your input data at timestep "t", numpy array of shape (n_x, m).</span></span><br><span class="line"><span class="string">    a_prev -- Hidden state at timestep "t-1", numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">    c_prev -- Memory state at timestep "t-1", numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wi -- Weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bi -- Bias of the save gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wc -- Weight matrix of the first "tanh", numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bc --  Bias of the first "tanh", numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wo -- Weight matrix of the focus gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bo --  Bias of the focus gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">                        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a_next -- next hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    c_next -- next memory state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    yt_pred -- prediction at timestep "t", numpy array of shape (n_y, m)</span></span><br><span class="line"><span class="string">    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilda),</span></span><br><span class="line"><span class="string">          c stands for the memory value</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve parameters from "parameters"</span></span><br><span class="line">    Wf = parameters[<span class="string">"Wf"</span>]</span><br><span class="line">    bf = parameters[<span class="string">"bf"</span>]</span><br><span class="line">    Wi = parameters[<span class="string">"Wi"</span>]</span><br><span class="line">    bi = parameters[<span class="string">"bi"</span>]</span><br><span class="line">    Wc = parameters[<span class="string">"Wc"</span>]</span><br><span class="line">    bc = parameters[<span class="string">"bc"</span>]</span><br><span class="line">    Wo = parameters[<span class="string">"Wo"</span>]</span><br><span class="line">    bo = parameters[<span class="string">"bo"</span>]</span><br><span class="line">    Wy = parameters[<span class="string">"Wy"</span>]</span><br><span class="line">    by = parameters[<span class="string">"by"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of xt and Wy</span></span><br><span class="line">    n_x, m = xt.shape</span><br><span class="line">    n_y, n_a = Wy.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Concatenate a_prev and xt (≈3 lines)</span></span><br><span class="line">    <span class="comment"># concat = np.zeros([n_x+n_a, m])</span></span><br><span class="line">    <span class="comment"># concat[:n_a, :] = a_prev</span></span><br><span class="line">    <span class="comment"># concat[n_a:, :] = xt</span></span><br><span class="line">    concat = np.vstack((a_prev, xt))</span><br><span class="line">    <span class="comment"># Compute values for ft, it, cct, c_next, ot, a_next using the formulas given figure (4) (≈6 lines)</span></span><br><span class="line">    ft = sigmoid(np.dot(Wf, concat) + bf)</span><br><span class="line">    it = sigmoid(np.dot(Wi, concat) + bi)</span><br><span class="line">    cct = np.tanh(np.dot(Wc, concat) + bc)</span><br><span class="line">    c_next = ft*c_prev + it*cct</span><br><span class="line">    ot = sigmoid(np.dot(Wo, concat) + bo)</span><br><span class="line">    a_next = ot * np.tanh(c_next)</span><br><span class="line">    <span class="comment"># Compute prediction of the LSTM cell (≈1 line)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a_next, c_next, yt_pred, cache</span><br></pre></td></tr></table></figure>
<h4 id="3-2-Forward-pass-for-LSTM"><a href="#3-2-Forward-pass-for-LSTM" class="headerlink" title="3.2- Forward pass for LSTM"></a>3.2- Forward pass for LSTM</h4><p>Now that you have implemented one step of an LSTM, you can now iterate this over this using a for-loop to process a sequence of Tx inputs.</p>
<p><img src="https://upload-images.jianshu.io/upload_images/8636110-53b7e99a279cda10.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_forward</span><span class="params">(x, a0, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the forward propagation of the recurrent neural network using an LSTM-cell described in Figure (3).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- Input data for every time-step, of shape (n_x, m, T_x).</span></span><br><span class="line"><span class="string">    a0 -- Initial hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wi -- Weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bi -- Bias of the save gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wc -- Weight matrix of the first "tanh", numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bc -- Bias of the first "tanh", numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wo -- Weight matrix of the focus gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bo -- Bias of the focus gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">                        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Initialize "caches", which will track the list of all the caches</span></span><br><span class="line">    caches = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of xt and Wy (≈2 lines)</span></span><br><span class="line">    n_x, m, T_x = x.shape</span><br><span class="line">    n_y, n_a = parameters[<span class="string">"Wy"</span>].shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize "a", "c" and "y" with zeros (≈3 lines)</span></span><br><span class="line">    a = np.zeros((n_a, m, T_x))</span><br><span class="line">    c = np.zeros((n_a, m, T_x))</span><br><span class="line">    y = np.zeros((n_y, m, T_x))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize a_next and c_next (≈2 lines)</span></span><br><span class="line">    a_next = a0</span><br><span class="line">    c_next = np.zeros((n_a, m))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop over all time-steps</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T_x):</span><br><span class="line">        <span class="comment"># Update next hidden state, next memory state, compute the prediction, get the cache (≈1 line)</span></span><br><span class="line">        a_next, c_next, yt_pred, cache = lstm_cell_forward(x[:,:,t],a_next,c_next,parameters)</span><br><span class="line">        <span class="comment"># Save the value of the new "next" hidden state in a (≈1 line)</span></span><br><span class="line">        a[:,:,t] = a_next</span><br><span class="line">        <span class="comment"># Save the value of the prediction in y (≈1 line)</span></span><br><span class="line">        y[:,:,t] = yt_pred</span><br><span class="line">        <span class="comment"># Save the value of the next cell state (≈1 line)</span></span><br><span class="line">        c[:,:,t] = c_next</span><br><span class="line">        <span class="comment"># Append the cache into caches (≈1 line)</span></span><br><span class="line">        caches.append(cache)</span><br><span class="line">        </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    caches = (caches, x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a, y, c, caches</span><br></pre></td></tr></table></figure>
<h3 id="4-Backward-propagation-for-LSTM"><a href="#4-Backward-propagation-for-LSTM" class="headerlink" title="4- Backward propagation for LSTM"></a>4- Backward propagation for LSTM</h3><p>占坑。</p>
<h2 id="Character-level-language-model-Dinosaurus-land"><a href="#Character-level-language-model-Dinosaurus-land" class="headerlink" title="Character level language model - Dinosaurus land"></a>Character level language model - Dinosaurus land</h2>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/python/" rel="tag"><i class="fa fa-tag"></i> python</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/23/deep-learningwx/" rel="next" title="占坑-目标检测">
                <i class="fa fa-chevron-left"></i> 占坑-目标检测
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/04/27/deep-learningw13/" rel="prev" title="第13周-自然语言处理与词嵌入">
                第13周-自然语言处理与词嵌入 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="DesmonDay" />
          <p class="site-author-name" itemprop="name">DesmonDay</p>
           
              <p class="site-description motion-element" itemprop="description">主攻方向：NLP</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">120</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">14</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">12</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/DesmonDay" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#为什么用序列模型：Why-sequence-models"><span class="nav-number">1.</span> <span class="nav-text">为什么用序列模型：Why sequence models?</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#数学符号：Notation"><span class="nav-number">2.</span> <span class="nav-text">数学符号：Notation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Motivation-Example"><span class="nav-number">2.1.</span> <span class="nav-text">Motivation Example</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Representing-words"><span class="nav-number">2.2.</span> <span class="nav-text">Representing words</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#循环神经网络模型"><span class="nav-number">3.</span> <span class="nav-text">循环神经网络模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Why-not-a-standard-network"><span class="nav-number">3.1.</span> <span class="nav-text">Why not a standard network?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Recurrent-Neural-Network"><span class="nav-number">3.2.</span> <span class="nav-text">Recurrent Neural Network</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Forward-Propagation"><span class="nav-number">3.3.</span> <span class="nav-text">Forward Propagation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Simplified-RNN-notation"><span class="nav-number">3.4.</span> <span class="nav-text">Simplified RNN notation</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Back-propagation-through-time"><span class="nav-number">4.</span> <span class="nav-text">Back propagation through time</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#不同类型的RNN"><span class="nav-number">5.</span> <span class="nav-text">不同类型的RNN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Examples-of-RNN-architectures"><span class="nav-number">5.1.</span> <span class="nav-text">Examples of RNN architectures</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Summary-of-RNN"><span class="nav-number">5.2.</span> <span class="nav-text">Summary of RNN</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#语言模型和序列生成"><span class="nav-number">6.</span> <span class="nav-text">语言模型和序列生成</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Language-modeling-with-an-RNN"><span class="nav-number">6.1.</span> <span class="nav-text">Language modeling with an RNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN-model"><span class="nav-number">6.2.</span> <span class="nav-text">RNN model</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#对新序列采样：sampling-novel-sequences"><span class="nav-number">7.</span> <span class="nav-text">对新序列采样：sampling novel sequences</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Sampling-a-sequence-from-a-trained-RNN"><span class="nav-number">7.1.</span> <span class="nav-text">Sampling a sequence from a trained RNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Character-level-language-model"><span class="nav-number">7.2.</span> <span class="nav-text">Character-level language model</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Vanishing-gradients-with-RNNs"><span class="nav-number">8.</span> <span class="nav-text">Vanishing gradients with RNNs</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#GRU-Gated-Recurrent-Unit"><span class="nav-number">9.</span> <span class="nav-text">GRU: Gated Recurrent Unit</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#RNN-unit"><span class="nav-number">9.1.</span> <span class="nav-text">RNN unit</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GRU-simplified"><span class="nav-number">9.2.</span> <span class="nav-text">GRU(simplified)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Full-GRU"><span class="nav-number">9.3.</span> <span class="nav-text">Full GRU</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考其他博客的解释"><span class="nav-number">9.4.</span> <span class="nav-text">参考其他博客的解释</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#长短时记忆网络：LSTM"><span class="nav-number">10.</span> <span class="nav-text">长短时记忆网络：LSTM</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM反向传播推导-未验证"><span class="nav-number">10.1.</span> <span class="nav-text">LSTM反向传播推导(未验证)</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#双向RNN：Bidirectional-RNN-BRNN"><span class="nav-number">11.</span> <span class="nav-text">双向RNN：Bidirectional RNN(BRNN)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#深层循环神经网络-Deep-RNNs"><span class="nav-number">12.</span> <span class="nav-text">深层循环神经网络: Deep RNNs</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#本周作业"><span class="nav-number">13.</span> <span class="nav-text">本周作业</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Building-your-Recurrent-Neural-Network-Step-by-Step"><span class="nav-number">13.1.</span> <span class="nav-text">Building your Recurrent Neural Network - Step by Step</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Forward-propagation-for-the-basic-Recurrent-Neural-Network"><span class="nav-number">13.1.1.</span> <span class="nav-text">1-Forward propagation for the basic Recurrent Neural Network</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-RNN-cell"><span class="nav-number">13.1.1.1.</span> <span class="nav-text">1.1- RNN cell</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-RNN-forward-pass"><span class="nav-number">13.1.1.2.</span> <span class="nav-text">1.2- RNN forward pass</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-backward-propagation-for-the-basic-Recurrent-Neural-Network"><span class="nav-number">13.1.2.</span> <span class="nav-text">2-backward propagation for the basic Recurrent Neural Network</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-Basic-RNN-backward-pass"><span class="nav-number">13.1.2.1.</span> <span class="nav-text">2.1- Basic RNN backward pass</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Forward-propagation-of-Long-Short-Term-Memory-LSTM-network"><span class="nav-number">13.1.3.</span> <span class="nav-text">3- Forward propagation of Long Short-Term Memory(LSTM) network</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-LSTM-cell"><span class="nav-number">13.1.3.1.</span> <span class="nav-text">3.1- LSTM cell</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-Forward-pass-for-LSTM"><span class="nav-number">13.1.3.2.</span> <span class="nav-text">3.2- Forward pass for LSTM</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-Backward-propagation-for-LSTM"><span class="nav-number">13.1.4.</span> <span class="nav-text">4- Backward propagation for LSTM</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Character-level-language-model-Dinosaurus-land"><span class="nav-number">13.2.</span> <span class="nav-text">Character level language model - Dinosaurus land</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">DesmonDay</span>
</div>



<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv"></span>
</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->



  


  




	





  





  






  





  

  

  

  

  

  

</body>
</html>


