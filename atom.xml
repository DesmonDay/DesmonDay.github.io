<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>DesmonDay&#39;s Blog</title>
  
  <subtitle>一只小辣鸡的自我拯救之路</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://github.com/DesmonDay/"/>
  <updated>2019-04-15T12:49:09.194Z</updated>
  <id>https://github.com/DesmonDay/</id>
  
  <author>
    <name>DesmonDay</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Word Embedding教程</title>
    <link href="https://github.com/DesmonDay/2019/04/15/NLP%E5%AD%A6%E4%B9%A01/"/>
    <id>https://github.com/DesmonDay/2019/04/15/NLP学习1/</id>
    <published>2019-04-15T07:26:13.000Z</published>
    <updated>2019-04-15T12:49:09.194Z</updated>
    
    <content type="html"><![CDATA[<p>参考李理的免费书籍《深度学习理论与实战：提高篇》进行学习，这里记录一些比较重要的笔记。注：本博客记录仅用于个人学习。原博客地址：<a href="https://fancyerii.github.io/2019/03/14/dl-book/" target="_blank" rel="noopener">https://fancyerii.github.io/2019/03/14/dl-book/</a> </p><h1 id="词的表示方法"><a href="#词的表示方法" class="headerlink" title="词的表示方法"></a>词的表示方法</h1><p>不同于更底层的图像和声音信号，语言是高度抽象的离散符号系统。为了能够使用神经网络来解决NLP任务，几乎所有的深度学习模型都会在第一步把离散的符号变成向量。我们希望把一个词映射到“语义”空间的一个点，使得相似的词的距离较近而不相似的较远。为此，通常用向量来表示一个点，该向量称为词向量。</p><h1 id="one-hot向量"><a href="#one-hot向量" class="headerlink" title="one-hot向量"></a>one-hot向量</h1><p>最简单的表示方法为one-hot。假设我们的词典大小为4，即一共有4个词（实际上为成千上万）。则词典中的每个词对应一个下标，每个词都用长度为4的向量表示，只有对应的下标为1，其余为0。如下图，第一个词是[1,0,0,0]，而第三个词是[0,0,1,0]。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-228b8c0619b4b24b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>然而，one-hot向量表示的问题是不满足我们的期望——相似的词的距离较近而不相似的较远。对于one-hot向量来说，相同的词距离是0，而不同的词距离是1，显然不正确。比如apple和pear的距离显然要比apple和cat的距离要近，但在one-hot表示里apple和其他所有词的距离都是1。</p><p>另外，one-hot也是一个高维的稀疏向量。而我们希望用一个低维的稠密向量来表示一个词，并且希望每一维都是表示某种语义，比如第一维代表水果（假设），那么apple和pear在这一维的值比较大，而cat的值比较小。这样apple和pear的距离就比cat和apple的距离要近。</p><h1 id="神经网络语言模型"><a href="#神经网络语言模型" class="headerlink" title="神经网络语言模型"></a>神经网络语言模型</h1><p>如何学习到比较好的词向量？最早的词向量可以追溯到神经网络语言模型。首先需要了解语言模型的概念和传统的基于统计得N-gram语言模型。</p><p>给定词序列$w_1,…,w_k$，语言模型会计算这个序列的概率，根据条件概率的定义，我们可以把联合概率分解为如下的条件概率：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-4e42aec1a0f47495.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>实际的语言模型很难考虑特别长的历史，通常我们会限定当前词的概率值依赖于之前的N-1个词，这就是所谓的N-Gram语言模型：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-2e89b69d1f069aa0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>在实际的应用中N的取值通常是2-5。</p><p>我们通常用困惑度（Perplexity）来衡量语言模型的好坏，其基本思想是：给测试集的句子赋予较高的概率值的语言模型较好，当语言模型训练完之后，测试集中的句子都是正常的句子，那么训练好的模型就是在测试集上的概率越高越好，公式如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-2b6cbebe6bffad2b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>由公式可知，句子概率越大，语言模型越好，困惑度越小。</p><p>而在语言模型的训练中，通常采用Perplexity的对数表达形式：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-0a55e734cde036d7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>N-gram语言模型可以通过最大似然方法来估计参数，假设$C(w_{k-2}w_{k-1}w_k)$表示3个词$w_{k-2}w_{k-1}w_{k}$连续出现在一起的次数，类似的$C(w_{k-2}w_{k-1})$表示两个词$w_{k-2}w_{k-1}$连续出现在一起的次数，那么：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e656b9ff23603a06.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>最大似然估计的最大问题是数据的稀疏性，如果3个词没有在训练数据中一起出现过，那么概率为0，但不在训练数据里出现不代表它不是合理的句子。实际我们一般会使用Discount和Backoff等平滑方法来改进最大似然估计。Discount的意思就是把一些高频N-gram的概率分配给从没有出现过的N-gram，Backoff是指如果N-gram没有出现过，那我们就使用(N-1)-gram来估计。比如Katz平滑方法公式如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-257a612981f9daf5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>其中，$d$是折扣系数，$\alpha (w_{k-1},w_{k-2})$是回退系数，而$C’$是一个阈值。当$C(w_{k-2}w_{k-1}w_k)$的出现频次高于阈值，则折扣系数为1；当其低于阈值时，则对其概率用$d$进行打折，而剩下的一部分概率则经过回退分配到那些没有在训练数据中出现过的$w_{k-2}w_{k-1}w_k$中。</p><p>接下来参考(<a href="http://blog.pluskid.org/?p=361" target="_blank" rel="noopener">http://blog.pluskid.org/?p=361</a>) ：折扣系数$d$是由我们定的，在Katz平滑中是根据出现次数$r$来选择系数$d$的。对于$r\in [1,C’]$，令$d_r = \frac{(r+1)n_{r+1}}{rn_r}$，其中$n_r$表示出现次数为$r$的trigram的个数，$n_{r+1}$类推。</p><p>而回退系数则要通过概率和等于1这样的一个等式来计算，具体来说，我们有<br><img src="https://upload-images.jianshu.io/upload_images/8636110-73141b42e44f8685.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>于是，<br><img src="https://upload-images.jianshu.io/upload_images/8636110-763777484ee23424.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>N-gram语言模型有两个比较大的问题：</p><ol><li>N不能太大，否则需要存储的N-gram太多，因此它无法考虑长距离的依赖。比如”I grew up in France… I speak fluent_”，我们想猜测fluent后面哪个词的可能性大。如果只看”speak fluent”，那么French, English和Chinese的概率都是一样大，但是通过前面的”I grew up in Frence”，我们可以知道French的概率要大得多。这个问题会通过之后的RNN/LSTM/GRU等模型来进行一定程度的解决。</li><li>泛化能力差，因为它完全基于词的共现。比如训练数据中有“我在北京”，但没有“我在上海”，那么$p(上海|在)$的概率就会比$p(北京|在)$小很多。但实际上上海和北京作为地名，都可以出现在“在”的后面。这个问题和one-hot问题类似，原因在于我们把北京和上海当成了完全不同的东西，但我们希望它们是类似的。</li></ol><p>通常，把一个词表示成一个低维稠密的向量能够解决这个问题，通过上下文，模型能够知道北京和上海经常出现在相似的上下文里，因此模型能用相似的向量来表示这两个不同的词。神经网络表示如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-832759817fc69b74.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这个模型的输入是当前要预测的词，比如用前两个词预测当前词$w_t$。模型首先用lookup table把一个词变成一个向量，然后把这两个词的向量拼接成一个大的向量，输入神经网络，最后使用softmax输出预测每个词的概率。</p><p>Lookup table等价于one-hot向量乘以Embedding矩阵。假设我们有3个词，词向量的维度是5维，那么Embedding矩阵就是(3, 5)的矩阵，比如：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-daa42fea9634ffc9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>这个矩阵的每一行表示一个词的词向量，那么我们要获得第二个词的词向量，可用如下矩阵乘法来提取：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e8c63668351c9f38.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>但是这样的实现并不高效，我们只需要”复制”第二行就可以了，因此大部分深度学习框架都提供了Lookup table的操作，用于从一个矩阵中提取某一行或者某一列。这个Embedding矩阵不是固定的，它也是神经网络的参数之一。通过语言模型的学习，我们就可以得到这个Embedding矩阵，从而得到词向量。</p><h1 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h1><p>我们可以使用语言模型（或者其他任务比如机器翻译）来获得词向量，但是语言模型的训练很慢（机器翻译则更慢，而且还需要监督的标注数据），因此说词向量是这些任务的副产品。而Mikolov等人提出Word2Vec的方法，其就是直接用于训练词向量，而且速度更快。</p><p>Word2Vec的基本思想就是分布假设(distributional hypothesis)：如果两个词的上下文相似，那么这两个词的语义就相似。上下文有很多粒度，比如文档的粒度，也就是一个词的上下文是所有与它出现在同一个文档中的词，或者是较细的粒度，比如当前词前后固定大小的窗口。如下图所示，written的上下文是前后两个词，即”Potter is by J.K.”这4个词。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-56a32e835303f660.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>除了我们即将介绍的Word2Vec，还有很多其它方法也可以利用上述假设学习词向量。所有通过Distributional假设学习到的(向量)表示都叫做Distributional表示(Representation)。</p><p>注意，还有一个很像的属于叫分布表示(distributed representation)。它指的是用稠密的低维向量来表示一个词的语义，即把语义“分散”到不同的维度上。与之相对的是one-hot表示，它的语义集中在高维的稀疏的某一维上。</p><p>这里省略Word2Vec的介绍，因为我有其他的补充材料可查看，就不多写了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;参考李理的免费书籍《深度学习理论与实战：提高篇》进行学习，这里记录一些比较重要的笔记。注：本博客记录仅用于个人学习。原博客地址：&lt;a href=&quot;https://fancyerii.github.io/2019/03/14/dl-book/&quot; target=&quot;_blank&quot;
      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/DesmonDay/categories/NLP/"/>
    
    
      <category term="python" scheme="https://github.com/DesmonDay/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>第6周-优化算法</title>
    <link href="https://github.com/DesmonDay/2019/04/14/deep-learningw6/"/>
    <id>https://github.com/DesmonDay/2019/04/14/deep-learningw6/</id>
    <published>2019-04-14T08:34:21.000Z</published>
    <updated>2019-04-16T00:27:22.053Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Mini-batch梯度下降"><a href="#Mini-batch梯度下降" class="headerlink" title="Mini-batch梯度下降"></a>Mini-batch梯度下降</h1><p>如果针对整个数据集进行梯度下降，如果数据集很大，那么速度就会很慢。因此，我们可以对训练集中的一部分进行梯度下降。因此可以将训练集分为多个mini-batch，针对每个mini-batch进行梯度下降。从图中，我们看到Batch vs Mini-Batch，这里对比的就是我们原本的针对整个训练集进行梯度下降，以及选取mini-batch进行梯度下降的两种算法的对比。另外，针对每个mini-batch，其标记符号为X^{1}…X^{t}.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b90b350915d8f58d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Mini-batch梯度下降的实现过程：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-48eeaa69726e431c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>从图中可知，我们在一次for循环中遍历所有的mini-batch样本，每次的计算仅针对X^{t}和Y^{t}来进行，整个的for循环称为一次epoch。如果我们要进行多次epoch，可以再添加一个针对epoch的for循环。</p><h1 id="理解Mini-batch梯度下降"><a href="#理解Mini-batch梯度下降" class="headerlink" title="理解Mini-batch梯度下降"></a>理解Mini-batch梯度下降</h1><p><img src="https://upload-images.jianshu.io/upload_images/8636110-2fb5165da85d56d2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>因此我们需要决定mini-batch的大小，一般有如下三种情况：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7a03ffbba9277896.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>三种选择下的不同梯度下降情况如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-6ca0ee33f2fc956d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ol><li>随机梯度下降：失去了向量化的速度优势，且梯度来回震荡剧烈；</li><li>Mini-Batch: 学习速度最快，并且不需要处理整个数据集；</li><li>Batch: 每一次迭代时间过长，需要处理整个数据集。</li></ol><p>那么如何选择合适的位于1与m之间的mini-batch大小呢？</p><ol><li>当数据集较小，如m&lt;=2000，则使用Batch梯度下降</li><li>当数据集较大，通常选择：64,128,256,512，1024等等（2的次数）（通常这里需要多加尝试，选择合适的值）</li><li>确保mini batch的大小在可用内存大小的范围内。</li></ol><h1 id="指数加权平均"><a href="#指数加权平均" class="headerlink" title="指数加权平均"></a>指数加权平均</h1><p>首先，我们需要知道平均数求法，即比如我们现在有100天的温度值，要求这100天的平均温度值，那么我们可以直接利用公式进行计算：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-21e01bea913eb11f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>何为指数加权平均，如何计算指数加权平均，下面给了一个关于温度的例子，可以看到得到的红线相比原来的蓝点要平滑许多。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-4d85231e7f47c235.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>改变$\beta$的作用，导致我们得到的指数加权平均数的不同：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c0c6d59de3f109ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>可以看到，当$\beta$越大，得到的曲线越平缓，但是却不能很好反映温度的变化；$\beta$越小，则曲线越接近温度的变化，但这样子得到的噪声或异常值也会有很多。因此，合适的$\beta$值，也会影响着算法的表现。</p><h1 id="理解指数加权平均"><a href="#理解指数加权平均" class="headerlink" title="理解指数加权平均"></a>理解指数加权平均</h1><p>指数加权平均是几种优化算法的关键步骤，而其实质上是将每日温度与指数衰减函数相乘，然后求和。形象化的图形解释如下图：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-4a5ff2b6a6d30d45.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>另外，按照粗略的计算，当$\beta=0.9$时，相当于计算了10天的平均温度，当$\beta=0.98$时，相当于计算了50天的平均温度。</p><p>实际的实现：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d4bb770fc95e9dc4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>可以看到实现很简单，并且内存占用也很少，因此其效率也很高。尽管这不是最精确的计算，因为最精确的计算时直接计算温度的平均值，这往往会得到更好的预测，但这种做法的缺点是如果保存所有最近的温度数据和过去的天数温度，这会占用很多内存。因此在深度学习中往往使用指数加权平均数来计算。</p><h1 id="指数加权平均的偏差修正-Bias-correction-in-exponentially-weighted-average"><a href="#指数加权平均的偏差修正-Bias-correction-in-exponentially-weighted-average" class="headerlink" title="指数加权平均的偏差修正-Bias correction in exponentially weighted average"></a>指数加权平均的偏差修正-Bias correction in exponentially weighted average</h1><p>有一种偏差修正的技巧，能够使我们通过指数加权平均计算的平均数更加准确。如下图，当$\beta=0.9$时，我们得到红线；当$\beta=0.98$时，如果我们直接使用上面所写的原公式计算，即$v_t = \beta v_{t-1} + (1-\beta)\theta_t$，我们实际上得到的是紫线，而其实更合理的是绿色线。可以看到紫色曲线的起点较低，因此我们需要处理这种情况。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-859ebd4c5aa72ba2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>举原公式的一个例子，我们设置$v_0$初值为0，所以计算$v_1$时，由于$v_0=0$，而且我们第一天的温度为40的话，那么计算出来的$v_1$实际上很小，不符合实际情况，继续计算下去的结果也均会偏小。因此我们可以有方法来让估测更好，特别是在估测初期。我们将估计的值不再表示为$v_t$，而是表示为$\frac{v_t}{1-\beta ^t}$。因此当t比较小时，分母较小，我们可以消除偏差；当t比较大时，分母接近1，则还原为原式。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-befc97b423e2f524.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="动量梯度下降-Gradient-descent-with-momentum"><a href="#动量梯度下降-Gradient-descent-with-momentum" class="headerlink" title="动量梯度下降-Gradient descent with momentum"></a>动量梯度下降-Gradient descent with momentum</h1><p>有一种叫做Momentum的优化算法，它的运行速度总是比标准的梯度下降算法要快。其简单的思想就是，计算梯度的指数加权平均数，并利用该梯度来更新权重。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-985686082610c848.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>假设我们要对上图的成本函数J做梯度下降，其中红点为最优点，蓝点为我们的起点。如果我们采用梯度下降法，那么整个过程就如下图的蓝线，其频繁的上下波动明显减慢了梯度下降法的速度。为了避免大幅度的波动（紫线），我们也需要采用一个较小的学习率。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-61f572480ad43f6a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>因此，我们可以采用动量梯度下降的方法，其对应的步骤如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-dbd732b1528e2151.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>我们可以形象地把整个过程当做一个从山上滚下来的球的加速运动，其中，以$V_{db}$的计算式为例，$d_b$起到了加速度的作用，而原来的$V_{db}$则表示速度，$\beta$则表现为摩擦力，保证了球不会无限加速下去。在这个例子中，球获得了动量，因此也称为动量梯度下降。</p><p>由上图算法步骤，我们可知超参数包括学习率$\alpha$和指数加权参数$\beta$，而$\beta$最常用的值为0.9（在梯度下降中则表示平均了前十次迭代的梯度，另外0.9是很棒的鲁棒数）。在实际中，我们在使用梯度下降或M<br>omentum的时候，通常不需要考虑偏差修正。另外，$db$和$dW$最开始初始化为零向量。另外，$v_{dW}=\beta v_{dw}+(1-\beta)dW$可以写成另一种算法$v_{dW}=\beta v_{dw}+dW$，即去掉了$1-\beta$。但大多数情况下，原有的表达形式更方便，效果也更好一点。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Mini-batch梯度下降&quot;&gt;&lt;a href=&quot;#Mini-batch梯度下降&quot; class=&quot;headerlink&quot; title=&quot;Mini-batch梯度下降&quot;&gt;&lt;/a&gt;Mini-batch梯度下降&lt;/h1&gt;&lt;p&gt;如果针对整个数据集进行梯度下降，如果数据集很
      
    
    </summary>
    
      <category term="深度学习" scheme="https://github.com/DesmonDay/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://github.com/DesmonDay/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>第5周-深度学习的实用层面</title>
    <link href="https://github.com/DesmonDay/2019/04/14/deep-learningw5/"/>
    <id>https://github.com/DesmonDay/2019/04/14/deep-learningw5/</id>
    <published>2019-04-14T06:05:22.000Z</published>
    <updated>2019-04-14T06:37:14.420Z</updated>
    
    <content type="html"><![CDATA[<h1 id="训练-验证-测试集"><a href="#训练-验证-测试集" class="headerlink" title="训练/验证/测试集"></a>训练/验证/测试集</h1><p>在实际构建和训练深层神经网络的时候，我们往往要确定一些超参数，如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-713247709c45087d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在实际应用中，对数据集进行划分为训练集、验证集、测试集可以加速神经网络的集成，也可以更有效地衡量算法的偏差和方差，从而帮助我们更高效地选择合适的方法来优化算法。图中展示了在数据样本较少时，我们将其划分比例为70%-30%或60%-20%-20%；当大数据时，则验证集和测试集的比例往往会很小。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-75b51a7281b1f656.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>当然，我们有时候会面临训练集和测试集分布不均衡的情况。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-efce84cda9a9e303.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如上图举例，我们的训练集来自网页上的图片，可能都是很高清的，而测试集和验证集的图片都来自用户自己拍摄的，很显然图片的分布就不同。面对这种情况，我们需要遵循的一个准则就是：Make sure dev and test set come from same distribution.</p><p>另外，实际应用中也可以不需要测试集，只使用训练集和验证集（有些人可能就会称作训练集和测试集）。</p><h1 id="偏差-方差-Bias-Variance"><a href="#偏差-方差-Bias-Variance" class="headerlink" title="偏差/方差 - Bias/Variance"></a>偏差/方差 - Bias/Variance</h1><p>我们可以通过训练集和验证集上的误差来确定算法的偏差和方差的高低情况，再根据具体的情况，如高偏差高方差、低偏差高方差等来判断下一步应该如何进行算法的优化。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ba80c1f989a721b5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>下图的猫分类的偏差方差衡量的前提有两个；1. 基本误差很小，即Optimal(Bayes) error很小；2. train和dev set的分布相同。然而，我们首先查看训练集误差，若误差大，说明高偏差，反之为低偏差；再看验证集误差，若误差大，则高方差，反之为低方差。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-bb4f07d38e066e76.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>接着，吴老师又举了一个高偏差和高方差的具体例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f396c27fd87c080f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们可以看到，由于这个分类器基本是线性拟合，所以其拟合程度低，说明为高偏差；但同时，它也有着过拟合的部分，如图的两个曲折部分，因此也具有高方差。在高维数据分类中，这种情况非常常见：有些区域偏差高，有些区域方差高。</p><h1 id="机器学习基础-Basic-Recipie-for-ML"><a href="#机器学习基础-Basic-Recipie-for-ML" class="headerlink" title="机器学习基础 - Basic Recipie for ML"></a>机器学习基础 - Basic Recipie for ML</h1><p>一般，我们会先查看是否High bias(training set performance)? 如果是，则有以下方法来进行调整：训练更深更大的神经网络、训练更长时间、或者修改我们的神经网络结构。最后达到至少能够拟合训练数据的目的。</p><p>接着，我们查看是否Hign variance(dev set performance)? 如果是，则有以下方法：获得更多的数据、正则化、修改神经网络结构。</p><p>当偏差和方差都比较小时，则我们的工作也基本完成。在以前，我们会特别注重”Bias Variance trade-off”，但是如今的深度学习并不需要特别看重这个问题，因为我们其实可以再减少偏差的同时不损害方差（利用正则化之类的方法）。</p><h1 id="正则化-Regularization"><a href="#正则化-Regularization" class="headerlink" title="正则化 - Regularization"></a>正则化 - Regularization</h1><p>当我们出现过拟合的结果，则高方差，那么我们第一个想到的方法应该是正则化。另一个方法则是得到更多的数据，但这往往比较难。而正则化通常可以避免过拟合。</p><p>以Logistic回归为例，我们可以在图中看到L2-范数和L1-范数的写法，其中lambda为正则化参数，而L2-范数相比L1-范数而言更为常用。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-784b3faf89a9b5ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>对神经网络而言，我们不再称为L2-范数，而称为Frobenius Norm。因为这里其范数的计算方式为一个矩阵中所有元素的平方和。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-3e53a9bda80463ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>另外，我们可以注意到，由于我们的cost函数发生变化，则多了正则项，那么dW的计算方式也有了变化。具体计算公式如图所示，并且我们可以发现，加了正则化项后，我们的参数W也是往减少的趋势走，我们称其为”Weight Decay”。</p><h2 id="为什么正则化可以减少过拟合"><a href="#为什么正则化可以减少过拟合" class="headerlink" title="为什么正则化可以减少过拟合"></a>为什么正则化可以减少过拟合</h2><p>这里吴老师给了两个比较直观的解释。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-a15998e3aef2b7cb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如图，当我们将正则化参数lambda设置得足够大，那么W会接近0，从而使得很多隐藏单元的影响变小了，因此网络结构也变得简单，从而可以减少过拟合。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-38ecfcc88978ab40.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>又如上图，如果我们的lambda增大，那么W会较小，从而Z也会减小。如果Z的范围小，则tanh激活函数近似于线性函数，从而使得神经网络类似于线性神经网络，因此可以减小过拟合。</p><h2 id="Dropout-Regularization"><a href="#Dropout-Regularization" class="headerlink" title="Dropout Regularization"></a>Dropout Regularization</h2><p>此部分参考总结：<a href="https://blog.csdn.net/u012328159/article/details/80210363" target="_blank" rel="noopener">https://blog.csdn.net/u012328159/article/details/80210363</a></p><p>为了防止过拟合的问题，我们最常使用的手段就是L2正则化，即在代价函数后面加一个L2正则项。dropout正则化是Srivastava在2014年提出来的：Dropout: A Simple Way to Prevent Neural Networks from Overfitting。Dropout的思想其实非常简单粗暴：对于网络的每一层，随机的丢弃一些单元。如下图所示:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b620859b7502897a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如何实现正则化，吴老师举了Inverted dropout的例子来进行阐述：从技术实现方面来看下dropout正则项，这里最重要的一个参数就是keep_prob，称作保留概率（同样，1−keep_prob1−keep_prob则为丢弃概率），比如某一层的 keep_prob=0.8，则意味着某一层随机的保留80%的神经单元（也即有20%的单元被丢弃）。通常实现dropout regularization的技术称为 inverted dropout，假设对于第三层，则inverted dropout的具体实现为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">d3 = np.random.rand(a3.shape[<span class="number">0</span>],a3.shape[<span class="number">1</span>]) &lt; keep_prob</span><br><span class="line">a3 = np.multiply(a3, d3)</span><br><span class="line">a3 = a3 / keep_prob</span><br><span class="line">z4 = np.dot(w4, a3) + b4</span><br></pre></td></tr></table></figure></p><p>对于上述第三行代码的解释：因为有1−keep_prob的单元失活了，这样a3的期望值也就减少了1−keep_prob，所以我们要用a3/keep_prob，这样a3的期望值不变。这就是inverted dropout。</p><p>我们用两个动态图来演示下dropout的过程（素材来自ng的deep learning课）：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ec4d349d9efbcfad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-a1c3c77880d41203.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>另外要注意的是，我们在测试阶段的时候是不使用Dropout的。</p><h2 id="理解Dropout"><a href="#理解Dropout" class="headerlink" title="理解Dropout"></a>理解Dropout</h2><p>为什么Dropout能够减小过拟合，吴老师给出了两个比较直观的解释：</p><ul><li>正是因为在每一层随机地丢弃了一些单元，所以相当于训练出来的网络要比原有的网络小得多，这在一定程度上解释了避免过拟合的问题。</li><li>如下图所示的一个简单单层网络，因为每一个特征都有可能被丢弃，所以整个网络不会偏向于某一个特征（把某特征的权重的值赋的很大），会把每一个特征的权重都赋的很小，这就有点类似于L2正则化了，能够起到减轻过拟合的作用。 （压缩权重）</li></ul><p>另外，不同层的keep_prob可以设置得不同，这就有些类似于正则化中的参数lambda。</p><p>简单总结：如果你担心某些层比其他层更容易发生过拟合，那么可以把某些层的keep_prob设置得低一些，而缺点是为了使用交叉验证，我们要搜索更多的超参数；另一种方案则是在一些层上应用dropout，而一些层则不用。dropout在计算机视觉领域用得很频繁，因为我们往往足够的数据，因此经常容易发生过拟合，那么dropout就是必然会使用的。另外需要记住，dropout是正则化中的一种方法，它可以帮助预防过拟合。不过，dropout的使用使得我们的cost function不再明确，因此我们的一个做法就是，先去掉dropout函数，画出cost function图确保是递减的，然后再使用dropout。</p><h2 id="其他正则化方法"><a href="#其他正则化方法" class="headerlink" title="其他正则化方法"></a>其他正则化方法</h2><ol><li>Data augmentation-数据增强</li></ol><p>如果我们无法获得更多的数据，以图片为例，我们可以增加其他方向不同的图片，或者随机翻转和裁剪图片，额外生成假数据。尽管这种做法不如增加一组新图片，因为存在一些冗余，但是这样做也节省了获取更多猫咪图片的花费；对于数字，我们也可以随机旋转或扭转数字来扩增数据。如以下例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-91b91c7f61140ea1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ol><li>Early stopping</li></ol><p>我们可以画出迭代过程中的训练误差和在验证集上的误差，通过观察验证误差，来提前结束迭代。这样子得到的参数W不至于过大，因此也减少了过拟合。但是early-stopping的一个主要缺点就是我们无法独立地处理优化cost function和防止过拟合这两个过程，因为提前停止梯度下降，我们实际上也停止了cost funciton的优化，因此我们实际上是用一种方法来同时解决两个问题。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-8db9fe9dc325e0bf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们也可以选择不同的正则化参数lambda，但这意味着更多的训练和尝试，计算代价会很大。因此Early stopping就不需要尝试那么多的lambda。</p><h1 id="标准化输入-Normalize-input"><a href="#标准化输入-Normalize-input" class="headerlink" title="标准化输入-Normalize input"></a>标准化输入-Normalize input</h1><p>训练神经网络时，提高训练速度的方法之一是对输入进行归一化。假设我们有一个训练集,它有两个输入特征,所以输入特征x是二维的,这是数据集的散点图。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-12733db941f0f44f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>归一化需要两个步骤：零均值化、归一化方差。</p><p>第一步：零均值化。subtract out or to zero out the mean 计算出u即x(i)的均值。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-1f523947cb70ca6e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>随后x = x-u。即通过移动训练集，直到其为零均值化。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b8ff7f5e486c6d33.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>第二步：归一化方差。</p><p>如上图所示，特征1的方差比特征2的要大很多。计算出方差sigma^2，如下图：</p><p>上一步我们已经完成了零均值化，接下来将所有数据都除以向量sigma^2.最后数据分布如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-1cf9df8abbc940c3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>最后注意，如果我们用了此种方法来对训练集的特征进行归一化，那么对于测试集也要使用同样的u和sigma做归一化，而不是在训练集和测试集上分别评估出不同的u和sigma。</p><p><strong>疑惑</strong>：为什么是除以方差，而不是除以标准差？</p><h2 id="为什么我们要对输入进行标准化"><a href="#为什么我们要对输入进行标准化" class="headerlink" title="为什么我们要对输入进行标准化"></a>为什么我们要对输入进行标准化</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-4a2ffb823b61cb8a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>标准化后，cost function更容易优化，更快达到迭代目标，即前提是特征的范围基本一致，而不是相差很大。</p><h1 id="梯度消失与梯度爆炸-Vanishing-exploding-gradients"><a href="#梯度消失与梯度爆炸-Vanishing-exploding-gradients" class="headerlink" title="梯度消失与梯度爆炸-Vanishing/exploding gradients"></a>梯度消失与梯度爆炸-Vanishing/exploding gradients</h1><p>在本节中，周老师举了一个较为简单的例子来解释梯度爆炸和梯度消失的问题。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-d9e550e711f9d0c8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>为了简单起见，每个隐藏层的结点数都为2，并且设置激活函数为线性激活函数，且b=0。在例子中可以看到，如果我们将W设置得稍微比I(全1矩阵)大一些，最后得到的预测结果y呈指数型增长，即出现指数爆炸；而若是W比I稍微小一些，得到的结果也是指数级下降，即出现梯度消失。这个问题长久以来都是深层神经网络发展的一个阻力。</p><h1 id="深层神经网络的权重初始化"><a href="#深层神经网络的权重初始化" class="headerlink" title="深层神经网络的权重初始化"></a>深层神经网络的权重初始化</h1><p>我们想出了一个不完整的解决方案，有助于我们为神经网络更谨慎的选择随机初始化参数。</p><p>一般而言，为了预防Z过大或过小，随着n越大，w_i应该越小，则最合理的方法是设置w_i = 1/n，其中n表示输入神经元的特征数。因此我们实际上一般写作：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-a429a52e392636b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>其中n^{l-1}即为我们输入到第l层神经单元的数目。</p><p>通常，relu激活函数的W_i应该设置为2/n比较合适，而tanh函数的见下图：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-5ab0a9239083c607.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们可以给上述参数再增加一个乘法参数，但上述方差参数的调优优先级并不高。通过上述的方法，我们确实降低的梯度爆炸和梯度消失问题。</p><h1 id="梯度的数值逼近-Numerical-approximation-of-gradients"><a href="#梯度的数值逼近-Numerical-approximation-of-gradients" class="headerlink" title="梯度的数值逼近-Numerical approximation of gradients"></a>梯度的数值逼近-Numerical approximation of gradients</h1><p>为了实现梯度检验，我们首先说说如何对计算梯度做数值逼近。吴老师在这节主要是利用了斜率的概念来计算导数，用实际的计算例子来验证梯度计算得是否正确，如下图：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d7d6a7c99eff4439.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们在检验的时候用如下公式更加准确：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-75636009046c847a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h1><p>梯度检验能够帮我们节省很多时间，帮我们发现在反向传播过程中的bug，接下来我们看看如何利用它来调试或检验backprop的实施是否正确。</p><p>假设网络中含有下列参数，W1和b1…Wl和bl。为了执行梯度检验，首先我们将所有参数转换成一个巨大的向量数据，将矩阵W转换成向量之后，做连接运算，得到一个巨型向量theta。我们的代价函数J是所有W和b的函数，因此我们得到了J(theta)。接着，我们可以同样把求得的dW1和db1…dWl和dbl转换成一个新的向量，用他们作为dtheta，它与theta具有相同维度。因此现在的问题是，dtheta和代价函数J的梯度有什么关系？<br><img src="https://upload-images.jianshu.io/upload_images/8636110-05f969f5800bcde9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>接下来就是实施梯度检验的过程，英语中简称为”grad check”。首先我们要清楚J是超参数theta的一个函数，不论theta的维度是多少，为了实施梯度检验，我们要做的是循环执行，对每个i也就是对每个theta的组成元素计算dtheta_approx(i)的值，我们要使用的是双边误差的公式：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-4bfc7d49becd3bc0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>从上节课的梯度数值逼近，我们知道dtheta_approx(i)的值应该逼近dtheta(i)的值，而dtheta(i)即为代价函数的偏导数。通过对每个i执行这个运算，我们可以得到两个向量，即dtheta的逼近值dtheta_approx和dtheta本身。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b4eba04454e0a5ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>利用上式计算两个向量之间的距离，再利用右边的值大小判断是否正确。在实施神经网络的时候，我们经常要执行foreprop和backprop，如果我们发现梯度检验有一个较大的值，那么我们就可以怀疑存在bug，因此需要进行调试。</p><h2 id="关于梯度检验实现的提示"><a href="#关于梯度检验实现的提示" class="headerlink" title="关于梯度检验实现的提示"></a>关于梯度检验实现的提示</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-360fe300127922bf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>一点解释：</p><ol><li>不要在训练的时候进行梯度检验。</li><li>如果算法在梯度检验的时候发生了错误，我们要查看不同的可能导致错误的组成部分，比如dbl或者dWl。</li><li>注意正则化，如果我们在成本函数中加了正则项，那么在梯度检验求梯度的时候也要求正则项的梯度。</li><li>不要与Dropout一块使用，因为Dropout会随机地使一些隐藏单元不起作用。</li><li>只有当W和b接近0时，梯度下降的实施是正确的，因此我们在随机初始化过程中运行梯度检验，再训练网络。</li></ol><h1 id="本周作业"><a href="#本周作业" class="headerlink" title="本周作业"></a>本周作业</h1><h2 id="Initialization"><a href="#Initialization" class="headerlink" title="Initialization"></a>Initialization</h2><p>A well chosen intialization can:</p><pre><code>- Speed up the convergence of gradient descent- Increase the odds of gradient descent converging to a lower training (and generalization) error</code></pre><p>Import packages and the planar dataset:<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">from</span> init_utils <span class="keyword">import</span> sigmoid, relu, compute_loss, forward_propagation, backward_propagation</span><br><span class="line"><span class="keyword">from</span> init_utils <span class="keyword">import</span> update_parameters, predict, load_dataset, plot_decision_boundary, predict_dec</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">7.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># load image dataset: blue/red dots in circles</span></span><br><span class="line">train_X, train_Y, test_X, test_Y = load_dataset()</span><br></pre></td></tr></table></figure></p><h3 id="1-Neural-Network-model"><a href="#1-Neural-Network-model" class="headerlink" title="1-Neural Network model"></a>1-Neural Network model</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, Y, learning_rate = <span class="number">0.01</span>, num_iterations = <span class="number">15000</span>, print_cost = True, initialization = <span class="string">"he"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate for gradient descent </span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations to run gradient descent</span></span><br><span class="line"><span class="string">    print_cost -- if True, print the cost every 1000 iterations</span></span><br><span class="line"><span class="string">    initialization -- flag to choose which initialization to use ("zeros","random" or "he")</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">        </span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = [] <span class="comment"># to keep track of the loss</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>] <span class="comment"># number of examples</span></span><br><span class="line">    layers_dims = [X.shape[<span class="number">0</span>], <span class="number">10</span>, <span class="number">5</span>, <span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters dictionary.</span></span><br><span class="line">    <span class="keyword">if</span> initialization == <span class="string">"zeros"</span>:</span><br><span class="line">        parameters = initialize_parameters_zeros(layers_dims)</span><br><span class="line">    <span class="keyword">elif</span> initialization == <span class="string">"random"</span>:</span><br><span class="line">        parameters = initialize_parameters_random(layers_dims)</span><br><span class="line">    <span class="keyword">elif</span> initialization == <span class="string">"he"</span>:</span><br><span class="line">        parameters = initialize_parameters_he(layers_dims)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line">        a3, cache = forward_propagation(X, parameters)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Loss</span></span><br><span class="line">        cost = compute_loss(a3, Y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backward propagation.</span></span><br><span class="line">        grads = backward_propagation(X, Y, cache)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the loss every 1000 iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration &#123;&#125;: &#123;&#125;"</span>.format(i, cost))</span><br><span class="line">            costs.append(cost)</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># plot the loss</span></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per hundreds)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="2-Zero-initialization"><a href="#2-Zero-initialization" class="headerlink" title="2-Zero initialization"></a>2-Zero initialization</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_zeros</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    layer_dims -- python array (list) containing the size of each layer.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (layers_dims[1], 1)</span></span><br><span class="line"><span class="string">                    ...</span></span><br><span class="line"><span class="string">                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])</span></span><br><span class="line"><span class="string">                    bL -- bias vector of shape (layers_dims[L], 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layers_dims)            <span class="comment"># number of layers in the network</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.zeros((layers_dims[l],layers_dims[l<span class="number">-1</span>]))</span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layers_dims[l], <span class="number">1</span>))</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>Run the following code to train your model on 15,000 iterations using zeros initialization.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y, initialization = <span class="string">"zeros"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the train set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure><p>Result:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-67175d9b586dbce4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-5d79d87b5d330532.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>The model is predicting 0 for every example.</p><p>In general, initializing all the weights to zero results in the network failing to break symmetry. This means that every neuron in each layer will learn the same thing, and you might as well be training a neural network with  n[l]=1  for every layer, and the network is no more powerful than a linear classifier such as logistic regression.</p><p><strong>Remember</strong>:</p><ul><li>The weights W[l] should be initialized randomly to break symmetry.</li><li>It is however okay to initialize the biases b[l] to zeros. Symmetry is still broken so long as W[l] is initialized randomly.</li></ul><h3 id="3-Random-initialization"><a href="#3-Random-initialization" class="headerlink" title="3- Random initialization"></a>3- Random initialization</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_random</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    layer_dims -- python array (list) containing the size of each layer.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (layers_dims[1], 1)</span></span><br><span class="line"><span class="string">                    ...</span></span><br><span class="line"><span class="string">                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])</span></span><br><span class="line"><span class="string">                    bL -- bias vector of shape (layers_dims[L], 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">3</span>)               <span class="comment"># This seed makes sure your "random" numbers will be the as ours</span></span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layers_dims)            <span class="comment"># integer representing the number of layers</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layers_dims[l],layers_dims[l<span class="number">-1</span>])*<span class="number">10</span></span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layers_dims[l],<span class="number">1</span>))</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>Result:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-2731639cbe8165e1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-fc5fe0ffe08338a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong> Observations </strong></p><ul><li>The cost starts very high. This is because with large random-valued weights, the last activation (sigmoid) outputs results that are very close to 0 or 1 for some examples, and when it gets that example wrong it incurs a very high loss for that example. Indeed, when  log(a[3])=log(0), the loss goes to infinity.</li><li>Poor initialization can lead to vanishing/exploding gradients, which also slows down the optimization algorithm.</li><li>If you train this network longer you will see better results, but initializing with overly large random numbers slows down the optimization.</li></ul><p><strong> In summary: </strong></p><ul><li>Initializing weights to very large random values does not work well.</li><li>Hopefully intializing with small random values does better. The important question is: how small should be these random values be? Lets find out in the next part!</li></ul><h3 id="4-He-initialization"><a href="#4-He-initialization" class="headerlink" title="4- He initialization"></a>4- He initialization</h3><p>Finally, try “He Initialization”; this is named for the first author of He et al., 2015.</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-98f736c820e171c2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_he</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    layer_dims -- python array (list) containing the size of each layer.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (layers_dims[1], 1)</span></span><br><span class="line"><span class="string">                    ...</span></span><br><span class="line"><span class="string">                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])</span></span><br><span class="line"><span class="string">                    bL -- bias vector of shape (layers_dims[L], 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layers_dims) - <span class="number">1</span> <span class="comment"># integer representing the number of layers</span></span><br><span class="line">     </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L + <span class="number">1</span>):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layers_dims[l],layers_dims[l<span class="number">-1</span>])*np.sqrt(<span class="number">2</span>/layers_dims[l<span class="number">-1</span>])</span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layers_dims[l],<span class="number">1</span>))</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>Result:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-305e6f7d962816f1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-ccc49b3765571c43.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong> Observation </strong></p><ul><li>The model with He initialization separates the blue and the red dots very well in a small number of iterations.</li></ul><h3 id="5-Conclusions"><a href="#5-Conclusions" class="headerlink" title="5- Conclusions"></a>5- Conclusions</h3><p><img src="https://upload-images.jianshu.io/upload_images/8636110-f61a6518bbd38752.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>What we need to remember:</p><ul><li>Different initializations lead to different results.</li><li>Random initialization is used to break symmetry and make sure different hidden units can learn different things.</li><li>Don’t intialize to values that are too large.</li><li>He initialization works well for networks with ReLU activations.</li></ul><h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><p>import packages:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import packages</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> reg_utils <span class="keyword">import</span> sigmoid, relu, plot_decision_boundary, initialize_parameters, load_2D_dataset, predict_dec</span><br><span class="line"><span class="keyword">from</span> reg_utils <span class="keyword">import</span> compute_cost, predict, forward_propagation, backward_propagation, update_parameters</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">7.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br></pre></td></tr></table></figure></p><p><strong> Problem Statement </strong> You have just been hired as an AI expert by the French Football Corporation. They would like you to recommend positions where France’s goal keeper should kick the ball so that the French team’s players can then hit it with their head.</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-55d3ce441f5f8b6f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>They give you the following 2D dataset from France’s past 10 games.<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_X, train_Y, test_X, test_Y = load_2D_dataset()</span><br></pre></td></tr></table></figure></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-3bc560f1523d3b04.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Each dot corresponds to a position on the football field where a football player has hit the ball with his/her head after the French goal keeper has shot the ball from the left side of the football field.</p><ul><li>If the dot is blue, it means the French player managed to hit the ball with his/her head</li><li>If the dot is red, it means the other team’s player hit the ball with their head</li></ul><p><strong>Your goal</strong>: Use a deep learning model to find the positions on the field where the goalkeeper should kick the ball.</p><p><strong>Analysis of the dataset</strong>: This dataset is a little noisy, but it looks like a diagonal line separating the upper left half (blue) from the lower right half (red) would work well. </p><p>You will first try a non-regularized model. Then you’ll learn how to regularize it and decide which model you will choose to solve the French Football Corporation’s problem. </p><h3 id="1-Non-regularized-model"><a href="#1-Non-regularized-model" class="headerlink" title="1- Non-regularized model"></a>1- Non-regularized model</h3><p>You will use the following neural network (already implemented for you below). This model can be used: </p><ul><li>in <em>regularization mode</em> — by setting the <code>lambd</code> input to a non-zero value. We use “<code>lambd</code>“ instead of “<code>lambda</code>“ because “<code>lambda</code>“ is a reserved keyword in Python. </li><li>in <em>dropout mode</em> — by setting the <code>keep_prob</code> to a value less than one</li></ul><p>You will first try the model without any regularization. Then, you will implement:</p><ul><li><em>L2 regularization</em> — functions: “<code>compute_cost_with_regularization()</code>“ and “<code>backward_propagation_with_regularization()</code>“</li><li><em>Dropout</em> — functions: “<code>forward_propagation_with_dropout()</code>“ and “<code>backward_propagation_with_dropout()</code>“</li></ul><p>In each part, you will run this model with the correct inputs so that it calls the functions you’ve implemented. Take a look at the code below to familiarize yourself with the model.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, Y, learning_rate = <span class="number">0.3</span>, num_iterations = <span class="number">30000</span>, print_cost = True, lambd = <span class="number">0</span>, keep_prob = <span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the optimization</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    print_cost -- If True, print the cost every 10000 iterations</span></span><br><span class="line"><span class="string">    lambd -- regularization hyperparameter, scalar</span></span><br><span class="line"><span class="string">    keep_prob - probability of keeping a neuron active during drop-out, scalar.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learned by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">        </span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = []                            <span class="comment"># to keep track of the cost</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]                        <span class="comment"># number of examples</span></span><br><span class="line">    layers_dims = [X.shape[<span class="number">0</span>], <span class="number">20</span>, <span class="number">3</span>, <span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters dictionary.</span></span><br><span class="line">    parameters = initialize_parameters(layers_dims)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line">        <span class="keyword">if</span> keep_prob == <span class="number">1</span>: <span class="comment"># 无dropout</span></span><br><span class="line">            a3, cache = forward_propagation(X, parameters)</span><br><span class="line">        <span class="keyword">elif</span> keep_prob &lt; <span class="number">1</span>:</span><br><span class="line">            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Cost function</span></span><br><span class="line">        <span class="keyword">if</span> lambd == <span class="number">0</span>: <span class="comment"># 无L2正则</span></span><br><span class="line">            cost = compute_cost(a3, Y)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># Backward propagation.</span></span><br><span class="line">        <span class="keyword">assert</span>(lambd==<span class="number">0</span> <span class="keyword">or</span> keep_prob==<span class="number">1</span>)    <span class="comment"># it is possible to use both L2 regularization and dropout, </span></span><br><span class="line">                                            <span class="comment"># but this assignment will only explore one at a time</span></span><br><span class="line">        <span class="keyword">if</span> lambd == <span class="number">0</span> <span class="keyword">and</span> keep_prob == <span class="number">1</span>:</span><br><span class="line">            grads = backward_propagation(X, Y, cache)</span><br><span class="line">        <span class="keyword">elif</span> lambd != <span class="number">0</span>:</span><br><span class="line">            grads = backward_propagation_with_regularization(X, Y, cache, lambd)</span><br><span class="line">        <span class="keyword">elif</span> keep_prob &lt; <span class="number">1</span>:</span><br><span class="line">            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the loss every 10000 iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration &#123;&#125;: &#123;&#125;"</span>.format(i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (x1,000)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>Let’s train the model without any regularization, and observe the accuracy on the train/test sets.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the training set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure><p><img src="https://upload-images.jianshu.io/upload_images/8636110-3cd473ea1ea143ca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-b551f869daf57480.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>The non-regularized model is obviously overfitting the training set. It is fitting the noisy points! Lets now look at two techniques to reduce overfitting.</p><h3 id="2-L2-Regularization"><a href="#2-L2-Regularization" class="headerlink" title="2- L2 Regularization"></a>2- L2 Regularization</h3><p><img src="https://upload-images.jianshu.io/upload_images/8636110-c2875ba8ba446530.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost_with_regularization</span><span class="params">(A3, Y, parameters, lambd)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the cost function with L2 regularization. See formula (2) above.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector, of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing parameters of the model</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost - value of the regularized loss function (formula (2))</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    W3 = parameters[<span class="string">"W3"</span>]</span><br><span class="line">    </span><br><span class="line">    cross_entropy_cost = compute_cost(A3, Y) <span class="comment"># This gives you the cross-entropy part of the cost</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    L2_regularization_cost = (np.sum(np.square(W1))+np.sum(np.square(W2))+np.sum(np.square(W3)))*(lambd/(<span class="number">2</span>*m))</span><br><span class="line">    <span class="comment">### END CODER HERE ###</span></span><br><span class="line">    </span><br><span class="line">    cost = cross_entropy_cost + L2_regularization_cost</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation_with_regularization</span><span class="params">(X, Y, cache, lambd)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the backward propagation of our baseline model to which we added an L2 regularization.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector, of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    cache -- cache output from forward_propagation()</span></span><br><span class="line"><span class="string">    lambd -- regularization hyperparameter, scalar</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    dW3 = <span class="number">1.</span>/m * np.dot(dZ3, A2.T) + W3*(lambd/m)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    db3 = <span class="number">1.</span>/m * np.sum(dZ3, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; <span class="number">0</span>))</span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    dW2 = <span class="number">1.</span>/m * np.dot(dZ2, A1.T) + W2*(lambd/m)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    db2 = <span class="number">1.</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; <span class="number">0</span>))</span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    dW1 = <span class="number">1.</span>/m * np.dot(dZ1, X.T) + W1*(lambd/m)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    db1 = <span class="number">1.</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;<span class="string">"dZ3"</span>: dZ3, <span class="string">"dW3"</span>: dW3, <span class="string">"db3"</span>: db3,<span class="string">"dA2"</span>: dA2,</span><br><span class="line">                 <span class="string">"dZ2"</span>: dZ2, <span class="string">"dW2"</span>: dW2, <span class="string">"db2"</span>: db2, <span class="string">"dA1"</span>: dA1, </span><br><span class="line">                 <span class="string">"dZ1"</span>: dZ1, <span class="string">"dW1"</span>: dW1, <span class="string">"db1"</span>: db1&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><p>Train and test:<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y, lambd = <span class="number">0.7</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the train set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure></p><p>Result:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-1caff43cb140270a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-83ff2ceab15f3524.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>Observations</strong>:</p><ul><li>The value of $\lambda$ is a hyperparameter that you can tune using a dev set.</li><li>L2 regularization makes your decision boundary smoother. If $\lambda$ is too large, it is also possible to “oversmooth”, resulting in a model with high bias.</li></ul><p><strong>What is L2-regularization actually doing?</strong>:</p><p>L2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes. </p><p><strong>What you should rememeber</strong> —— the implications</p><ul><li>The cost computation: A regularization term is added to the cost</li><li>The backpropagation function: There are extra terms in the gradients with respect to weight matrices</li><li>Weights end up smaller (“weight decay”): Weights are pushed to smaller values.</li></ul><h3 id="3-Dropout"><a href="#3-Dropout" class="headerlink" title="3- Dropout"></a>3- Dropout</h3><p>Finally, dropout is a widely used regularization technique that is specific to deep learning. <strong>It randomly shuts down some neurons in each iteration</strong>. </p><p>When you shut some neurons down, you actually modify your model. The idea behind drop-out is that at each iteration, you train a different model that uses only a subset of your neurons. With dropout, your neurons thus become less sensitive to the activation of one other specific neuron, because that other neuron might be shut down at any time.</p><h4 id="3-1-Forward-propagation"><a href="#3-1-Forward-propagation" class="headerlink" title="3.1- Forward propagation"></a>3.1- Forward propagation</h4><p>注意随机函数用的是np.random.rand，获得0到1之间的随机数；而不是randn。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation_with_dropout</span><span class="params">(X, parameters, keep_prob = <span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the forward propagation: LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset, of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (20, 2)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (20, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (3, 20)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (3, 1)</span></span><br><span class="line"><span class="string">                    W3 -- weight matrix of shape (1, 3)</span></span><br><span class="line"><span class="string">                    b3 -- bias vector of shape (1, 1)</span></span><br><span class="line"><span class="string">    keep_prob - probability of keeping a neuron active during drop-out, scalar</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A3 -- last activation value, output of the forward propagation, of shape (1,1)</span></span><br><span class="line"><span class="string">    cache -- tuple, information stored for computing the backward propagation</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># retrieve parameters</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    W3 = parameters[<span class="string">"W3"</span>]</span><br><span class="line">    b3 = parameters[<span class="string">"b3"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = relu(Z1)</span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 4 lines)         # Steps 1-4 below correspond to the Steps 1-4 described above. </span></span><br><span class="line">    D1 = np.random.rand(A1.shape[<span class="number">0</span>],A1.shape[<span class="number">1</span>])                                         <span class="comment"># Step 1: initialize matrix D1 = np.random.rand(..., ...)</span></span><br><span class="line">    D1 = D1 &lt; keep_prob                                         <span class="comment"># Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)</span></span><br><span class="line">    A1 = A1 * D1                                        <span class="comment"># Step 3: shut down some neurons of A1</span></span><br><span class="line">    A1 = A1 / keep_prob                                         <span class="comment"># Step 4: scale the value of neurons that haven't been shut down</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = relu(Z2)</span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 4 lines)</span></span><br><span class="line">    D2 = np.random.rand(A2.shape[<span class="number">0</span>],A2.shape[<span class="number">1</span>])                                         <span class="comment"># Step 1: initialize matrix D2 = np.random.rand(..., ...)</span></span><br><span class="line">    D2 = D2 &lt; keep_prob                                      <span class="comment"># Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)</span></span><br><span class="line">    A2 = A2 * D2                                        <span class="comment"># Step 3: shut down some neurons of A2</span></span><br><span class="line">    A2 = A2 / keep_prob                                         <span class="comment"># Step 4: scale the value of neurons that haven't been shut down</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    Z3 = np.dot(W3, A2) + b3</span><br><span class="line">    A3 = sigmoid(Z3)</span><br><span class="line">    </span><br><span class="line">    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A3, cache</span><br></pre></td></tr></table></figure></p><h4 id="3-2-Backward-propagation-with-dropout"><a href="#3-2-Backward-propagation-with-dropout" class="headerlink" title="3.2- Backward propagation with dropout"></a>3.2- Backward propagation with dropout</h4><p>Instruction: Backpropagation with dropout is actually quite easy. You will have to carry out 2 Steps:</p><ol><li>You had previously shut down some neurons during forward propagation, by applying a mask $D^{[1]}$ to <code>A1</code>. In backpropagation, you will have to shut down the same neurons, by reapplying the same mask $D^{[1]}$ to <code>dA1</code>. </li><li>During forward propagation, you had divided <code>A1</code> by <code>keep_prob</code>. In backpropagation, you’ll therefore have to divide <code>dA1</code> by <code>keep_prob</code> again (the calculus interpretation is that if $A^{[1]}$ is scaled by <code>keep_prob</code>, then its derivative $dA^{[1]}$ is also scaled by the same <code>keep_prob</code>).</li></ol><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation_with_dropout</span><span class="params">(X, Y, cache, keep_prob)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the backward propagation of our baseline model to which we added dropout.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset, of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector, of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    cache -- cache output from forward_propagation_with_dropout()</span></span><br><span class="line"><span class="string">    keep_prob - probability of keeping a neuron active during drop-out, scalar</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y</span><br><span class="line">    dW3 = <span class="number">1.</span>/m * np.dot(dZ3, A2.T)</span><br><span class="line">    db3 = <span class="number">1.</span>/m * np.sum(dZ3, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    dA2 = dA2 * D2              <span class="comment"># Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation</span></span><br><span class="line">    dA2 = dA2 / keep_prob              <span class="comment"># Step 2: Scale the value of neurons that haven't been shut down</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; <span class="number">0</span>))</span><br><span class="line">    dW2 = <span class="number">1.</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = <span class="number">1.</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    dA1 = dA1 * D1              <span class="comment"># Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation</span></span><br><span class="line">    dA1 = dA1 / keep_prob              <span class="comment"># Step 2: Scale the value of neurons that haven't been shut down</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; <span class="number">0</span>))</span><br><span class="line">    dW1 = <span class="number">1.</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1.</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;<span class="string">"dZ3"</span>: dZ3, <span class="string">"dW3"</span>: dW3, <span class="string">"db3"</span>: db3,<span class="string">"dA2"</span>: dA2,</span><br><span class="line">                 <span class="string">"dZ2"</span>: dZ2, <span class="string">"dW2"</span>: dW2, <span class="string">"db2"</span>: db2, <span class="string">"dA1"</span>: dA1, </span><br><span class="line">                 <span class="string">"dZ1"</span>: dZ1, <span class="string">"dW1"</span>: dW1, <span class="string">"db1"</span>: db1&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><p>Train and test:<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y, keep_prob = <span class="number">0.86</span>, learning_rate = <span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the train set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure></p><p>Result:<img src="https://upload-images.jianshu.io/upload_images/8636110-375b4aa1cbb89014.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-bcd49e6ed1b6b2d1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>What you should remember about dropout:</strong></p><ul><li>Dropout is a regularization technique.</li><li>You only use dropout during training. Don’t use dropout (randomly eliminate nodes) during test time.</li><li>Apply dropout both during forward and backward propagation.</li><li>During training time, divide each dropout layer by keep_prob to keep the same expected value for the activations. For example, if keep_prob is 0.5, then we will on average shut down half the nodes, so the output will be scaled by 0.5 since only the remaining half are contributing to the solution. Dividing by 0.5 is equivalent to multiplying by 2. Hence, the output now has the same expected value. You can check that this works even when keep_prob is other values than 0.5. </li></ul><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><p><img src="https://upload-images.jianshu.io/upload_images/8636110-75bf68c731402015.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Note that regularization hurts training set performance! This is because it limits the ability of the network to overfit to the training set. But since it ultimately gives better test accuracy, it is helping your system.</p><p><strong>What we need to remember</strong>:</p><ul><li>Regularization will help you reduce overfitting.</li><li>Regularization will drive your weights to lower values.</li><li>L2 regularization and Dropout are two very effective regularization techniques.</li></ul><h2 id="Gradient-Checking"><a href="#Gradient-Checking" class="headerlink" title="Gradient Checking"></a>Gradient Checking</h2><p>import packages:<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> gc_utils <span class="keyword">import</span> sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector</span><br></pre></td></tr></table></figure></p><p><strong>1) How does gradient checking work?</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-16af127a400268af.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>2) 1-dimensional gradient checking</strong><br><img src="https://upload-images.jianshu.io/upload_images/8636110-89a1e5d5f434b40b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(x, theta)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the linear forward propagation (compute J) presented in Figure 1 (J(theta) = theta * x)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- a real-valued input</span></span><br><span class="line"><span class="string">    theta -- our parameter, a real number as well</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    J -- the value of function J, computed using the formula J(theta) = theta * x</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    J = theta * x</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> J</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span><span class="params">(x, theta)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the derivative of J with respect to theta (see Figure 1).</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- a real-valued input</span></span><br><span class="line"><span class="string">    theta -- our parameter, a real number as well</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dtheta -- the gradient of the cost with respect to theta</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    dtheta = x</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dtheta</span><br></pre></td></tr></table></figure></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-f71a8784ae28956f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_check</span><span class="params">(x, theta, epsilon = <span class="number">1e-7</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation presented in Figure 1.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- a real-valued input</span></span><br><span class="line"><span class="string">    theta -- our parameter, a real number as well</span></span><br><span class="line"><span class="string">    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    difference -- difference (2) between the approximated gradient and the backward propagation gradient</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute gradapprox using left side of formula (1). epsilon is small enough, you don't need to worry about the limit.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 5 lines)</span></span><br><span class="line">    thetaplus = theta + epsilon                               <span class="comment"># Step 1</span></span><br><span class="line">    thetaminus = theta - epsilon                              <span class="comment"># Step 2</span></span><br><span class="line">    J_plus = forward_propagation(x,thetaplus)                                  <span class="comment"># Step 3</span></span><br><span class="line">    J_minus = forward_propagation(x,thetaminus)                                 <span class="comment"># Step 4</span></span><br><span class="line">    gradapprox = (J_plus-J_minus)/(<span class="number">2</span>*epsilon)                             <span class="comment"># Step 5</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Check if gradapprox is close enough to the output of backward_propagation()</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    grad = backward_propagation(x, theta)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    numerator = np.linalg.norm(grad - gradapprox)                             <span class="comment"># Step 1'</span></span><br><span class="line">    denominator = np.linalg.norm(grad)+np.linalg.norm(gradapprox)                          <span class="comment"># Step 2'</span></span><br><span class="line">    difference = numerator / denominator                           <span class="comment"># Step 3'</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> difference &lt; <span class="number">1e-7</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"The gradient is correct!"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"The gradient is wrong!"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> difference</span><br></pre></td></tr></table></figure><p>If the difference is smaller that the 1e-7, then we can have high confidence that we’ve correctly computed the gradient in backward_propagation().</p><p><strong>3)N-dimensional gradient checking</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-6dd5258b2f733fd7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如何对多层神经网络进行梯度检测？<br><img src="https://upload-images.jianshu.io/upload_images/8636110-50ee01ca15822334.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-965c5966c012dac0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_check_n</span><span class="params">(parameters, gradients, X, Y, epsilon = <span class="number">1e-7</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":</span></span><br><span class="line"><span class="string">    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters. </span></span><br><span class="line"><span class="string">    x -- input datapoint, of shape (input size, 1)</span></span><br><span class="line"><span class="string">    y -- true "label"</span></span><br><span class="line"><span class="string">    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    difference -- difference (2) between the approximated gradient and the backward propagation gradient</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Set-up variables</span></span><br><span class="line">    parameters_values, _ = dictionary_to_vector(parameters)</span><br><span class="line">    grad = gradients_to_vector(gradients)</span><br><span class="line">    num_parameters = parameters_values.shape[<span class="number">0</span>]</span><br><span class="line">    J_plus = np.zeros((num_parameters, <span class="number">1</span>))</span><br><span class="line">    J_minus = np.zeros((num_parameters, <span class="number">1</span>))</span><br><span class="line">    gradapprox = np.zeros((num_parameters, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute gradapprox</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_parameters):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute J_plus[i]. Inputs: "parameters_values, epsilon". Output = "J_plus[i]".</span></span><br><span class="line">        <span class="comment"># "_" is used because the function you have to outputs two parameters but we only care about the first one</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 3 lines)</span></span><br><span class="line">        thetaplus = np.copy(parameters_values)                                     <span class="comment"># Step 1</span></span><br><span class="line">        thetaplus[i][<span class="number">0</span>] = thetaplus[i][<span class="number">0</span>] + epsilon                           <span class="comment"># Step 2</span></span><br><span class="line">        J_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))                                <span class="comment"># Step 3</span></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute J_minus[i]. Inputs: "parameters_values, epsilon". Output = "J_minus[i]".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 3 lines)</span></span><br><span class="line">        thetaminus = np.copy(parameters_values)                                        <span class="comment"># Step 1</span></span><br><span class="line">        thetaminus[i][<span class="number">0</span>] = thetaplus[i][<span class="number">0</span>] - epsilon                              <span class="comment"># Step 2        </span></span><br><span class="line">        J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus))                                    <span class="comment"># Step 3</span></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute gradapprox[i]</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">        gradapprox[i] = (J_plus[i]-J_minus[i])/(<span class="number">2</span>*epsilon)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compare gradapprox to backward propagation gradients by computing difference.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    numerator = np.linalg.norm(grad - gradapprox)                                           <span class="comment"># Step 1'</span></span><br><span class="line">    denominator = np.linalg.norm(gradapprox) + np.linalg.norm(grad)                                         <span class="comment"># Step 2'</span></span><br><span class="line">    difference = numerator / denominator                                         <span class="comment"># Step 3'</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> difference &gt; <span class="number">1e-7</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"\033[93m"</span> + <span class="string">"There is a mistake in the backward propagation! difference = "</span> + str(difference) + <span class="string">"\033[0m"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"\033[92m"</span> + <span class="string">"Your backward propagation works perfectly fine! difference = "</span> + str(difference) + <span class="string">"\033[0m"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> difference</span><br></pre></td></tr></table></figure><p>最后运行结果出现difference大于1e-7，说明我们的反向传播函数中出现错误，需要到原函数中进行查找。</p><p><strong>Note</strong></p><ul><li>Gradient Checking is slow. For this reason, we don’t run gradient checking at every iteration during training. Just a few times to check if the gradient is correct.</li><li>Gradient Checking, at least as we’ve presented it, doesn’t work with dropout. You would usually run the gradient check algorithm without dropout to make sure your backprop is correct, then add dropout.</li></ul><p><strong>What we need to rememeber</strong></p><ul><li>Gradient checking verifies closeness between the gradients from backpropagation and the numerical approximation of the gradient (computed using forward propagation).</li><li>Gradient checking is slow, so we don’t run it in every iteration of training. You would usually run it only to make sure your code is correct, then turn it off and use backprop for the actual learning process.</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;训练-验证-测试集&quot;&gt;&lt;a href=&quot;#训练-验证-测试集&quot; class=&quot;headerlink&quot; title=&quot;训练/验证/测试集&quot;&gt;&lt;/a&gt;训练/验证/测试集&lt;/h1&gt;&lt;p&gt;在实际构建和训练深层神经网络的时候，我们往往要确定一些超参数，如下：&lt;br&gt;&lt;img
      
    
    </summary>
    
      <category term="深度学习" scheme="https://github.com/DesmonDay/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://github.com/DesmonDay/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>第4周-深层神经网络</title>
    <link href="https://github.com/DesmonDay/2019/04/10/deep-learningw4/"/>
    <id>https://github.com/DesmonDay/2019/04/10/deep-learningw4/</id>
    <published>2019-04-10T10:59:44.000Z</published>
    <updated>2019-04-12T07:22:08.041Z</updated>
    
    <content type="html"><![CDATA[<h1 id="深层神经网络介绍"><a href="#深层神经网络介绍" class="headerlink" title="深层神经网络介绍"></a>深层神经网络介绍</h1><p><img src="https://upload-images.jianshu.io/upload_images/8636110-b259b31dc3bdd95a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>一些符号表示：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c0ce4005cfac8297.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="深层网络中的前向传播"><a href="#深层网络中的前向传播" class="headerlink" title="深层网络中的前向传播"></a>深层网络中的前向传播</h1><p>从计算可知，我们需要显式地使用一个for循环来计算每一层的前向传播输出。另外，右侧为向量化的表示方式。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-2a85981119325313.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="核对矩阵中的维数"><a href="#核对矩阵中的维数" class="headerlink" title="核对矩阵中的维数"></a>核对矩阵中的维数</h1><p>这里总结了一下计算过程中我们的参数W和b的维数，以及Z和A的维数。</p><p>首先是参数的维数：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-1a17e18191a86898.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>接着是Z和A的维数，这里指出的是向量化的结果，其中m为样本数目：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-3e2c280ac12ec663.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="为什么要使用深层表示"><a href="#为什么要使用深层表示" class="headerlink" title="为什么要使用深层表示"></a>为什么要使用深层表示</h1><p>吴老师在这里举了三个例子来解释，分别是图像、语音和电路系统。以图像为例，我们通常由简单到复杂， 先识别图像的边缘，再识别图像的局部，最后组成成图像的整体，深层神经网络就是这样一层层地增加识别的难度；同理，对于语音识别，我们从音调的高低，再组合成声音的基本单元：音位，再到单词，最后到词组、句子，一步步地组合成我们需要的，而深层神经网络也是这样对应下来。</p><p>最后举了电路系统的例子，用来解释我们可以用深层神经网络更好地计算一些数学公式。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ee9768b637f1f234.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="搭建深层神经网络块"><a href="#搭建深层神经网络块" class="headerlink" title="搭建深层神经网络块"></a>搭建深层神经网络块</h1><p>输入输出的整个流程图（包括前向传播和后向传播）：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c6f643511f987015.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>注意到一个细节，就是我们会把前向函数计算出来的Z值缓存起来，便于在反向计算时使用。</p><h1 id="参数和超参数"><a href="#参数和超参数" class="headerlink" title="参数和超参数"></a>参数和超参数</h1><p>注意，超参数的选择会影响参数的值。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-aca7792266fb55da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>超参数需要调参，很大程度上要基于经验选择，在一定范围内找到最优值。</p><h1 id="本周作业"><a href="#本周作业" class="headerlink" title="本周作业"></a>本周作业</h1><h2 id="Building-your-Deep-Neural-Network-Step-by-Step"><a href="#Building-your-Deep-Neural-Network-Step-by-Step" class="headerlink" title="Building your Deep Neural Network: Step by Step"></a>Building your Deep Neural Network: Step by Step</h2><h3 id="1-导入包"><a href="#1-导入包" class="headerlink" title="1. 导入包"></a>1. 导入包</h3><p>Import all the packages that we will need during the assignment.</p><ul><li>numpy is the main package for scientific computing with Python.</li><li>matplotlib is a library to plot graphs in Python.</li><li>dnn_utils provides some necessary functions for this notebook.</li><li>testCases provides some test cases to assess the correctness of your functions</li><li>np.random.seed(1) is used to keep all the random function calls consistent. It will help us grade your work. Please don’t change the seed.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases_v2 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> dnn_utils_v2 <span class="keyword">import</span> sigmoid, sigmoid_backward, relu, relu_backward</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="2-任务大纲"><a href="#2-任务大纲" class="headerlink" title="2. 任务大纲"></a>2. 任务大纲</h3><p>To build your neural network, you will be implementing several “helper functions”. </p><ul><li>Initialize the parameters for a two-layer network and for an  LL -layer neural network.</li><li>Implement the forward propagation module (shown in purple in the figure below).<ul><li>Complete the LINEAR part of a layer’s forward propagation step (resulting in  Z^[l] ).</li><li>We give you the ACTIVATION function (relu/sigmoid).</li><li>Combine the previous two steps into a new [LINEAR-&gt;ACTIVATION] forward function.</li><li>Stack the [LINEAR-&gt;RELU] forward function L-1 time (for layers 1 through L-1) and add a [LINEAR-&gt;SIGMOID] at the end (for the final layer L). This gives you a new L_model_forward function.</li></ul></li><li>Compute the loss.</li><li>Implement the backward propagation module (denoted in red in the figure below).<ul><li>Complete the LINEAR part of a layer’s backward propagation step.</li><li>We give you the gradient of the ACTIVATE function (relu_backward/sigmoid_backward)</li><li>Combine the previous two steps into a new [LINEAR-&gt;ACTIVATION] backward function.</li><li>Stack [LINEAR-&gt;RELU] backward L-1 times and add [LINEAR-&gt;SIGMOID] backward in a new L_model_backward function</li></ul></li><li>Finally update the parameters.</li></ul><p>以图片形式表示：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-8baf676325cae7ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="3-初始化"><a href="#3-初始化" class="headerlink" title="3. 初始化"></a>3. 初始化</h3><p>两层神经网络：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    n_x -- size of the input layer</span></span><br><span class="line"><span class="string">    n_h -- size of the hidden layer</span></span><br><span class="line"><span class="string">    n_y -- size of the output layer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (n_h, n_x)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (n_h, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = np.random.randn(n_h,n_x)*<span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h,<span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y,n_h)*<span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y,<span class="number">1</span>))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span>(b1.shape == (n_h, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span>(b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure></p><p>L层神经网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    layer_dims -- python array (list) containing the dimensions of each layer in our network</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="string">                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])</span></span><br><span class="line"><span class="string">                    bl -- bias vector of shape (layer_dims[l], 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">3</span>) <span class="comment"># 为了作业检查是否正确而设定的种子</span></span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layer_dims)            <span class="comment"># number of layers in the network</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layer_dims[l],layer_dims[l<span class="number">-1</span>])*<span class="number">0.01</span></span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layer_dims[l], <span class="number">1</span>))</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'W'</span> + str(l)].shape == (layer_dims[l], layer_dims[l<span class="number">-1</span>]))</span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'b'</span> + str(l)].shape == (layer_dims[l], <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="4-前向传播模块"><a href="#4-前向传播模块" class="headerlink" title="4. 前向传播模块"></a>4. 前向传播模块</h3><h4 id="4-1-Linear-Forward"><a href="#4-1-Linear-Forward" class="headerlink" title="4.1 Linear Forward"></a>4.1 Linear Forward</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span><span class="params">(A, W, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the linear part of a layer's forward propagation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span><br><span class="line"><span class="string">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class="line"><span class="string">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z -- the input of the activation function, also called pre-activation parameter </span></span><br><span class="line"><span class="string">    cache -- a python dictionary containing "A", "W" and "b" ; stored for computing the backward pass efficiently</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    Z = np.dot(W,A)+b</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(Z.shape == (W.shape[<span class="number">0</span>], A.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (A, W, b)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z, cache</span><br></pre></td></tr></table></figure><h4 id="4-2-Linear-Activation-Forward"><a href="#4-2-Linear-Activation-Forward" class="headerlink" title="4.2 Linear-Activation Forward"></a>4.2 Linear-Activation Forward</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span><span class="params">(A_prev, W, b, activation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span><br><span class="line"><span class="string">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class="line"><span class="string">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="string">    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- the output of the activation function, also called the post-activation value </span></span><br><span class="line"><span class="string">    cache -- a python dictionary containing "linear_cache" and "activation_cache";</span></span><br><span class="line"><span class="string">             stored for computing the backward pass efficiently</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">        <span class="comment"># Inputs: "A_prev, W, b". Outputs: "A, activation_cache".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">        A, activation_cache = sigmoid(Z) <span class="comment"># 这里的cache为Z</span></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        <span class="comment"># Inputs: "A_prev, W, b". Outputs: "A, activation_cache".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">        A, activation_cache = relu(Z)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (A.shape == (W.shape[<span class="number">0</span>], A_prev.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache) <span class="comment"># linear_cache: A,W,b; activation_cache: Z</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure><h4 id="4-3-L-layer-Model"><a href="#4-3-L-layer-Model" class="headerlink" title="4.3 L-layer Model"></a>4.3 L-layer Model</h4><p><img src="https://upload-images.jianshu.io/upload_images/8636110-60120c093b83d009.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- data, numpy array of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- output of initialize_parameters_deep()</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    AL -- last post-activation value</span></span><br><span class="line"><span class="string">    caches -- list of caches containing:</span></span><br><span class="line"><span class="string">                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)</span></span><br><span class="line"><span class="string">                the cache of linear_sigmoid_forward() (there is one, indexed L-1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    caches = []</span><br><span class="line">    A = X </span><br><span class="line">    L = len(parameters) // <span class="number">2</span>  <span class="comment"># number of layers in the neural network</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Implement [LINEAR -&gt; RELU]*(L-1). Add "cache" to the "caches" list.</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        A_prev = A</span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        A, cache = linear_activation_forward(A_prev, parameters[<span class="string">"W"</span>+str(l)], parameters[<span class="string">"b"</span>+str(l)], <span class="string">"relu"</span>)</span><br><span class="line">        caches.append(cache)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Implement LINEAR -&gt; SIGMOID. Add "cache" to the "caches" list.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    AL, cache = linear_activation_forward(A, parameters[<span class="string">"W"</span>+str(L)], parameters[<span class="string">"b"</span>+str(L)], <span class="string">"sigmoid"</span>)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(AL.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br></pre></td></tr></table></figure><h3 id="5-Cost-function"><a href="#5-Cost-function" class="headerlink" title="5. Cost function"></a>5. Cost function</h3><p><img src="https://upload-images.jianshu.io/upload_images/8636110-b7a72fc204f22a2d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the cost function defined by equation (7).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost -- cross-entropy cost</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute loss from AL and y.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 lines of code)</span></span><br><span class="line">    cost = -np.sum(Y*np.log(AL) + (<span class="number">1</span>-Y)*np.log(<span class="number">1</span>-AL)) / m</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    cost = np.squeeze(cost) <span class="comment"># To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).</span></span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><h3 id="6-反向传播模块"><a href="#6-反向传播模块" class="headerlink" title="6. 反向传播模块"></a>6. 反向传播模块</h3><p><img src="https://upload-images.jianshu.io/upload_images/8636110-435066cf1d671938.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Now, similar to forward propagation, you are going to build the backward propagation in three steps:</p><ul><li>LINEAR backward</li><li>LINEAR -&gt; ACTIVATION backward where ACTIVATION computes the derivative of either the ReLU or sigmoid activation</li><li>[LINEAR -&gt; RELU] X (L-1) -&gt; LINEAR -&gt; SIGMOID backward (whole model)</li></ul><h4 id="6-1-Linear-backward"><a href="#6-1-Linear-backward" class="headerlink" title="6.1 Linear backward"></a>6.1 Linear backward</h4><p><img src="https://upload-images.jianshu.io/upload_images/8636110-0362454b836ff631.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_backward</span><span class="params">(dZ, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the linear portion of backward propagation for a single layer (layer l)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dZ -- Gradient of the cost with respect to the linear output (of current layer l)</span></span><br><span class="line"><span class="string">    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class="line"><span class="string">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class="line"><span class="string">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    A_prev, W, b = cache</span><br><span class="line">    m = A_prev.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></span><br><span class="line">    dW = np.dot(dZ,A_prev.T)/m</span><br><span class="line">    db = np.sum(dZ,axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)/m <span class="comment"># 这里要注意！！！</span></span><br><span class="line">    dA_prev = np.dot(W.T,dZ)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> (dA_prev.shape == A_prev.shape)</span><br><span class="line">    <span class="keyword">assert</span> (dW.shape == W.shape)</span><br><span class="line">    <span class="keyword">assert</span> (db.shape == b.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure><h4 id="6-2-Linear-Activation-backward"><a href="#6-2-Linear-Activation-backward" class="headerlink" title="6.2 Linear-Activation backward"></a>6.2 Linear-Activation backward</h4><p><img src="https://upload-images.jianshu.io/upload_images/8636110-4528f7692864b295.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span><span class="params">(dA, cache, activation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- post-activation gradient for current layer l </span></span><br><span class="line"><span class="string">    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently</span></span><br><span class="line"><span class="string">    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class="line"><span class="string">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class="line"><span class="string">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    linear_cache, activation_cache = cache</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        dZ = relu_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure><h4 id="6-3-L-Model-Backward"><a href="#6-3-L-Model-Backward" class="headerlink" title="6.3 L-Model Backward"></a>6.3 L-Model Backward</h4><p><img src="https://upload-images.jianshu.io/upload_images/8636110-11097ec0e5799543.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-d23ec336460f48b1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL, Y, caches)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    AL -- probability vector, output of the forward propagation (L_model_forward())</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat)</span></span><br><span class="line"><span class="string">    caches -- list of caches containing:</span></span><br><span class="line"><span class="string">                every cache of linear_activation_forward() with "relu" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)</span></span><br><span class="line"><span class="string">                the cache of linear_activation_forward() with "sigmoid" (it's caches[L-1])</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    grads -- A dictionary with the gradients</span></span><br><span class="line"><span class="string">             grads["dA" + str(l)] = ...</span></span><br><span class="line"><span class="string">             grads["dW" + str(l)] = ...</span></span><br><span class="line"><span class="string">             grads["db" + str(l)] = ...</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    L = len(caches) <span class="comment"># the number of layers</span></span><br><span class="line">    m = AL.shape[<span class="number">1</span>]</span><br><span class="line">    Y = Y.reshape(AL.shape) <span class="comment"># after this line, Y is the same shape as AL</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initializing the backpropagation</span></span><br><span class="line">    dAL = - (np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL))</span><br><span class="line">    current_cache = caches[L<span class="number">-1</span>]</span><br><span class="line">    grads[<span class="string">"dA"</span> + str(L)], grads[<span class="string">"dW"</span> + str(L)], grads[<span class="string">"db"</span> + str(L)] = linear_activation_backward(dAL,current_cache,<span class="string">"sigmoid"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> reversed(range(L<span class="number">-1</span>)):</span><br><span class="line">        current_cache = caches[l]</span><br><span class="line">        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class="string">"dA"</span>+str(l+<span class="number">2</span>)], current_cache, <span class="string">"relu"</span>)</span><br><span class="line">        grads[<span class="string">"dA"</span> + str(l+<span class="number">1</span>)] = dA_prev_temp</span><br><span class="line">        grads[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = dW_temp</span><br><span class="line">        grads[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = db_temp</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><h4 id="6-4-Update-Parameters"><a href="#6-4-Update-Parameters" class="headerlink" title="6.4 Update Parameters"></a>6.4 Update Parameters</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using gradient descent</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients, output of L_model_backward</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">                  parameters["W" + str(l)] = ... </span></span><br><span class="line"><span class="string">                  parameters["b" + str(l)] = ...</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    L = len(parameters) // <span class="number">2</span>  <span class="comment"># number of layers in the neural network</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)] = parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"b"</span>+str(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">"db"</span>+str(l+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h2 id="Deep-Neural-Network-for-Image-Classification-Application"><a href="#Deep-Neural-Network-for-Image-Classification-Application" class="headerlink" title="Deep Neural Network for Image Classification: Application"></a>Deep Neural Network for Image Classification: Application</h2><h3 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">from</span> dnn_app_utils_v2 <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">train_x_orig, train_y, test_x_orig, test_y, classes = load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Explore your dataset</span></span><br><span class="line">m_train = train_x_orig.shape[<span class="number">0</span>]</span><br><span class="line">num_px = train_x_orig.shape[<span class="number">1</span>]</span><br><span class="line">m_test = test_x_orig.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of training examples: "</span> + str(m_train))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of testing examples: "</span> + str(m_test))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Each image is of size: ("</span> + str(num_px) + <span class="string">", "</span> + str(num_px) + <span class="string">", 3)"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_x_orig shape: "</span> + str(train_x_orig.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_y shape: "</span> + str(train_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_x_orig shape: "</span> + str(test_x_orig.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_y shape: "</span> + str(test_y.shape))</span><br></pre></td></tr></table></figure><p><img src="https://upload-images.jianshu.io/upload_images/8636110-bfae12fb41ebccea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Reshape the training and test exmaples</span></span><br><span class="line">train_x_flatten = train_x_orig.reshape(train_x_orig.shape[<span class="number">0</span>],<span class="number">-1</span>).T <span class="comment"># The "-1" makes reshape flatten the remaining dimensions</span></span><br><span class="line">test_x_flatten = test_x_orig.reshape(test_x_orig.shape[<span class="number">0</span>],<span class="number">-1</span>).T</span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize data to have feature values between 0 and 1.</span></span><br><span class="line">train_x = train_x_flatten/<span class="number">255.</span></span><br><span class="line">test_x = test_x_flatten/<span class="number">255.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_x's shape: "</span> + str(train_x.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_x's shape: "</span> + str(test_x.shape))</span><br></pre></td></tr></table></figure><h3 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h3><h4 id="1-2-layer-neural-network"><a href="#1-2-layer-neural-network" class="headerlink" title="1. 2-layer neural network"></a>1. 2-layer neural network</h4><p><img src="https://upload-images.jianshu.io/upload_images/8636110-c612af784c677de0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>将要用到的函数：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-6a1485fb36d28b1c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>模型的构建<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### CONSTANTS DEFINING THE MODEL ####</span></span><br><span class="line">n_x = <span class="number">12288</span>     <span class="comment"># num_px * num_px * 3</span></span><br><span class="line">n_h = <span class="number">7</span></span><br><span class="line">n_y = <span class="number">1</span></span><br><span class="line">layers_dims = (n_x, n_h, n_y)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two_layer_model</span><span class="params">(X, Y, layers_dims, learning_rate=<span class="number">0.0075</span>, num_iterations=<span class="number">3000</span>, print_cost=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a two-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (n_x, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    layers_dims -- dimensions of the layers (n_x, n_h, n_y)</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">    print_cost -- If set to True, this will print the cost every 100 iterations </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- a dictionary containing W1, W2, b1, and b2</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    np.random=seed(<span class="number">1</span>)</span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = []</span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (n_x, n_h, n_y) = layers_dims</span><br><span class="line"></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line"></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        A1, cache1 = linear_activation_forward(X, W1, b1, <span class="string">"relu"</span>)</span><br><span class="line">        A2, cache2 = linear_activation_forward(A1, W2, b2, <span class="string">"sigmoid"</span>)</span><br><span class="line"></span><br><span class="line">        cost = compute_cost(A2, Y)</span><br><span class="line"></span><br><span class="line">        dA2 = - (np.divide(Y, A2) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - A2))</span><br><span class="line"></span><br><span class="line">        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, <span class="string">"sigmoid"</span>)</span><br><span class="line">        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, <span class="string">"relu"</span>)</span><br><span class="line"></span><br><span class="line">        grads[<span class="string">"dW1"</span>] = dW1</span><br><span class="line">        grads[<span class="string">'db1'</span>] = db1</span><br><span class="line">        grads[<span class="string">'dW2'</span>] = dW2</span><br><span class="line">        grads[<span class="string">'db2'</span>] = db2</span><br><span class="line"></span><br><span class="line">        <span class="comment">## Update parameters</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line"></span><br><span class="line">        W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">        b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">        W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">        b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration &#123;&#125;: &#123;&#125;"</span>.format(i, np.squeeze(cost)))</span><br><span class="line">            costs.append(cost)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line"></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure></p><p>训练模型：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = <span class="number">2500</span>, print_cost=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure></p><p>预测结果：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">predictions_train = predict(train_x, train_y, parameters)</span><br><span class="line"></span><br><span class="line">predictions_test = predict(test_x, test_y, parameters)</span><br></pre></td></tr></table></figure></p><p>Note: You may notice that running the model on fewer iterations (say 1500) gives better accuracy on the test set. This is called “early stopping” and we will talk about it in the next course. Early stopping is a way to prevent overfitting.</p><h4 id="2-L-layer-neural-network"><a href="#2-L-layer-neural-network" class="headerlink" title="2. L-layer neural network"></a>2. L-layer neural network</h4><p><img src="https://upload-images.jianshu.io/upload_images/8636110-b7a1501a32fd62ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>要用到的函数：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d929d0a3fb804874.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>模型的构建：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### CONSTANTS ###</span></span><br><span class="line">layers_dims = [<span class="number">12288</span>, <span class="number">20</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">1</span>] <span class="comment">#  5-layer model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_layer_mode</span><span class="params">(X, Y, layers_dims, learning_rate = <span class="number">0.0075</span>, num_iterations=<span class="number">3000</span>, print_cost=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    print_cost -- if True, it prints the cost every 100 steps</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    costs = []</span><br><span class="line"></span><br><span class="line">    parameters = initialize_parameters_deep(layers_dims)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        AL, caches = L_model_forward(X, parameters)</span><br><span class="line"></span><br><span class="line">        cost = compute_cost(AL, Y)</span><br><span class="line"></span><br><span class="line">        grads = L_model_backward(AL, Y, caches)</span><br><span class="line"></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p><p>训练模型：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = <span class="number">2500</span>, print_cost = <span class="keyword">True</span>)</span><br></pre></td></tr></table></figure></p><p>预测：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pred_train = predict(train_x, train_y, parameters)</span><br><span class="line">pred_test = predict(test_x, test_y, parameters)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;深层神经网络介绍&quot;&gt;&lt;a href=&quot;#深层神经网络介绍&quot; class=&quot;headerlink&quot; title=&quot;深层神经网络介绍&quot;&gt;&lt;/a&gt;深层神经网络介绍&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://upload-images.jianshu.io/upl
      
    
    </summary>
    
      <category term="深度学习" scheme="https://github.com/DesmonDay/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://github.com/DesmonDay/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>第3周-浅层神经网络</title>
    <link href="https://github.com/DesmonDay/2019/04/09/deep-learningw3/"/>
    <id>https://github.com/DesmonDay/2019/04/09/deep-learningw3/</id>
    <published>2019-04-09T11:04:06.000Z</published>
    <updated>2019-04-12T07:24:08.250Z</updated>
    
    <content type="html"><![CDATA[<p>下面进入第三周的学习内容。</p><h1 id="神经网络概述"><a href="#神经网络概述" class="headerlink" title="神经网络概述"></a>神经网络概述</h1><p><img src="https://upload-images.jianshu.io/upload_images/8636110-23bd9f4bd923a2c1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这里将神经网络与Logistic回归进行了一个简单的对比，即神经网络也需要前向传播和后向传播。另外，每一层的参数用右上标的一个方括号表示，即方括号用来区分不同的层；如果右上标是圆括号，则指具体的一个样本。</p><h1 id="神经网络表示"><a href="#神经网络表示" class="headerlink" title="神经网络表示"></a>神经网络表示</h1><p>下图是一个简单的双层神经网络的表示（即单隐层网络），其中也包括了一些通用的符号标记方法。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-08d351090e1131ac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="计算神经网络的输出"><a href="#计算神经网络的输出" class="headerlink" title="计算神经网络的输出"></a>计算神经网络的输出</h1><p>图中给出了第一层隐藏层的输入和输出过程，我们可以通过将参数w进行堆叠化，再与X向量相乘，与b向量相加，得到对应的输出Z^[1]和a^[1]。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-fe5a64b67e9273f3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>更简单的一个解释如下图，可以看到W,a,b,z的维度的相同点与不同点。我们固定输入特征为列向量，则隐藏层参数的维数则为（隐藏层结点个数，特征总数）。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-42f3688fd171800f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="多个样本的向量化"><a href="#多个样本的向量化" class="headerlink" title="多个样本的向量化"></a>多个样本的向量化</h1><p>如果没有进行向量化，那么我们要用如下的一个for循环来实现神经网络的前向传播：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-901146b4df18ad31.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如果进行向量化，就像下图红框中标出的，这是就不需要使用for循环了，而是一次性就可以计算所有样本。即我们的关键是，将X向量化，这样也能使得Z,A向量化。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7f01ef2229759bbd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>下面我们横向看A矩阵，矩阵A会扫过不同的训练样本，而竖向则经过当前隐藏层的不同单元；再以X矩阵为例，横向对应不同的训练样本，竖向则对应同一样本的不同输入特征（相当于输入层的不同单元）。</p><h2 id="形象解释"><a href="#形象解释" class="headerlink" title="形象解释"></a>形象解释</h2><p>接下来，对于Z^[1] = W^[1]X + b^[1]这样的向量化做了一个形象的解释。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b59d08b05b300678.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>简单的回顾，可以看到右下角的公式可以一次性计算所有的样本，而不需要使用for循环。但是我们会注意到，仍然有地方需要使用到for循环，即遍历所有的隐藏层来计算对应的输出。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-a4c6d5572ef3ea5d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="激活函数-Activation-functions"><a href="#激活函数-Activation-functions" class="headerlink" title="激活函数-Activation functions"></a>激活函数-Activation functions</h1><p>神经网络常见的几种激活函数：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e18af2b5b92895ac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这里有几点需要注意的，以及来自经验者的一些经验法则：</p><ol><li>如果是做二元分类，即输出值为0或1，则Sigmoid函数可以用作输出层的激活函数，其他情况下都不要使用Sigmoid函数。</li><li>一般的默认激活函数为ReLu(线性修正单元)，不过它的一个小缺点是当z小于0时，导数为0，因此有人提出了Leaky ReLu，不过挺少人使用的。</li><li>搭建神经网络时面临很多选择，如隐藏层单元数、激活函数、如何初始化权重等，当我们不确定哪些选择更有效时，可以使用交叉验证集尝试不同的选择，然后再找到最合适的来使用。</li></ol><h2 id="为什么神经网络需要非线性激活函数"><a href="#为什么神经网络需要非线性激活函数" class="headerlink" title="为什么神经网络需要非线性激活函数"></a>为什么神经网络需要非线性激活函数</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-35353f2a739253f6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如图，如果我们的隐藏层激活函数都使用恒等激活函数（线性激活函数），那么我们最终得到的只是输入的线性组合，即线性隐藏层毫无作用，因为两个线性函数的组合就是线性组合。因此，我们需要使用非线性激活函数才能使神经网络有效。当然，如果我们需要的真实输出y属于实数，那么我们可以在输出层使用线性激活函数，但是隐藏层则绝对不可以（例外是有关于一些压缩机制）。</p><h2 id="激活函数的导数"><a href="#激活函数的导数" class="headerlink" title="激活函数的导数"></a>激活函数的导数</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-0386034fffecbf96.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="神经网络的梯度下降公式"><a href="#神经网络的梯度下降公式" class="headerlink" title="神经网络的梯度下降公式"></a>神经网络的梯度下降公式</h1><p><img src="https://upload-images.jianshu.io/upload_images/8636110-fa731da059042b15.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h2><p>省略</p><h2 id="向量化写法"><a href="#向量化写法" class="headerlink" title="向量化写法"></a>向量化写法</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-7c8807e8d8e7d89a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h1><p>对于Logistic回归来说，我们可以将权重全部初始化为0，但是如果将神经网络的权重都初始化为0，则是行不通的。从图中可以知道，如果权重均为0，那么同一隐藏层中的两个隐藏层结点完全对称，即完全相同，因此并没有起到神经网络隐藏层的作用。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-4ff87e6dbe5cfc37.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>解决方法是进行随机初始化，而且应该初始化成小的参数。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-af4811753c122bd9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们使用np.random.randn(a,b)函数来进行随机初始化，注意到有个0.01的常数。对于浅层神经网络而言，常数设置为0.01也可以；但对于深层神经网络，则应该设置得更小。</p><h1 id="本周作业"><a href="#本周作业" class="headerlink" title="本周作业"></a>本周作业</h1><p>这周作业是创建一个单隐层神经网络，遇到了两个大坑！！！这两个大坑都是由于遇到了类似(100,)这样的向量，导致程序无法正常运行。今后在遇到bug的时候，第一个要想到这个问题！接下来记录一下整个的创建过程。</p><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-fc9be5f724a72f8e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这里就遇到了坑一，我们需要通过reshape函数来确认向量的维度才行。</p><h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h2><p>这里不再是上周作业那样自己从头到尾创建，而是直接调用sklearn库中的函数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">clf = sklearn.linear_model.LogisticRegressionCV();</span><br><span class="line">clf.fit(X.T, Y.T);</span><br></pre></td></tr></table></figure></p><p>其测试结果如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c3d9403ef759e289.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="Neural-Network-model"><a href="#Neural-Network-model" class="headerlink" title="Neural Network model"></a>Neural Network model</h2><p>我们的神经网络模型图形如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-2e2bf1e20cf5a83a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>接下来是整个的网络构建过程。</p><h3 id="确定神经网络结构-Defining-the-neural-network-structure"><a href="#确定神经网络结构-Defining-the-neural-network-structure" class="headerlink" title="确定神经网络结构-Defining the neural network structure"></a>确定神经网络结构-Defining the neural network structure</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_sizes</span><span class="params">(X, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- labels of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    n_x -- the size of the input layer</span></span><br><span class="line"><span class="string">    n_h -- the size of the hidden layer</span></span><br><span class="line"><span class="string">    n_y -- the size of the output layer</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></span><br><span class="line">    n_x = X.shape[<span class="number">0</span>] <span class="comment"># size of input layer</span></span><br><span class="line">    n_h = <span class="number">4</span></span><br><span class="line">    n_y = Y.shape[<span class="number">0</span>] <span class="comment"># size of output layer</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> (n_x, n_h, n_y)</span><br></pre></td></tr></table></figure><h3 id="初始化模型参数-Initialize-the-model’s-parameters"><a href="#初始化模型参数-Initialize-the-model’s-parameters" class="headerlink" title="初始化模型参数-Initialize the model’s parameters"></a>初始化模型参数-Initialize the model’s parameters</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    n_x -- size of the input layer</span></span><br><span class="line"><span class="string">    n_h -- size of the hidden layer</span></span><br><span class="line"><span class="string">    n_y -- size of the output layer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    params -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (n_h, n_x)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (n_h, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">2</span>) <span class="comment"># we set up a seed so that your output matches ours although the initialization is random.</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = np.random.randn(n_h,n_x)*<span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h,<span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y,n_h)*<span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y,<span class="number">1</span>))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span> (b1.shape == (n_h, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span> (W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span> (b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="循环过程-The-Loop"><a href="#循环过程-The-Loop" class="headerlink" title="循环过程-The Loop"></a>循环过程-The Loop</h3><p>前向传播过程：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    X -- input data of size (n_x, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters (output of initialization function)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A2 -- The sigmoid output of the second activation</span></span><br><span class="line"><span class="string">    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2"</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary "parameters"</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Implement Forward Propagation to calculate A2 (probabilities)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = np.tanh(Z1)</span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = sigmoid(Z2)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">assert</span>(A2.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line">    </span><br><span class="line">    cache = &#123;<span class="string">"Z1"</span>: Z1,</span><br><span class="line">             <span class="string">"A1"</span>: A1,</span><br><span class="line">             <span class="string">"Z2"</span>: Z2,</span><br><span class="line">             <span class="string">"A2"</span>: A2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A2, cache</span><br></pre></td></tr></table></figure></p><p>计算成本函数：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(A2, Y, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the cross-entropy cost given in equation (13)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters W1, b1, W2 and b2</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost -- cross-entropy cost given equation (13)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = Y.shape[<span class="number">1</span>] <span class="comment"># number of example</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the cross-entropy cost</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(<span class="number">1</span>-A2),<span class="number">1</span>-Y)</span><br><span class="line">    cost = -np.sum(logprobs) / m</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    cost = np.squeeze(cost)     <span class="comment"># makes sure cost is the dimension we expect. </span></span><br><span class="line">                                <span class="comment"># E.g., turns [[17]] into 17 </span></span><br><span class="line">    <span class="keyword">assert</span>(isinstance(cost, float))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure></p><p>后向传播：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span><span class="params">(parameters, cache, X, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation using the instructions above.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing our parameters </span></span><br><span class="line"><span class="string">    cache -- a dictionary containing "Z1", "A1", "Z2" and "A2".</span></span><br><span class="line"><span class="string">    X -- input data of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients with respect to different parameters</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># First, retrieve W1 and W2 from the dictionary "parameters".</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Retrieve also A1 and A2 from dictionary "cache".</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    A1 = cache[<span class="string">"A1"</span>]</span><br><span class="line">    A2 = cache[<span class="string">"A2"</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Backward propagation: calculate dW1, db1, dW2, db2. </span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 6 lines of code, corresponding to 6 equations on slide above)</span></span><br><span class="line">    dZ2 = A2 - Y</span><br><span class="line">    dW2 = np.dot(dZ2, A1.T) / m</span><br><span class="line">    db2 = np.sum(dZ2, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>) / m</span><br><span class="line">    dZ1 = np.multiply(np.dot(W2.T, dZ2), <span class="number">1</span>-np.power(A1,<span class="number">2</span>))</span><br><span class="line">    dW1 = np.dot(dZ1, X.T) / m</span><br><span class="line">    db1 = np.sum(dZ1, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>) / m</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dW1"</span>: dW1,</span><br><span class="line">             <span class="string">"db1"</span>: db1,</span><br><span class="line">             <span class="string">"dW2"</span>: dW2,</span><br><span class="line">             <span class="string">"db2"</span>: db2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure></p><p>参数更新：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate = <span class="number">1.2</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Updates parameters using the gradient descent update rule given above</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Retrieve each parameter from the dictionary "parameters"</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve each gradient from the dictionary "grads"</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    dW1 = grads[<span class="string">"dW1"</span>]</span><br><span class="line">    db1 = grads[<span class="string">"db1"</span>]</span><br><span class="line">    dW2 = grads[<span class="string">"dW2"</span>]</span><br><span class="line">    db2 = grads[<span class="string">"db2"</span>]</span><br><span class="line">    <span class="comment">## END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Update rule for each parameter</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 -= learning_rate * dW1</span><br><span class="line">    b1 -= learning_rate * db1</span><br><span class="line">    W2 -= learning_rate * dW2</span><br><span class="line">    b2 -= learning_rate * db2</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure></p><h3 id="整合最终模型-nn-model"><a href="#整合最终模型-nn-model" class="headerlink" title="整合最终模型-nn_model()"></a>整合最终模型-nn_model()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nn_model</span><span class="params">(X, Y, n_h, num_iterations = <span class="number">10000</span>, print_cost=False)</span>:</span></span><br><span class="line">    Y = Y.reshape((<span class="number">1</span>,<span class="number">-1</span>))</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- dataset of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- labels of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    n_h -- size of the hidden layer</span></span><br><span class="line"><span class="string">    num_iterations -- Number of iterations in gradient descent loop</span></span><br><span class="line"><span class="string">    print_cost -- if True, print the cost every 1000 iterations</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    n_x = layer_sizes(X, Y)[<span class="number">0</span>]</span><br><span class="line">    n_y = layer_sizes(X, Y)[<span class="number">2</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: "n_x, n_h, n_y". Outputs = "W1, b1, W2, b2, parameters".</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 5 lines of code)</span></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">         </span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">        <span class="comment"># Forward propagation. Inputs: "X, parameters". Outputs: "A2, cache".</span></span><br><span class="line">        A2, cache = forward_propagation(X, parameters)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Cost function. Inputs: "A2, Y, parameters". Outputs: "cost".</span></span><br><span class="line">        cost = compute_cost(A2, Y, parameters)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Backpropagation. Inputs: "parameters, cache, X, Y". Outputs: "grads".</span></span><br><span class="line">        grads = backward_propagation(parameters, cache, X, Y)</span><br><span class="line"> </span><br><span class="line">        <span class="comment"># Gradient descent parameter update. Inputs: "parameters, grads". Outputs: "parameters".</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate=<span class="number">1.2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the cost every 1000 iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="预测-Predictions"><a href="#预测-Predictions" class="headerlink" title="预测-Predictions"></a>预测-Predictions</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(parameters, X)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Using the learned parameters, predicts a class for each example in X</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></span><br><span class="line"><span class="string">    X -- input data of size (n_x, m)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    predictions -- vector of predictions of our model (red: 0 / blue: 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    A2, cache = forward_propagation(X, parameters)</span><br><span class="line">    <span class="comment"># predictions = [y&gt;0.5 for y in A2] # 这里要多学习！！</span></span><br><span class="line">    predictions = (A2 &gt; <span class="number">0.5</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> predictions</span><br><span class="line"></span><br><span class="line">parameters = nn_model(X, Y, n_h = <span class="number">4</span>, num_iterations = <span class="number">10000</span>, print_cost=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the decision boundary</span></span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict(parameters, x.T), X, Y)</span><br><span class="line">plt.title(<span class="string">"Decision Boundary for hidden layer size "</span> + str(<span class="number">4</span>))</span><br></pre></td></tr></table></figure><p>而具体的计算公式可以参考前面的内容。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;下面进入第三周的学习内容。&lt;/p&gt;
&lt;h1 id=&quot;神经网络概述&quot;&gt;&lt;a href=&quot;#神经网络概述&quot; class=&quot;headerlink&quot; title=&quot;神经网络概述&quot;&gt;&lt;/a&gt;神经网络概述&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://upload-images.j
      
    
    </summary>
    
      <category term="深度学习" scheme="https://github.com/DesmonDay/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://github.com/DesmonDay/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Attention is all you need</title>
    <link href="https://github.com/DesmonDay/2019/04/09/Attention-is-all-you-need/"/>
    <id>https://github.com/DesmonDay/2019/04/09/Attention-is-all-you-need/</id>
    <published>2019-04-09T05:28:21.000Z</published>
    <updated>2019-04-10T10:46:15.110Z</updated>
    
    <content type="html"><![CDATA[<p>阅读的第一篇NLP文章，大名鼎鼎的Transformer。由于缺乏较多的基础，因此这里也会参考别人的读书笔记进行阅读。</p><p>seq2seq是根据一个输入序列x，来生成另一个输出序列y。seq2seq有很多的应用，例如机器翻译，文档摘取，问答系统以及语音识别等等。在翻译中，输入序列是待翻译的文本，输出序列是翻译后的文本；在问答系统中，输入序列是提出的问题，而输出序列是答案；在语音识别中输入序列是一段语音，输出序列是翻译后的文字。（很贴合实际的一个问题！！）</p><h1 id="本文需要掌握的基础知识"><a href="#本文需要掌握的基础知识" class="headerlink" title="本文需要掌握的基础知识"></a>本文需要掌握的基础知识</h1><p>Seq2Seq基本概念、RNN、LSTM、CNN</p><h1 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h1><p>抛弃传统的encoder-decoder模型必须结合RNN或者CNN的固有模式，只使用Attention。文章的主要目的在于减少计算量和提高并行效率的同时不损害最终的实验结果，提出了两个新的Attention机制。</p><h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p><img src="https://upload-images.jianshu.io/upload_images/8636110-ea03b32ce715661b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>主流的处理序列问题，例如机器翻译，文档摘要，对话系统，QA等都是使用Encoder-Decoder框架，其中编码器：从单词序列到句子表示，解码器：从句子表示转化为单词序列分布。</p><p>传统的编码器解码器一般使用RNN，这也是机器翻译中最经典的模型，但RNN的缺点是：难以处理长序列句子，并且无法实现并行，且面临着对齐问题。之后这种Encoder-Decoder模型发展一般从三个方面入手：input的方向性——单向或双向，深层——单层或多层，类型——LSTM，RNN或GRU。然而，这种模型潜在的问题如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-59ddcfc298799285.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>神经网络需要能够将源语句的所有必要信息压缩成固定长度的向量，这可能使得神经网络难以应付长序列的句子，特别是那些比训练语料库中的句子更长的句子；<strong>每个时间步的输出需要依赖于前面时间步的输出</strong>，这使得模型没有办法并行，效率低；仍然面临对齐问题。</p><p>之后有人提出将CNN运用到deep NLP中，CNN不能直接用于处理变长的序列样本但可以实现并行计算。完全基于CNN的Seq2Seq模型虽然可以并行实现，但非常占内存，很多的trick，大数据量上参数调整并不容易。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7d91ff435291706d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>以上这些缺点的话就是由于无论输入如何变化，encoder给出的都是一个固定维数的向量，存在信息损失；在生成文本时，生成每个词所用到的语义向量都是一样的，这显然有些过于简单。为了解决上面提到的问题，一种可行的方案是引入attention mechanism。</p><p>随后就引入了Attention机制：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-3b4189ca93f4b58a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>深度学习中的Attention机制就是模拟人脑的注意力模型，举个简单例子，当我们观赏一幅画时，虽然我们可以看到整幅画的全貌，但我们深入观察时只注意到其中的一小部分，这个时候人的大脑主要关注在这一小块图案上，也就是说这个时候人脑对整幅图的关注并不是均衡的，是有一定的权重区分的。这就是深度学习里的Attention Model的核心思想。</p><p>所谓注意力机制，就是说在生成每个词的时候，对不同的输入词给予不同的关注权重。我们可以看一下上面👆这幅图——通过注意力机制，我们将输入句子编码为一个向量序列，并自适应地选择这些向量的一个子集，同时对译文进行译码，例如where are you——&gt;你在哪？现在我们在翻译“你”的时候给”you”更多的权重，那么就可以有效的解决对齐问题。</p><h1 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h1><p><img src="https://upload-images.jianshu.io/upload_images/8636110-d671a2e5ca27c7de.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Neural machine translation by jointly learning to align and translate这篇论文首先将注意力机制运用在NLP上，提出了soft Attention Model，并将其应用到机器翻译上面。其中，Soft是在求注意力分配概率分布的时候，对于输入句子X中任意一个单词都给出一个概率，即概率分布。加入注意力机制的模型表现确实更好，但存在一定问题：attention mechanism通常和RNN结合使用，我们都知道RNN依赖t-1的历史信息来计算t时刻的信息，因此不能并行实现，计算效率比较低，特别是训练样本量非常大的时候。</p><p>CNN+Attention: Abstractive Sentence Summarization with Attentive Recurrent Neural Networks, 2016NAACL<br><img src="https://upload-images.jianshu.io/upload_images/8636110-bca7113e475bae14.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>基于CNN的Seq2Seq+attention的优点：基于CNN的Seq2Seq模型具有基于RNN的Seq2Seq模型捕捉long distance dependency的能力，此外，最大的优点是可以并行化实现，效率比基于RNN的Seq2Seq模型高。缺点：计算量与观测序列X和输出序列Y的长度成正比。</p><h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p><img src="https://upload-images.jianshu.io/upload_images/8636110-e18a7a0c36bf8da2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="参考出处"><a href="#参考出处" class="headerlink" title="参考出处"></a>参考出处</h1><p><a href="https://blog.csdn.net/u013713117/article/details/56014634" target="_blank" rel="noopener">https://blog.csdn.net/u013713117/article/details/56014634</a></p><p><a href="https://www.jianshu.com/p/3f2d4bc126e6" target="_blank" rel="noopener">https://www.jianshu.com/p/3f2d4bc126e6</a></p><p>代码讲解：<a href="https://www.jianshu.com/p/b1030350aadb" target="_blank" rel="noopener">https://www.jianshu.com/p/b1030350aadb</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;阅读的第一篇NLP文章，大名鼎鼎的Transformer。由于缺乏较多的基础，因此这里也会参考别人的读书笔记进行阅读。&lt;/p&gt;
&lt;p&gt;seq2seq是根据一个输入序列x，来生成另一个输出序列y。seq2seq有很多的应用，例如机器翻译，文档摘取，问答系统以及语音识别等等。在
      
    
    </summary>
    
      <category term="论文阅读" scheme="https://github.com/DesmonDay/categories/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="paper" scheme="https://github.com/DesmonDay/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>Seq-embedding</title>
    <link href="https://github.com/DesmonDay/2019/04/04/Seq-embedding/"/>
    <id>https://github.com/DesmonDay/2019/04/04/Seq-embedding/</id>
    <published>2019-04-03T19:28:33.000Z</published>
    <updated>2019-04-03T19:35:08.408Z</updated>
    
    <content type="html"><![CDATA[<h1 id="用BERT生成句子向量"><a href="#用BERT生成句子向量" class="headerlink" title="用BERT生成句子向量"></a>用BERT生成句子向量</h1><p>一开始我用的是Linux虚拟机环境，后来发现内存不够，因此改为Windows10. 接下来是调用别人完成的一个接口。</p><ol><li><p>tensorflow版本要大于等于1.10, numpy也要更新</p></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install bert-serving-server  <span class="comment"># server</span></span><br><span class="line">pip install bert-serving-client  <span class="comment"># client, independent of `bert-serving-server`</span></span><br></pre></td></tr></table></figure></li><li><p>下载中文预训练模型，<a href="https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip，" target="_blank" rel="noopener">https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip，</a> 直接解压，得到对应的文件夹。</p></li><li><p>得到上面文件夹的绝对路径，输入命令（我用相对路径出错了）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bert-serving-start -model_dir=绝对路径 -num_work=2</span><br></pre></td></tr></table></figure></li></ol><p>例如我的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bert-serving-start -model_dir=E:\PostGraduate\NLP\chinese_L-12_H-768_A-12\ -num_worker=1</span><br></pre></td></tr></table></figure></p><p>完成后即开启了BERT服务，此时的命令窗口不可关闭，否则相当于关闭服务。</p><ol><li>python运行<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 本地运行</span></span><br><span class="line"><span class="keyword">from</span> bert_serving.client <span class="keyword">import</span> BertClient</span><br><span class="line">bc = BertClient()</span><br><span class="line">bc.encode([<span class="string">'开心的一天'</span>,<span class="string">'今天真开心'</span>,<span class="string">'开心！'</span>]) <span class="comment"># 文档建议一次性将需要的句子作为列表输入，而不是用for循环，这样效率较高</span></span><br></pre></td></tr></table></figure></li></ol><p>结果：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-9cc6d50a847295d9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="用ERNIE生成句子向量"><a href="#用ERNIE生成句子向量" class="headerlink" title="用ERNIE生成句子向量"></a>用ERNIE生成句子向量</h1><p>要用到PaddlePaddle，在Linux下似乎比较好，所以又转向了虚拟机…</p><ol><li>下载预训练模型: <a href="https://ernie.bj.bcebos.com/ERNIE_stable.tgz" target="_blank" rel="noopener">https://ernie.bj.bcebos.com/ERNIE_stable.tgz</a> 解压得到params文件夹</li><li>Linux安装PaddlePaddle: <a href="http://www.paddlepaddle.org/" target="_blank" rel="noopener">http://www.paddlepaddle.org/</a> 老师可以在此网站查看自己的电脑合适的安装方式，我是用pip装的。</li><li>下载需要的ERNIE源文件，需要的话我可以发给老师</li><li><p>完成脚本文件<br>注意：因为我没有cuda，所以use_cuda那里改为了false；${MODEL_PATH}我填的是模型的绝对路径；${TASK_DATA_PATH}我填的是数据的绝对路径，注意数据有特定的格式，而且必须是tsv文件。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> FLAGS_sync_nccl_allreduce=1</span><br><span class="line"><span class="built_in">export</span> CUDA_VISIBLE_DEVICES=7</span><br><span class="line"></span><br><span class="line">python -u ernir_encoder.py \</span><br><span class="line">                   --use_cuda <span class="literal">true</span> \</span><br><span class="line">                   --batch_size 32 \</span><br><span class="line">                   --output_dir <span class="string">"./test"</span> \</span><br><span class="line">                   --init_pretraining_params <span class="variable">$&#123;MODEL_PATH&#125;</span>/params \</span><br><span class="line">                   --data_set <span class="variable">$&#123;TASK_DATA_PATH&#125;</span>/lcqmc/dev.tsv \</span><br><span class="line">                   --vocab_path config/vocab.txt \</span><br><span class="line">                   --max_seq_len 128 \</span><br><span class="line">                   --ernie_config_path config/ernie_config.json</span><br></pre></td></tr></table></figure></li><li><p>建立文件夹test，执行上述bash文件，可以分别得到cls_emb.npy（存储句子embeddings）和top_layer_emb.npy（存储句子的token embeddings）。</p></li><li>读取npy文件内容：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-45050b69f313f3b3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></li></ol><h1 id="相似度比较"><a href="#相似度比较" class="headerlink" title="相似度比较"></a>相似度比较</h1><p>测试的句子：</p><pre><code>- 我可以给您介绍一下免费拿手机的活动吗？ 您可以给我介绍一下免费拿手机的活动吗？- 我可以给您介绍一下不要钱就能拿手机的活动吗？ 我来给您介绍一下免费就能得到好手机的活动- 我该叫你妈妈什么？ 我应该怎么称呼你妈妈？- 爸爸的儿子我应该叫什么？ 儿子的爸爸我该怎么叫？</code></pre><p>三种方法的比对结果：</p><div class="table-container"><table><thead><tr><th>seq1</th><th>seq2</th><th>LASER</th><th>BERT</th><th>ERNIE</th></tr></thead><tbody><tr><td>我可以给您介绍一下免费拿手机的活动吗？</td><td>您可以给我介绍一下免费拿手机的活动吗？</td><td>0.9675291</td><td>0.9962016</td><td>0.9903929 </td></tr><tr><td>我可以给您介绍一下不要钱就能拿手机的活动吗？</td><td>我来给您介绍一下免费就能得到好手机的活动。</td><td>0.810219</td><td>0.94765455</td><td>0.8651513</td></tr><tr><td>我该叫你妈妈什么？</td><td>我应该怎么称呼你妈妈？</td><td>0.9234945</td><td>0.9519638</td><td>0.9113665 </td></tr><tr><td>爸爸的儿子我应该叫什么？</td><td>儿子的爸爸我该怎么叫？</td><td>0.9514671</td><td>0.9730811</td><td>0.9775459</td></tr></tbody></table></div><p>给我的感觉是，三种模型都是基于词来进行句子向量的实现的，而没有考虑到句子的含义，比如最后那一句，爸爸的儿子，儿子的爸爸，这两句三个模型的相似度都很高。感觉不太妥当。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;用BERT生成句子向量&quot;&gt;&lt;a href=&quot;#用BERT生成句子向量&quot; class=&quot;headerlink&quot; title=&quot;用BERT生成句子向量&quot;&gt;&lt;/a&gt;用BERT生成句子向量&lt;/h1&gt;&lt;p&gt;一开始我用的是Linux虚拟机环境，后来发现内存不够，因此改为Win
      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/DesmonDay/categories/NLP/"/>
    
    
      <category term="embedding" scheme="https://github.com/DesmonDay/tags/embedding/"/>
    
  </entry>
  
  <entry>
    <title>第2周-神经网络基础</title>
    <link href="https://github.com/DesmonDay/2019/04/01/deep-learningw2/"/>
    <id>https://github.com/DesmonDay/2019/04/01/deep-learningw2/</id>
    <published>2019-04-01T09:26:58.000Z</published>
    <updated>2019-04-12T07:24:16.486Z</updated>
    
    <content type="html"><![CDATA[<p>下面进入神经网络基础的学习，这部分大多已经学过了，这里就当作是一次复习。</p><h1 id="二元分类-Binary-Classification"><a href="#二元分类-Binary-Classification" class="headerlink" title="二元分类-Binary Classification"></a>二元分类-Binary Classification</h1><p>图片用红绿蓝三个通道，通过这些输入预测是否是猫。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-58570aa3a16bf715.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>将这张猫的图片表示成输入的特征向量：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d344493c07cb5f2d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>对应的记号：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-075f444ce29f4d83.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>需要注意的是X是每个样本列向量的堆叠。</p><h1 id="Logistic回归-Logistic-Regression"><a href="#Logistic回归-Logistic-Regression" class="headerlink" title="Logistic回归-Logistic Regression"></a>Logistic回归-Logistic Regression</h1><p>logistic回归用于解决二元分类问题。这里要注意，P(y=1|x)指训练样本为x时，y为1的概率。在下图写的Output那里，最初我们是直接用一个线性分类器，但是因为我们希望输出的是概率，而w^T+b可能大于1，可能小于0，因此要添加sigmoid函数将其值域压缩到(0,1)。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-32b12b2fb0f2047d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>另外，红色部分那里是另一种表示形式，将b作为其中一个参数theta_0。在实现神经网络时，最好用蓝色部分的符号表示。</p><p>更清晰的表示如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-afc4b3675c063183.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="logistic回归损失函数-Logistic-Regression-cost-function"><a href="#logistic回归损失函数-Logistic-Regression-cost-function" class="headerlink" title="logistic回归损失函数-Logistic Regression cost function"></a>logistic回归损失函数-Logistic Regression cost function</h1><p><img src="https://upload-images.jianshu.io/upload_images/8636110-f81af276c1811d76.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>具体公式推导可以见上图。我们可以发现，最开始我们写的损失函数L是误差平方公式，然而这个函数的优化形式是非凸函数，如果求解会有多个局部最优解。使用梯度下降可能就找不到全局最优解。因此需要进一步地对损失函数进行研究。</p><ol><li>损失函数(loss function)定义为 L(y’, y) = -(ylogy’+(1-y)log(1-y’))，并且当y=1时，希望y’尽量大；当y=0时，希望y’尽量小。损失函数仅适用于单个的训练样本。</li><li>代价函数(cost function)如上图中所写，是整个训练集的平均损失函数。我们的目标就是找到合适的参数w和b来最小化cost function的值。</li></ol><h1 id="梯度下降法-Gradient-Descent"><a href="#梯度下降法-Gradient-Descent" class="headerlink" title="梯度下降法-Gradient Descent"></a>梯度下降法-Gradient Descent</h1><p>梯度下降法，初始化参数时可以随机初始化，因为对应的成本函数是凸函数，因此总会到达全局最优解。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-fc767b037a75dcde.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>更新公式：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f210c7aa4f359e44.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>这里要注意导数的符号表示，当变量为两个或多于两个时，使用花体符号；如果只有一个变量，可以用d表示。当我们要在代码中实现导数时，可以直接写成dw,db。</p><h1 id="计算图-Computation-Graph"><a href="#计算图-Computation-Graph" class="headerlink" title="计算图-Computation Graph"></a>计算图-Computation Graph</h1><p>一个简单的计算图例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-68afab65388cfbed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>其中，计算图是指用蓝色箭头画出来的从左到右的计算。</p><h1 id="计算图的导数计算-Derivatives-with-a-Computation-Graph"><a href="#计算图的导数计算-Derivatives-with-a-Computation-Graph" class="headerlink" title="计算图的导数计算-Derivatives with a Computation Graph"></a>计算图的导数计算-Derivatives with a Computation Graph</h1><p>这一小节，吴老师用一个简单的例子介绍了求导的链式法则以及反向传播的概念，并且也告诉我们在编程中如果需要求关于某个变量的导数，可以直接写作da, dvar之类的。 我们先求出了dv，然后利用dv求出da, du，接着再利用所求可以继续求出da, db, dc，即反向传播。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7825abe0a5eee6b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-b2ae3c7b8c474f00.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>因此，一个计算图就是从左到右的计算成本函数J，再从反向计算导数。</p><h1 id="Logistic回归中的梯度下降-Logistic-Regression-Gradient-Descent"><a href="#Logistic回归中的梯度下降-Logistic-Regression-Gradient-Descent" class="headerlink" title="Logistic回归中的梯度下降-Logistic Regression Gradient Descent"></a>Logistic回归中的梯度下降-Logistic Regression Gradient Descent</h1><p>本节介绍的是单个样本的Logistic回归中的梯度下降过程。如图，从右往左计算导数，其中还用到了求导的链式法则。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7b061c83f3839af5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="M个样本上进行logistic回归的梯度下降过程"><a href="#M个样本上进行logistic回归的梯度下降过程" class="headerlink" title="M个样本上进行logistic回归的梯度下降过程"></a>M个样本上进行logistic回归的梯度下降过程</h1><p>首先，我们需要回忆logistic回归的cost function如下。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d070f51fd777450f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>假设我们要求dw1，对应的公式如下。可以发现，我们需要累加每个样本对应的dw，最后还需要求平均。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-2bf7423664b0b8c6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>因此最终可以得到如下的更新过程：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-75dfcee80a8ec6bd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>然而，这样的更新过程有两个缺点：一是需要遍历所有的样本(i=1,2,…,m)；二是需要遍历所有的特征(dw1,dw2)。也就是说，需要两个for循环，然而在代码中显式地使用for循环会使效果低下，因此我们的解决方法是：Vectorization，向量化。</p><h1 id="向量化-Vectorization"><a href="#向量化-Vectorization" class="headerlink" title="向量化-Vectorization"></a>向量化-Vectorization</h1><p>向量化通常是消除我们代码中显示for循环语句的艺术。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-075ab455f7acdfd3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>从图中可以看到，左边是非向量化写法，即用for循环来实现矩阵乘法；而右边则是向量化写法，使用python中的numpy库来实现。下面给出在juypter notebook中实现的两种写法，对比其时间差异：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-2eb987492d9501b3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>显然，向量化的计算快了许多。可扩展深度学习一般是在GPU上运行的，而jupyter notebook是基于CPU的。GPU擅长SIMD指令(Single Instruction Multiple Data，但之灵多数据流)，而CPU的表现也不差。因此，我们看到numpy的向量化可以加速代码运行。因此，我们可以得到一个经验法则：不要显式地使用for循环。</p><h2 id="More-exmaples"><a href="#More-exmaples" class="headerlink" title="More exmaples"></a>More exmaples</h2><p>Neural network programming guideline: Whenever possible, avoid explicit for-loops.</p><ol><li><p>一个矩阵和向量的乘法计算：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-0794a79f9ba514d3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p></li><li><p>计算向量中每个元素的指数运算：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-11142b52a693d999.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p></li></ol><p>因此上述例子也告诉我们：在实现代码时，先看看我们能否用numpy的内置函数，而不是使用for循环。</p><p>接着，我们对logistic回归中的一个有关特征的循环进行向量化：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-23875c59a63e8893.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="向量化Logistic回归"><a href="#向量化Logistic回归" class="headerlink" title="向量化Logistic回归"></a>向量化Logistic回归</h1><h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-7569387a10a5a5db.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>从图中可以看到，我们可以将Z和A用向量化的方法得到对应的矩阵，即一次性计算了所有样本的w.T+b和a，而不需要使用for循环。</p><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>首先，吴老师使用向量化计算出参数b和w的梯度db,dw：注意左边是for循环做法，右边是向量化做法。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d3a9af1f9958495a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>接着是整个Logistic回归的向量化算法过程，将X作为矩阵参与计算：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ffe6d306dac83a4f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>可以发现，最后梯度下降的迭代仍然需要一个for循环来实现，这部分则是无法向量化的。</p><h1 id="Python中的广播-Broadcasting-in-Python"><a href="#Python中的广播-Broadcasting-in-Python" class="headerlink" title="Python中的广播-Broadcasting in Python"></a>Python中的广播-Broadcasting in Python</h1><p>首先，周老师给出了一个计算百分比的例子，用来说明广播的作用：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-76c074e494865c1c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>对应的python代码如下，可以发现，我们先使用了sum函数，这里axis=0时表示按列相加，而axis=1则表示按行相加；另外，这里用到广播的地方是percentage的计算。而老师说这里的reshape是可以去掉的，但是这个reshape其实也起到了一个保证我们的矩阵维数不会出错的作用。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-066827da288afd80.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>更多的广播例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7740e442d0adffd5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>广播的一些常用法则：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e79ce24e4500f7ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="A-note-on-python-numpy-vectors"><a href="#A-note-on-python-numpy-vectors" class="headerlink" title="A note on python/numpy vectors"></a>A note on python/numpy vectors</h1><h2 id="消除代码中秩为1的数组"><a href="#消除代码中秩为1的数组" class="headerlink" title="消除代码中秩为1的数组"></a>消除代码中秩为1的数组</h2><p>写向量时，不要这样写：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = np.random.randn(<span class="number">5</span>)</span><br><span class="line"><span class="comment"># a.shape = (5,) # 既不是行向量，也不是列向量</span></span><br></pre></td></tr></table></figure></p><p>而如果我们要把上述a转换成向量，可以用reshape函数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = a.reshape((<span class="number">5</span>,<span class="number">1</span>))</span><br><span class="line">a = a.reshape((<span class="number">1</span>,<span class="number">5</span>))</span><br></pre></td></tr></table></figure></p><p>应该这样写：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = np.random.randn(<span class="number">5</span>,<span class="number">1</span>) <span class="comment"># a.shape=(5,1) column vector</span></span><br><span class="line">a = np.random.randn(<span class="number">1</span>,<span class="number">5</span>) <span class="comment"># a.shape=(1,5) row vector</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span>(a.shape == (<span class="number">5</span>,<span class="number">1</span>)) <span class="comment"># 确保这是一个向量，而且执行速度很快</span></span><br></pre></td></tr></table></figure></p><h2 id="不要害羞，使用reshape或assert来保证维度不出错"><a href="#不要害羞，使用reshape或assert来保证维度不出错" class="headerlink" title="不要害羞，使用reshape或assert来保证维度不出错"></a>不要害羞，使用reshape或assert来保证维度不出错</h2><h1 id="Logistic回归中成本函数的证明"><a href="#Logistic回归中成本函数的证明" class="headerlink" title="Logistic回归中成本函数的证明"></a>Logistic回归中成本函数的证明</h1><p>Lost function函数的来由，我们最小化L(a,y)，实际上就是最大化logP(y|x)<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7ce97fae643e8ca0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>而对于整个训练集的成本函数：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-73f51176fb37e694.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在假设样本集为独立同分布的前提下，通过最大似然估计，可以知道我们在最小化cost function的同时，也在最大化似然估计。</p><h1 id="本周作业"><a href="#本周作业" class="headerlink" title="本周作业"></a>本周作业</h1><p>参考别人记录的作业内容，这里就顺便抄题目了！</p><h2 id="Part-1-Python-Basics-with-Numpy-optional-assignment"><a href="#Part-1-Python-Basics-with-Numpy-optional-assignment" class="headerlink" title="Part 1: Python Basics with Numpy (optional assignment)"></a>Part 1: Python Basics with Numpy (optional assignment)</h2><p>What we need to Remember:</p><ul><li>np.exp(x) works for any np.array x and applies the exponential function to every coordinate</li><li><p>the sigmoid function and its gradient</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: sigmoid</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># this means you can access numpy functions by writing np.function() instead of numpy.function()</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the sigmoid of x</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- A scalar or numpy array of any size</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    s -- sigmoid(x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    s = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_derivative</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.</span></span><br><span class="line"><span class="string">    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- A scalar or numpy array</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    ds -- Your computed gradient.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    s = sigmoid(x)</span><br><span class="line">    ds = s*(<span class="number">1</span>-s)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> ds</span><br></pre></td></tr></table></figure></li><li><p>image2vector is commonly used in deep learning</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GRADED FUNCTION: image2vector</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">image2vector</span><span class="params">(image)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    image -- a numpy array of shape (length, height, depth)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    v -- a vector of shape (length*height*depth, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    v = image.reshape(image.shape[<span class="number">0</span>]*image.shape[<span class="number">1</span>]*image.shape[<span class="number">2</span>], <span class="number">1</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> v</span><br></pre></td></tr></table></figure></li><li><p>np.reshape is widely used. In the future, you’ll see that keeping your matrix/vector dimensions straight will go toward eliminating a lot of bugs.</p></li><li>numpy has efficient built-in functions</li><li>broadcasting is extremely useful</li><li>Note that np.dot() performs a matrix-matrix or matrix-vector multiplication. This is different from np.multiply() and the * operator (which is equivalent to .* in Matlab/Octave), which performs an element-wise multiplication.</li><li>np.dot(x,x) = 所有对应位置元素相乘之和。</li><li>Vectorization is very important in deep learning. It provides computational efficiency and clarity.</li><li><p>You have reviewed the L1 and L2 loss.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L1</span><span class="params">(yhat, y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    yhat -- vector of size m (predicted labels)</span></span><br><span class="line"><span class="string">    y -- vector of size m (true labels)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    loss -- the value of the L1 loss function defined above</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    loss = np.sum(abs(y - yhat))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L2</span><span class="params">(yhat, y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    yhat -- vector of size m (predicted labels)</span></span><br><span class="line"><span class="string">    y -- vector of size m (true labels)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    loss -- the value of the L2 loss function defined above</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    loss = np.dot(y-yhat, y-yhat)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure></li><li><p>You are familiar with many numpy functions such as np.sum, np.dot, np.multiply, np.maximum, etc…</p></li></ul><h3 id="np-dot-np-outer-np-multiply"><a href="#np-dot-np-outer-np-multiply" class="headerlink" title="np.dot(),np.outer(),np.multiply(),*"></a>np.dot(),np.outer(),np.multiply(),*</h3><ol><li>np.dot()如果碰到的是秩为1的数组，那么执行的是对应位置的元素相乘再相加；如果遇到的是秩不为1的数组，那么执行的是矩阵相乘。需要注意的是矩阵与矩阵相乘是秩为2，矩阵和向量相乘秩为1。</li><li>np.multiply()表示的是数据和矩阵相应位置相乘，输出和输出的结果shape一致。</li><li>np.outer()表示的是两个向量相乘，拿第一个向量的元素<strong>分别</strong>与第二个向量所有元素相乘得到结果的一行。</li><li>*对数组执行的是对应位置相乘（成本函数里的就是这么计算！！！而不是用np.dot），对矩阵执行的是矩阵相乘。</li></ol><h2 id="Part-2-Logistic-Regression-with-a-Neural-Network-mindset"><a href="#Part-2-Logistic-Regression-with-a-Neural-Network-mindset" class="headerlink" title="Part 2: Logistic Regression with a Neural Network mindset"></a>Part 2: Logistic Regression with a Neural Network mindset</h2><p>这部分作业是完成一个Logistic回归算法，来分辨图片是否为猫。</p><h3 id="数据集预处理"><a href="#数据集预处理" class="headerlink" title="数据集预处理"></a>数据集预处理</h3><p>很多时候，我们经常会遇到的bug是有关于矩阵/向量维数的，因此我们必须保证清楚了解自己设置的矩阵维数是否正确。因此在写代码的过程中，时不时使用X.shape查看矩阵/向量的维数。</p><p>有关X.reshape()的一个小技巧：A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b ∗ c ∗ d, a) is to use: 这样子之后，矩阵的每一列都是一个样本。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_flatten = X.reshape(X.shape[<span class="number">0</span>], <span class="number">-1</span>).T      <span class="comment"># X.T is the transpose of X</span></span><br></pre></td></tr></table></figure></p><p><strong>What we need to remember:</strong><br>Common steps for pre-processing a new dataset are:</p><ul><li>Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, …)</li><li>Reshape the datasets such that each example is now a vector of size(num_px*num_px*3,1)</li><li>“Standardize” the data 标准化</li></ul><h3 id="Gereral-Architecture-of-the-learning-algorithm"><a href="#Gereral-Architecture-of-the-learning-algorithm" class="headerlink" title="Gereral Architecture of the learning algorithm"></a>Gereral Architecture of the learning algorithm</h3><p><img src="https://upload-images.jianshu.io/upload_images/8636110-20e5400369b44374.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-7ac0d47a6a9ba9c9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>关键步骤：</p><ul><li>Initialize the parameters of the model</li><li>Learn the parameters for the model by minimizing the cost  </li><li>Use the learned parameters to make predictions (on the test set)</li><li>Analyse the results and conclude</li></ul><h3 id="算法的各个模块"><a href="#算法的各个模块" class="headerlink" title="算法的各个模块"></a>算法的各个模块</h3><p>主要步骤：</p><ol><li>Define the model structure (such as number of input features)</li><li>Initialize the model’s parameters</li><li>Loop:<ul><li>Calculate current loss (forward propagation)</li><li>Calculate current gradient (backward propagation)</li><li>Update parameters (gradient descent)</li></ul></li></ol><h4 id="Helper-functions-Sigmoid"><a href="#Helper-functions-Sigmoid" class="headerlink" title="Helper functions-Sigmoid"></a>Helper functions-Sigmoid</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the sigmoid of z</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    z -- A scalar or numpy array of any size.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    s -- sigmoid(z)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    s = <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> s</span><br></pre></td></tr></table></figure><h4 id="初始化参数-Initializing-parameters"><a href="#初始化参数-Initializing-parameters" class="headerlink" title="初始化参数-Initializing parameters"></a>初始化参数-Initializing parameters</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_with_zeros</span><span class="params">(dim)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    dim -- size of the w vector we want (or number of parameters in this case)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    w -- initialized vector of shape (dim, 1)</span></span><br><span class="line"><span class="string">    b -- initialized scalar (corresponds to the bias)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    w = np.zeros((dim, <span class="number">1</span>))</span><br><span class="line">    b = <span class="number">0</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(w.shape == (dim, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(isinstance(b, float) <span class="keyword">or</span> isinstance(b, int))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> w, b</span><br></pre></td></tr></table></figure><h4 id="Forward-and-Backward-propagation"><a href="#Forward-and-Backward-propagation" class="headerlink" title="Forward and Backward propagation"></a>Forward and Backward propagation</h4><p>要用到的公式：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f065864226e11d7e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这里有个地方我以前经常搞混，就是不知道什么时候用np.dot，np.multiply和*，现在清楚了：普通的矩阵乘法就是用np.dot，而对应位置相乘，如上图里面cost function的计算，则要用到np.multiply或者*。弄清楚了就不会错了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">propagate</span><span class="params">(w, b, X, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the cost function and its gradient for the propagation explained above</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of size (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return:</span></span><br><span class="line"><span class="string">    cost -- negative log-likelihood cost for logistic regression</span></span><br><span class="line"><span class="string">    dw -- gradient of the loss with respect to w, thus same shape as w</span></span><br><span class="line"><span class="string">    db -- gradient of the loss with respect to b, thus same shape as b</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Tips:</span></span><br><span class="line"><span class="string">    - Write your code step by step for the propagation. np.log(), np.dot()</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>] <span class="comment"># 样本数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># FORWARD PROPAGATION (FROM X TO COST)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    A = sigmoid(np.dot(w.T, X) + b)</span><br><span class="line">    cost = -(<span class="number">1.0</span>/m) * np.sum(Y*np.log(A) + (<span class="number">1</span>-Y)*np.log(<span class="number">1</span>-A))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># BACKWARD PROPAGATION (TO FIND GRAD)</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    dw = np.dot(X, (A-Y).T) / m</span><br><span class="line">    db = np.sum(A-Y) / m</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">assert</span>(dw.shape == w.shape)</span><br><span class="line">    <span class="keyword">assert</span>(db.dtype == float)</span><br><span class="line">    cost = np.squeeze(cost)</span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,</span><br><span class="line">             <span class="string">"db"</span>: db&#125;</span><br><span class="line">    <span class="keyword">return</span> grads, cost</span><br></pre></td></tr></table></figure><h4 id="Optimization-更新"><a href="#Optimization-更新" class="headerlink" title="Optimization-更新"></a>Optimization-更新</h4><p>利用更新公式: theta = theta - alpha * dtheta<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize</span><span class="params">(w, b, X, Y, num_iterations, learning_rate, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    This function optimizes w and b by running a gradient descent algorithm</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of shape (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">    print_cost -- True to print the loss every 100 steps</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    params -- dictionary containing the weights w and bias b</span></span><br><span class="line"><span class="string">    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function</span></span><br><span class="line"><span class="string">    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Tips:</span></span><br><span class="line"><span class="string">    You basically need to write down two steps and iterate through them:</span></span><br><span class="line"><span class="string">        1) Calculate the cost and the gradient for the current parameters. Use propagate().</span></span><br><span class="line"><span class="string">        2) Update the parameters using gradient descent rule for w and b.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    costs = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Cost and gradient calculation (≈ 1-4 lines of code)</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### </span></span><br><span class="line">        grads, cost = propagate(w, b, X, Y)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Retrieve derivatives from grads</span></span><br><span class="line">        dw = grads[<span class="string">"dw"</span>]</span><br><span class="line">        db = grads[<span class="string">"db"</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># update rule (≈ 2 lines of code)</span></span><br><span class="line">        <span class="comment">### START CODE HERE ###</span></span><br><span class="line">        w = w - learning_rate*dw</span><br><span class="line">        b = b - learning_rate*db</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Record the costs</span></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the cost every 100 training examples</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">    </span><br><span class="line">    params = &#123;<span class="string">"w"</span>: w,</span><br><span class="line">              <span class="string">"b"</span>: b&#125;</span><br><span class="line">    </span><br><span class="line">    grads = &#123;<span class="string">"dw"</span>: dw,</span><br><span class="line">             <span class="string">"db"</span>: db&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> params, grads, costs</span><br></pre></td></tr></table></figure></p><h4 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(w, b, X)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span></span><br><span class="line"><span class="string">    b -- bias, a scalar</span></span><br><span class="line"><span class="string">    X -- data of size (num_px * num_px * 3, number of examples)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    Y_prediction = np.zeros((<span class="number">1</span>,m))</span><br><span class="line">    w = w.reshape(X.shape[<span class="number">0</span>], <span class="number">1</span>) <span class="comment"># 保证维数正确</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute vector "A" predicting the probabilities of a cat being present in the picture</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    A = sigmoid(np.dot(w.T,X) + b)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(A.shape[<span class="number">1</span>]):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert probabilities A[0,i] to actual predictions p[0,i]</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">        <span class="keyword">if</span> A[<span class="number">0</span>][i] &gt; <span class="number">0.5</span>: </span><br><span class="line">            Y_prediction[<span class="number">0</span>][i] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            Y_prediction[<span class="number">0</span>][i] = <span class="number">0</span></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(Y_prediction.shape == (<span class="number">1</span>, m))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Y_prediction</span><br></pre></td></tr></table></figure><h3 id="合并模块-Merge-all-functions-into-a-model"><a href="#合并模块-Merge-all-functions-into-a-model" class="headerlink" title="合并模块-Merge all functions into a model"></a>合并模块-Merge all functions into a model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_test, Y_test, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.5</span>, print_cost = False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Builds the logistic regression model by calling the function you've implemented previously</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)</span></span><br><span class="line"><span class="string">    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)</span></span><br><span class="line"><span class="string">    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)</span></span><br><span class="line"><span class="string">    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)</span></span><br><span class="line"><span class="string">    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters</span></span><br><span class="line"><span class="string">    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()</span></span><br><span class="line"><span class="string">    print_cost -- Set to true to print the cost every 100 iterations</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    d -- dictionary containing information about the model.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize parameters with zeros (≈ 1 line of code)</span></span><br><span class="line">    w, b = initialize_with_zeros(X_train.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Gradient descent (≈ 1 line of code)</span></span><br><span class="line">    params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve parameters w and b from dictionary "parameters"</span></span><br><span class="line">    w = params[<span class="string">"w"</span>]</span><br><span class="line">    b = params[<span class="string">"b"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Predict test/train set examples (≈ 2 lines of code)</span></span><br><span class="line">    Y_prediction_train = predict(w, b, X_train)</span><br><span class="line">    Y_prediction_test = predict(w, b, X_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Print train/test Errors</span></span><br><span class="line">    print(<span class="string">"train accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_train - Y_train)) * <span class="number">100</span>))</span><br><span class="line">    print(<span class="string">"test accuracy: &#123;&#125; %"</span>.format(<span class="number">100</span> - np.mean(np.abs(Y_prediction_test - Y_test)) * <span class="number">100</span>))</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    d = &#123;<span class="string">"costs"</span>: costs,</span><br><span class="line">         <span class="string">"Y_prediction_test"</span>: Y_prediction_test, </span><br><span class="line">         <span class="string">"Y_prediction_train"</span> : Y_prediction_train, </span><br><span class="line">         <span class="string">"w"</span> : w, </span><br><span class="line">         <span class="string">"b"</span> : b,</span><br><span class="line">         <span class="string">"learning_rate"</span> : learning_rate,</span><br><span class="line">         <span class="string">"num_iterations"</span>: num_iterations&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> d</span><br><span class="line"></span><br><span class="line">d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = <span class="number">2000</span>, learning_rate = <span class="number">0.005</span>, print_cost = <span class="keyword">True</span>) <span class="comment">#训练</span></span><br></pre></td></tr></table></figure><h3 id="有关学习率的进一步学习"><a href="#有关学习率的进一步学习" class="headerlink" title="有关学习率的进一步学习"></a>有关学习率的进一步学习</h3><p>If the learning rate is too large we may “overshoot” the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That’s why it is crucial to use a well-tuned(精心调整) learning rate.</p><p>In deep learning, 我们建议：</p><ol><li>Choose the learning rate that better minimizes the cost function. 选择合适的学习率。</li><li>If your model overfits, use other techniques to reduce overfitting. (We’ll talk about this in later videos.) 减少过拟合。</li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过这次任务，我们学习到：</p><ol><li>Preprocessing the dataset is important.</li><li>You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model().</li><li>Tuning the learning rate (which is an example of a “hyperparameter”) can make a big difference to the algorithm. You will see more examples of this later in this course!</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;下面进入神经网络基础的学习，这部分大多已经学过了，这里就当作是一次复习。&lt;/p&gt;
&lt;h1 id=&quot;二元分类-Binary-Classification&quot;&gt;&lt;a href=&quot;#二元分类-Binary-Classification&quot; class=&quot;headerlink&quot; tit
      
    
    </summary>
    
      <category term="深度学习" scheme="https://github.com/DesmonDay/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://github.com/DesmonDay/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>第1周-深度学习概论</title>
    <link href="https://github.com/DesmonDay/2019/04/01/deep-learningw1/"/>
    <id>https://github.com/DesmonDay/2019/04/01/deep-learningw1/</id>
    <published>2019-04-01T07:48:01.000Z</published>
    <updated>2019-04-12T07:24:22.514Z</updated>
    
    <content type="html"><![CDATA[<p>今天，愚人节。开始学习吴恩达的deep learning课程！！（这个不是愚人节节目）因为学费还是有点贵，所以打算在网易云课堂看对应的免费课程，只是这样就没法交作业批改了。因此作业这块会去看看别人博客里记录的作业内容，自己完成后对比一下别人的结果就好。哇，省了一大笔钱，真好。</p><h1 id="简单概念"><a href="#简单概念" class="headerlink" title="简单概念"></a>简单概念</h1><p>“深度学习”指的是训练神经网络，并且神经网络的规模比较大。依旧是房价预测的例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-a4c69d5a968935fb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>可以看到右下角，我们可以称其为最简单的神经网络结构，即单神经元网络(single neural network)。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-8916348065e0eb14.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>更大一点的神经网络，则将这些神经元进行堆叠，得到大一点的神经网络。仍然以房价预测为例，假设我们不仅知道size，还知道bedroom的数量，邮编(zip code / posted code)，wealth等等，得到以下神经网络：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-491f70f358ed02a9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>其中x是神经网络的输入，而y则是输出，中间节电成为隐藏结点。画成更简洁的形式如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-96dfed4dea7195ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>给定足够的训练数据(x,y)(training examples)，那么神经网络非常擅长于计算从X到Y的精准映射函数(figuring out functions that accurately map from x to y)。</p><h1 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h1><p>简单例子：神经网络在在线网络(online ad)取得了很大成功，同样的也是计算机视觉、语音识别、机器翻译、无人驾驶等领域有着巨大应用。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-a54dd1376b4fd78d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>因此我们需要机智且正确的选择X和y，才能解决我们特定的问题。房地产和在线广告，用的是相对标准的神经网络；图像邻域则常用卷积神经网络(CNN)；对于序列数据，如音频含有时间成分(一维时间序列)，语言，经常使用RNN；而无人驾驶(计算机视觉,radar)则需要更多复杂的神经网络。RNN非常适合用来处理一维序列数据（包含时间成分）。部分网络结构如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-5d488b501cd48aa8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>结构化数据和非结构化数据的区分：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-55d0951d45f80c93.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="为什么深度学习会兴起"><a href="#为什么深度学习会兴起" class="headerlink" title="为什么深度学习会兴起"></a>为什么深度学习会兴起</h1><p>普通机器学习无法处理大规模数据，而如今我们已经获得了海量数据，因此需要使用深度学习来进行大型训练。如以下例子:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-a301992dbb2aaf97.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>因此我们说，“规模”不断推动着神经网络的进步，这里的“规模”既指神经网络本身的规模（更多的隐藏层结点，更多的权重和连接），也指数据规模。符号标记：(X,y)训练数据，m表示数据集规模。当数据集较小时，一般比较手工设计组件(hand engineer feature)，如SVM表现上更好。因此在此图左侧，每个算法的性能定义得并不明确，更多取决于设计细节；只有在大数据邻域，我们可以看到神经网络稳定超越其他算法。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-54c712695baf94e5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>除了有数据规模的扩大和计算能力的增强，我们的算法也不断得到优化。如上图中举例Sigmoid函数和ReLu函数，因为Sigmoid函数在后半段的梯度接近0，因此会导致学习速度减慢（梯度下降法），因此通过改变激活函数为ReLu，可以使梯度下降的速度加快。</p><p>在实现神经网络时，迭代速度对效率影响很大，因此计算速度的提升有利于提高迭代速度。</p><h1 id="本部分的学习内容"><a href="#本部分的学习内容" class="headerlink" title="本部分的学习内容"></a>本部分的学习内容</h1><p><img src="https://upload-images.jianshu.io/upload_images/8636110-90d9e634b92545a6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="第一周作业"><a href="#第一周作业" class="headerlink" title="第一周作业"></a>第一周作业</h1><p>十道选择题，这里就不贴了。记录一些关键点：</p><ol><li>一个表现很好的深度学习模型，并不是仅靠经验就能立马找到的。虽然经验很重要，但要找到一个表现很好的模型，都需要尝试，修复，不断完善的一个过程。</li><li>一个团队如果能更快的用代码实现想法，或者写出来的代码（算法）更好，或者科学家研究出了更优秀的算法，上述循环过程都会更短，从而提高效率。</li><li>Leaky ReLu:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-41c4532d2e603676.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></li><li>激活函数相关：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7e7cda8575746ecb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></li><li>RNN相关：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-bfd240bf75000aa9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>选择AC。此题考查我们对RNN（循环神经网络）的了解。RNN在语音识别，语言建模，翻译，图片描述等问题上已经取得一定成功。它是一种监督学习，比如输入数据英文，标签为法文。RNN 可以被看做是同一神经网络的多次赋值，每个神经网络模块会把消息传递给下一个，所以它是链式的，链式的特征揭示了 RNN 本质上是与序列和列表相关的，所以它在解决sequence上是毫无问题的。要说哪个完全比另一个强，基本都是错的。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今天，愚人节。开始学习吴恩达的deep learning课程！！（这个不是愚人节节目）因为学费还是有点贵，所以打算在网易云课堂看对应的免费课程，只是这样就没法交作业批改了。因此作业这块会去看看别人博客里记录的作业内容，自己完成后对比一下别人的结果就好。哇，省了一大笔钱，真好
      
    
    </summary>
    
      <category term="深度学习" scheme="https://github.com/DesmonDay/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://github.com/DesmonDay/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>LASER-句子嵌入</title>
    <link href="https://github.com/DesmonDay/2019/04/01/LASER/"/>
    <id>https://github.com/DesmonDay/2019/04/01/LASER/</id>
    <published>2019-04-01T03:04:42.000Z</published>
    <updated>2019-04-01T07:47:51.876Z</updated>
    
    <content type="html"><![CDATA[<h1 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h1><p>最好是Linux环境，Windows没有试。</p><p>老师直接下载邮件附件的LASER压缩包(里面包括了已经下载好的模型)即可，直接Extract here。解压出来为LASER文件夹，文件夹的位置即为其安装位置，后面环境变量会用到。文件夹内部内容为下图：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-13a57c3e4782eb58.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="安装环境依赖"><a href="#安装环境依赖" class="headerlink" title="安装环境依赖"></a>安装环境依赖</h1><ol><li>python 3.6, numpy</li><li><p>pytorch 1.0(这个老师应该装了吧？)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cpu版本: conda install pytorch-cpu torchvision-cpu(我用的Anaconda，且环境是虚拟机，所以用的是这个命令)</span><br><span class="line">gpu版本: pip install torch torchvision</span><br></pre></td></tr></table></figure></li><li><p>Faiss(需要把镜像源换成 mirrors.ustc.edu.cn/ubuntu/ 才能正常下载)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install faiss</span><br></pre></td></tr></table></figure></li><li><p>jieba，中文分词器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install jieba</span><br></pre></td></tr></table></figure></li></ol><h1 id="其他必要步骤"><a href="#其他必要步骤" class="headerlink" title="其他必要步骤"></a>其他必要步骤</h1><h2 id="添加环境变量"><a href="#添加环境变量" class="headerlink" title="添加环境变量"></a>添加环境变量</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vim .bashrc</span><br><span class="line"></span><br><span class="line">在文件末尾添加，双引号内为LASER的安装目录</span><br><span class="line">LASER=&quot;/home/desmon/LASER&quot; </span><br><span class="line"></span><br><span class="line">source .baserc</span><br></pre></td></tr></table></figure><h1 id="测试结果"><a href="#测试结果" class="headerlink" title="测试结果"></a>测试结果</h1><p>因为老师只需要得到句子的嵌入和两个句子之间的相似度，这里我只介绍在embed文件夹中如何使用其预训练模型（其他的我也没试过，囧）。</p><ol><li>进入tasks/embed文件夹，将要计算相似度的句子写成文件，如下图input文件（这里举例为中文，也可以多个句子，每个句子要自成一行）。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f2c34371ba5cc00c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></li><li><p>命令：input-file和output-file即我们对应的输入输出文件，由于输出的格式为二进制，所以之后要处理。而LANGUAGE则为对应的语言，如 中文对应cmn, 英文对应eng，法语对应fr等等。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">bash ./embed.sh INPUT-FILE LANGUAGE OUTPUT-FILE</span><br><span class="line"></span><br><span class="line">例如：bash ./embed.sh input cmn output</span><br></pre></td></tr></table></figure></li><li><p>将二进制输出可视化，对应embed文件夹里的main.py，老师需要将里面的output改为自己设置的输出名字。</p></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity</span><br><span class="line"></span><br><span class="line">dim = <span class="number">1024</span></span><br><span class="line">X = np.fromfile(<span class="string">"output"</span>,dtype=np.float32,count=<span class="number">-1</span>)</span><br><span class="line">X.resize(X.shape[<span class="number">0</span>]//dim, dim)</span><br><span class="line">print(cosine_similarity(X))</span><br></pre></td></tr></table></figure><p>修改正确后直接运行此py文件，即可得到对应的余弦相似度矩阵。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;下载&quot;&gt;&lt;a href=&quot;#下载&quot; class=&quot;headerlink&quot; title=&quot;下载&quot;&gt;&lt;/a&gt;下载&lt;/h1&gt;&lt;p&gt;最好是Linux环境，Windows没有试。&lt;/p&gt;
&lt;p&gt;老师直接下载邮件附件的LASER压缩包(里面包括了已经下载好的模型)即可，直接E
      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/DesmonDay/categories/NLP/"/>
    
    
      <category term="embedding" scheme="https://github.com/DesmonDay/tags/embedding/"/>
    
  </entry>
  
  <entry>
    <title>hdoj3342-拓扑排序</title>
    <link href="https://github.com/DesmonDay/2019/03/10/hdoj3342/"/>
    <id>https://github.com/DesmonDay/2019/03/10/hdoj3342/</id>
    <published>2019-03-10T04:05:51.000Z</published>
    <updated>2019-03-10T04:08:09.705Z</updated>
    
    <content type="html"><![CDATA[<h1 id="题目描述-Legal-or-not"><a href="#题目描述-Legal-or-not" class="headerlink" title="题目描述-Legal or not"></a>题目描述-Legal or not</h1><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p>ACM-DIY is a large QQ group where many excellent acmers get together. It is so harmonious that just like a big family. Every day,many “holy cows” like HH, hh, AC, ZT, lcc, BF, Qinz and so on chat on-line to exchange their ideas. When someone has questions, many warm-hearted cows like Lost will come to help. Then the one being helped will call Lost “master”, and Lost will have a nice “prentice”. By and by, there are many pairs of “master and prentice”. But then problem occurs: there are too many masters and too many prentices, how can we know whether it is legal or not?</p><p>We all know a master can have many prentices and a prentice may have a lot of masters too, it’s legal. Nevertheless，some cows are not so honest, they hold illegal relationship. Take HH and 3xian for instant, HH is 3xian’s master and, at the same time, 3xian is HH’s master,which is quite illegal! To avoid this,please help us to judge whether their relationship is legal or not. </p><p>Please note that the “master and prentice” relation is transitive. It means that if A is B’s master ans B is C’s master, then A is C’s master.</p><h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>The input consists of several test cases. For each case, the first line contains two integers, N (members to be tested) and M (relationships to be tested)(2 &lt;= N, M &lt;= 100). Then M lines follow, each contains a pair of (x, y) which means x is y’s master and y is x’s prentice. The input is terminated by N = 0.<br>TO MAKE IT SIMPLE, we give every one a number (0, 1, 2,…, N-1). We use their numbers instead of their names.</p><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p>For each test case, print in one line the judgement of the messy relationship.<br>If it is legal, output “YES”, otherwise “NO”.</p><h2 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a>Sample Input</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">3 2</span><br><span class="line">0 1</span><br><span class="line">1 2</span><br><span class="line">2 2</span><br><span class="line">0 1</span><br><span class="line">1 0</span><br><span class="line">0 0</span><br></pre></td></tr></table></figure><h2 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a>Sample Output</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">YES</span><br><span class="line">NO</span><br></pre></td></tr></table></figure><h1 id="思路分析"><a href="#思路分析" class="headerlink" title="思路分析"></a>思路分析</h1><p>判断是否为有向无环图，使用拓扑排序的方法。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stack&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; edge[<span class="number">110</span>];</span><br><span class="line">    <span class="built_in">stack</span>&lt;<span class="keyword">int</span>&gt; S;</span><br><span class="line">    <span class="keyword">int</span> inDegree[<span class="number">110</span>];</span><br><span class="line">    <span class="keyword">int</span> n, m;</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%d%d"</span>, &amp;n, &amp;m) != EOF)&#123;</span><br><span class="line">        <span class="keyword">if</span>(n == <span class="number">0</span>) <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;n; i++)&#123;</span><br><span class="line">            edge[i].clear();</span><br><span class="line">            inDegree[i] = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span>(!S.empty()) S.pop();</span><br><span class="line">        <span class="keyword">while</span>(m--)&#123;</span><br><span class="line">            <span class="keyword">int</span> a, b;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d%d"</span>, &amp;a, &amp;b);</span><br><span class="line">            inDegree[b]++;</span><br><span class="line">            edge[a].push_back(b);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;n; i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(inDegree[i] == <span class="number">0</span>)&#123;</span><br><span class="line">                S.push(i);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> cnt = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span>(!S.empty())&#123;</span><br><span class="line">            <span class="keyword">int</span> x = S.top();</span><br><span class="line">            S.pop();</span><br><span class="line">            cnt++;</span><br><span class="line">            <span class="keyword">int</span> j;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;edge[x].size(); i++)&#123;</span><br><span class="line">                j = edge[x][i];</span><br><span class="line">                inDegree[j]--;</span><br><span class="line">                <span class="keyword">if</span>(inDegree[j] == <span class="number">0</span>) S.push(j);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(cnt == n) <span class="built_in">printf</span>(<span class="string">"YES\n"</span>);</span><br><span class="line">        <span class="keyword">else</span> <span class="built_in">printf</span>(<span class="string">"NO\n"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;题目描述-Legal-or-not&quot;&gt;&lt;a href=&quot;#题目描述-Legal-or-not&quot; class=&quot;headerlink&quot; title=&quot;题目描述-Legal or not&quot;&gt;&lt;/a&gt;题目描述-Legal or not&lt;/h1&gt;&lt;h2 id=&quot;Probl
      
    
    </summary>
    
      <category term="hdoj" scheme="https://github.com/DesmonDay/categories/hdoj/"/>
    
    
      <category term="C++" scheme="https://github.com/DesmonDay/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>hdoj2544-最短路径</title>
    <link href="https://github.com/DesmonDay/2019/03/10/hdoj2544/"/>
    <id>https://github.com/DesmonDay/2019/03/10/hdoj2544/</id>
    <published>2019-03-10T03:59:32.000Z</published>
    <updated>2019-03-10T04:12:15.939Z</updated>
    
    <content type="html"><![CDATA[<h1 id="题目描述-最短路"><a href="#题目描述-最短路" class="headerlink" title="题目描述-最短路"></a>题目描述-最短路</h1><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p>在每年的校赛里，所有进入决赛的同学都会获得一件很漂亮的t-shirt。但是每当我们的工作人员把上百件的衣服从商店运回到赛场的时候，却是非常累的！所以现在他们想要寻找最短的从商店到赛场的路线，你可以帮助他们吗？</p><h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>输入包括多组数据。每组数据第一行是两个整数N、M（N&lt;=100，M&lt;=10000），N表示成都的大街上有几个路口，标号为1的路口是商店所在地，标号为N的路口是赛场所在地，M则表示在成都有几条路。N=M=0表示输入结束。接下来M行，每行包括3个整数A，B，C（1&lt;=A,B&lt;=N,1&lt;=C&lt;=1000）,表示在路口A与路口B之间有一条路，我们的工作人员需要C分钟的时间走过这条路。<br>输入保证至少存在1条商店到赛场的路线。</p><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p>对于每组输入，输出一行，表示工作人员从商店走到赛场的最短时间</p><h2 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a>Sample Input</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">2 1</span><br><span class="line">1 2 3</span><br><span class="line">3 3</span><br><span class="line">1 2 5</span><br><span class="line">2 3 5</span><br><span class="line">3 1 2</span><br><span class="line">0 0</span><br></pre></td></tr></table></figure><h2 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a>Sample Output</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">3</span><br><span class="line">2</span><br></pre></td></tr></table></figure><h1 id="思路分析"><a href="#思路分析" class="headerlink" title="思路分析"></a>思路分析</h1><p>此题的结点数量为100，可以使用Floyed算法来解决，如下：<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> INF 10000 <span class="comment">// we can use -1</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="keyword">int</span> dist[<span class="number">101</span>][<span class="number">101</span>];</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n, m;</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">cin</span> &gt;&gt; n &gt;&gt; m)&#123;</span><br><span class="line">        <span class="keyword">if</span>(n == <span class="number">0</span> &amp;&amp; m == <span class="number">0</span>) <span class="keyword">break</span>;</span><br><span class="line">        fill(dist[<span class="number">0</span>],dist[<span class="number">0</span>]+<span class="number">101</span>*<span class="number">101</span>, INF);</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=n; i++) dist[i][i] = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> a,b,c;</span><br><span class="line">        <span class="keyword">while</span>(m--)&#123;</span><br><span class="line">            <span class="built_in">cin</span> &gt;&gt; a &gt;&gt; b &gt;&gt; c;;</span><br><span class="line">            dist[a][b] = c;</span><br><span class="line">            dist[b][a] = c;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> k=<span class="number">1</span>; k&lt;=n; k++)&#123; <span class="comment">//k,i,j</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=n; i++)&#123;</span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">1</span>; j&lt;=n; j++)&#123;</span><br><span class="line">                    <span class="keyword">if</span>(dist[i][k] == INF || dist[k][j] == INF) <span class="keyword">continue</span>;</span><br><span class="line">                    <span class="keyword">if</span>(dist[i][j] == INF || dist[i][j]&gt;dist[i][k]+dist[k][j])</span><br><span class="line">                        dist[i][j] = dist[i][k] + dist[k][j];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; dist[<span class="number">1</span>][n]&lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>当然，也可以用Dijkstra算法来写。Dijkstra算法适合单源最短路径，当结点数量比较大时得使用这种算法，只是写起来会相对比较麻烦。注意下面的newP这个点。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">E</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> next; <span class="comment">//neighbor</span></span><br><span class="line">    <span class="keyword">int</span> c; <span class="comment">//weight</span></span><br><span class="line">&#125;;</span><br><span class="line"><span class="built_in">vector</span>&lt;E&gt; edge[<span class="number">101</span>]; <span class="comment">//LinkedList</span></span><br><span class="line"><span class="keyword">bool</span> mark[<span class="number">101</span>];</span><br><span class="line"><span class="keyword">int</span> dis[<span class="number">101</span>];</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n, m;</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%d%d"</span>, &amp;n, &amp;m) != EOF)&#123;</span><br><span class="line">        <span class="keyword">if</span>(n == <span class="number">0</span> &amp;&amp; m == <span class="number">0</span>) <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=n; i++) edge[i].clear();<span class="comment">//initialize</span></span><br><span class="line">        <span class="keyword">while</span>(m--)&#123;</span><br><span class="line">            <span class="keyword">int</span> a, b, c;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d%d%d"</span>, &amp;a, &amp;b, &amp;c);</span><br><span class="line">            E temp;</span><br><span class="line">            temp.c = c;</span><br><span class="line">            temp.next = b;</span><br><span class="line">            edge[a].push_back(temp);</span><br><span class="line">            temp.next = a;</span><br><span class="line">            edge[b].push_back(temp);<span class="comment">//non-directional</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=n; i++)&#123;</span><br><span class="line">            dis[i] = <span class="number">-1</span>; <span class="comment">// not access</span></span><br><span class="line">            mark[i] = <span class="literal">false</span>; <span class="comment">// no belong to set K</span></span><br><span class="line">        &#125;</span><br><span class="line">        dis[<span class="number">1</span>] = <span class="number">0</span>;<span class="comment">//length = 0, the most near point</span></span><br><span class="line">        mark[<span class="number">1</span>] = <span class="literal">true</span>;</span><br><span class="line">        <span class="keyword">int</span> newP = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;n; i++)&#123;<span class="comment">//to define the rest n-1 point</span></span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;edge[newP].size(); j++)&#123; <span class="comment">//遍历邻居</span></span><br><span class="line">                <span class="keyword">int</span> t = edge[newP][j].next;</span><br><span class="line">                <span class="keyword">int</span> c = edge[newP][j].c;</span><br><span class="line">                <span class="keyword">if</span>(mark[t] == <span class="literal">true</span>) <span class="keyword">continue</span>;</span><br><span class="line">                <span class="keyword">if</span>(dis[t] == <span class="number">-1</span> || dis[t] &gt; dis[newP] + c)&#123;</span><br><span class="line">                    dis[t] = dis[newP] + c;<span class="comment">//update dis</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">int</span> min = <span class="number">123123123</span>;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">1</span>; j&lt;=n; j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(mark[j] == <span class="literal">true</span>) <span class="keyword">continue</span>;</span><br><span class="line">                <span class="keyword">if</span>(dis[j] == <span class="number">-1</span>) <span class="keyword">continue</span>;</span><br><span class="line">                <span class="keyword">if</span>(dis[j] &lt; min)&#123; <span class="comment">//找距离最短的点</span></span><br><span class="line">                    min = dis[j];</span><br><span class="line">                    newP = j;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            mark[newP] = <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, dis[n]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;题目描述-最短路&quot;&gt;&lt;a href=&quot;#题目描述-最短路&quot; class=&quot;headerlink&quot; title=&quot;题目描述-最短路&quot;&gt;&lt;/a&gt;题目描述-最短路&lt;/h1&gt;&lt;h2 id=&quot;Problem-Description&quot;&gt;&lt;a href=&quot;#Problem-De
      
    
    </summary>
    
      <category term="hdoj" scheme="https://github.com/DesmonDay/categories/hdoj/"/>
    
    
      <category term="C++" scheme="https://github.com/DesmonDay/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>hdoj2094-拓扑排序</title>
    <link href="https://github.com/DesmonDay/2019/03/10/hdoj2094/"/>
    <id>https://github.com/DesmonDay/2019/03/10/hdoj2094/</id>
    <published>2019-03-10T03:54:26.000Z</published>
    <updated>2019-03-10T03:59:08.944Z</updated>
    
    <content type="html"><![CDATA[<h1 id="题目描述-产生冠军"><a href="#题目描述-产生冠军" class="headerlink" title="题目描述-产生冠军"></a>题目描述-产生冠军</h1><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p>有一群人，打乒乓球比赛，两两捉对撕杀，每两个人之间最多打一场比赛。<br>球赛的规则如下：<br>如果A打败了B，B又打败了C，而A与C之间没有进行过比赛，那么就认定，A一定能打败C。<br>如果A打败了B，B又打败了C，而且，C又打败了A，那么A、B、C三者都不可能成为冠军。<br>根据这个规则，无需循环较量，或许就能确定冠军。你的任务就是面对一群比赛选手，在经过了若干场撕杀之后，确定是否已经实际上产生了冠军。</p><h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>输入含有一些选手群，每群选手都以一个整数n(n&lt;1000)开头，后跟n对选手的比赛结果，比赛结果以一对选手名字（中间隔一空格）表示，前者战胜后者。如果n为0，则表示输入结束。</p><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p>对于每个选手群，若你判断出产生了冠军，则在一行中输出“Yes”，否则在一行中输出“No”。</p><h2 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a>Sample Input</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">3</span><br><span class="line">Alice Bob</span><br><span class="line">Smith John</span><br><span class="line">Alice Smith</span><br><span class="line">5</span><br><span class="line">a c</span><br><span class="line">c d</span><br><span class="line">d e</span><br><span class="line">b e</span><br><span class="line">a d</span><br><span class="line">0</span><br></pre></td></tr></table></figure><h2 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a>Sample Output</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Yes</span><br><span class="line">No</span><br></pre></td></tr></table></figure><h1 id="思路分析"><a href="#思路分析" class="headerlink" title="思路分析"></a>思路分析</h1><p>这题貌似只要入度为0的点唯一，就能产生冠军了。。而按照拓扑排序的原先步骤那样写的话，结果又是不正确的，不明白。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// hdoj 2094</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stack&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;map&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//入度为0的点唯一说明有冠军</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line">    <span class="built_in">map</span>&lt;<span class="built_in">string</span>, <span class="keyword">int</span>&gt; M;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; edge[<span class="number">1000</span>];</span><br><span class="line">    <span class="built_in">stack</span>&lt;<span class="keyword">int</span>&gt; S;</span><br><span class="line">    <span class="keyword">int</span> inDegree[<span class="number">1000</span>];</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">cin</span> &gt;&gt; n &amp;&amp; n)&#123;</span><br><span class="line">        M.clear();</span><br><span class="line">        <span class="comment">//S.clear(); 没有此函数</span></span><br><span class="line">        <span class="keyword">while</span>(!S.empty()) S.pop();</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;<span class="number">2</span>*n; i++)&#123;</span><br><span class="line">            edge[i].clear();</span><br><span class="line">            inDegree[i] = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> k = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;n; i++)&#123;</span><br><span class="line">            <span class="built_in">string</span> a, b;</span><br><span class="line">            <span class="built_in">cin</span> &gt;&gt; a &gt;&gt; b;</span><br><span class="line">            <span class="keyword">if</span>(M.count(a) == <span class="number">0</span>) M[a] = k++;</span><br><span class="line">            <span class="keyword">if</span>(M.count(b) == <span class="number">0</span>) M[b] = k++;</span><br><span class="line">            inDegree[M[b]] += <span class="number">1</span>;</span><br><span class="line">            edge[M[a]].push_back(M[b]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">        for(int i=0; i&lt;k; i++)&#123;</span></span><br><span class="line"><span class="comment">            if(inDegree[i] == 0) S.push(i);</span></span><br><span class="line"><span class="comment">        &#125;</span></span><br><span class="line"><span class="comment">        int cnt = 0;</span></span><br><span class="line"><span class="comment">        while(!S.empty())&#123;</span></span><br><span class="line"><span class="comment">            int x = S.top();</span></span><br><span class="line"><span class="comment">            S.pop();</span></span><br><span class="line"><span class="comment">            cnt++;</span></span><br><span class="line"><span class="comment">            for(int i=0; i&lt;edge[x].size(); i++)&#123;</span></span><br><span class="line"><span class="comment">                inDegree[edge[x][i]]--;</span></span><br><span class="line"><span class="comment">                if(inDegree[edge[x][i]] == 0) S.push(edge[x][i]);</span></span><br><span class="line"><span class="comment">            &#125;</span></span><br><span class="line"><span class="comment">        &#125;</span></span><br><span class="line"><span class="comment">        for(int i=0; i&lt;k; i++)&#123;</span></span><br><span class="line"><span class="comment">            cout &lt;&lt; inDegree[i] &lt;&lt; " ";</span></span><br><span class="line"><span class="comment">        &#125; */</span></span><br><span class="line">        <span class="keyword">int</span> cnt = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;k; i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(inDegree[i] == <span class="number">0</span>) cnt++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(cnt == <span class="number">1</span>) <span class="built_in">cout</span> &lt;&lt; <span class="string">"Yes"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="built_in">cout</span> &lt;&lt; <span class="string">"No"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;题目描述-产生冠军&quot;&gt;&lt;a href=&quot;#题目描述-产生冠军&quot; class=&quot;headerlink&quot; title=&quot;题目描述-产生冠军&quot;&gt;&lt;/a&gt;题目描述-产生冠军&lt;/h1&gt;&lt;h2 id=&quot;Problem-Description&quot;&gt;&lt;a href=&quot;#Proble
      
    
    </summary>
    
      <category term="hdoj" scheme="https://github.com/DesmonDay/categories/hdoj/"/>
    
    
      <category term="C++" scheme="https://github.com/DesmonDay/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>hdoj2037-贪心</title>
    <link href="https://github.com/DesmonDay/2019/03/10/hdoj2037/"/>
    <id>https://github.com/DesmonDay/2019/03/10/hdoj2037/</id>
    <published>2019-03-10T03:48:22.000Z</published>
    <updated>2019-03-10T03:53:07.062Z</updated>
    
    <content type="html"><![CDATA[<h1 id="题目描述-今年暑假不AC"><a href="#题目描述-今年暑假不AC" class="headerlink" title="题目描述-今年暑假不AC"></a>题目描述-今年暑假不AC</h1><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p>“今年暑假不AC？”<br>“是的。”<br>“那你干什么呢？”<br>“看世界杯呀，笨蛋！”<br>“@#$%^&amp;*%…”</p><p>确实如此，世界杯来了，球迷的节日也来了，估计很多ACMer也会抛开电脑，奔向电视了。<br>作为球迷，一定想看尽量多的完整的比赛，当然，作为新时代的好青年，你一定还会看一些其它的节目，比如新闻联播（永远不要忘记关心国家大事）、非常6+7、超级女生，以及王小丫的《开心辞典》等等，假设你已经知道了所有你喜欢看的电视节目的转播时间表，你会合理安排吗？（目标是能看尽量多的完整节目）</p><h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>输入数据包含多个测试实例，每个测试实例的第一行只有一个整数n(n&lt;=100)，表示你喜欢看的节目的总数，然后是n行数据，每行包括两个数据Ti_s,Ti_e (1&lt;=i&lt;=n)，分别表示第i个节目的开始和结束时间，为了简化问题，每个时间都用一个正整数表示。n=0表示输入结束，不做处理。</p><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p>对于每个测试实例，输出能完整看到的电视节目的个数，每个测试实例的输出占一行。</p><h2 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a>Sample Input</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">12</span><br><span class="line">1 3</span><br><span class="line">3 4</span><br><span class="line">0 7</span><br><span class="line">3 8</span><br><span class="line">15 19</span><br><span class="line">15 20</span><br><span class="line">10 15</span><br><span class="line">8 18</span><br><span class="line">6 12</span><br><span class="line">5 10</span><br><span class="line">4 14</span><br><span class="line">2 9</span><br><span class="line">0</span><br></pre></td></tr></table></figure><h2 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a>Sample Output</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">5</span><br></pre></td></tr></table></figure><h1 id="思路分析"><a href="#思路分析" class="headerlink" title="思路分析"></a>思路分析</h1><p>使用贪心法则，比较麻烦的是怎么做最贪心，且结果最好，因为贪心策略并不是每次都显而易见的。可以通过分析知道，最优解中，第一个观看的节目一定是所有节目里结束时间最早的节目。因此，要按照结束时间进行升序排序。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">TVshow</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> start;</span><br><span class="line">    <span class="keyword">int</span> end;</span><br><span class="line">    <span class="keyword">bool</span> <span class="keyword">operator</span> &lt;(<span class="keyword">const</span> TVshow &amp;A) <span class="keyword">const</span>&#123; <span class="comment">//用于排序，结束时间越早排前面</span></span><br><span class="line">        <span class="keyword">return</span> end &lt; A.end;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;buf[<span class="number">100</span>];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;n) != EOF &amp;&amp; n != <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;n; i++)&#123;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d %d"</span>, &amp;buf[i].start, &amp;buf[i].end);</span><br><span class="line">        &#125;</span><br><span class="line">        sort(buf, buf+n);</span><br><span class="line">        <span class="keyword">int</span> idx = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">int</span> cnt = <span class="number">0</span>, currentTime = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span>(idx &lt; n)&#123;</span><br><span class="line">            <span class="keyword">if</span>(currentTime &lt;= buf[idx].start)&#123;</span><br><span class="line">                currentTime = buf[idx].end;</span><br><span class="line">                cnt ++;</span><br><span class="line">            &#125;</span><br><span class="line">            idx++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; cnt &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;题目描述-今年暑假不AC&quot;&gt;&lt;a href=&quot;#题目描述-今年暑假不AC&quot; class=&quot;headerlink&quot; title=&quot;题目描述-今年暑假不AC&quot;&gt;&lt;/a&gt;题目描述-今年暑假不AC&lt;/h1&gt;&lt;h2 id=&quot;Problem-Description&quot;&gt;&lt;a h
      
    
    </summary>
    
      <category term="hdoj" scheme="https://github.com/DesmonDay/categories/hdoj/"/>
    
    
      <category term="C++" scheme="https://github.com/DesmonDay/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>hdoj1856-并查集</title>
    <link href="https://github.com/DesmonDay/2019/03/10/hdoj1856/"/>
    <id>https://github.com/DesmonDay/2019/03/10/hdoj1856/</id>
    <published>2019-03-10T03:43:00.000Z</published>
    <updated>2019-03-10T03:47:52.497Z</updated>
    
    <content type="html"><![CDATA[<h1 id="题目描述-More-is-better"><a href="#题目描述-More-is-better" class="headerlink" title="题目描述-More is better"></a>题目描述-More is better</h1><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p>Mr Wang wants some boys to help him with a project. Because the project is rather complex, the more boys come, the better it will be. Of course there are certain requirements.</p><p>Mr Wang selected a room big enough to hold the boys. The boy who are not been chosen has to leave the room immediately. There are 10000000 boys in the room numbered from 1 to 10000000 at the very beginning. After Mr Wang’s selection any two of them who are still in this room should be friends (direct or indirect), or there is only one boy left. Given all the direct friend-pairs, you should decide the best way.</p><h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>The first line of the input contains an integer n (0 ≤ n ≤ 100 000) - the number of direct friend-pairs. The following n lines each contains a pair of numbers A and B separated by a single space that suggests A and B are direct friends. (A ≠ B, 1 ≤ A, B ≤ 10000000)</p><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p>The output in one line contains exactly one integer equals to the maximum number of boys Mr Wang may keep. </p><h2 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a>Sample Input</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">4</span><br><span class="line">1 2</span><br><span class="line">3 4</span><br><span class="line">5 6</span><br><span class="line">1 6</span><br><span class="line">4</span><br><span class="line">1 2</span><br><span class="line">3 4</span><br><span class="line">5 6</span><br><span class="line">7 8</span><br></pre></td></tr></table></figure><h2 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a>Sample Output</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">4</span><br><span class="line">2</span><br></pre></td></tr></table></figure><p>Hint</p><p>A and B are friends(direct or indirect), B and C are friends(direct or indirect),<br>then A and C are also friends(indirect).</p><p>In the first sample {1,2,5,6} is the result.<br>In the second sample {1,2},{3,4},{5,6},{7,8} are four kinds of answers.</p><h1 id="思路分析"><a href="#思路分析" class="headerlink" title="思路分析"></a>思路分析</h1><p>本题使用并查集来解决。就是看这题能够形成的最大簇的数量是多少。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt; </span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">10000001</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> Tree[N];</span><br><span class="line"><span class="keyword">int</span> sum[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">FindRoot</span><span class="params">(<span class="keyword">int</span> i)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(Tree[i] == <span class="number">-1</span>) <span class="keyword">return</span> i;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="keyword">int</span> temp = FindRoot(Tree[i]);</span><br><span class="line">        Tree[i] = temp;</span><br><span class="line">        <span class="keyword">return</span> temp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n, a, b;</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;n) != EOF)&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;N; i++)&#123;</span><br><span class="line">            Tree[i] = <span class="number">-1</span>;  <span class="comment">//全部初始化为-1</span></span><br><span class="line">            sum[i] = <span class="number">1</span>; <span class="comment">//初始化为1 </span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span>(n--)&#123;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%d%d"</span>, &amp;a, &amp;b);</span><br><span class="line">            a = FindRoot(a); <span class="comment">// a结点所在的根 </span></span><br><span class="line">            b = FindRoot(b);</span><br><span class="line">            <span class="keyword">if</span>(a != b)&#123;</span><br><span class="line">                Tree[a] = b; <span class="comment">//b是根节点！ </span></span><br><span class="line">                sum[b] += sum[a];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> ans = <span class="number">1</span>; <span class="comment">// 至少有1个男孩 </span></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; N; i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(Tree[i] == <span class="number">-1</span> &amp;&amp; ans &lt; sum[i])  <span class="comment">//搞清楚谁大于，谁小于！ </span></span><br><span class="line">                ans = sum[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, ans);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;题目描述-More-is-better&quot;&gt;&lt;a href=&quot;#题目描述-More-is-better&quot; class=&quot;headerlink&quot; title=&quot;题目描述-More is better&quot;&gt;&lt;/a&gt;题目描述-More is better&lt;/h1&gt;&lt;h2 i
      
    
    </summary>
    
      <category term="hdoj" scheme="https://github.com/DesmonDay/categories/hdoj/"/>
    
    
      <category term="C++" scheme="https://github.com/DesmonDay/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>hdoj1285-拓扑排序</title>
    <link href="https://github.com/DesmonDay/2019/03/10/hdoj1285/"/>
    <id>https://github.com/DesmonDay/2019/03/10/hdoj1285/</id>
    <published>2019-03-10T03:38:02.000Z</published>
    <updated>2019-03-10T03:42:21.469Z</updated>
    
    <content type="html"><![CDATA[<h1 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h1><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p>有N个比赛队（1&lt;=N&lt;=500），编号依次为1，2，3，。。。。，N进行比赛，比赛结束后，裁判委员会要将所有参赛队伍从前往后依次排名，但现在裁判委员会不能直接获得每个队的比赛成绩，只知道每场比赛的结果，即P1赢P2，用P1，P2表示，排名时P1在P2之前。现在请你编程序确定排名。</p><h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>输入有若干组，每组中的第一行为二个数N（1&lt;=N&lt;=500），M；其中N表示队伍的个数，M表示接着有M行的输入数据。接下来的M行数据中，每行也有两个整数P1，P2表示即P1队赢了P2队。</p><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p>给出一个符合要求的排名。输出时队伍号之间有空格，最后一名后面没有空格。</p><p>其他说明：符合条件的排名可能不是唯一的，此时要求输出时编号小的队伍在前；输入数据保证是正确的，即输入数据确保一定能有一个符合要求的排名。</p><h2 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a>Sample Input</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">4 3</span><br><span class="line">1 2</span><br><span class="line">2 3</span><br><span class="line">4 3</span><br></pre></td></tr></table></figure><h2 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a>Sample Output</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1 2 4 3</span><br></pre></td></tr></table></figure><h1 id="思路分析"><a href="#思路分析" class="headerlink" title="思路分析"></a>思路分析</h1><p>本次用拓扑排序来解决。设置一个vector数组来保存有向图，另外需要设置入度数组来查找入度为0的点。同时，设置一个队列来进行遍历，另外注意，符合条件的排名可能不是唯一的，此时要求输出时编号小的队伍在前，因此使用小顶堆，优先弹出的是入度为0且编号较小的号码。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;queue&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; edge[<span class="number">501</span>];</span><br><span class="line">    <span class="comment">//符合条件的排名可能不是唯一的，此时要求输出时编号小的队伍在前；</span></span><br><span class="line">    <span class="comment">//因此使用小顶堆，优先弹出的是入度为0且编号较小的号码。 </span></span><br><span class="line">    priority_queue&lt;<span class="keyword">int</span>, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;, greater&lt;<span class="keyword">int</span>&gt;&gt; q;</span><br><span class="line">    <span class="keyword">int</span> inDegree[<span class="number">501</span>];</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; res;</span><br><span class="line">    <span class="keyword">int</span> n, m; <span class="comment">//n为队伍个数，m为输入行数 </span></span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">cin</span> &gt;&gt; n &gt;&gt; m)&#123;</span><br><span class="line">        res.clear();</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=n; i++)&#123;</span><br><span class="line">            edge[i].clear();</span><br><span class="line">            inDegree[i] = <span class="number">0</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span>(!q.empty()) q.pop();</span><br><span class="line">        <span class="keyword">while</span>(m--)&#123;</span><br><span class="line">            <span class="keyword">int</span> a, b;</span><br><span class="line">            <span class="built_in">cin</span> &gt;&gt; a &gt;&gt; b;</span><br><span class="line">            inDegree[b] += <span class="number">1</span>;</span><br><span class="line">            edge[a].push_back(b);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=n; i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(inDegree[i] == <span class="number">0</span>) q.push(i);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span>(!q.empty())&#123;</span><br><span class="line">            <span class="keyword">int</span> newP = q.top(); <span class="comment">// 优先队列的顶部用top()，而不是用front()</span></span><br><span class="line">            q.pop();</span><br><span class="line">            res.push_back(newP);</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;edge[newP].size(); i++)&#123;</span><br><span class="line">                inDegree[edge[newP][i]]--;</span><br><span class="line">                <span class="keyword">if</span>(inDegree[edge[newP][i]] == <span class="number">0</span>)&#123;</span><br><span class="line">                    q.push(edge[newP][i]);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;res.size(); i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(i == <span class="number">0</span>) <span class="built_in">cout</span> &lt;&lt; res[i];</span><br><span class="line">            <span class="keyword">else</span> <span class="built_in">cout</span> &lt;&lt; <span class="string">" "</span> &lt;&lt; res[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;题目描述&quot;&gt;&lt;a href=&quot;#题目描述&quot; class=&quot;headerlink&quot; title=&quot;题目描述&quot;&gt;&lt;/a&gt;题目描述&lt;/h1&gt;&lt;h2 id=&quot;Problem-Description&quot;&gt;&lt;a href=&quot;#Problem-Description&quot; class
      
    
    </summary>
    
      <category term="hdoj" scheme="https://github.com/DesmonDay/categories/hdoj/"/>
    
    
      <category term="C++" scheme="https://github.com/DesmonDay/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>hdoj1253-BFS</title>
    <link href="https://github.com/DesmonDay/2019/03/10/hdoj1253/"/>
    <id>https://github.com/DesmonDay/2019/03/10/hdoj1253/</id>
    <published>2019-03-10T03:31:09.000Z</published>
    <updated>2019-03-10T03:46:57.535Z</updated>
    
    <content type="html"><![CDATA[<h1 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h1><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p>Ignatius被魔王抓走了,有一天魔王出差去了,这可是Ignatius逃亡的好机会.</p><p>魔王住在一个城堡里,城堡是一个A*B*C的立方体,可以被表示成A个B*C的矩阵,刚开始Ignatius被关在(0,0,0)的位置,离开城堡的门在(A-1,B-1,C-1)的位置,现在知道魔王将在T分钟后回到城堡,Ignatius每分钟能从一个坐标走到相邻的六个坐标中的其中一个.现在给你城堡的地图,请你计算出Ignatius能否在魔王回来前离开城堡(只要走到出口就算离开城堡,如果走到出口的时候魔王刚好回来也算逃亡成功),如果可以请输出需要多少分钟才能离开,如果不能则输出-1.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c87493aba0e9347c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>输入数据的第一行是一个正整数K,表明测试数据的数量.每组测试数据的第一行是四个正整数A,B,C和T(1&lt;=A,B,C&lt;=50,1&lt;=T&lt;=1000),它们分别代表城堡的大小和魔王回来的时间.然后是A块输入数据(先是第0块,然后是第1块,第2块……),每块输入数据有B行,每行有C个正整数,代表迷宫的布局,其中0代表路,1代表墙.(如果对输入描述不清楚,可以参考Sample Input中的迷宫描述,它表示的就是上图中的迷宫)</p><p>特别注意:本题的测试数据非常大,请使用scanf输入,我不能保证使用cin能不超时.在本OJ上请使用Visual C++提交.</p><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p>对于每组测试数据,如果Ignatius能够在魔王回来前离开城堡,那么请输出他最少需要多少分钟,否则输出-1.</p><h2 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a>Sample Input</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1</span><br><span class="line">3 3 4 20</span><br><span class="line">0 1 1 1</span><br><span class="line">0 0 1 1</span><br><span class="line">0 1 1 1</span><br><span class="line">1 1 1 1</span><br><span class="line">1 0 0 1</span><br><span class="line">0 1 1 1</span><br><span class="line">0 0 0 0</span><br><span class="line">0 1 1 0</span><br><span class="line">0 1 1 0</span><br></pre></td></tr></table></figure><h2 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a>Sample Output</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">11</span><br></pre></td></tr></table></figure><h1 id="思路分析"><a href="#思路分析" class="headerlink" title="思路分析"></a>思路分析</h1><p>这题相当于走迷宫的题，走迷宫一般用广搜！！广度优先搜索，即在遍历解答树时使每次状态转移时扩展出尽可能多的新状态，并且按照各个状态出现的先后顺序依次扩展它们。当搜索过程中第一次查找到状态中坐标为终点的结点，其记录的时间即为所需最短时间。</p><p>另外，广搜使用队列来实现。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 广度优先搜索，同时，在扩展的时候注意剪枝，即将不可能的状态不进行扩展。</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;queue&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 为了实现各个状态按照其被查找到的顺序依次转移扩展，使用队列;</span></span><br><span class="line"><span class="comment">// 即将每次扩展得到的新状态放入队列中，待排在其之前的状态都被扩展完成后，该状态才能得到扩展</span></span><br><span class="line"><span class="keyword">bool</span> mark[<span class="number">50</span>][<span class="number">50</span>][<span class="number">50</span>]; <span class="comment">//标记数组，如果已经得到过包含坐标(x,y,z)的状态后，则置为true </span></span><br><span class="line"><span class="keyword">int</span> maze[<span class="number">50</span>][<span class="number">50</span>][<span class="number">50</span>]; <span class="comment">//保存立方体信息 </span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用结构体保存每一个状态 </span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">State</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> x, y, z; <span class="comment">//位置坐标</span></span><br><span class="line">    <span class="keyword">int</span> t; <span class="comment">// 所需时间 </span></span><br><span class="line">&#125;; </span><br><span class="line"></span><br><span class="line"><span class="built_in">queue</span>&lt;State&gt; Q; <span class="comment">//队列，队列中的元素为状态</span></span><br><span class="line"><span class="keyword">int</span> go[][<span class="number">3</span>] = &#123;<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">-1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">-1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">-1</span>&#125;; <span class="comment">//坐标变换数组(x+go[i][0],y+go[i][1],z+go[i][2])</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">BFS</span><span class="params">(<span class="keyword">int</span> a, <span class="keyword">int</span> b, <span class="keyword">int</span> c)</span></span>&#123; <span class="comment">//广度优先搜索 </span></span><br><span class="line">    <span class="keyword">while</span>(!Q.empty())&#123;</span><br><span class="line">        State now = Q.front();</span><br><span class="line">        Q.pop();</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;<span class="number">6</span>; i++)&#123; <span class="comment">//依次扩展其六个相邻结点 </span></span><br><span class="line">            <span class="keyword">int</span> nx = now.x + go[i][<span class="number">0</span>];</span><br><span class="line">            <span class="keyword">int</span> ny = now.y + go[i][<span class="number">1</span>];</span><br><span class="line">            <span class="keyword">int</span> nz = now.z + go[i][<span class="number">2</span>];</span><br><span class="line">            <span class="keyword">if</span>(nx &lt; <span class="number">0</span> || nx &gt;= a || ny &lt; <span class="number">0</span> || ny &gt;= b || nz &lt; <span class="number">0</span> || nz &gt;= c) <span class="keyword">continue</span>; <span class="comment">//丢弃该坐标</span></span><br><span class="line">            <span class="keyword">if</span>(maze[nx][ny][nz] == <span class="number">1</span>) <span class="keyword">continue</span>; <span class="comment">//若该位置为墙，丢弃</span></span><br><span class="line">            <span class="keyword">if</span>(mark[nx][ny][nz] == <span class="literal">true</span>) <span class="keyword">continue</span>; <span class="comment">//若包含该坐标的状态已经被得到过，丢弃</span></span><br><span class="line">            State tmp;</span><br><span class="line">            tmp.x = nx;</span><br><span class="line">            tmp.y = ny;</span><br><span class="line">            tmp.z = nz;</span><br><span class="line">            tmp.t = now.t + <span class="number">1</span>;</span><br><span class="line">            Q.push(tmp);</span><br><span class="line">            mark[nx][ny][nz] = <span class="literal">true</span>; <span class="comment">//标记该坐标</span></span><br><span class="line">            <span class="keyword">if</span>(nx == a<span class="number">-1</span> &amp;&amp; ny == b<span class="number">-1</span> &amp;&amp; nz == c<span class="number">-1</span>) <span class="keyword">return</span> tmp.t; </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> T;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;T);</span><br><span class="line">    <span class="keyword">while</span>(T--)&#123;</span><br><span class="line">        <span class="keyword">int</span> a, b, c, t;</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d%d%d%d"</span>, &amp;a, &amp;b, &amp;c, &amp;t);</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;a; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;b; j++)&#123;</span><br><span class="line">                <span class="keyword">for</span>(<span class="keyword">int</span> k=<span class="number">0</span>; k&lt;c; k++)&#123;</span><br><span class="line">                    <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;maze[i][j][k]);</span><br><span class="line">                    mark[i][j][k] = <span class="literal">false</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">while</span>(!Q.empty()) Q.pop();</span><br><span class="line">        mark[<span class="number">0</span>][<span class="number">0</span>][<span class="number">0</span>] = <span class="literal">true</span>; <span class="comment">//标记起点</span></span><br><span class="line">        State tmp;</span><br><span class="line">        tmp.t = tmp.y = tmp.x = tmp.z = <span class="number">0</span>; <span class="comment">//初始化状态</span></span><br><span class="line">        Q.push(tmp);</span><br><span class="line">        <span class="keyword">int</span> rec = BFS(a, b, c);</span><br><span class="line">        <span class="keyword">if</span>(rec &lt;= t) <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, rec);</span><br><span class="line">        <span class="keyword">else</span> <span class="built_in">printf</span>(<span class="string">"-1\n"</span>); </span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;题目描述&quot;&gt;&lt;a href=&quot;#题目描述&quot; class=&quot;headerlink&quot; title=&quot;题目描述&quot;&gt;&lt;/a&gt;题目描述&lt;/h1&gt;&lt;h2 id=&quot;Problem-Description&quot;&gt;&lt;a href=&quot;#Problem-Description&quot; class
      
    
    </summary>
    
      <category term="hdoj" scheme="https://github.com/DesmonDay/categories/hdoj/"/>
    
    
      <category term="C++" scheme="https://github.com/DesmonDay/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>hdoj1016-素数环递归</title>
    <link href="https://github.com/DesmonDay/2019/03/10/hdoj1016/"/>
    <id>https://github.com/DesmonDay/2019/03/10/hdoj1016/</id>
    <published>2019-03-10T03:25:54.000Z</published>
    <updated>2019-03-10T03:30:22.954Z</updated>
    
    <content type="html"><![CDATA[<h1 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h1><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p>A ring is compose of n circles as shown in diagram. Put natural number 1, 2, …, n into each circle separately, and the sum of numbers in two adjacent circles should be a prime.</p><p>Note: the number of first circle should always be 1.</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-b21057e80ad98f20.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>n (0 &lt; n &lt; 20).</p><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p>The output format is shown as sample below. Each row represents a series of circle numbers in the ring beginning from 1 clockwisely and anticlockwisely. The order of numbers must satisfy the above requirements. Print solutions in lexicographical order.</p><p>You are to write a program that completes above process.</p><p>Print a blank line after each case.</p><h2 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a>Sample Input</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">6</span><br><span class="line">8</span><br></pre></td></tr></table></figure><h2 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a>Sample Output</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Case 1:</span><br><span class="line">1 4 3 2 5 6</span><br><span class="line">1 6 5 2 3 4</span><br><span class="line"></span><br><span class="line">Case 2:</span><br><span class="line">1 2 3 8 5 6 7 4</span><br><span class="line">1 2 5 8 3 4 7 6</span><br><span class="line">1 4 7 6 5 8 3 2</span><br><span class="line">1 6 7 4 3 8 5 2</span><br></pre></td></tr></table></figure><h1 id="思路分析"><a href="#思路分析" class="headerlink" title="思路分析"></a>思路分析</h1><p>这题对我来说有些难，最主要是这个递归的过程有点难懂。还是要多看看代码才行，贴出来的代码还是能懂的，但是自己写却比较困难了。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> ans[<span class="number">22</span>];</span><br><span class="line"><span class="keyword">int</span> visit[<span class="number">22</span>];</span><br><span class="line"><span class="keyword">int</span> n;</span><br><span class="line"><span class="keyword">int</span> prime[] = &#123;<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">11</span>,<span class="number">13</span>,<span class="number">17</span>,<span class="number">19</span>,<span class="number">23</span>,<span class="number">29</span>,<span class="number">31</span>,<span class="number">37</span>,<span class="number">41</span>&#125;;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">judgePrime</span><span class="params">(<span class="keyword">int</span> x)</span></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;<span class="number">13</span>; i++)&#123;</span><br><span class="line">        <span class="keyword">if</span>(prime[i] == x) <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">DFS</span><span class="params">(<span class="keyword">int</span> x)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(x == n)&#123;</span><br><span class="line">        <span class="keyword">if</span>(judgePrime(<span class="number">1</span>+ans[n]))&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=n; i++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(i != <span class="number">1</span>) <span class="built_in">printf</span>(<span class="string">" "</span>);</span><br><span class="line">                <span class="built_in">printf</span>(<span class="string">"%d"</span>, ans[i]);</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">        &#125; </span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">2</span>; i&lt;=n; i++)&#123;</span><br><span class="line">            <span class="keyword">if</span>(!visit[i] &amp;&amp; judgePrime(i + ans[x]))&#123;</span><br><span class="line">                visit[i] = <span class="number">1</span>;</span><br><span class="line">                ans[x+<span class="number">1</span>] = i;</span><br><span class="line">                DFS(x+<span class="number">1</span>);</span><br><span class="line">                visit[i] = <span class="number">0</span>; <span class="comment">// 这里需要重置为0，以便于下次递归入口使用。</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> cas = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;n) !=EOF)&#123;</span><br><span class="line">        cas++;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;<span class="number">22</span>; i++) visit[i] = <span class="number">0</span>;</span><br><span class="line">        ans[<span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"Case %d:\n"</span>, cas);</span><br><span class="line">        visit[<span class="number">1</span>] = <span class="literal">true</span>;</span><br><span class="line">        DFS(<span class="number">1</span>);</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;题目描述&quot;&gt;&lt;a href=&quot;#题目描述&quot; class=&quot;headerlink&quot; title=&quot;题目描述&quot;&gt;&lt;/a&gt;题目描述&lt;/h1&gt;&lt;h2 id=&quot;Problem-Description&quot;&gt;&lt;a href=&quot;#Problem-Description&quot; class
      
    
    </summary>
    
      <category term="hdoj" scheme="https://github.com/DesmonDay/categories/hdoj/"/>
    
    
      <category term="C++" scheme="https://github.com/DesmonDay/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>hdoj1009-贪心</title>
    <link href="https://github.com/DesmonDay/2019/03/10/hdoj1009/"/>
    <id>https://github.com/DesmonDay/2019/03/10/hdoj1009/</id>
    <published>2019-03-10T03:17:58.000Z</published>
    <updated>2019-03-10T03:22:17.957Z</updated>
    
    <content type="html"><![CDATA[<h1 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h1><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p>FatMouse prepared M pounds of cat food, ready to trade with the cats guarding the warehouse containing his favorite food, JavaBean.<br>The warehouse has N rooms. The i-th room contains J[i] pounds of JavaBeans and requires F[i] pounds of cat food. FatMouse does not have to trade for all the JavaBeans in the room, instead, he may get J[i]<em> a% pounds of JavaBeans if he pays F[i]</em> a% pounds of cat food. Here a is a real number. Now he is assigning this homework to you: tell him the maximum amount of JavaBeans he can obtain.</p><h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>The input consists of multiple test cases. Each test case begins with a line containing two non-negative integers M and N. Then N lines follow, each contains two non-negative integers J[i] and F[i] respectively. The last test case is followed by two -1’s. All integers are not greater than 1000.</p><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p>For each test case, print in a single line a real number accurate up to 3 decimal places, which is the maximum amount of JavaBeans that FatMouse can obtain.</p><h2 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a>Sample Input</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">5 3</span><br><span class="line">7 2</span><br><span class="line">4 3</span><br><span class="line">5 2</span><br><span class="line">20 3</span><br><span class="line">25 18</span><br><span class="line">24 15</span><br><span class="line">15 10</span><br><span class="line">-1 -1</span><br></pre></td></tr></table></figure><h2 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a>Sample Output</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">13.333</span><br><span class="line">31.500</span><br></pre></td></tr></table></figure><h1 id="思路分析"><a href="#思路分析" class="headerlink" title="思路分析"></a>思路分析</h1><p>题目不难，使用贪心解决即可。但最关键的是，如何找到一个合适的贪心策略。比如这里，最合适的当然是能够找到用更少的钱买到更多的食物，因此还需要计算物品的性价比。再将性价比降序排序。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">goods</span>&#123;</span></span><br><span class="line">    <span class="keyword">double</span> j; <span class="comment">//该物品总重</span></span><br><span class="line">    <span class="keyword">double</span> f; <span class="comment">//该物品总价值</span></span><br><span class="line">    <span class="keyword">double</span> s; <span class="comment">//该物品性价比</span></span><br><span class="line">    <span class="keyword">bool</span> <span class="keyword">operator</span> &lt;(<span class="keyword">const</span> goods &amp;A) <span class="keyword">const</span>&#123;</span><br><span class="line">        <span class="keyword">return</span> s &gt; A.s;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;buf[<span class="number">1000</span>];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">double</span> m; <span class="comment">//总钱数</span></span><br><span class="line">    <span class="keyword">int</span> n;    <span class="comment">//n种物品</span></span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%lf%d"</span>, &amp;m, &amp;n) != EOF)&#123;</span><br><span class="line">        <span class="keyword">if</span>(m == <span class="number">-1</span> &amp;&amp; n == <span class="number">-1</span>) <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;n; i++)&#123;</span><br><span class="line">            <span class="built_in">scanf</span>(<span class="string">"%lf%lf"</span>, &amp;buf[i].j, &amp;buf[i].f);</span><br><span class="line">            buf[i].s = buf[i].j / buf[i].f; <span class="comment">//计算性价比</span></span><br><span class="line">        &#125;</span><br><span class="line">        sort(buf, buf+n); <span class="comment">//性价比降序排序，贪心算法</span></span><br><span class="line">        <span class="keyword">int</span> idx = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">double</span> ans = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span>(m &gt; <span class="number">0</span> &amp;&amp; idx &lt; n)&#123;</span><br><span class="line">            <span class="keyword">if</span>(m &gt;= buf[idx].f)&#123;</span><br><span class="line">                ans += buf[idx].j;</span><br><span class="line">                m -= buf[idx].f;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">else</span>&#123;</span><br><span class="line">                ans += m*buf[idx].s;</span><br><span class="line">                m = <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            idx++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%.3f\n"</span>,ans);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;题目描述&quot;&gt;&lt;a href=&quot;#题目描述&quot; class=&quot;headerlink&quot; title=&quot;题目描述&quot;&gt;&lt;/a&gt;题目描述&lt;/h1&gt;&lt;h2 id=&quot;Problem-Description&quot;&gt;&lt;a href=&quot;#Problem-Description&quot; class
      
    
    </summary>
    
      <category term="hdoj" scheme="https://github.com/DesmonDay/categories/hdoj/"/>
    
    
      <category term="C++" scheme="https://github.com/DesmonDay/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>hdoj1241-DFS</title>
    <link href="https://github.com/DesmonDay/2019/03/10/hdoj1241/"/>
    <id>https://github.com/DesmonDay/2019/03/10/hdoj1241/</id>
    <published>2019-03-10T03:14:23.000Z</published>
    <updated>2019-03-10T03:47:17.030Z</updated>
    
    <content type="html"><![CDATA[<h1 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h1><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p>The GeoSurvComp geologic survey company is responsible for detecting underground oil deposits. GeoSurvComp works with one large rectangular region of land at a time, and creates a grid that divides the land into numerous square plots. It then analyzes each plot separately, using sensing equipment to determine whether or not the plot contains oil. A plot containing oil is called a pocket. If two pockets are adjacent, then they are part of the same oil deposit. Oil deposits can be quite large and may contain numerous pockets. Your job is to determine how many different oil deposits are contained in a grid. </p><h2 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h2><p>The input file contains one or more grids. Each grid begins with a line containing m and n, the number of rows and columns in the grid, separated by a single space. If m = 0 it signals the end of the input; otherwise 1 &lt;= m &lt;= 100 and 1 &lt;= n &lt;= 100. Following this are m lines of n characters each (not counting the end-of-line characters). Each character corresponds to one plot, and is either <code>*&#39;, representing the absence of oil, or</code>@’, representing an oil pocket.</p><h2 id="Output"><a href="#Output" class="headerlink" title="Output"></a>Output</h2><p>For each grid, output the number of distinct oil deposits. Two different pockets are part of the same oil deposit if they are adjacent horizontally, vertically, or diagonally. An oil deposit will not contain more than 100 pockets.</p><h2 id="Sample-Input"><a href="#Sample-Input" class="headerlink" title="Sample Input"></a>Sample Input</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">1 1</span><br><span class="line">*</span><br><span class="line">3 5</span><br><span class="line">*@*@*</span><br><span class="line">**@**</span><br><span class="line">*@*@*</span><br><span class="line">1 8</span><br><span class="line">@@****@*</span><br><span class="line">5 5 </span><br><span class="line">****@</span><br><span class="line">*@@*@</span><br><span class="line">*@**@</span><br><span class="line">@@@*@</span><br><span class="line">@@**@</span><br><span class="line">0 0</span><br></pre></td></tr></table></figure><h2 id="Sample-Output"><a href="#Sample-Output" class="headerlink" title="Sample Output"></a>Sample Output</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">2</span><br></pre></td></tr></table></figure><h1 id="思路分析"><a href="#思路分析" class="headerlink" title="思路分析"></a>思路分析</h1><p>使用DFS深搜即可解决，要注意的是，相邻点不仅仅包括直接相邻，还包括间接相邻。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 遇到不合理的，则跳过，不用进行遍历 </span></span><br><span class="line"><span class="keyword">int</span> m, n;</span><br><span class="line"><span class="keyword">char</span> grid[<span class="number">100</span>][<span class="number">100</span>];</span><br><span class="line"><span class="keyword">bool</span> mark[<span class="number">100</span>][<span class="number">100</span>];</span><br><span class="line"></span><br><span class="line"><span class="comment">// 实际上应该为8个相邻点，而不单纯指直接相邻 </span></span><br><span class="line"><span class="keyword">int</span> go[][<span class="number">2</span>] = &#123;<span class="number">1</span>,<span class="number">0</span>,<span class="number">-1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">-1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">-1</span>,<span class="number">-1</span>,<span class="number">-1</span>,<span class="number">-1</span>,<span class="number">1</span>&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">DFS</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span></span>&#123;</span><br><span class="line">    mark[x][y] = <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;<span class="number">8</span>; i++)&#123;</span><br><span class="line">        <span class="keyword">int</span> nx = x + go[i][<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">int</span> ny = y + go[i][<span class="number">1</span>];</span><br><span class="line">        <span class="keyword">if</span>(mark[nx][ny] == <span class="literal">true</span>) <span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">if</span>(grid[nx][ny] == <span class="string">'*'</span>) <span class="keyword">continue</span>;</span><br><span class="line">        <span class="keyword">if</span>(nx &lt; <span class="number">0</span> || nx &gt;= m || ny &lt; <span class="number">0</span> || ny &gt;= n) <span class="keyword">continue</span>;</span><br><span class="line">        DFS(nx, ny);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">"%d%d"</span>, &amp;m, &amp;n))&#123;</span><br><span class="line">        <span class="keyword">if</span>(m == <span class="number">0</span> &amp;&amp; n == <span class="number">0</span>) <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;m; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;n; j++)&#123;</span><br><span class="line">                <span class="built_in">cin</span> &gt;&gt; grid[i][j];</span><br><span class="line">                mark[i][j] = <span class="literal">false</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span> cnt = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;m; i++)&#123;</span><br><span class="line">            <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;n; j++)&#123;</span><br><span class="line">                <span class="keyword">if</span>(mark[i][j] == <span class="literal">true</span>) <span class="keyword">continue</span>;</span><br><span class="line">                <span class="keyword">if</span>(grid[i][j] == <span class="string">'*'</span>) <span class="keyword">continue</span>; <span class="comment">//跳过，无油 </span></span><br><span class="line">                DFS(i, j);</span><br><span class="line">                cnt++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d\n"</span>, cnt);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;题目描述&quot;&gt;&lt;a href=&quot;#题目描述&quot; class=&quot;headerlink&quot; title=&quot;题目描述&quot;&gt;&lt;/a&gt;题目描述&lt;/h1&gt;&lt;h2 id=&quot;Problem-Description&quot;&gt;&lt;a href=&quot;#Problem-Description&quot; class
      
    
    </summary>
    
      <category term="hdoj" scheme="https://github.com/DesmonDay/categories/hdoj/"/>
    
    
      <category term="C++" scheme="https://github.com/DesmonDay/tags/C/"/>
    
  </entry>
  
</feed>
