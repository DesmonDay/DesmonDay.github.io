<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>DesmonDay&#39;s Blog</title>
  
  <subtitle>一只小辣鸡的自我拯救之路</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://github.com/DesmonDay/"/>
  <updated>2020-02-22T08:23:21.199Z</updated>
  <id>https://github.com/DesmonDay/</id>
  
  <author>
    <name>DesmonDay</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>预训练语言模型</title>
    <link href="https://github.com/DesmonDay/2020/02/22/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    <id>https://github.com/DesmonDay/2020/02/22/预训练语言模型/</id>
    <published>2020-02-22T07:46:28.000Z</published>
    <updated>2020-02-22T08:23:21.199Z</updated>
    
    <content type="html"><![CDATA[<p>本文记录一些关于预训练语言模型讲解比较好的中文博客，有时间再补充自己的理解。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文记录一些关于预训练语言模型讲解比较好的中文博客，有时间再补充自己的理解。&lt;/p&gt;

      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/DesmonDay/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>特征工程</title>
    <link href="https://github.com/DesmonDay/2020/02/19/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    <id>https://github.com/DesmonDay/2020/02/19/特征工程/</id>
    <published>2020-02-19T13:20:53.000Z</published>
    <updated>2020-02-19T13:55:09.773Z</updated>
    
    <content type="html"><![CDATA[<p>本笔记参考二水马的rebirth。</p><h1 id="1-特征选择"><a href="#1-特征选择" class="headerlink" title="1. 特征选择"></a>1. 特征选择</h1><h2 id="1-1-特征来源"><a href="#1-1-特征来源" class="headerlink" title="1.1 特征来源"></a>1.1 特征来源</h2><ol><li>业务已整理好的数据，需要找出适合问题的特征</li><li>高级特征，需要从业务特征中去寻找⾼级数据特征</li></ol><h2 id="1-2-选择合适的特征"><a href="#1-2-选择合适的特征" class="headerlink" title="1.2 选择合适的特征"></a>1.2 选择合适的特征</h2><p>第⼀步是找到该领域懂业务的<strong>专家</strong>，让他们给⼀些建议，这些特征就是我们的特征的第⼀候选集。</p><p>在尝试降维之前，有必要用特征工程的方法选择出较重要的特征结合，这些方法不会用到领域知识，而仅是统计学的方法。</p><p>特征选择方法一般分为三类：</p><ul><li><p>过滤法选择特征</p><ol><li><strong>方差筛选</strong>：方差越大的特征，则认为是比较有用；若方差较小，如小于1，则该特征对算法作用没那么大。若某特征方差为0，则其所有样本的特征取值相同，应当舍弃该特征。实际应用中，应指定方差阈值，当方差小于该阈值特征则筛选掉。</li><li><strong>相关系数</strong>：主要⽤于输出连续值的监督学习算法中。我们分别计算所有训练集中各个特征与输出值之间的相关系数，设定⼀个阈值，选择相关系数较⼤的部分特征。</li><li>假设检验：如卡方检验。卡方检验可以检验某个特征分布和输出值分布之间的相关性。除此之外，还可以用F检验和t检验。</li><li>互信息：从信息熵角度分析各个特征和输出值之间的关系评分。互信息值越大，说明该特征和输出值之间的相关性越⼤，越需要保留。</li></ol></li><li><p>包装法选择特征</p><p>思想是选择⼀个⽬标函数来⼀步步的筛选特征。最常⽤的包装法是递归消除特征法(recursive feature elimination, 以下简称RFE)。递归消除特征法使⽤⼀个机器学习模型来进⾏多轮训练，每轮训练后，消除若干权值系数的对应的特征，再基于新的特征集进⾏下⼀轮训练。</p><p>经典方法：SVM-RFE方法。以⽀持向量机来做RFE 的机器学习模型选择特征，假设有n 个特征。</p><ol><li>它在第⼀轮训练的时候，会选择所有的特征来训练，得到分类的超平⾯$wx+b$；</li><li>选择出$w$中分量的平⽅值$w_i^2$最小的那个序号$i$对应的特征，将其排除；</li><li>对剩下的$n-1$个特征和输出值，重新训练SVM，直到剩下的特征数满足需求。</li></ol></li><li><p>嵌入法选择特征</p><ol><li>使⽤全部特征，使⽤L1 正则化和L2 正则化来选择特征。正则化惩罚项越⼤，那么模型的系数就会越⼩。</li><li>当正则化惩罚项⼤到⼀定的程度的时候，部分特征系数会变成0，当正则化惩罚项继续增⼤到⼀定程度时，所有的特征系数都会趋于0。⼀部分特征系数会更容易先变成0，这部分系数就是可以筛掉的。即选择特征系数较⼤的特征。</li><li>⽤的L1 正则化和L2 正则化来选择特征的基学习器是逻辑回归，此外也可以使⽤决策树或者GBDT。</li><li>可以得到特征系数coef或者得到特征重要度(feature importances)的算法才可以作为嵌入法的基学习器。</li></ol></li></ul><h2 id="1-3-寻找高级特征（交叉特征）"><a href="#1-3-寻找高级特征（交叉特征）" class="headerlink" title="1.3 寻找高级特征（交叉特征）"></a>1.3 寻找高级特征（交叉特征）</h2><p>⽐如有车的路程特征和时间间隔特征，我们就可以得到车的平均速度这个⼆级特征。根据车的速度特征，我们就可以得到车的加速度这个三级特征，根据车的加速度特征，我们就可以得到车的加加<br>速度这个四级特征。也就是说，⾼级特征可以⼀直寻找下去。</p><ol><li>特征加和：计算累积</li><li>特征之差：计算增量</li><li>特征乘积：价格与销量得到营业额</li><li>特征除商：销售额除以商品数量得到均价</li></ol><h2 id="1-4-特征选择小结"><a href="#1-4-特征选择小结" class="headerlink" title="1.4 特征选择小结"></a>1.4 特征选择小结</h2><p>特征选择是特征⼯程的第⼀步，它关系到机器学习算法的上限。因此原则是<strong>尽量不错过⼀个可能有⽤的特征，但是也不滥⽤太多的特征</strong>。</p><h1 id="2-特征表达"><a href="#2-特征表达" class="headerlink" title="2. 特征表达"></a>2. 特征表达</h1><h2 id="2-1-缺失值处理"><a href="#2-1-缺失值处理" class="headerlink" title="2.1 缺失值处理"></a>2.1 缺失值处理</h2><p>连续值：均值填充、中位数填充</p><p>离散值：选择所有有该特征值的样本中出现最频繁的类别值，填充缺失值。</p><h2 id="2-2-特殊特征处理"><a href="#2-2-特殊特征处理" class="headerlink" title="2.2 特殊特征处理"></a>2.2 特殊特征处理</h2><ol><li>日期时间<ul><li><strong>时间差值法</strong>：计算出所有样本的时间到某⼀个未来时间（或过去时间）之间的数值差距，转换成<strong>连续值</strong>；</li><li><strong>细分特征</strong>：年、⽉、⽇、周、时，将⼀个时间特征转化为若⼲个<strong>离散特征</strong>，这种⽅法在分析具有明显时间趋势的问题⽐较好⽤。</li><li><strong>权重法</strong>：即根据时间的新旧得到⼀个权重值。⽐如对于商品，三个⽉前购买的设置⼀个较低的权重，最近三天购买的设置⼀个中等的权重，在三个⽉内但是三天前的设置⼀个较⼤的权重。还有其他的设置权重的⽅法，这个要根据要解决的问题来灵活确定。</li></ul></li><li>地理位置<ul><li>细分特征：城市、区县、街道</li><li>经纬度：如果需要判断用户分布区域，一般处理成连续值比较好，这时可以将地址处理成经度和纬度的连续特征。</li></ul></li></ol><h2 id="2-3-离散特征的连续化处理"><a href="#2-3-离散特征的连续化处理" class="headerlink" title="2.3 离散特征的连续化处理"></a>2.3 离散特征的连续化处理</h2><ol><li>one-hot向量</li><li>embedding</li></ol><h2 id="2-4-离散特征的离散化处理"><a href="#2-4-离散特征的离散化处理" class="headerlink" title="2.4 离散特征的离散化处理"></a>2.4 离散特征的离散化处理</h2><ol><li>one-hot向量</li><li>dummy coding</li><li>根据背景知识划分：⽐如，商品的销量对应的特征，原始特征是季节春夏秋冬。可以将其转化为淡季和旺季这样的⼆值特征，⽅便建模。</li></ol><h2 id="2-5-连续特征的离散化处理"><a href="#2-5-连续特征的离散化处理" class="headerlink" title="2.5 连续特征的离散化处理"></a>2.5 连续特征的离散化处理</h2><ol><li>等频分桶</li><li>等值分桶</li><li>GBDT+LR</li></ol><h1 id="3-数据归一化"><a href="#3-数据归一化" class="headerlink" title="3. 数据归一化"></a>3. 数据归一化</h1><h2 id="3-1-为什么需要数据归一化"><a href="#3-1-为什么需要数据归一化" class="headerlink" title="3.1 为什么需要数据归一化"></a>3.1 为什么需要数据归一化</h2><ol><li>归⼀化后加快了梯度下降求最优解的速度</li><li>归⼀化有可能提⾼精度</li></ol><h2 id="3-2-常见数据归一化方法"><a href="#3-2-常见数据归一化方法" class="headerlink" title="3.2 常见数据归一化方法"></a>3.2 常见数据归一化方法</h2><ol><li><p>线性归一化：</p><script type="math/tex; mode=display">x' = \frac{x-min}{max-min}</script><p>缺点：一旦测试集有特征小于min或大于max的数据，则会导致max和min发生变化，需要重新计算。</p></li><li><p>标准化：z-score标准化。</p><script type="math/tex; mode=display">x' = \frac{x - mean}{std}</script></li></ol><ol><li><p>非线性归一化：取指数、对数、正切、等频分桶、等值分桶</p></li><li><p>L1/L2范数标准化：如果只是为了统⼀量纲，那么可以通过L2 范数整体标准化。</p></li><li><p>中心化：PCA 降维时候的预处理，均值变为零，⽅差不变</p><script type="math/tex; mode=display">x' = x-mean</script></li></ol><h3 id="3-3-PCA-主成分分析"><a href="#3-3-PCA-主成分分析" class="headerlink" title="3.3 PCA(主成分分析)"></a>3.3 PCA(主成分分析)</h3><p>两种推导：</p><ol><li>样本点到这个超平面的距离足够近</li><li>样本点在这个超平面的投影能尽可能地分开</li></ol><p>PCA算法流程：</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-110fb4b24415592a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png" style="zoom:67%;"></p><p><strong>问题</strong>：能否用PCA 代替隐含层的降维操作</p><p>假设我们有⼀个如下图所⽰的隐藏层。隐藏层在这个⽹络中起到了⼀定的降维作⽤。假如现在我们⽤另⼀种维度下降的⽅法，⽐如说主成分分析法(PCA) 来替代这个隐藏层。那么，这两者的输出效果是⼀样的吗？</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-7f81a305f78c2195.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>答：不一样。因为PCA 提取的是<strong>数据分布⽅差⽐较⼤</strong>的⽅向，隐藏层可以提取有预测能⼒的特征。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本笔记参考二水马的rebirth。&lt;/p&gt;
&lt;h1 id=&quot;1-特征选择&quot;&gt;&lt;a href=&quot;#1-特征选择&quot; class=&quot;headerlink&quot; title=&quot;1. 特征选择&quot;&gt;&lt;/a&gt;1. 特征选择&lt;/h1&gt;&lt;h2 id=&quot;1-1-特征来源&quot;&gt;&lt;a href=&quot;#1-
      
    
    </summary>
    
      <category term="找工作" scheme="https://github.com/DesmonDay/categories/%E6%89%BE%E5%B7%A5%E4%BD%9C/"/>
    
    
      <category term="work" scheme="https://github.com/DesmonDay/tags/work/"/>
    
  </entry>
  
  <entry>
    <title>实习复习过程</title>
    <link href="https://github.com/DesmonDay/2020/02/19/%E5%AE%9E%E4%B9%A0%E5%A4%8D%E4%B9%A0%E8%BF%87%E7%A8%8B/"/>
    <id>https://github.com/DesmonDay/2020/02/19/实习复习过程/</id>
    <published>2020-02-19T02:11:46.000Z</published>
    <updated>2020-02-19T13:22:45.498Z</updated>
    
    <content type="html"><![CDATA[<p>目前的复习进度：</p><ol><li>剑指offer已刷完</li><li>机器学习复习中</li><li>项目继续准备中</li><li>需要开始刷leetcode</li></ol><p>机器学习所用复习资料：</p><ol><li>西瓜书</li><li>二水马的笔记rebirth</li><li>susht师姐的知乎专栏</li><li>各种博客：刘建平Pinard</li></ol><p>深度学习复习：</p><ol><li>二水马的笔记rebirth</li><li>之前看吴恩达课程所做的笔记</li><li>还需自己再熟悉pytorch和tensorflow常用函数</li></ol><p>项目：</p><ol><li>驱动型对话数据集</li><li>文本对抗比赛</li><li>本科毕设——推荐相关</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;目前的复习进度：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;剑指offer已刷完&lt;/li&gt;
&lt;li&gt;机器学习复习中&lt;/li&gt;
&lt;li&gt;项目继续准备中&lt;/li&gt;
&lt;li&gt;需要开始刷leetcode&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;机器学习所用复习资料：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;西瓜书&lt;/li&gt;
      
    
    </summary>
    
      <category term="找工作" scheme="https://github.com/DesmonDay/categories/%E6%89%BE%E5%B7%A5%E4%BD%9C/"/>
    
    
      <category term="work" scheme="https://github.com/DesmonDay/tags/work/"/>
    
  </entry>
  
  <entry>
    <title>Transformer笔记</title>
    <link href="https://github.com/DesmonDay/2019/09/21/Transformer%E7%AC%94%E8%AE%B0/"/>
    <id>https://github.com/DesmonDay/2019/09/21/Transformer笔记/</id>
    <published>2019-09-21T07:52:02.000Z</published>
    <updated>2020-02-22T14:21:16.169Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Transformer笔记"><a href="#Transformer笔记" class="headerlink" title="Transformer笔记"></a>Transformer笔记</h1><p>本笔记主要供自己复习，只记录一些关键的点。参考链接：<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html#prelims" target="_blank" rel="noopener">http://nlp.seas.harvard.edu/2018/04/03/attention.html#prelims</a></p><h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p>一般的神经序列模型都包含encoder-decoder架构。其中，encoder将输入序列$(x_1,x_2,..,x_n)$的符号表示(symbol representations)映射到连续表示序列$z=(z_1,z_2,…z_n)$。给定$z$，decoder随后一次一个元素地生成输出序列$(y_1,y_2,…,y_m)$。在每个步骤中，模型是自动回归(auto-regressive)，在生成下一个时，把先前生成的符号作为附加输入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A standard Encoder-Decoder architecture. Base for this and many </span></span><br><span class="line"><span class="string">    other models.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder, src_embed, tgt_embed, generator)</span>:</span></span><br><span class="line">        super(EncoderDecoder, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.src_embed = src_embed</span><br><span class="line">        self.tgt_embed = tgt_embed</span><br><span class="line">        self.generator = generator</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src, tgt, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Take in and process masked src and target sequences."</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask,</span><br><span class="line">                            tgt, tgt_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, src, src_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, memory, src_mask, tgt, tgt_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Define standard linear + softmax generation step."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.proj(x), dim=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><p>Transformer的模型架构如下：</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-79468beb5ae88542.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="Encoder-and-Decoder-Stack"><a href="#Encoder-and-Decoder-Stack" class="headerlink" title="Encoder and Decoder Stack"></a>Encoder and Decoder Stack</h2><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>Transformer的encoder由$N=6$个独立块组成，可看模型框架图得知，位于图的左半部分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clones</span><span class="params">(module, N)</span>:</span></span><br><span class="line">    <span class="string">"Produce N identical layers."</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> range(N)])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Core encoder is a stack of N layers"</span></span><br><span class="line">    <span class="comment"># 此处是Encoder部分，是由6个layers组成的stack</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="string">"Pass the input (and mask) through each layer in turn."</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure><p>另外，每个layer(块)包括了两个sub-layers. 其中，第一个layer是多头注意力机制，第二个layer是简单的全连接层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Encoder is made up of self-attn and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(EncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="string">"Follow Figure 1 (left) for connections."</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure><p>另外，从模型图中可以看到，每个子layer中都有残差连接部分，随后跟随着layer norm层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, dropout)</span>:</span></span><br><span class="line">        super(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, sublayer)</span>:</span></span><br><span class="line">        <span class="string">"Apply residual connection to any sublayer with the same size."</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Construct a layernorm module (See citation for details)."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, features, eps=<span class="number">1e-6</span>)</span>:</span></span><br><span class="line">        super(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        mean = x.mean(<span class="number">-1</span>, keepdim=<span class="keyword">True</span>)</span><br><span class="line">        std = x.std(<span class="number">-1</span>, keepdim=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br></pre></td></tr></table></figure><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>decoder也是由6个块构成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Generic N layer decoder with masking."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure><p>除了每个encoder层中的两个子层之外，decoder还插入第三子层，其对堆叠encoder的输出执行多头注意（multi-head attention）。与编码器类似，我们在每个子层周围使用残差连接（residual connections），然后进行层规范化（layer normalization）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, src_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Follow Figure 1 (right) for connections."</span></span><br><span class="line">        m = memory</span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure><p>另外，需要修改解码器中的自注意子层（self-attention sub-layer）以防止位置出现在后续位置（subsequent positions）。这种掩蔽与输出嵌入偏移一个位置的事实相结合，<strong>确保了位置i的预测仅依赖于小于i的位置处的已知输出</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsequent_mask</span><span class="params">(size)</span>:</span></span><br><span class="line">    <span class="string">"Mask out subsequent positions."</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">'uint8'</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(subsequent_mask) == <span class="number">0</span></span><br></pre></td></tr></table></figure><p>我们可以通过图示来展示这个掩码的作用（其中，显示了每个tgt单词（行）允许查看的位置（列））：</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-a7ebf9dec279cb0f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>An attention function can be described as <strong>mapping a query and a set of key-value pairs to an output</strong>, where the query, keys, values, and output are all vectors. The output is computed as <strong>a weighted sum of the values</strong>, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p><h3 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h3><p><img src="https://upload-images.jianshu.io/upload_images/8636110-b8e83271fed25135.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这个Attention机制称为”Scaled Dot-Product Attention”。对应公式如下：</p><script type="math/tex; mode=display">Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</script><p>实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(query, key, value, mask=None, dropout=None)</span>:</span></span><br><span class="line">    <span class="string">"Compute 'Scaled Dot Product Attention'"</span></span><br><span class="line">    d_k = query.size(<span class="number">-1</span>)</span><br><span class="line">    scores = torch.matmul(query, key.transpose(<span class="number">-2</span>, <span class="number">-1</span>)) \</span><br><span class="line">             / math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="number">-1e9</span>)</span><br><span class="line">    p_attn = F.softmax(scores, dim = <span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure><p>截止参考博客发出来的时候，比较常用的两种attention机制分别是 additive attention（使用具有单隐层的神经网络来计算compatibility function）和dot-product (multiplicative) attention（这种attention和scaled dot-product attention基本一致，除了多了分母部分）。两者相比，后者的运行速度和空间存储利用更好。</p><p>之所以除$\sqrt{d_k}$，解释如下，主要是为了消除点乘的值可能会太大，导致softmax函数进行低梯度空间的情况：</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-c15390dfbff52bea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="Multi-head-Attention"><a href="#Multi-head-Attention" class="headerlink" title="Multi-head Attention"></a>Multi-head Attention</h3><p><img src="https://upload-images.jianshu.io/upload_images/8636110-77539b2534ce68d4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png" style="zoom: 50%;"></p><script type="math/tex; mode=display">MultiHead(Q,K,V) = Concat(head_1,...,head_h)W^O \\where\ head_i = Attention(QW_i^Q, KW_i^K,VW_i^V)</script><p>其中，投影矩阵为$W_i^Q \in \mathbb{R}^{d_{model} \times d_k}$,  $W_i^K \in \mathbb{R}^{d_{model} \times d_k}$, $W_i^V \in \mathbb{R}^{d_{model} \times d_v}$。在Transformer中，一共用了8个头，其中$d_k=d_v=d_{model}/h$。由于每个头部的维数减少，总的计算成本与全维度的单头部注意的计算成本相似。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, d_model, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">"Take in model size and number of heads."</span></span><br><span class="line">        super(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment"># We assume d_v always equals d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="keyword">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, mask=None)</span>:</span></span><br><span class="line">        <span class="string">"Implements Figure 2"</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k </span></span><br><span class="line">        query, key, value = \</span><br><span class="line">            [l(x).view(nbatches, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> zip(self.linears, (query, key, value))]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class="line">                                 dropout=self.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3) "Concat" using a view and apply a final linear. </span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous() \</span><br><span class="line">             .view(nbatches, <span class="number">-1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="keyword">return</span> self.linears[<span class="number">-1</span>](x)</span><br></pre></td></tr></table></figure><h3 id="模型中Attention的应用"><a href="#模型中Attention的应用" class="headerlink" title="模型中Attention的应用"></a>模型中Attention的应用</h3><ol><li><p>在encoder-decoder层中，queries来自先前时刻的decoder输出，而keys和values来自encoder层的输出。这样可以允许每个位置的decoder输出能够注意到输入序列的全部位置，这模仿了典型的encoder-decoder注意机制模型。</p></li><li><p>encoder层使用了自注意力机制，其中，query,key,value的值都相同，即encoder的输出。这样的话，每个位置的encoder都可以与encoder之前的所有时刻有关联。而<strong>self-attention</strong>的实际意义是：在序列内部做Attention，寻找序列内部的联系。</p></li><li><p>在decoder层也使用了自注意力机制，即允许解码器中的每个时刻可以关注在该时刻之前的所有时刻。为了保持decoder层的自回归特性，需要防止解码器中的信息向左流动（因为是并行训练，防止看到后面的信息），因此要通过mask掉输入中所有非法连接的值（设置为负无穷大）。具体可以参考下图，感觉讲的应该有点道理，之后再补充苏剑林老师的理解。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-cd697e7bf806f27f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p></li></ol><h3 id="Attention机制的好处"><a href="#Attention机制的好处" class="headerlink" title="Attention机制的好处"></a>Attention机制的好处</h3><p>Attention层的好处是能够一步到位捕捉到全局的联系，因为它直接把序列两两比较（代价是计算量变为$O(n^2)$，当然由于是纯矩阵运算，这个计算量也不是很严重）；相比之下，RNN需要一步步递推才能捕捉到，而CNN则需要通过层叠来扩大感受野，这是Attention层的明显优势。</p><h2 id="Position-wise-Feed-Forward-Network"><a href="#Position-wise-Feed-Forward-Network" class="headerlink" title="Position-wise Feed-Forward Network"></a>Position-wise Feed-Forward Network</h2><p>每个子层都包含了全连接FFN，分别独立的应用于每个position中。其实它的作用有点类似卷积核大小为1的情况。输入和输出的维度都是$d_{model}=512$，中间隐层维度为$d_{ff}=2048$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implements FFN equation."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_ff, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        super(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br></pre></td></tr></table></figure><h2 id="Embeddings-and-Softmax"><a href="#Embeddings-and-Softmax" class="headerlink" title="Embeddings and Softmax"></a>Embeddings and Softmax</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Embeddings, self).__init__()</span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure><h2 id="Postional-Encoding"><a href="#Postional-Encoding" class="headerlink" title="Postional Encoding"></a>Postional Encoding</h2><p>由于模型中即没有recurrence和convolution，为了可以利用语句中的词序，我们必须将位置信息想办法加进去。因此，模型中对在encoder和decoder的输入处加入了positional encoding。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-627bc132cf18fd28.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>对应公式如下:</p><script type="math/tex; mode=display">PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}}) \\PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}})</script><p>其中，$pos$是position，$i$是维度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implement the PE function."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the positional encodings once in log space.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], </span><br><span class="line">                         requires_grad=<span class="keyword">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure><h2 id="完整模型"><a href="#完整模型" class="headerlink" title="完整模型"></a>完整模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span><span class="params">(src_vocab, tgt_vocab, N=<span class="number">6</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">               d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="string">"Helper: Construct a model from hyperparameters."</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), </span><br><span class="line">                             c(ff), dropout), N),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">        Generator(d_model, tgt_vocab))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This was important from their code. </span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line">tmp_model = make_model(<span class="number">10</span>, <span class="number">10</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>接下里就是模型的训练部分了，不进行讲解。</p><h2 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h2><p>要学会这种写法。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-775d4829b9fb130b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NoamOpt</span>:</span></span><br><span class="line">    <span class="string">"Optim wrapper that implements rate."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model_size, factor, warmup, optimizer)</span>:</span></span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line">        self._step = <span class="number">0</span></span><br><span class="line">        self.warmup = warmup</span><br><span class="line">        self.factor = factor</span><br><span class="line">        self.model_size = model_size</span><br><span class="line">        self._rate = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"Update parameters and rate"</span></span><br><span class="line">        self._step += <span class="number">1</span></span><br><span class="line">        rate = self.rate()</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.optimizer.param_groups:</span><br><span class="line">            p[<span class="string">'lr'</span>] = rate</span><br><span class="line">        self._rate = rate</span><br><span class="line">        self.optimizer.step()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rate</span><span class="params">(self, step = None)</span>:</span></span><br><span class="line">        <span class="string">"Implement `lrate` above"</span></span><br><span class="line">        <span class="keyword">if</span> step <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            step = self._step</span><br><span class="line">        <span class="keyword">return</span> self.factor * \</span><br><span class="line">            (self.model_size ** (<span class="number">-0.5</span>) *</span><br><span class="line">            min(step ** (<span class="number">-0.5</span>), step * self.warmup ** (<span class="number">-1.5</span>)))</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_std_opt</span><span class="params">(model)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> NoamOpt(model.src_embed[<span class="number">0</span>].d_model, <span class="number">2</span>, <span class="number">4000</span>,</span><br><span class="line">            torch.optim.Adam(model.parameters(), lr=<span class="number">0</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>))</span><br><span class="line"></span><br><span class="line">opts = [NoamOpt(<span class="number">512</span>, <span class="number">1</span>, <span class="number">4000</span>, <span class="keyword">None</span>), </span><br><span class="line">        NoamOpt(<span class="number">512</span>, <span class="number">1</span>, <span class="number">8000</span>, <span class="keyword">None</span>),</span><br><span class="line">        NoamOpt(<span class="number">256</span>, <span class="number">1</span>, <span class="number">4000</span>, <span class="keyword">None</span>)]</span><br><span class="line">plt.plot(np.arange(<span class="number">1</span>, <span class="number">20000</span>), [[opt.rate(i) <span class="keyword">for</span> opt <span class="keyword">in</span> opts] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">20000</span>)])</span><br><span class="line">plt.legend([<span class="string">"512:4000"</span>, <span class="string">"512:8000"</span>, <span class="string">"256:4000"</span>])</span><br></pre></td></tr></table></figure><p><img src="https://upload-images.jianshu.io/upload_images/8636110-cf0ab027799b58a2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><h3 id="Label-Smoothing"><a href="#Label-Smoothing" class="headerlink" title="Label Smoothing"></a>Label Smoothing</h3><p>在训练中，使用了label smoothing。这种做法会使perplexity增大，as the model learns to be more unsure, but improves accuracy and BLEU score. （中文好差，不知道怎么翻译得好一些）。标签平滑的优势是能够防止模型追求确切概率而不影响模型学习正确分类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LabelSmoothing</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implement label smoothing."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, padding_idx, smoothing=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        super(LabelSmoothing, self).__init__()</span><br><span class="line">        self.criterion = nn.KLDivLoss(size_average=<span class="keyword">False</span>)</span><br><span class="line">        self.padding_idx = padding_idx</span><br><span class="line">        self.confidence = <span class="number">1.0</span> - smoothing</span><br><span class="line">        self.smoothing = smoothing</span><br><span class="line">        self.size = size</span><br><span class="line">        self.true_dist = <span class="keyword">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, target)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> x.size(<span class="number">1</span>) == self.size</span><br><span class="line">        true_dist = x.data.clone()</span><br><span class="line">        true_dist.fill_(self.smoothing / (self.size - <span class="number">2</span>)) <span class="comment"># 将该tensor用指定的数值填充</span></span><br><span class="line">        true_dist.scatter_(<span class="number">1</span>, target.data.unsqueeze(<span class="number">1</span>), self.confidence)</span><br><span class="line">        true_dist[:, self.padding_idx] = <span class="number">0</span></span><br><span class="line">        mask = torch.nonzero(target.data == self.padding_idx)</span><br><span class="line">        <span class="keyword">if</span> mask.dim() &gt; <span class="number">0</span>:</span><br><span class="line">            true_dist.index_fill_(<span class="number">0</span>, mask.squeeze(), <span class="number">0.0</span>)</span><br><span class="line">        self.true_dist = true_dist</span><br><span class="line">        <span class="keyword">return</span> self.criterion(x, Variable(true_dist, requires_grad=<span class="keyword">False</span>))</span><br></pre></td></tr></table></figure><ul><li><p>scatter_函数理解举例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line"> <span class="comment">#0.4319  0.6500  0.4080  0.8760  0.2355</span></span><br><span class="line"> <span class="comment">#0.2609  0.4711  0.8486  0.8573  0.1029</span></span><br><span class="line"><span class="comment"># LongTensor的shape刚好与x的shape对应，也就是LongTensor每个index指定x中一个数据的填充位置。</span></span><br><span class="line"><span class="comment"># dim=0，表示按行填充，主要理解按行填充。</span></span><br><span class="line">torch.zeros(<span class="number">3</span>, <span class="number">5</span>).scatter_(<span class="number">0</span>, torch.LongTensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]]), x)</span><br><span class="line"><span class="comment"># 举例LongTensor中的第0行第2列index=2，表示在第2行（从0开始）进行填充填充，对应到zeros(3, 5)中就是位置（2,2）。 所以此处要求zeros(3, 5)的列数要与x列数相同，而LongTensor中的index最大值应与zeros(3, 5)行数相一致。</span></span><br><span class="line"><span class="number">0.4319</span>  <span class="number">0.4711</span>  <span class="number">0.8486</span>  <span class="number">0.8760</span>  <span class="number">0.2355</span></span><br><span class="line"><span class="number">0.0000</span>  <span class="number">0.6500</span>  <span class="number">0.0000</span>  <span class="number">0.8573</span>  <span class="number">0.0000</span></span><br><span class="line"><span class="number">0.2609</span>  <span class="number">0.0000</span>  <span class="number">0.4080</span>  <span class="number">0.0000</span>  <span class="number">0.1029</span></span><br><span class="line">参考链接：https://www.cnblogs.com/dogecheng/p/<span class="number">11938009.</span>html，有公式说明</span><br></pre></td></tr></table></figure><p>scatter()一般可以用来对标签进行<strong>one-hot编码</strong>，举例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class_num = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">4</span></span><br><span class="line">label = torch.LongTensor(batch_size, <span class="number">1</span>).random_() % class_num</span><br><span class="line"><span class="comment">#tensor([[6],</span></span><br><span class="line"><span class="comment">#        [0],</span></span><br><span class="line"><span class="comment">#        [3],</span></span><br><span class="line"><span class="comment">#        [2]])</span></span><br><span class="line">torch.zeros(batch_size, class_num).scatter_(<span class="number">1</span>, label, <span class="number">1</span>)</span><br><span class="line"><span class="comment">#tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])</span></span><br></pre></td></tr></table></figure></li></ul><p>普通的label smoothing写法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">new_labels = (<span class="number">1.0</span> - label_smoothing) * one_hot_labels + label_smoothing / num_classes</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Transformer笔记&quot;&gt;&lt;a href=&quot;#Transformer笔记&quot; class=&quot;headerlink&quot; title=&quot;Transformer笔记&quot;&gt;&lt;/a&gt;Transformer笔记&lt;/h1&gt;&lt;p&gt;本笔记主要供自己复习，只记录一些关键的点。参考链接：
      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/DesmonDay/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>HMM-隐马尔可夫模型</title>
    <link href="https://github.com/DesmonDay/2019/09/05/HMM-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/"/>
    <id>https://github.com/DesmonDay/2019/09/05/HMM-隐马尔可夫模型/</id>
    <published>2019-09-05T06:45:10.000Z</published>
    <updated>2020-02-22T14:15:08.863Z</updated>
    
    <content type="html"><![CDATA[<p>参考：<a href="https://www.cnblogs.com/pinard/p/6945257.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6945257.html</a></p><h1 id="HMM"><a href="#HMM" class="headerlink" title="HMM"></a>HMM</h1><h2 id="1-什么问题需要HMM模型"><a href="#1-什么问题需要HMM模型" class="headerlink" title="1. 什么问题需要HMM模型"></a>1. 什么问题需要HMM模型</h2><p>使⽤HMM 模型时我们的问题⼀般有这两个特征：</p><ul><li>问题是基于序列的，⽐如时间序列，或者状态序列。</li><li>问题中有两类数据<ul><li>⼀类序列数据是可以观测到的，即观测序列；</li><li>另⼀类数据是不能观察到的，即隐藏状态序列，简称状态序列。</li></ul></li></ul><h2 id="2-HMM定义"><a href="#2-HMM定义" class="headerlink" title="2. HMM定义"></a>2. HMM定义</h2><ul><li><p>隐藏状态集合：$Q={q_1,q_2,…,q_N}$， 观察状态集合：$V={v_1,v_2,…,v_M}$</p></li><li><p>对于长度为$T$的序列，对应状态序列和观测序列为：</p><script type="math/tex; mode=display">I = {i_1,i_2,...,i_T} i_t \in Q\\O = {o_1,o_2,...,o_T}, o_t \in V</script></li><li><p>两个重要假设</p><ul><li><p>齐次马尔科夫链假设：任意时刻的隐藏状态只依赖于前一个隐藏状态，表示为：</p><script type="math/tex; mode=display">a_{ij} = P(i_{t+1} = q_j | i_t = q_i)</script><p>对应状态转移矩阵：$A=[a_{ij}]_{N\times N}$</p></li><li><p>观测独立性假设：任意时刻的观察状态只依赖于当前时刻的隐藏状态，观测状态生成概率为：</p><script type="math/tex; mode=display">b_j(k) = P(o_t=v_k|i_t=q_j)</script></li></ul></li><li><p>一个HMM可表示为三元组：$\lambda = (A,B,\prod)$</p></li></ul><h2 id="3-HMM三个基本问题"><a href="#3-HMM三个基本问题" class="headerlink" title="3. HMM三个基本问题"></a>3. HMM三个基本问题</h2><ul><li><p>估计观测序列概率（似然问题）：</p><ul><li><p>给定模型和观测序列，计算观测序列在该模型下出现的概率</p></li><li><p>求解方法为<strong>前向后向算法</strong>(均为动态规划)：可参考<a href="https://www.cnblogs.com/pinard/p/6955871.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6955871.html</a></p></li><li><p>前向算法总结：</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-9b33930cdbc358da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p></li><li><p>后向算法总结：</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-3414527f99d702bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p></li></ul></li><li><p>模型参数学习问题（学习问题）：</p><ul><li><p>给定观测序列，估计模型的参数，使得在该模型下观测序列出现的概率最大。</p></li><li><p>求解方法为<strong>基于EM算法的鲍姆-韦尔奇算法</strong>。</p></li><li><p>鲍姆-韦尔奇算法总结：</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-3f288ea5c331c5b8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p></li></ul></li><li><p>预测问题（解码问题）：</p><ul><li><p>给定模型和观测序列，求在给定观测序列的条件下，最可能出现的状态序列。</p></li><li><p>求解方法为<strong>基于动态规划的维特比算法</strong>。</p></li><li><p>维特比算法总结：</p><p><img src="C:\Users\90866\AppData\Roaming\Typora\typora-user-images\1582182663650.png" alt="1582182663650" style="zoom:80%;"></p></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;参考：&lt;a href=&quot;https://www.cnblogs.com/pinard/p/6945257.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.cnblogs.com/pinard/p/6945257.html&lt;/
      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/DesmonDay/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>Optimizer的选择</title>
    <link href="https://github.com/DesmonDay/2019/08/23/Optimizer%E7%9A%84%E9%80%89%E6%8B%A9/"/>
    <id>https://github.com/DesmonDay/2019/08/23/Optimizer的选择/</id>
    <published>2019-08-23T06:31:42.000Z</published>
    <updated>2020-02-19T07:23:09.242Z</updated>
    
    <content type="html"><![CDATA[<p>在深度学习中，选择合适的优化器不仅会加速整个神经网络训练过程，并且会避免在训练的过程中碰到鞍点。⽂中会结合自己的使⽤情况，对使⽤过的优化器提出⼀些自己的理解。参考二水马的笔记rebirth。</p><h1 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h1><p>SGD是非常常见的优化方法，每次迭代计算Mini-batch的梯度，再对参数进行更新。公式：</p><script type="math/tex; mode=display">v_t = \mu \nabla_theta J(\theta) \\\theta = \theta - v_t</script><p>缺点：对于损失方程有比较严重的振荡，并且容易收敛到局部最小值。</p><h1 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h1><p>为了克服SGD振荡比较严重的问题，Momentum将物理中的动量概念引入SGD当中，通过积累之前的动量来替代梯度。</p><script type="math/tex; mode=display">v_t = \gamma v_{t-1} + \mu \nabla_{\theta} J(\theta) \\\theta = \theta - v_t</script><p>其中，$v$是初始速度，$\mu$是学习率，$\gamma$是动量参数。</p><p>相较于SGD，Momentum 就相当于在从⼭坡上不停的向下⾛，当没有阻⼒的话，它的动量会越来越⼤，但是如果遇到了阻⼒，速度就会变小。也就是说，在训练的时候，在梯度⽅向不变的维度上，训练速度变快；梯度⽅向有所改变的维度上，更新速度变慢，这样就可以加快收敛并减小振荡。</p><h1 id="Nesterov"><a href="#Nesterov" class="headerlink" title="Nesterov"></a>Nesterov</h1><p>加速梯度更新。但在随机梯度的情况下，Nesterov动量没有改进收敛率。</p><script type="math/tex; mode=display">v_t = \gamma * v_{t-1} + \mu * \nabla_{theta}[\frac{1}{m}L(f(x^{(i)};\theta+\gamma v),y^{(i)})] \\\theta = \theta - v_t</script><h1 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h1><p>Adagrad算法，独立地适应所有模型参数的学习率。相较于SGD，Adagrad相当于<strong>对学习率多加了一个约束</strong>，即</p><script type="math/tex; mode=display">计算梯度：g = \frac{1}{m}\nabla_{\theta}\sum_iL(f(x^{(i)};\theta),y^{(i)})\\累积平方梯度：r_{t+1} = r_{t} + g \odot g \\计算参数更新：\nabla \theta = -\frac{\epsilon}{\sqrt{\delta+r}} \odot g \\应用更新：\theta = \theta + \nabla\theta</script><p>在训练开始时积累梯度平方会导致有效学习率过早和过量的减小。</p><h1 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h1><p>和AdaGrad相比，使用移动平均引入了一个新的超参数$\rho$，用来控制移动平均的长度范围。</p><script type="math/tex; mode=display">计算梯度：g = \frac{1}{m}\nabla_{\theta}\sum_iL(f(x^{(i)};\theta),y^{(i)})\\累积平方梯度：r_{t+1} = \rho r_{t} + (1-\rho)g \odot g \\计算参数更新：\nabla \theta = -\frac{\epsilon}{\sqrt{\delta+r}} \odot g \\应用更新：\theta = \theta + \nabla\theta</script><h1 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h1><p>Adam 是⼀个结合了Momentum 与Adagrad 的产物，它既考虑到了利⽤动量项来加速训练过程，又考虑到对于学习率的约束。利⽤梯度的⼀阶矩估计和⼆阶矩估计动态调整每个参数的学习率。Adam 的优点主要在于经过偏置校正后，每⼀次迭代学习率都有个确定范围，使得参数⽐较平稳。其公式为：</p><script type="math/tex; mode=display">计算梯度：g = \frac{1}{m}\nabla_{\theta}\sum_iL(f(x^{(i)};\theta),y^{(i)})\\更新有偏一阶矩估计：s = \rho_1 s + (1 - \rho_1)g \\更新有偏二阶矩估计：r = \rho_2 r + (1 - \rho_2)g\odot g\\修正一阶矩的偏差：\hat{s} = \frac{s}{1 - \rho_1^t}\\修正二阶矩的偏差：\hat{r} = \frac{r}{1 - \rho_2^t} \\计算更新：\nabla \theta = - \epsilon \frac{\hat{s}}{\sqrt{\hat{r}}+\delta}\\应用更新：\theta = \theta + \nabla\theta</script><p>其中，$\rho_1$和$\rho_2$为矩估计的指数衰减速率。</p><p>Adam通常被认为对超参数的选择相当鲁棒，尽管学习率有时需要从建议的默认修改。</p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>通过实践证明，Adam 结合了Adagrad 善于处理稀疏梯度和Momentum 善于处理⾮平稳⽬标的优点，相较于其他⼏种优化器效果更好。同时，我们也注意到很多论⽂中都会引⽤SGD，Adagrad作为优化函数。但相较于其他⽅法，在实践中，SGD 需要更多的训练时间以及可能会被困到鞍点的<br>缺点，都制约了它在很多真实数据上的表现。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在深度学习中，选择合适的优化器不仅会加速整个神经网络训练过程，并且会避免在训练的过程中碰到鞍点。⽂中会结合自己的使⽤情况，对使⽤过的优化器提出⼀些自己的理解。参考二水马的笔记rebirth。&lt;/p&gt;
&lt;h1 id=&quot;SGD&quot;&gt;&lt;a href=&quot;#SGD&quot; class=&quot;he
      
    
    </summary>
    
      <category term="深度学习" scheme="https://github.com/DesmonDay/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://github.com/DesmonDay/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Batch Normalization</title>
    <link href="https://github.com/DesmonDay/2019/08/19/Batch-Normalization/"/>
    <id>https://github.com/DesmonDay/2019/08/19/Batch-Normalization/</id>
    <published>2019-08-19T03:54:39.000Z</published>
    <updated>2020-02-19T06:38:35.314Z</updated>
    
    <content type="html"><![CDATA[<h2 id="使用BN的原因和BN的原理"><a href="#使用BN的原因和BN的原理" class="headerlink" title="使用BN的原因和BN的原理"></a>使用BN的原因和BN的原理</h2><p>批归一化实际上是为了解决“Internal Covariate Shift”问题，即在深度神经网络在训练过程中使得每一层神经网络的输入保持相同分布。</p><p>其基本思想为：因为深度神经网络在做非线性变换前的激活输入值，在随着网络深度加深或者训练过程中，其分布逐渐发生偏移或者变动，这就导致”梯度消失“问题，从而训练收敛慢。</p><p>为什么会造成“梯度消失”问题？这是因为，变化的整体分布逐渐往非线性函数的取值区间的上下限两端靠近，我们可以观察sigmoid函数和tanh函数的图像，其上下限两端的梯度均接近0. 而接近上下限则会导致梯度接近0， 从而发生“梯度消失”现象。</p><p>因此，BN所做的事就是通过一定的规范化手段，把每层神经网络任意神经元的输入值分布强行拉回到均值为0，方差为1的标准正态分布。其中，$\epsilon$是为了控制分母为正。</p><script type="math/tex; mode=display">x_i' = \frac{x_i-\mu}{\sqrt{\sigma^2+\epsilon}}</script><p>然而，如果强行对数据进行缩放，可能会导致一些问题（毕竟强行改变了人家的分布），因此，BN增加了<em>scale</em>和<em>shift</em>的操作。其核心思想是找到一个线性和非线性的较好平衡点，既能享受非线性的较强表达能力的好处，又避免太靠非线性区两头使得网络收敛速度太慢。</p><script type="math/tex; mode=display">y_i = scale * x_i' +shift</script><p>既然BN的名称为batch normalization，那其表现就会受到batch size的影响：</p><ol><li>size太小，算出的$\mu$和$\sigma$不准确，影响归一化，导致性能下降；</li><li>size太小，则内存放不下。</li></ol><h2 id="BN放置的位置"><a href="#BN放置的位置" class="headerlink" title="BN放置的位置"></a>BN放置的位置</h2><p>许多人会纠结Batch Normalization操作放置的位置，这里做一个确定性的结论：通常位于X=WU+B激活输入值获得之后，非线性函数变换（激活函数）之前，图示如下：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180405213955224-1791925244.png" alt="img"></p><h2 id="其他归一化手段"><a href="#其他归一化手段" class="headerlink" title="其他归一化手段"></a>其他归一化手段</h2><p>除了BN，还有layer normalization、Group Normalization和Instance Normalization。</p><p>我们参考susht师姐整理的内容进行总结：</p><p><img src="https://pic4.zhimg.com/v2-91307a71718b2ce805678a612625676f_r.jpg" alt="preview"></p><p>其实这些方法只是归一化的方向不一样，不再沿batch方向归一化，他们的不同点就在于归一化的方向不一样。</p><p>BN：批量归一化，往batch方向做归一化，归一化维度是[N，H，W]</p><p>LN：层次归一化，往channel方向做归一化，归一化维度为[C，H，W]</p><p>IN：实例归一化，只在一个channel内做归一化，归一化维度为[H，W]</p><p>GN：介于LN和IN之间，在channel方向分group来做归一化，归一化的维度为[C//G , H, W]</p><h2 id="深度学习框架中使用BN"><a href="#深度学习框架中使用BN" class="headerlink" title="深度学习框架中使用BN"></a>深度学习框架中使用BN</h2><p>这里贴一下一点注意事项。</p><p>在pytorch中使用BN很简单，但注意，对应模型在训练和验证（测试）两种情况，需要区分model.train()和model.eval()。另外贴一下使用方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nn.Conv2d(in_channel, out_channel, <span class="number">3</span>, stride, <span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line">nn.BatchNorm2d(out_channel)<span class="comment">#BatchNorm2d最常用于卷积网络中(防止梯度消失或爆炸)，设置的参数就是卷积的输出通道数</span></span><br><span class="line">nn.ReLU(inplace=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>在tensorflow中BN的实现主要有三个：</p><ol><li>tf.nn.batch_normalization</li><li>tf.layers.batch_normalization</li><li>tf.contrib.layers.batch_norm</li></ol><p>参考资料：</p><ol><li><a href="https://www.cnblogs.com/guoyaohua/p/8724433.html" target="_blank" rel="noopener">https://www.cnblogs.com/guoyaohua/p/8724433.html</a></li><li><a href="https://zhuanlan.zhihu.com/p/36101196" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/36101196</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;使用BN的原因和BN的原理&quot;&gt;&lt;a href=&quot;#使用BN的原因和BN的原理&quot; class=&quot;headerlink&quot; title=&quot;使用BN的原因和BN的原理&quot;&gt;&lt;/a&gt;使用BN的原因和BN的原理&lt;/h2&gt;&lt;p&gt;批归一化实际上是为了解决“Internal Cova
      
    
    </summary>
    
    
      <category term="python" scheme="https://github.com/DesmonDay/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>第14周-序列模型和注意力机制</title>
    <link href="https://github.com/DesmonDay/2019/04/28/deep-learningw14/"/>
    <id>https://github.com/DesmonDay/2019/04/28/deep-learningw14/</id>
    <published>2019-04-28T06:12:48.000Z</published>
    <updated>2019-05-01T16:41:06.622Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基础模型"><a href="#基础模型" class="headerlink" title="基础模型"></a>基础模型</h1><h2 id="Sequence-to-sequence-model-encoder-decoder-network"><a href="#Sequence-to-sequence-model-encoder-decoder-network" class="headerlink" title="Sequence to sequence model(encoder-decoder network)"></a>Sequence to sequence model(encoder-decoder network)</h2><p>论文标题：</p><ol><li>Sequence to sequence learning with neural networks, 2014</li><li>Learning phrase representations using RNN encoder-decoder for statistical machine translation, 2014</li></ol><p>假设我们要翻译以下句子（法语-&gt;英语）：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ba79ab26ed9d15e5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>首先，我们需要建立一个网络，该网络称为<strong>encoder network</strong>(编码网络)，它是一个RNN网络，其中RNN的单元可以是GRU，也可以是LSTM。每次只向该网络中输入一个法语单词，将整个句子输入完毕后，RNN会输出一个向量来代表这个输入序列。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-260763f4c22287ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在之后，我们需要建立一个<strong>decoder network</strong>(解码网络)，它以编码网络的输出作为输入，然后可以被训练为每次输出一个翻译后的单词。一直到它输出序列的结尾或者句子结尾标记，那么这个解码网络的工作就结束了。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-5b82df19496a649c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在给出足够的法语和英语文本的情况下，如果我们训练这个模型，通过输入一个法语句子来输出对应的英语翻译，这个模型将会非常有效。</p><h2 id="Image-captioning"><a href="#Image-captioning" class="headerlink" title="Image captioning"></a>Image captioning</h2><p>论文标题：</p><ol><li>Deep captioning with multimodal recurrent neural network, 2014</li><li>Show and tell: Neural image caption generator, 2014</li><li>Deep visual-semantic alignments for generating image descriptions, 2015</li></ol><p>还有一个非常类似的结构被用来做图像描述。</p><p>比如给定一张图片，如这张猫的图片，我们希望网络能够自动地输出该图片的描述：一只猫坐在椅子上。方法如下：在之前的图像课程中我们知道了如何将图片输入到卷积神经网络中，比如一个预训练的AlexNet结构，然后让其学习图片的编码或者学习图片的一系列特征。如下，如果我们去掉最后的Softmax层，那么这个预训练的AlexNet结构会给我们一个4096维的特征向量，而向量表示的就是这只猫的图片。。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e11143623397661a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>因此，这个预训练网络可以是图像的编码网络(encoder network)，接着我们可以把输出的向量输入到RNN中，而RNN要做的就是生成图像的描述，这与我们之前所讲的英语法语翻译的结构很类似。事实证明这个方法在图像描述的领域中很有效，特别是当我们想生成的描述不是很长时。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e966cf9e737be645.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>现在我们知道了Seq2Seq模型和图像描述模型是怎样运作的，不过这两个模型的运作方式有一些不同，主要体现在如何用语言模型合成新的文本，并生成对应序列的方面。</p><h1 id="Picking-the-most-likely-sentence"><a href="#Picking-the-most-likely-sentence" class="headerlink" title="Picking the most likely sentence"></a>Picking the most likely sentence</h1><p>在seq2seq机器翻译模型和我们在第一周课程里所用的语言模型之间有很多相似的地方，也有很多重要的区别。</p><p>我们可以把机器翻译想成是建立一个条件语言模型。下图先给出我们在学习序列模型第一周语言模型时所给出的网络结构：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-028837dd1aeb5362.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>这个模型可以让我们估计句子的可能性，我们也可以用其生成一个新的句子。</p><p>而机器翻译模型长的是下面这样，其中绿色表示encoder网络，而紫色表示decoder网络。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-6eaff6c73ec5c1c8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>所以机器翻译模型与语言模型很相似，不同在于语言模型总是以零向量开始，而encoder网络会计算出一系列句子并将其输入到decoder网络中。因此我们称其为条件语言模型。相比语言模型是输出任意句子的概率，翻译模型会输出句子的英文翻译，这取决于输入的法语句子。也就是说，我们估计这句英文翻译的概率，比如”Jane is visiting Africa in September”，这句翻译取决于法语句子”Jane visited I’Afrique en septembre”，这就是英语句子相对于输入的法语句子的可能性，所以它是条件语言模型。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-dc75067c047e5cb0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>下面我们希望能够利用上面的模型实现法译英。通过输入的法语句子，模型会告诉我们各种英文翻译所对应的可能性（概率）。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-41c54e23bd9b6c05.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>其中，x在这里是法语句子，而y这里对应不同英语句子翻译的概率。显然我们不希望它随机地进行输出，如果我们从这个分布中进行取样，我们可能会得到各种不同的翻译，可能有的糟糕，有的还可以：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-47f3012dd9af9f24.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>所以当我们使用这个模型来进行翻译时，我们并不是从得到的分布中进行随机采样，而是要找到一个英语句子y，使得条件概率最大化。所以在开发机器翻译系统时，我们需要做的一件事就是想出一个算法，用来找到合适的y值，使得该项最大化。而解决这个问题的算法称为Beam Search(束搜索)。</p><h2 id="Why-not-a-greedy-search"><a href="#Why-not-a-greedy-search" class="headerlink" title="Why not a greedy search?"></a>Why not a greedy search?</h2><p>在下一节介绍束搜索前，我们需要了解为什么不使用贪心搜索。这是一种来自计算机科学的算法，在生成第一个词的分布之后，它会根据我们的条件语言模型挑选出最有可能的第一个词，放入机器翻译模型中，接着它会继续挑选出最有可能的其他词。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ef67140fe31da49b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>但是我们真正需要的是一次性挑选出整个单词序列，从y<1>、y<2>到y<ty>，来使得整体的概率最大化。所以这种贪心算法是先挑出最好的第一个词，在这之后再挑最好的第二个词，这样一直挑选下去，而这种方法其实并不管用。</ty></2></1></p><p>为了证明整个观点，我们考虑下面两种翻译：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-a1897897502347e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>显然第一个翻译要好于第二个，所以我们希望机器翻译模型可以说第一个句子的p(y|x)比第二个句子高。但如果贪心算法挑选出了”Jane is”这两个词，而由于”is going”在英语中更加常见，所以对于法语句子来说”Jane is going”要比”Jane is visiting”会有更高的概率，所以很有可能如果我们仅仅根据前两个词来估计第三个词的可能性，我们最后得到的会是第二个句子，但这个句子显然比第一个差。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-1f24d333c8447e43.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>当然，在英语中各种词汇的组合数量还有许多，所以如果我们的字典中有10,000个单词，并且我们的翻译可能有10个词长，那么可能的组合就是10,000^10那么多。所以可能的句子数量非常巨大，我们不可能计算每一种组合的可能性，所以此时最常用的办法是用一个近似的搜索算法，这个近似的搜索算法做的就是尽力挑选句子使得条件概率最大化。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-93646929bb4c91fc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>尽管这不可能保证找到的句子是条件概率最大化，但这也足够了。</p><p>因此接下来，我们需要一个合适的搜索算法。</p><h1 id="集束搜索：Beam-Search"><a href="#集束搜索：Beam-Search" class="headerlink" title="集束搜索：Beam Search"></a>集束搜索：Beam Search</h1><h2 id="基本的Beam-Search"><a href="#基本的Beam-Search" class="headerlink" title="基本的Beam Search"></a>基本的Beam Search</h2><p>下面用我们之前所举的法语句子为例讲述beam search算法。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d6364f761df9c972.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>首先我们用这个网络结构来评估第一个单词的概率值，而贪婪算法只会挑出最有可能的一个单词然后继续，而集束搜索则会考虑多个选择。因此集束搜索会有一个参数B，全称为<strong>Beam width</strong>，在本例中<strong>设置为3</strong>。这就意味着集束搜索会一次性考虑3个可能的选择。比如对于第一个单词有不同选择的可能，最后找到in、jane、september是最有可能的三个选项，那么集束搜索算法会把结果存到计算机内存中，以便后面尝试用这三个词。</p><p>因此为了执行beam search的<strong>第一步</strong>，我们需要输入法语句子到编码网络，然后第一步，也就是解码网络的softmax层会输出10,000个概率值，在这10,000个输出的概率值取出前三个存起来。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-36f761c6dcd6b259.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在将in、jane、september作为第一个单词的三个可能选择后，beam search所做的<strong>第二步</strong>是针对每个选择，考虑第二个单词是什么。为了评估第二个词的概率值，我们会用下面这个神经网络。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-a47eb4c0d493f4da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>将y<1>（假设为in）的输出作为输入，从而可以评估第二个单词的概率。需要注意的是，在第二步里我们更关心的是找到最可能第一个和第二个单词对，而不是仅仅第二个单词具有最大概率。而具体的计算公式如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-88c2c2b2e3947490.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>因此对于第一个单词的三个候选选择，我们可以保存它们对应的概率值，然后再乘以第二个概率值，从而得到两个单词对的概率值。</1></p><p>对应预测单词jane后的第二个单词，相似网络如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c757813680bb6dfc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>同样的，对于september:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c1d4f41783d8b5dc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>那么我们通过上述的第二步，最后可以得到10,000*3种结果，也就是30,000个可能的结果，即集束宽乘以词汇表大小。而我们要做的就是评估这30,000个选择，然后<strong>选出前三个选择</strong>，假设选出来的是in september、jane is和jane visits。那么我们注意到，第一个单词的september已经没有可能了，但现在我们还是有3个选择。另外，由于我们的集束宽为3，因此我们会有三个网络副本，每个网络的第一个单词不同。</p><p>接下来简要讲第三步，类似上面的网络：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-540574efd8bb8f9b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>再继续同样的步骤，直到达到EOS标记。注意到，如果B=1，那么集束搜索就相当于前面所讲的贪婪搜索。</p><h2 id="改进的集束搜索：Refinements-to-beam-search"><a href="#改进的集束搜索：Refinements-to-beam-search" class="headerlink" title="改进的集束搜索：Refinements to beam search"></a>改进的集束搜索：Refinements to beam search</h2><h3 id="Length-normalization"><a href="#Length-normalization" class="headerlink" title="Length normalization"></a>Length normalization</h3><p>我们知道束搜索所做的是最大化下面的乘积概率：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f40580d0e5bf2929.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如果计算这些概率值，而这些概率值通常是远小于1，将它们相乘最后会造成数值下溢。因此在实际操作时我们不会真的计算乘积，而是取log值。而最大化这个log求和的概率值，也相当于最大化上面的乘积，得到相同的结果。因此通过取log，我们会得到一个数值上更稳定的算法，不容易出现数值的舍入误差。因为对数函数是严格单调递增的函数，而我们要最大化P(y|x)，因此最大化logP(y|x)和最大化P(y|x)的结果一样。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-6e1b1849b35f0bc5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>而这个目标函数有一个潜在的缺点，就是它会更偏向简短的翻译结果，因为短句子的概率是由更少的小于1的数字乘积得到的，对取log后的目标函数也是一样。因此我们对目标函数做另一个改变，即对这些数值做归一化，从而消除长短句的影响：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ea1cede76ed7b9dc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>但通常上，我们不会直接除以Ty，而是会采取更加柔和的方法，即在Ty上加上指数alpha，alpha=0.7。如果取alpha=1，就是直接除以Ty；而如果alpha=0，那么便没有归一化。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-633005637a01cdda.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>需要明确的是，这个超参数alpha（可调整）的设置并没有理论验证，而是大家都发现在实践中效果很好，所以很多人会这么做。</p><p>总结，在集束搜索中，我们会得到许多不同长度的句子，从1到可能最大设置为30之类的。然后我们针对这些所有的可能的输出句子，用上式对它们打分，取出概率最大的几个句子，再对这些束搜索得到的句子计算这个目标函数，最后从经过评估的这些句子中，挑选出在归一化的log概率目标函数上得分最高的一个。（A normalized log likelihood objective)</p><h3 id="Beam-search-discusstion"><a href="#Beam-search-discusstion" class="headerlink" title="Beam search discusstion"></a>Beam search discusstion</h3><p>如何选择B呢？如果B选择大，那么我们的计算代价也会很大，因为我们要把更多的可能选择保存起来。因此这里总结一下关于如何选择beam width的想法。</p><p>首先，我们知道B的大小会有以下影响：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e800f8190cd7616a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>对一般的产品系统而言，B一般选择为10；而对于科研而言，人们想压榨出全部的性能，也经常能够看到大家用束宽为1000或者3000。因此在实现我们的应用时，尝试不同的束宽值，当B很大的时候，性能提高会越来越小。因此对一般应用而言，我们可以从1到3到10这样调整。</p><p>另外，与BFS和DFS进行对比：Unlike exact search algorithms like BFS(Breadth First Search) and DFS(Depth First Search), Beam Search runs faster but is not guaranteed to find exact maximum for argmax_y(y|x)。</p><h2 id="集束搜索的误差分析：Error-analysis-on-beam-search"><a href="#集束搜索的误差分析：Error-analysis-on-beam-search" class="headerlink" title="集束搜索的误差分析：Error analysis on beam search"></a>集束搜索的误差分析：Error analysis on beam search</h2><p>集束搜索是一种近似搜索算法，也被称作启发式搜索算法。它不总是输出可能性最大的句子，而仅记录着B为前3或者10或是100种可能。如果束搜索算法出现错误，我们就可以用误差分析来发现问题的来源，看看是束搜索算法出现问题，还是我们的RNN模型出现问题。</p><p>假设给定以下例子，其中y*为最标准答案，而y^则是我们算法预测的结果，很明显是糟糕的翻译，其改变了句子的原意。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-728349452f845fdf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>而我们的模型有两部分，一个部分是RNN模型（即encoder-decoder模型），另一部分是束搜索算法。我们需要知道是哪一部分出现了问题。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-057605270bee8438.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>为了知道是哪个部分出现问题，我们需要使用RNN模型计算P(y*|X)和yP(y^|x)，需要注意我们要比较的是归一化后的最优化目标函数值。我们可以有以下结论:</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-d5b5fd3c92e66e87.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>整个误差分析过程如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-792e8d1399886acb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>经过这样的误差分析过程，我们就可以对验证集(dev set)中每一个错误例子，即算法输出了比人工翻译更差的结果的情况，我们可以尝试确定这些错误是搜索算法出了问题，还是生成目标函数的RNN模型出了问题。只有当我们发现束搜索算法造成了大部分错误时，才值得花费努力增大集束宽度；如果我们发现是RNN出错，我们可以考虑是增加正则化还是获取更多训练数据，亦或是尝试其他的网络结构。</p><p>这就是束搜索中的误差分析。这个特定的误差分析过程是十分有用的，它可以用于分析近似最佳算法（如束搜索算法），这些算法被用来优化学习算法（例如序列到序列模型/RNN）输出的目标函数。</p><h1 id="BLEU分数"><a href="#BLEU分数" class="headerlink" title="BLEU分数"></a>BLEU分数</h1><p>论文标题：A method for automatic evaluation of machine translation, 2002</p><h2 id="Evaluating-machine-translation"><a href="#Evaluating-machine-translation" class="headerlink" title="Evaluating machine translation"></a>Evaluating machine translation</h2><p>机器翻译的一大难题是比如给定一个法语句子，可以有多种英文翻译，而且都同样好。那么当具有多种同样好的翻译结果时，应该怎样评估一个机器翻译系统呢？常见的办法是通过BLEU得分来解决。</p><p>比如下面的一个法语句子，两个翻译结果都很好：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-87d9fd7fedc737fc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>而BLEU得分做的就是给定一个机器生成的翻译，它能够自动地计算一个分数来衡量机器翻译的好坏。而只要我们的翻译和人工翻译的结果足够接近，那么它就会得到一个高的BLEU分数。另外，BLEU的全称是bilingual evaluation understudy(双语评估替补)。BLEU分数背后的理念是观察机器生成的翻译，然后看<strong>生成的词是否出现在至少一个人工翻译参考之中</strong>。因此这些人工翻译的参考会出现包含在验证集或是测试集中。</p><p>下面看一个极端的例子。我们假设机器翻译系统(MT)输出了这样一个结果，显然是很糟糕的翻译。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c2f353e76c000a49.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>衡量机器翻译输出质量的方法之一是观察输出结果的每一个词看是否出现在人工翻译参考之中，这被称为机器翻译的精确度(a precision of the machine translation output)。这个情况下，机器翻译输出了7个词，并且每个词都出现在了参考1或参考2中，因此看上去每个词都很合理，所以这个输出的精确度为7/7。然而这显然不正确，因此这就是为什么把出现在参考中的词在MT输出的所有词中所占的比例作为精确度评估标准并不是很有用的原因。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ef76a7acc543f250.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>因此，我们取而代之的是用<strong>改良后的精确度评估方法</strong>(modified precision)，我们把每一个单词的记分上限定为它在参考句子中出现的最多次数。因此在本例中，the在参考1中出现两次，在参考2中出现1次，因此其得分上限为2。因此我们说，这个输出句子的得分为2/7，其中分母就是7个词中单词the总共出现的次数，而分子就是单词the出现的计数，<strong>达到上限(在参考句子中的最大出现次数)时就截断计数</strong>。</p><h2 id="Bleu-score-on-bigrams"><a href="#Bleu-score-on-bigrams" class="headerlink" title="Bleu score on bigrams"></a>Bleu score on bigrams</h2><p>在前面我们都只考虑了单个的单词，但我们也需要考虑成对的单词。接下来我们定义二元词组上的(bigram:指相邻的两个单词)的BLEU得分。（也有可能为三元词组trigram）</p><p>还是上面的例子，但是我们假设机器翻译出了较之前好一些的结果。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-849fb66f9bfba93c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>在该翻译上的二元词组有如下几个：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c469b3db983836f3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>分别计算二元词组的出现次数以及他们的截断值，以及最后计算得到的精确度为：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-edb69916491fd3f4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>更加具体的公式如下，评估的是一元词组乃至n元词组，从而知道机器翻译结果与人工参考翻译相似重复的程度。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-bb36541a937dd52e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>另外，我们可以确信，如果机器翻译结果与人工翻译结果一致，那么P1等等都会等于1。因此为了达到1.0的改良精确度，我们只需要与人工翻译结果其中的一个完全一致就好了。</p><h2 id="Bleu-details"><a href="#Bleu-details" class="headerlink" title="Bleu details"></a>Bleu details</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-b4d68bb849c47616.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>我们会加一个BP = “brevity penalty”(简单惩罚)。事实表明如果我们输出了一个非常短的翻译，那么它更容易得到了一个非常短的翻译，那么它会更容易得到一个高精确度，因为输出的大部分词都可能出现的人工参考中。因此BP就是一个调整因子，来惩罚输出了太短输出结果的翻译系统。</p><p>在之前的讲解中，我们知道拥有单一实数评估指标(a single real number evaluation metric)的重要性，它使得我们可以尝试不同想法，然后选取最好的。因此，BLEU分数对于机器翻译来说，具有革命性的原因是因为它有一个相当不错的（尽管不是完美的）但是非常好的单一实数评估指标，从而<strong>加快了整个机器翻译领域的进程</strong>。实践中，很少人会从零实现一个BLEU分数，如今有很多开源的实现结果。不过今天，BLEU分数被用来评估许多生成文本(generate text)的系统，比如机器翻译系统，也有图像描述系统，即我们用神经网络生成图像描述，然后用BLEU分数看看是否与参考描述相符。</p><p>不过，BLEU没有用于语音识别，因为在语音识别中通常只有一个答案，评估语音识别结果我们可以看是否十分相近或是字字正确(exactly word for word correct)。不过，在图像描述应用中，对于同一图片的不同描述，可能是同样好的。总之，BLEU很重要，能够自动评估翻译结果，从而帮助加快算法开发进程。</p><h1 id="注意力模型"><a href="#注意力模型" class="headerlink" title="注意力模型"></a>注意力模型</h1><p>论文标题：</p><ol><li>Neural Machine Translation by Jointly Learning to Align and Translate, 2014（首次提出attention）</li><li>Show attention and tell: neural image caption generation with visual attention, 2015</li></ol><p>注意力模型已经是深度学习领域中最重要的思想之一。下面开始解释。</p><p>·<br>假设给定一个很长的法语句子。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-a09bbaee23505414.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>在我们的神经网络中，这个绿色的编码器要做的就是读整个句子并且记忆，然后在感知机中传递，而对于紫色的解码网络则将生成英文翻译。其实，人工翻译并不会通过读整个法语句子，再记忆里面的东西，然后从零开始机械式地翻译成一个英语句子。人工翻译首先可能会先翻译出句子的部分，再看下一部分，并翻译，再一直这样下去。我们可以通过一点点地翻译，因为记忆整个长句子是非常困难的。</p><p>因此对于上面的网络，它对于短句子翻译的效果很好，能够有较高的BLEU分数，但对相对长的句子（比如大于30或40个单词的句子），它的表现就会变差。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-19e88609b7b49855.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>而有了注意力模型，我们会发现它的翻译过程和人类很像，即一次只翻译句子的一部分，因此我们也不会看到BLEU分数巨大的下倾(huge dip)。而这个下倾实际上衡量了神经网络记忆一个长句子的能力，这是我们不希望看到的。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-68ad521726cf8790.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="Attention-model-intuition"><a href="#Attention-model-intuition" class="headerlink" title="Attention model intuition"></a>Attention model intuition</h2><p>我们举一个短句来理解注意力模型。我们用双向RNN来对句子中的单词特征集进行计算，然后用另一个RNN来进行翻译。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-1d17f2792463f5d1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>当尝试生成句子的第一个单词翻译时，我们的输入包括S<0>，以及对句子中上下文单词的注意力。直观来想，RNN向前进，一次生成一个词，直到最后生成<code>&lt;EOS&gt;</code>，而alpha&lt;t,t’&gt;告诉我们，当我们在翻译第t个英文词时，应该将多少注意力放置在t’个法语单词上，这允许它在每个时间步会去看周围词距内的法语词要花多少注意力。</0></p><h2 id="Attention-model"><a href="#Attention-model" class="headerlink" title="Attention model"></a>Attention model</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-e56e95d532c2d1d6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这个alpha参数告诉我们上下文有多少取决于我们得到的特征或者我们从不同时间步得到的激活值。所以我们定义上下文的方式实际上来源于被注意力权重加权的不同时间步中的特征值（激活值）。因此更加公式化的注意力权重将会满足非负的条件，它们加起来等于1。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-33d87cf4e97e1f09.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>另外，我们也会输入上下文，对应的公式为：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-486d469a8d6d4a25.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>其中，a&lt;t’&gt;等于前向和后向的激活值：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-5e1da806cf47270a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>因此，alpha<t,t'>就表示y<t>应该在t'时花在a<t'>上的注意力(amount of "attention" y<t> should pay to a<t'>)。换句话说，当我们在t处生成输出词，我们应该花多少注意力在第t’个输入词上面，这是生成输出的其中一步。随后下一个时间步，我们会生成第二个输出，相似的，我们有了一个新的注意力权重集，再找到一个新的方式将他们相加，这就产生了一个新的上下文，允许我们生成第二个词。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c87c2396dbff39fe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></t'></t></t'></t></t,t'></p><p>为了更加清楚，这里采用原论文的图片进行理解：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-25c60a522ee34b85.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>接下来需要了解如何计算注意力权重集。</p><h2 id="计算注意力权重"><a href="#计算注意力权重" class="headerlink" title="计算注意力权重"></a>计算注意力权重</h2><p>从上一节，我们知道：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-bd3f4d025fbe2dfe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>其中，y<t>表示第t个输出词，a&lt;t’&gt;表示第t’时间步的激活值。</t></p><p>我们用下面的式子来计算alpha&lt;t,t’&gt;。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f2e3fc1c0afdd9e3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>因此在此之前，我们需要计算e<t,t'>，然后用softmax计算出上述注意力权重值，保证和为1。如何计算e呢，我们用下面的这个小型神经网络：![image.png](https://upload-images.jianshu.io/upload_images/8636110-2161008f31e0f7e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)其中，s<t-1>就是神经网络在上个时间步的隐藏状态，然后a<t'>则为另一个输入。直观来想就是，如果我们想要决定要花多少注意力在t’的激活值上，那么它在很大程度上取决于我们上一个时间步的隐藏状态的激活值。但我们还没有当前状态的激活值，所以我们会看看生成上一个翻译的RNN的隐藏状态，然后对于每一个位置都看看他们的特征值，因此a<t,t'>和e<t,t'>取决于这两个量。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-68990f07ce95f08d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></t,t'></t,t'></t'></t-1></t,t'></p><p>这个算法的缺点是它要花费三次方的时间，即时间复杂度为O(n^3)。因此如果我们有Tx个输入单词和Ty个输出单词，那么注意力参数的总数会是Tx * Ty。但是在机器翻译的应用中，输入和输出的句子一般不会太长，不过现在也有很多研究在尝试减少这样的消耗。另外也有一篇类似的image captioning论文运用了类似的思想。</p><h2 id="Attention-examples"><a href="#Attention-examples" class="headerlink" title="Attention examples"></a>Attention examples</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-0c7b833ad052eb00.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="语音识别：Speech-recognition"><a href="#语音识别：Speech-recognition" class="headerlink" title="语音识别：Speech recognition"></a>语音识别：Speech recognition</h1><p><img src="https://upload-images.jianshu.io/upload_images/8636110-c4a16133c6266c81.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>首先简单介绍语音识别的流程。假设我们说一个音频片段为”the quick brown fox”，<br><img src="https://upload-images.jianshu.io/upload_images/8636110-54c9846e5b68a13f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>我们希望一个语音识别算法(speech recognition algorithm)，通过输入这段音频，然后输出音频的文本内容。考虑到人的耳朵并不会处理声音的原始波形，而是通过一种特殊的物理结构来测量这些不同频率和强度的声波。因此音频数据的常见预处理步骤，就是运行这个原始的音频片段，然后生成一个声谱图(spectrogram)。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-127a36d36e11c98d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>横轴是时间，纵轴是声音的频率，而图中的不同颜色显示了声波能量的大小(the amount of energy)，也就是不同的时间和频率上这些声音有多大。</p><p>以前的语音学家都是通过基本的音位单元(basic units of sound called phonemes)来表示音频(audio)，然后在end-to-end模型中，我们发现这种音位表示法已经不再必要了，而是可以构建一个系统，通过向系统中输入音频片段(audio clip)，然后直接输出音频的文本。使这种方法成为可能的一件事是使用一个很大的数据集，所以语音研究的数据集一般会超过300个小时，而商业系统已经训练出了上万个小时的数据。在文本音频数据集(transribe audio data sets)同时包含x和y，通过深度学习算法大大推进了语音识别的进程。接下来讲解如何建立一个语音识别系统。</p><h2 id="Attention-model-for-speech-recognition"><a href="#Attention-model-for-speech-recognition" class="headerlink" title="Attention model for speech recognition"></a>Attention model for speech recognition</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-d311db6470d6cfcf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们可以这样做：在横轴上，也就是在输入音频的不同时间帧上，我们可以用注意力模型来输出文本描述。</p><h2 id="CTC-cost-for-speech-recognition"><a href="#CTC-cost-for-speech-recognition" class="headerlink" title="CTC cost for speech recognition"></a>CTC cost for speech recognition</h2><p>论文标题：Connectionist Temporal Classification: Labeling unsegmented sequence data with recurrent neural networks, 2006.</p><p>CTC的全称为Connectionist Temporal Classification。它的算法思想如下：</p><p>假如语音片段内容是某人说”the quick brown fox”，这时候我们使用一个新的网络，结构如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-493bffae94c71bd6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>其中，其输入的x和输出的y数量是一样的，另外，尽管这里画的只是一个简单的单向RNN结构，但实际上它有可能是双向的LSTM结构或者双向的GRU结构，并且通常是很深的模型。需要注意，在语音识别中，通常输入的时间步数量(the number of input time steps)要比输出的时间步数量(the number of output time steps)多出很多。举例：假设有一段10秒的音频，并且特征是100赫兹的，即每秒有100个样本，于是这段10秒的音频片段就有1000个输入。但我们的输出就没有1000个字母或字符，这时候<strong>CTC损失函数</strong>允许RNN生成这样的输出：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-9dbfe3efc6dd3cfa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>这句输出对应的是”the q”。<strong>CTC损失函数的一个基本规则是将空白符之间的重复字符折叠起来</strong>，注意空白符和空格符是不一样的。比如，上图中我们发现空格符是短下划线两边还有两个竖杠，从而用于区分空白符和空格符。</p><p>因此，尽管我们的文本只有”the quick brown fox”包括空格一共有19个字符，但在这样的情况下，通常允许神经网络有重复的字符和插入空白符来使其强制输出1000个字符。</p><h1 id="触发字检测：Trigger-word-detection"><a href="#触发字检测：Trigger-word-detection" class="headerlink" title="触发字检测：Trigger word detection"></a>触发字检测：Trigger word detection</h1><p><img src="https://upload-images.jianshu.io/upload_images/8636110-c4af0063d7252e6b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>下面简单介绍一个能够实现的算法：</p><p>如果刚探测到一个触发字，将对应时间的目标标签设为1，之后的设为0；如果再遇到触发词，也设置为1。很明显这是一个非平衡数据集，0的数量比1多很多。</p><p>解决方法：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-74dfb154c9462061.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>实际上没听懂这里，看看编程能不能懂。</p><h1 id="本周作业"><a href="#本周作业" class="headerlink" title="本周作业"></a>本周作业</h1><h1 id="完成，向NLP继续进军。"><a href="#完成，向NLP继续进军。" class="headerlink" title="完成，向NLP继续进军。"></a>完成，向NLP继续进军。</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;基础模型&quot;&gt;&lt;a href=&quot;#基础模型&quot; class=&quot;headerlink&quot; title=&quot;基础模型&quot;&gt;&lt;/a&gt;基础模型&lt;/h1&gt;&lt;h2 id=&quot;Sequence-to-sequence-model-encoder-decoder-network&quot;&gt;&lt;a hr
      
    
    </summary>
    
      <category term="深度学习" scheme="https://github.com/DesmonDay/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://github.com/DesmonDay/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>第13周-自然语言处理与词嵌入</title>
    <link href="https://github.com/DesmonDay/2019/04/27/deep-learningw13/"/>
    <id>https://github.com/DesmonDay/2019/04/27/deep-learningw13/</id>
    <published>2019-04-27T12:47:25.000Z</published>
    <updated>2019-05-01T15:46:05.977Z</updated>
    
    <content type="html"><![CDATA[<h1 id="词嵌入-Word-Embedding"><a href="#词嵌入-Word-Embedding" class="headerlink" title="词嵌入: Word Embedding"></a>词嵌入: Word Embedding</h1><p>我们之前用的词向量表示法为one-hot向量，但这种表示方法存在很大的缺陷，我们用o_3455表示该向量。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b69751a957a9523b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>比如，苹果和梨具有相似性，但用one-hot向量表示的话，神经网络无法捕捉他们之间的相似性。这是因为两个不同one-hot向量的内积为0，即不同单词之间的距离相同。而很明显，苹果和梨的距离，是要小于苹果和国家的距离的。</p><p>因此我们考虑用特征化后的向量来表示词，举个简单例子，假设有很多不同的特征，使我们得到新的词嵌入表示。我们用e_5391来表示特征化后的向量，此时使用这种向量表示，苹果和橙子之间的距离就很接近了，且算法会发现苹果和橙子要比苹果和国家更相似。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-dad888a611f8f1e8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>当然，实际上我们学习的特征向量是比较复杂的，而不是一个个具体的特征。如果我们学习到了300维的词嵌入，我们通常会把这300维向量嵌入到一个二维空间，从而实现可视化。通常我们用t-SNE算法来实现。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-72b76b6b8ad9cad1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="使用词嵌入：Using-word-embeddings"><a href="#使用词嵌入：Using-word-embeddings" class="headerlink" title="使用词嵌入：Using word embeddings"></a>使用词嵌入：Using word embeddings</h2><p>先举一个命名实体识别的例子。假设我们的训练集很小，甚至都不知道durian(榴莲)这个词。那么我们可以用事先训练出来的词向量。<br>学习词向量的算法会从大量的文本中学习，所以即使我们的训练集很小，如果我们使用已训练出来的词向量，那么结果也不会差。这也算是迁移学习的一种，即我们将从大量文本中学习到的词向量，应用到自己的任务上。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-3fca17c45498af4c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>在本例中，我们可以通过后文的farmer来意识到前面的单词是人名，因此最好使用双向RNN模型。</p><p>下面总结用词嵌入做迁移学习的步骤。</p><ol><li>Learn word embeddings from large text corpus.(1-100B words), or download pre-trained embedding online.</li><li>Transfer embedding to new task with smaller training set.(say, 100k words.) 比如用一个300维的词嵌入来表示单词，这样的好处是可以用更低维度的特征向量来代替原来的10,000维的one-hot向量，相比之下低维向量会更加紧凑。</li><li>Optional: Continue to finetune the word embeddings with new data. 这一步一般在我们的数据集比较大的时候才做。</li></ol><p>当我们的训练集相对较小时，词嵌入的作用最明显，因此其广泛用于NLP领域。比如它已经用在了命名实体识别、文本摘要、文本解析、指代消解里，这些都是很标准的NLP任务；词嵌入在语言模型、机器翻译领域用得少一些，尤其是我们做语言模型或者机器翻译任务时，这些任务我们有大量的数据。在其他的迁移学习场景下也一样，如果我们从某一任务A迁移到某一个任务B，只有A中有大量数据，B中数据少时，迁移才有用。</p><p>这里还举了与人脸识别的不同。对于人脸识别，我们训练一个Siamese网络来学习不同人脸的128维表示，然后比较编码结果来判断两个图片是否为同一个人脸。只要输入一个人脸，就返回一个编码结果。两者对比，人脸编码可能涉及海量的图片，而自然语言处理有一个固定的词汇表，像没有出现的就标记为”\<unk\>“。</unk\></p><h2 id="词嵌入的特性"><a href="#词嵌入的特性" class="headerlink" title="词嵌入的特性"></a>词嵌入的特性</h2><p>词嵌入的一个特性是可以帮助实现类比推理。我们希望词嵌入可以捕捉单词的特征表示，假如我们提出一个问题，man对应woman，那么king对应什么？用词嵌入可以实现这种推导。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ce068fbd6d430d76.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>如图，通过e_man-e_woman和e_king-e_queen，我们发现他们的主要差异，通过向量表示，可以发现是gender(性别)上的差异。所以得出这种类比推理的结果方法是，当算法被问及man对应woman，那king对应什么时，算法所做的就是计算e_man-e_woman，然后找出一个向量，使得e_man-e_woman约等于e_king-e_?。也就是说，当这个新词为queen时，式子近似相等。这种思想帮助很多研究者对词嵌入领域建立了更深刻的理解。</p><p>论文标题：Linguistic regularities in continuous space word representations, 2013.</p><p>实现类比推理的方法即找到相似度最大的单词：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-4f6bfafe877960c8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>另外，注意到我们用t-SNE算法映射到的二维空间，不一定能够像左图那样呈现出一个平行四边形，因为这种映射是使用了一种很复杂的非线性映射。</p><p>下面列举常用的相似度函数：</p><ol><li>余弦相似度（常用来衡量词嵌入之间的相似度）：<br> <img src="https://upload-images.jianshu.io/upload_images/8636110-676805ea293c9357.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></li><li>欧式距离：<br> <img src="https://upload-images.jianshu.io/upload_images/8636110-dd85369ffbc4f0ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></li></ol><p>通过在大量的语料库上学习，词嵌入算法可以发现像下面这样的类比推理(analogy reasoning)模式：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-4de132349dfb7b1b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="词嵌入矩阵"><a href="#词嵌入矩阵" class="headerlink" title="词嵌入矩阵"></a>词嵌入矩阵</h2><p>将嵌入矩阵E与单词对应的one-hot矩阵相乘，我们可以得到对应单词的词向量，即E·O_j = E_j。但一般我们实际上是用专门的函数，即找出对应的列，来找到对应单词的词向量，这样更高效，比如Keras有个函数叫keras.layers.Embedding可以实现。我们在学习的时候，会随机初始化E矩阵，然后通过梯度下降方法来求出E。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ada21eecc44bdd3f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="具体算法学习词嵌入"><a href="#具体算法学习词嵌入" class="headerlink" title="具体算法学习词嵌入"></a>具体算法学习词嵌入</h2><p>论文标题：A neural probabilistic language model, 2003.</p><p>介绍一个早期最成功的用于学习嵌入矩阵E的NLP模型，比如假定给出四个单词，预测下一个单词会是什么。：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b3958ee014798af9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>Other context/target pairs</strong>: </p><p>在前面我们知道了算法预测出了某个单词juice，我们将其称为target words，它是通过前面的context(last 4 words)学到的。所以如果我们的目标是学习一个嵌入向量，研究人员已经尝试过很多不同类型的上下文，<strong>如果我们要构建一个语言模型，那么一般选取目标词之前的几个词作为上下文；但如果我们的目标不是学习语言模型本身，而是学习词嵌入，那么我们可以选择其他上下文。</strong></p><p>比如，我们可以提出一个学习问题，而它的上下文是左边和右边的各四个词，即我们可以把target word左右两个的词作为上下文。因此我们的算法获得了a glass of orange和to go along with，然后要求预测出中间这个词。提出这样一个问题，这个问题需要将左边4个词和右边4个词的嵌入向量提供给神经网络，来预测中间的单词是什么。或者上下文只有前一个词的嵌入向量，然后用来预测下一个词。或者上下文可以是附近的一个词，比如glass，利用它来推导juice。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-cef310702e888287.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>而这种利用上下文前后两个单词的思想与Word2Vec中的skip gram模型一致，下文会介绍Word2Vec.</p><h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><p>论文标题：Efficient estimation of word representations in vector space, 2013.</p><p>假设给定一个句子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-bebd48a7b239949a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>在skip gram模型中，我们要做的是抽取上下文和目标词配对，来构造一个监督学习问题。而我们的上下文不一定总是在目标词之前离得最近的n个单词。我们会随机选一个词作为context word，如orange，然后我们要做的是随机在一定词距内(比如context word前后5个词或10个词内）选择目标词target word，比如juice。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-bccd935a4e58cc20.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>因此我们构造一个监督学习问题，它给定context word，要求你预测在这个词正负10个词距或者5个词距内随机选择的某个目标词。而构造这个监督学习问题的目标并不是解决这个监督学习问题本身，而是我们想要用这个学习问题来学到一个好的词嵌入模型。</p><h3 id="Skip-gram模型"><a href="#Skip-gram模型" class="headerlink" title="Skip gram模型"></a>Skip gram模型</h3><p>假设我们的单词总数量为10,000.那么给定一个context word作为输入，我们要求预测出target word。（这里之所以叫skip-gram，就是因为我们预测的是context word从左数或者右数的某个target word。）<br><img src="https://upload-images.jianshu.io/upload_images/8636110-54695b88b17d7d4b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们的网络结构是这样的：输入context word的one-hot向量，然后经过嵌入矩阵的相乘得到e_c，再通过一个Softmax单元得到目标词的one-hot表示。因此这里的参数有两个部分，一个是嵌入矩阵本身，一个是softmax单元本身的参数。所以我们的网络结构和具体Softmax函数如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-6fa505f529777475.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>而Softmax函数的损失函数如下，注意到y和y_hat都是one-hot向量：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ee88a2a23705f4a9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>而Skip-gram模型实际上存在一些问题，尤其是在softmax模型中，每次我们想要计算概率时，我们需要对词汇表的所有词做求和运算，而这个词汇表可能会很大，那么分母的求和操作就会相当慢。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-9b88697bbc563b43.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>解决方法有Hierachical softmax（分级的Softmax分类器），简单来说就是不是一开始就确定到底是属于哪一类，而是先告诉我们该词是属于哪一级别，相当于利用一个树形结构。我们使用的是霍夫曼树，相当于使用了二元分类，即二元逻辑回归的方法。在实践中，分级的Softmax分类器会被构造成常用词在顶部，而不常用词则在树的深部，即不对称的二叉树（不同的经验法则）。更具体的解释：<a href="https://www.cnblogs.com/pinard/p/7243513.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/7243513.html</a><br><img src="https://upload-images.jianshu.io/upload_images/8636110-0e29831514e81939.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>接下来我们需要理解，怎么对上下文C进行采样？一旦我们对上下文进行采样，那么目标词t就会在上下文前后词距比如10的单词中进行采样。一种选择是我们可以在语料库中随机均匀地采样，这样做我们会发现有一些词像the/a/of/and会频繁出现，而其他的apple/durian的词则很少出现，这是我们不希望的情况（很多时间更新频繁词）；因此我们通常采用一些启发式方法来平衡频繁词和普通词的采样。</p><h3 id="CBOW模型-Continuous-Bag-of-Words"><a href="#CBOW模型-Continuous-Bag-of-Words" class="headerlink" title="CBOW模型(Continuous Bag-of-Words)"></a>CBOW模型(Continuous Bag-of-Words)</h3><p>CBOW模型通过获得中间词的上下文，然后用这些周围的词去预测中间的词。</p><h3 id="负采样：Negative-Sampling"><a href="#负采样：Negative-Sampling" class="headerlink" title="负采样：Negative Sampling"></a>负采样：Negative Sampling</h3><p><img src="https://upload-images.jianshu.io/upload_images/8636110-bebd48a7b239949a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>负采样可以比较好的解决Skip gram模型的计算问题。我们在这个算法中要做的是构造一个新的监督学习问题，比如给定一对单词，如orange和juice，我们要去预测这是否是context-target pair？在这个例子中，orange和juice就是一个正样本，即为1。而比如orange和king，我们将其视为负样本，即为0。所以我们要做的是采样得到一个context word和target word，也就是表中的第一样，给出了一个正样本；接着我们再使用相同的上下文词，在词典中随机选取几个词（如果我们的词在上下文词的词距中也没关系），作为负样本。接着我们构造的监督问题就是输入这对词，然后去预测目标的标签，即预测输出y。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-1d0c7b574085f460.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>我们的目的就是区分这两个词是通过对靠近的两个词采样，还是随机采样的。我们的目的就是区分两种不同的采样方式。</p><p>所以这就是如何生成训练集的方法。而如何选择K呢？如果是小数据集，那么K从5到20比较好，但如果是大数据集，那么K为2-5。在本例中，我们选择K为4.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b7fdd6568e488ecc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>对应的模型：原本我们使用的是Softmax分类器，但是计算成本过高。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b0b75a550cdb0e21.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>因此我们采用负采样的方法来进行训练。此时我们的负采样输入和输出分别为：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-708c9138ab4bf471.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>所以我们会使用<strong>二元逻辑回归分类器</strong>来判断是否为正样本还是负样本。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-81e6db3c34f1e23c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>因此整个的Skip gram模型优化后如下。（给定上下文词orange，通过与嵌入矩阵E相乘得到嵌入向量，然后我们会得到10,000个可能的logistic回归问题，其中一个将会是用来判断目标词是否是juice的分类器，而其他的可能用来预测king是否是目标词之类的。把他们看成10,000个二元分类器，但并不是每次迭代都训练全部的10,000个，即<strong>每次迭代我们只训练其他的K+1个分类器</strong>。这样每次迭代的成本要比Softmax小很多。）<br><img src="https://upload-images.jianshu.io/upload_images/8636110-0ecb9a6fe27f1b58.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这个算法还有一个细节，即如何选择负采样的样本？我们可以对候选的目标词进行采样，可以根据其在语料中的经验频率进行采样（会导致the,of,and等多次被采样），而另一个极端就是用1除以词汇表总词数，均匀且随机地抽取负样本。而论文的作者发现的一个经验方法是既不用经验频率，也不是均匀采样，而可以用介于他们之间的方法。他们做的是对词频的3/4次方除以整体的值进行采样。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d6754994b242345a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="GloVe词向量"><a href="#GloVe词向量" class="headerlink" title="GloVe词向量"></a>GloVe词向量</h2><p>论文标题：Global vector for word representation, 2014.</p><p>在之前，我们曾通过挑选语料库中位置相近的两个词，列举出词对，即上下文和目标词，而GloVe算法做的是使其关系明确化。假设X_ij是单词i在单词j上下文中出现的次数，那么这里i和j的功能就和t和c的功能一样，所以我们可以认为X_ij等同于X_tc。根据上下文和目标词的定义，我们可以得出X_ij等于X_ji的结论。事实上，如果我们将上下文和目标词的范围定义为出现于左右各1词以内的话，就会有对称关系，但如果上下文总是目标词前一个词的话，那就不对称了。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-aab282e606d3b427.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>对于GloVe算法，我们可以定义上下文和目标词Wie任意两个位置相近的单词，假设是左右各10个词额距离，那么X_ij就是一个能够获取单词i和单词j出现位置相近时或者彼此接近的频率的计算器。</p><p>Glove模型做的就是最小化他们之间的差距：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-dd3925abbecc8c53.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>而公式中的点乘就是要告诉我们这两个单词之间有多少联系，t和c之间有多紧密，i和j之间联系程度如何，换句话说他们同时出现的频率是多少，这是由X_ij影响的。接着我们需要解决参数theta和e的问题，然后用梯度下降法来最小化上面的公式。需要补充的细节是，如果X_ij=0，那么log0是未定义的，所以我们添加了一个额外的加权项f(X_ij)(weighting term)。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ebbe984ad95ececd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>如果X_ij等于0，我们会约定0log0=0。因此这个求和公式表明，这个和仅是一个上下文和目标词关系里连续出现至少一次的词对的和。f(X_ij)的另一个作用是，有些词在英语里出现十分频繁，比如this,is,of,a等等，这些词称为”停止词”，在频繁词和不常用词之间也会有一个连续体(continumm: 相邻两者相似但起首与末尾截然不同的)。另外也有一些不常用词，比如durion，但我们还是想考虑在内，但又不像常用词那么频繁。因此，这个加权项f(X_ij)就可以是一个函数，给予这些出现频率不同的词不同的权重。（具体可以看GloVe算法的论文）</p><p>最后一个关于此算法有趣的事是theta和e是完全堆成的。因此有一种训练算法的方法是一致地初始化theta和e，然后使用梯度下降来最小化输出。当每个词都处理完之后去平均值，所以给定一个词w，我们有：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ebd4600cd0ec5c0a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="A-note-on-the-featurization-view-of-embeddings"><a href="#A-note-on-the-featurization-view-of-embeddings" class="headerlink" title="A note on the featurization view of embeddings"></a>A note on the featurization view of embeddings</h2><p>在前面讲词嵌入的时候，我们是用下面这样一个简单的例子来解释的。但是，实际上我们训练出来的词向量很难对每个维度有这样清楚的理解，即很难知道哪个轴代表gender之类的。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ef901ebc6cf6b990.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>例如，对于我们在前面学到的GloVe算法：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-70c70df5fa9db494.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>我们可以乘积项利用线性代数的知识表示为：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-105665c3bc9f3fd4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>我们就知道，我们不能保证这些用来表示特征的轴能够等同于人类可能简单理解的轴。具体而言，第一个特征可能是gender/roya/age/foot/等的组合，它也许是名词或是一个行为动词和其他所有特征的组合，所以很难看出独立组成部分。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-4e8e1a43202079f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>尽管有这种类型的线性变换，但是这个平行四边形映射也说明我们解决了这个问题。因此尽管存在特征量潜在的任意线性变换，我们最终还是能学习出解决类似问题的平行四边形映射。</p><h1 id="情感分类：Sentiment-classification"><a href="#情感分类：Sentiment-classification" class="headerlink" title="情感分类：Sentiment classification"></a>情感分类：Sentiment classification</h1><p>问题阐述，一般是对评论进行分类：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d77d210646ecbd19.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>情感分类的一个最大挑战是可能标记的训练集没有那么多。对于情感分类任务来说，训练集大小从10,000到100,000个单词都很常见，也可能小于10,000个单词，而使用词嵌入能够带来更好的效果，尤其是只有很小的训练集时。</p><p>一个简单的情感分类的模型如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f71bba9251809c51.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>注意到取平均的操作使得我们的模型可以适用于任意长短的评论。</p><p>而这个算法的一个问题是没有考虑词序。尤其是，对于这样一个负面的评价”Completely lacking in good taste, good service, and good ambience.”，但由于good出现了很多次，那么仅仅平均或求和得到的嵌入向量可能会多次出现good的含义，因此我们的分类器可能会认为这是一个正面的评价。所以我们可以用一个RNN模型来做情感分类。</p><h2 id="RNN-for-sentiment-classification"><a href="#RNN-for-sentiment-classification" class="headerlink" title="RNN for sentiment classification"></a>RNN for sentiment classification</h2><p>我们可以使用如下的RNN模型：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-a347f55b5b01ff85.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>这个模型就是我们之前所介绍过的many-to-one结构。有了这样的模型，考虑词的顺序，这样就会有更好的效果了。</p><h1 id="词嵌入除偏：Debias-word-embedding"><a href="#词嵌入除偏：Debias-word-embedding" class="headerlink" title="词嵌入除偏：Debias word embedding"></a>词嵌入除偏：Debias word embedding</h1><p>论文标题：Man is computer programmer as woman is homemaker? Debiasing word embeddings 2016</p><p>现在机器学习和人工智能算法正渐渐地被信任用以辅助或者指定极其重要的决策，因此我们想尽可能地确保它们不受<strong>非预期形式偏见</strong>影响，比如性别歧视(gender bias)、种族歧视(ethnic bias)等等。本节会展示词嵌入中一些有关减少或是消除这些形式的偏见的方法。</p><p>常见词嵌入bias如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-055bea92b4d8f957.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>具体来说，word embeddings can reflect gender, ethnicity, age, sexual orientation, and other biases of the text used to train the model. 通常，这些偏见都和社会经济状态相关。</p><p>假设下面这些词的嵌入画在平面图如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-90897e106c8e4e4b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>第一步我们要做的是identify bias direction（定义步），比如采用取平均的方法：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d90327669786993e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>（实际上取平均的算法过于简单，原论文里不是这样做的，而是做了奇异值分解，从而确定了bias direction，也就是偏见的方向。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b1729f55eb91a9bc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>第二步是Neutralize（中和步）: For every word that is not definitional（定义不明确）, project to get rid of bias，比如图片上的babysitter、docter。对于这样的词，我们可以减小将它们在bias direction上进行处理，来减少或消除它们性别歧视趋势的成分。</p><p>第三步是Equalize pairs(均衡步)。比如我们有这样的词对grandmother和grandfather，girl和boy，我们只希望这些词对的不同体现在性别上，确保和babysitter和docter之类的词有相似的距离。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-158cd369401bcab3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>所以论文作者所做的就是尝试训练一个分类器，来尝试解决哪些词是明确定义的，哪些词不是。结果表明大部分英文单词在性别方面是没有明确定义的，而只有一部分词不是性别中立的。</p><h1 id="本周作业"><a href="#本周作业" class="headerlink" title="本周作业"></a>本周作业</h1><h2 id="Operations-on-word-vectors"><a href="#Operations-on-word-vectors" class="headerlink" title="Operations on word vectors"></a>Operations on word vectors</h2><h3 id="1-Cosine-similarity"><a href="#1-Cosine-similarity" class="headerlink" title="1- Cosine similarity"></a>1- Cosine similarity</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cosine_similarity</span><span class="params">(u, v)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Cosine similarity reflects the degree of similariy between u and v</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        u -- a word vector of shape (n,)          </span></span><br><span class="line"><span class="string">        v -- a word vector of shape (n,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        cosine_similarity -- the cosine similarity between u and v defined by the formula above.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    distance = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Compute the dot product between u and v (≈1 line)</span></span><br><span class="line">    dot = np.dot(u,v)</span><br><span class="line">    <span class="comment"># Compute the L2 norm of u (≈1 line)</span></span><br><span class="line">    norm_u = np.sqrt(np.sum(u*u))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute the L2 norm of v (≈1 line)</span></span><br><span class="line">    norm_v = np.sqrt(np.sum(v*v))</span><br><span class="line">    <span class="comment"># Compute the cosine similarity defined by formula (1) (≈1 line)</span></span><br><span class="line">    cosine_similarity = dot / (norm_u * norm_v)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cosine_similarity</span><br></pre></td></tr></table></figure><h3 id="2-Word-analogy-task"><a href="#2-Word-analogy-task" class="headerlink" title="2- Word analogy task"></a>2- Word analogy task</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">complete_analogy</span><span class="params">(word_a, word_b, word_c, word_to_vec_map)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Performs the word analogy task as explained above: a is to b as c is to ____. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    word_a -- a word, string</span></span><br><span class="line"><span class="string">    word_b -- a word, string</span></span><br><span class="line"><span class="string">    word_c -- a word, string</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary that maps words to their corresponding vectors. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># convert words to lower case</span></span><br><span class="line">    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Get the word embeddings v_a, v_b and v_c (≈1-3 lines)</span></span><br><span class="line">    e_a, e_b, e_c = word_to_vec_map[word_a],word_to_vec_map[word_b],word_to_vec_map[word_c]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    words = word_to_vec_map.keys()</span><br><span class="line">    max_cosine_sim = <span class="number">-100</span>              <span class="comment"># Initialize max_cosine_sim to a large negative number</span></span><br><span class="line">    best_word = <span class="keyword">None</span>                   <span class="comment"># Initialize best_word with None, it will help keep track of the word to output</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># loop over the whole word vector set</span></span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> words: <span class="comment"># w is string</span></span><br><span class="line">        <span class="comment"># to avoid best_word being one of the input words, pass on them.</span></span><br><span class="line">        <span class="keyword">if</span> w <span class="keyword">in</span> [word_a, word_b, word_c] :</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">### START CODE HERE ###</span></span><br><span class="line">        <span class="comment"># Compute cosine similarity between the combined_vector and the current word (≈1 line)</span></span><br><span class="line">        cosine_sim = cosine_similarity(e_b-e_a, word_to_vec_map[w]-e_c)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># If the cosine_sim is more than the max_cosine_sim seen so far,</span></span><br><span class="line">            <span class="comment"># then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (≈3 lines)</span></span><br><span class="line">        <span class="keyword">if</span> cosine_sim &gt; max_cosine_sim:</span><br><span class="line">            max_cosine_sim = cosine_sim</span><br><span class="line">            best_word = w</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> best_word</span><br></pre></td></tr></table></figure><h3 id="3-Debiasing-word-vectors-OPTIONAL-UNGRADED"><a href="#3-Debiasing-word-vectors-OPTIONAL-UNGRADED" class="headerlink" title="3- Debiasing word vectors (OPTIONAL/UNGRADED)"></a>3- Debiasing word vectors (OPTIONAL/UNGRADED)</h3><h4 id="3-1-Neutralize-bias-for-non-gender-specific-words"><a href="#3-1-Neutralize-bias-for-non-gender-specific-words" class="headerlink" title="3.1- Neutralize bias for non-gender specific words"></a>3.1- Neutralize bias for non-gender specific words</h4><p><img src="https://upload-images.jianshu.io/upload_images/8636110-d7339d5c8b5fe9e5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-ac20abc1877f2d6e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neutralize</span><span class="params">(word, g, word_to_vec_map)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Removes the bias of "word" by projecting it on the space orthogonal to the bias axis. </span></span><br><span class="line"><span class="string">    This function ensures that gender neutral words are zero in the gender subspace.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        word -- string indicating the word to debias</span></span><br><span class="line"><span class="string">        g -- numpy-array of shape (50,), corresponding to the bias axis (such as gender)</span></span><br><span class="line"><span class="string">        word_to_vec_map -- dictionary mapping words to their corresponding vectors.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        e_debiased -- neutralized word vector representation of the input "word"</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Select word vector representation of "word". Use word_to_vec_map. (≈ 1 line)</span></span><br><span class="line">    e = word_to_vec_map[word]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute e_biascomponent using the formula give above. (≈ 1 line)</span></span><br><span class="line">    </span><br><span class="line">    e_biascomponent = np.dot(e,g) / np.square(np.sqrt(np.sum(g**<span class="number">2</span>))) * g</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#e_biascomponent = np.dot(e, g) / np.square(np.linalg.norm(g)) * g</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Neutralize e by substracting e_biascomponent from it </span></span><br><span class="line">    <span class="comment"># e_debiased should be equal to its orthogonal projection. (≈ 1 line)</span></span><br><span class="line">    e_debiased = e - e_biascomponent</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> e_debiased</span><br></pre></td></tr></table></figure><h4 id="3-2-Equalization-algorithm-for-gender-specific-words"><a href="#3-2-Equalization-algorithm-for-gender-specific-words" class="headerlink" title="3.2- Equalization algorithm for gender-specific words"></a>3.2- Equalization algorithm for gender-specific words</h4><p>Next, lets see how debiasing can also be applied to word pairs such as “actress” and “actor.” Equalization is applied to pairs of words that you might want to have differ only through the gender property. As a concrete example, suppose that “actress” is closer to “babysit” than “actor.” By applying neutralizing to “babysit” we can reduce the gender-stereotype associated with babysitting. But this still does not guarantee that “actor” and “actress” are equidistant from “babysit.” The equalization algorithm takes care of this.</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-681c53121600f01a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>The derivation of the linear algebra to do this is a bit more complex. (See Bolukbasi et al., 2016 for details.) But the key equations are:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ac96125437c58c03.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">equalize</span><span class="params">(pair, bias_axis, word_to_vec_map)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Debias gender specific words by following the equalize method described in the figure above.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    pair -- pair of strings of gender specific words to debias, e.g. ("actress", "actor") </span></span><br><span class="line"><span class="string">    bias_axis -- numpy-array of shape (50,), vector corresponding to the bias axis, e.g. gender</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping words to their corresponding vectors</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    e_1 -- word vector corresponding to the first word</span></span><br><span class="line"><span class="string">    e_2 -- word vector corresponding to the second word</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Step 1: Select word vector representation of "word". Use word_to_vec_map. (≈ 2 lines)</span></span><br><span class="line">    w1, w2 = pair</span><br><span class="line">    e_w1, e_w2 = word_to_vec_map[w1], word_to_vec_map[w2]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: Compute the mean of e_w1 and e_w2 (≈ 1 line)</span></span><br><span class="line">    mu = (e_w1 + e_w2) / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 3: Compute the projections of mu over the bias axis and the orthogonal axis (≈ 2 lines)</span></span><br><span class="line">    mu_B = np.dot(mu, bias_axis) / np.sum(bias_axis**<span class="number">2</span>) * bias_axis</span><br><span class="line">    mu_orth = mu - mu_B</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 4: Set e1_orth and e2_orth to be equal to mu_orth (≈2 lines)</span></span><br><span class="line">    e_w1B = np.dot(e_w1, bias_axis) / np.sum(bias_axis**<span class="number">2</span>) * bias_axis</span><br><span class="line">    e_w2B = np.dot(e_w2, bias_axis) / np.sum(bias_axis**<span class="number">2</span>) * bias_axis</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Step 5: Adjust the Bias part of u1 and u2 using the formulas given in the figure above (≈2 lines)</span></span><br><span class="line">    corrected_e_w1B = np.sqrt(np.abs(<span class="number">1</span>-np.sum(mu_orth**<span class="number">2</span>))) * ((e_w1B-mu_B)/np.linalg.norm(e_w1-mu_orth-mu_B))</span><br><span class="line">    corrected_e_w2B = np.sqrt(np.abs(<span class="number">1</span>-np.sum(mu_orth**<span class="number">2</span>))) * ((e_w2B-mu_B)/np.linalg.norm(e_w2-mu_orth-mu_B))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 6: Debias by equalizing u1 and u2 to the sum of their projections (≈2 lines)</span></span><br><span class="line">    e1 = corrected_e_w1B + mu_orth</span><br><span class="line">    e2 = corrected_e_w2B + mu_orth</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> e1, e2</span><br></pre></td></tr></table></figure><p>Please feel free to play with the input words in the cell above, to apply equalization to other pairs of words.</p><p>These debiasing algorithms are very helpful for reducing bias, but are not perfect and do not eliminate all traces of bias. For example, one weakness of this implementation was that the bias direction g was defined using only the pair of words woman and man. As discussed earlier, if g were defined by computing g1=e_woman−e_man; g2=em_other−e_father; g3=e_girl−e_boy; and so on and averaging over them, you would obtain a better estimate of the “gender” dimension in the 50 dimensional word embedding space. Feel free to play with such variants as well.</p><h2 id="Emojify"><a href="#Emojify" class="headerlink" title="Emojify!"></a>Emojify!</h2><h3 id="1-Baseline-model-Emojifier-V1"><a href="#1-Baseline-model-Emojifier-V1" class="headerlink" title="1- Baseline model: Emojifier-V1"></a>1- Baseline model: Emojifier-V1</h3><h4 id="1-1-Dataset-EMOJISET"><a href="#1-1-Dataset-EMOJISET" class="headerlink" title="1.1- Dataset EMOJISET"></a>1.1- Dataset EMOJISET</h4><p>Let’s start by building a simple baseline classifier. </p><p>You have a tiny dataset (X, Y) where:</p><ul><li>X contains 127 sentences (strings)</li><li>Y contains a integer label between 0 and 4 corresponding to an emoji for each sentence</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/8636110-5988832092fefe2d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="1-2-Overview-of-the-Emojifier-V1"><a href="#1-2-Overview-of-the-Emojifier-V1" class="headerlink" title="1.2- Overview of the Emojifier-V1"></a>1.2- Overview of the Emojifier-V1</h4><p>In this part, you are going to implement a baseline model called “Emojifier-v1”.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-477d8bda868724b8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>To get our labels into a format suitable for training a softmax classifier, lets convert  YY  from its current shape current shape (m,1) into a “one-hot representation” (m,5), where each row is a one-hot vector giving the label of one example, You can do so using this next code snipper. Here, Y_oh stands for “Y-one-hot” in the variable names Y_oh_train and Y_oh_test:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Y_oh_train = convert_to_one_hot(Y_train, C = <span class="number">5</span>)</span><br><span class="line">Y_oh_test = convert_to_one_hot(Y_test, C = <span class="number">5</span>)</span><br></pre></td></tr></table></figure><h4 id="1-3-Implementing-Emojifier-V1"><a href="#1-3-Implementing-Emojifier-V1" class="headerlink" title="1.3- Implementing Emojifier-V1"></a>1.3- Implementing Emojifier-V1</h4><p>As shown in Figure (2), the first step is to convert an input sentence into the word vector representation, which then get averaged together. Similar to the previous exercise, we will use pretrained 50-dimensional GloVe embeddings. Run the following cell to load the word_to_vec_map, which contains all the vector representations.<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">word_to_index, index_to_word, word_to_vec_map = read_glove_vecs(<span class="string">'data/glove.6B.50d.txt'</span>)</span><br></pre></td></tr></table></figure></p><p>对词向量取平均：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentence_to_avg</span><span class="params">(sentence, word_to_vec_map)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Converts a sentence (string) into a list of words (strings). Extracts the GloVe representation of each word</span></span><br><span class="line"><span class="string">    and averages its value into a single vector encoding the meaning of the sentence.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    sentence -- string, one training example from X</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    avg -- average vector encoding information about the sentence, numpy-array of shape (50,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Step 1: Split sentence into list of lower case words (≈ 1 line) 小写</span></span><br><span class="line">    words = list(sentence.lower().split())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize the average word vector, should have the same shape as your word vectors.</span></span><br><span class="line">    avg = np.zeros((<span class="number">50</span>,))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: average the word vectors. You can loop over the words in the list "words".</span></span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> words:</span><br><span class="line">        avg += word_to_vec_map[w]</span><br><span class="line">    avg = avg / len(words)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> avg</span><br></pre></td></tr></table></figure></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-74b3e105e731b29c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>模型：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, Y, word_to_vec_map, learning_rate = <span class="number">0.01</span>, num_iterations = <span class="number">400</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Model to train word vector representations in numpy.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, numpy array of sentences as strings, of shape (m, 1)</span></span><br><span class="line"><span class="string">    Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1)</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation</span></span><br><span class="line"><span class="string">    learning_rate -- learning_rate for the stochastic gradient descent algorithm</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    pred -- vector of predictions, numpy-array of shape (m, 1)</span></span><br><span class="line"><span class="string">    W -- weight matrix of the softmax layer, of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">    b -- bias of the softmax layer, of shape (n_y,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define number of training examples</span></span><br><span class="line">    m = Y.shape[<span class="number">0</span>]                          <span class="comment"># number of training examples</span></span><br><span class="line">    n_y = <span class="number">5</span>                                 <span class="comment"># number of classes  </span></span><br><span class="line">    n_h = <span class="number">50</span>                                <span class="comment"># dimensions of the GloVe vectors </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters using Xavier initialization</span></span><br><span class="line">    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)</span><br><span class="line">    b = np.zeros((n_y,))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Convert Y to Y_onehot with n_y classes</span></span><br><span class="line">    Y_oh = convert_to_one_hot(Y, C = n_y) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Optimization loop</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(num_iterations):                       <span class="comment"># Loop over the number of iterations</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                                <span class="comment"># Loop over the training examples</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">            <span class="comment"># Average the word vectors of the words from the i'th training example</span></span><br><span class="line">            avg = sentence_to_avg(X[i], word_to_vec_map)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Forward propagate the avg through the softmax layer</span></span><br><span class="line">            z = np.dot(W,avg)+b</span><br><span class="line">            a = softmax(z)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute cost using the j'th training label's one hot representation and "A" (the output of the softmax)</span></span><br><span class="line">            cost = -np.sum(Y_oh[i]*np.log(a))</span><br><span class="line">            <span class="comment">### END CODE HERE ###</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Compute gradients </span></span><br><span class="line">            dz = a - Y_oh[i]</span><br><span class="line">            dW = np.dot(dz.reshape(n_y,<span class="number">1</span>), avg.reshape(<span class="number">1</span>, n_h))</span><br><span class="line">            db = dz</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Update parameters with Stochastic Gradient Descent</span></span><br><span class="line">            W = W - learning_rate * dW</span><br><span class="line">            b = b - learning_rate * db</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch: "</span> + str(t) + <span class="string">" --- cost = "</span> + str(cost))</span><br><span class="line">            pred = predict(X, Y, W, b, word_to_vec_map)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pred, W, b</span><br></pre></td></tr></table></figure></p><h4 id="1-4-Examining-test-set-performance"><a href="#1-4-Examining-test-set-performance" class="headerlink" title="1.4- Examining test set performance"></a>1.4- Examining test set performance</h4><p><img src="https://upload-images.jianshu.io/upload_images/8636110-22d4157616527d4b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-820057ddf5210855.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Printing the confusion matrix can also help understand which classes are more difficult for your model. A confusion matrix shows how often an example whose label is one class (“actual” class) is mislabeled by the algorithm with a different class (“predicted” class).<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f62b12a091ddf0cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>What you should remember from this part:</p><ul><li>Even with a 127 training examples, you can get a reasonably good model for Emojifying. This is due to the generalization power word vectors gives you.</li><li>Emojify-V1 will perform poorly on sentences such as “This movie is not good and not enjoyable” because it doesn’t understand combinations of words—it just averages all the words’ embedding vectors together, without paying attention to the ordering of words. You will build a better algorithm in the next part.</li></ul><h3 id="2-Emojifier-V2-Using-LSTMs-in-Keras"><a href="#2-Emojifier-V2-Using-LSTMs-in-Keras" class="headerlink" title="2- Emojifier-V2: Using LSTMs in Keras:"></a>2- Emojifier-V2: Using LSTMs in Keras:</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Input, Dropout, LSTM, Activation</span><br><span class="line"><span class="keyword">from</span> keras.layers.embeddings <span class="keyword">import</span> Embedding</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> sequence</span><br><span class="line"><span class="keyword">from</span> keras.initializers <span class="keyword">import</span> glorot_uniform</span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h4 id="2-1-Overview-of-the-model"><a href="#2-1-Overview-of-the-model" class="headerlink" title="2.1- Overview of the model"></a>2.1- Overview of the model</h4><p><img src="https://upload-images.jianshu.io/upload_images/8636110-9259a2dd421de22a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="2-2-Keras-and-mini-batching"><a href="#2-2-Keras-and-mini-batching" class="headerlink" title="2.2- Keras and mini-batching"></a>2.2- Keras and mini-batching</h4><p>In this exercise, we want to train Keras using mini-batches. However, most deep learning frameworks require that all sequences in the same mini-batch have the same length. This is what allows vectorization to work: If you had a 3-word sentence and a 4-word sentence, then the computations needed for them are different (one takes 3 steps of an LSTM, one takes 4 steps) so it’s just not possible to do them both at the same time.</p><p>The common solution to this is to use <strong>padding</strong>. Specifically, set a maximum sequence length, and pad all sequences to the same length. For example, of the maximum sequence length is 20, we could pad every sentence with “0”s so that each input sentence is of length 20. Thus, a sentence “i love you” would be represented as (e_i,e_love,e_you,0⃗ ,0⃗ ,…,0⃗ ). In this example, any sentences longer than 20 words would have to be truncated. One simple way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</p><h4 id="2-3-The-Embedding-layer"><a href="#2-3-The-Embedding-layer" class="headerlink" title="2.3- The Embedding layer"></a>2.3- The Embedding layer</h4><p>In Keras, the embedding matrix is represented as a “layer”, and maps positive integers (indices corresponding to words) into dense vectors of fixed size (the embedding vectors). It can be trained or initialized with a pretrained embedding. In this part, you will learn how to create an Embedding() layer in Keras, initialize it with the GloVe 50-dimensional vectors loaded earlier in the notebook. Because our training set is quite small, we will not update the word embeddings but will instead leave their values fixed. But in the code below, we’ll show you how Keras allows you to either train or leave fixed this layer.</p><p>The Embedding() layer takes an integer matrix of size (batch size, max input length) as input. This corresponds to sentences converted into lists of indices (integers), as shown in the figure below.</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-dcaf6e24664d4d8d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>The largest integer (i.e. word index) in the input should be no larger than the vocabulary size. The layer outputs an array of shape (batch size, max input length, dimension of word vectors).</p><p>The first step is to convert all your training sentences into lists of indices, and then zero-pad all these lists so that their length is the length of the longest sentence.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentences_to_indices</span><span class="params">(X, word_to_index, max_len)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.</span></span><br><span class="line"><span class="string">    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- array of sentences (strings), of shape (m, 1)</span></span><br><span class="line"><span class="string">    word_to_index -- a dictionary containing the each word mapped to its index</span></span><br><span class="line"><span class="string">    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">0</span>]                                   <span class="comment"># number of training examples</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Initialize X_indices as a numpy matrix of zeros and the correct shape</span></span><br><span class="line">    X_indices = np.zeros((m,max_len))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                               <span class="comment"># loop over training examples</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert the ith training sentence in lower case and split is into words. You should get a list of words.</span></span><br><span class="line">        sentence_words = list(X[i].lower().split())</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initialize j to 0</span></span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Loop over the words of sentence_words</span></span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> sentence_words:</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Set the (i,j)th entry of X_indices to the index of the correct word.</span></span><br><span class="line">            X_indices[i, j] = word_to_index[w]</span><br><span class="line">            <span class="comment"># Increment j to j + 1</span></span><br><span class="line">            j = j + <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X_indices</span><br></pre></td></tr></table></figure><p>Build the Embedding() layer in Keras, using pre-trained word vectors.<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pretrained_embedding_layer</span><span class="params">(word_to_vec_map,word_to_index)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.</span></span><br><span class="line"><span class="string">    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    embedding_layer -- pretrained layer Keras instance</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    vocab_len = len(word_to_index)+<span class="number">1</span></span><br><span class="line">    emb_dim = word_to_vec_map[<span class="string">"cucumber"</span>].shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)</span></span><br><span class="line">    emb_matrix = np.zeros((vocab_len, emb_dim)) <span class="comment">#所有单词的嵌入矩阵</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set each row "index" of the embedding matrix to be the word vector representation of the "index"th word of the vocabulary</span></span><br><span class="line">    <span class="keyword">for</span> word, index <span class="keyword">in</span> word_to_index.items():</span><br><span class="line">        emb_matrix[index, :] = word_to_vec_map[word]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define Keras embedding layer with the correct output/input sizes, make it trainable.</span></span><br><span class="line">    <span class="comment"># Use Embedding(...). Make sure to set trainable=False.</span></span><br><span class="line">    embedding_layer = Embedding(input_dim=vocab_len,output_dim=emb_dim,trainable=<span class="keyword">False</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the "None".</span></span><br><span class="line">    embedding_layer.build((<span class="keyword">None</span>,))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.</span></span><br><span class="line">    embedding_layer.set_weights([emb_matrix])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> embedding_layer</span><br></pre></td></tr></table></figure></p><h3 id="2-3-Building-the-Emojifier-V2"><a href="#2-3-Building-the-Emojifier-V2" class="headerlink" title="2.3- Building the Emojifier-V2"></a>2.3- Building the Emojifier-V2</h3><p><img src="https://upload-images.jianshu.io/upload_images/8636110-aee79f1c430ef952.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>注意LSTM的参数设值，比如return_sequences。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Emojify_V2</span><span class="params">(input_shape,word_to_index,word_to_index)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Function creating the Emojify-v2 model's graph.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    input_shape -- shape of the input, usually (max_len,)</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation</span></span><br><span class="line"><span class="string">    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a model instance in Keras</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).</span></span><br><span class="line">    sentence_indices = Input(input_shape, dtype=<span class="string">'int32'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create the embedding layer pretrained with GloVe Vectors (≈1 line)</span></span><br><span class="line">    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Propagate sentence_indices through your embedding layer, you get back the embeddings</span></span><br><span class="line">    embeddings = embedding_layer(sentence_indices) <span class="comment">#注意这里！</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Propagate the embeddings through an LSTM layer with 128-dimensional hidden state</span></span><br><span class="line">    <span class="comment"># Be careful, the returned output should be a batch of sequences.</span></span><br><span class="line">    X = LSTM(units=<span class="number">128</span>,return_sequences=<span class="keyword">True</span>)(embeddings) <span class="comment"># 注意参数设置，要一个个认真检查</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add dropout with a probability of 0.5</span></span><br><span class="line">    X = Dropout(rate=<span class="number">0.5</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Propagate X trough another LSTM layer with 128-dimensional hidden state</span></span><br><span class="line">    <span class="comment"># Be careful, the returned output should be a single hidden state, not a batch of sequences.</span></span><br><span class="line">    X = LSTM(units=<span class="number">128</span>,return_sequences=<span class="keyword">False</span>)(X)</span><br><span class="line">    <span class="comment"># Add dropout with a probability of 0.5</span></span><br><span class="line">    X = Dropout(rate=<span class="number">0.5</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.</span></span><br><span class="line">    X = Dense(units=<span class="number">5</span>)(X)</span><br><span class="line">    <span class="comment"># Add a softmax activation</span></span><br><span class="line">    X = Activation(<span class="string">'softmax'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create Model instance which converts sentence_indices into X.</span></span><br><span class="line">    model = Model(inputs=sentence_indices,outputs=X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-b4818af62aae12b2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>接下来的步骤：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)</span><br><span class="line">Y_train_oh = convert_to_one_hot(Y_train, C = <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">model.fit(X_train_indices, Y_train_oh, epochs = <span class="number">50</span>, batch_size = <span class="number">32</span>, shuffle=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)</span><br><span class="line">Y_test_oh = convert_to_one_hot(Y_test, C = <span class="number">5</span>)</span><br><span class="line">loss, acc = model.evaluate(X_test_indices, Y_test_oh)</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">"Test accuracy = "</span>, acc)</span><br></pre></td></tr></table></figure></p><p><strong>What we should remember</strong>:</p><ul><li>If you have an NLP task where the training set is small, using word embeddings can help your algorithm significantly. Word embeddings allow your model to work on words in the test set that may not even have appeared in your training set.</li><li>Training sequence models in Keras (and in most other deep learning frameworks) requires a few important details:<ul><li>To use mini-batches, the sequences need to be <strong>padded</strong> so that all the examples in a mini-batch have the same length.</li><li>An Embedding() layer can be initialized with pretrained values. These values can be either fixed or trained further on your dataset. If however your labeled dataset is small, it’s usually not worth trying to train a large pre-trained set of embeddings.</li><li>LSTM() has a flag called return_sequences to decide if you would like to return every hidden states or only the last one.</li><li>You can use Dropout() right after LSTM() to regularize your network.</li></ul></li></ul><p>总结：Keras使用还是不太熟练啊。另外，感觉自己缺乏编程能力，需要多练习，特别是针对深度学习框架。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;词嵌入-Word-Embedding&quot;&gt;&lt;a href=&quot;#词嵌入-Word-Embedding&quot; class=&quot;headerlink&quot; title=&quot;词嵌入: Word Embedding&quot;&gt;&lt;/a&gt;词嵌入: Word Embedding&lt;/h1&gt;&lt;p&gt;我们之前
      
    
    </summary>
    
      <category term="深度学习" scheme="https://github.com/DesmonDay/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://github.com/DesmonDay/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>第12周-循环神经网络(RNN)</title>
    <link href="https://github.com/DesmonDay/2019/04/23/deep-learningw12/"/>
    <id>https://github.com/DesmonDay/2019/04/23/deep-learningw12/</id>
    <published>2019-04-23T09:21:00.000Z</published>
    <updated>2019-05-01T15:47:53.333Z</updated>
    
    <content type="html"><![CDATA[<h1 id="为什么用序列模型：Why-sequence-models"><a href="#为什么用序列模型：Why-sequence-models" class="headerlink" title="为什么用序列模型：Why sequence models?"></a>为什么用序列模型：Why sequence models?</h1><p>首先，我们看几个序列数据的例子:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d932f29b3f3fbe30.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>所有的这些问题都可以作为使用标签数据(X,Y)作为训练集的监督学习。不过从上图，我们也可以知道这里的序列数据是非常不同的。有些输入输出都是序列，但长度不同；有的只有输入或输出才是序列，等等。</p><h1 id="数学符号：Notation"><a href="#数学符号：Notation" class="headerlink" title="数学符号：Notation"></a>数学符号：Notation</h1><h2 id="Motivation-Example"><a href="#Motivation-Example" class="headerlink" title="Motivation Example"></a>Motivation Example</h2><p>假设我们想要建立一个能够自动识别句中人名位置的序列模型。所以这就是一个命名实体识别问题，常用于搜索引擎。比如，索引过去24小时内所有新闻报道提及的人名。命名实体识别系统可以用来查找不同类型的文本中的人名、公司名、时间、地点、国家名、货币名等等。</p><p>假设我们的输入输出设置如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-37849034ae083b14.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-df6c867d4977f8d3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>其中，y用一个向量对应句子中的每个单词，如果该单词为人名，那么为1，否则为0。从技术层面角度，这并不是最好的输出形式，还有其他更加复杂的输出形式。它不仅能够表明输入词是否是人名的一部分，还能够告诉你这个人名从句子哪里开始和结束。</p><p>我们将用x<1>,x<2>,…,x<9>来索引句子中单词的位置，用x&lt;\t&gt;表示序列的中间位置。这里的t意味着它们是时序序列，输出同样的用y<1>,y<2>,…y<9>来索引位置。另外，我们用T_y表示句子的长度。为了表示训练样本i的序列中第t个单词(元素)，我们用X(i)&lt;\t&gt;来表示，再用Tx(i)表示第i个训练样本的输入序列长度，这种表示方法对输出序列也成立。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-12426e1eb0b70a16.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></9></2></1></9></2></1></p><h2 id="Representing-words"><a href="#Representing-words" class="headerlink" title="Representing words"></a>Representing words</h2><p>接下来探讨怎样表示一个句子里单个的词。首先，我们可能会做一张词汇表，将我们要表示的词按字典顺序放入。比如，这里有个词汇量为10000的词汇表，这对自然处理语言应用来说是非常小规模的。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-53233244b3a9e6c2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>接下来，我们可以用<strong>one-hot</strong>表示每个单词，这个每个单词用one-hot向量来表示：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-8fe77d39fca4e914.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们的目标是，用这样的表示方式表示X，用序列模型在X和目标输出Y之间学习建立一个映射，我们会把它当做监督学习的问题来做。</p><p>另一个需要注意的问题是，如果我们遇到了一个不在单词表中的单词，我们就创建一个新的标记<code>&quot;&lt;UNK&gt;&quot;</code>，来表示这个单词不在词汇表中。</p><h1 id="循环神经网络模型"><a href="#循环神经网络模型" class="headerlink" title="循环神经网络模型"></a>循环神经网络模型</h1><h2 id="Why-not-a-standard-network"><a href="#Why-not-a-standard-network" class="headerlink" title="Why not a standard network?"></a>Why not a standard network?</h2><p>A standard network:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-2260330da984017d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Problems:</p><ul><li>Inputs, outputs can be different lengths in different examples.</li><li>Doesn’t share features learned across different positions of text.类似卷积神经网络中所学到的，我们希望将部分图片里学到的内容快速推广到图片的其他部分，而我们希望对序列数据也有相似的效果。</li></ul><p>另外，用一个更好的representation能够让我们减少模型中的参数数量。</p><h2 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h2><p>RNN的基本结构如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-aa877c8fdbe61164.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>首先顺序读取序列中的第一个单词，并且尝试预测一个输出值；再读取第二个单词的时候，我们不仅仅利用这个单词进行预测，而且会用到前一个隐藏层的输出作为当前层的输入来预测。就这样按照顺序进行下去。另外，我们通常为设置一个伪激活值a<0>作为RNN的最初始输入，通常为0向量。另外在本例中，Tx=Ty，如果不等，则需要对网络结构进行调整。</0></p><p>循环神经网络是从左向右扫描数据，同时每个时间步的参数也是共享的。我们用W_ax来表示从x<1>到隐藏层的连接的一系列参数，并且每个时间步所使用的都是相同的参数W_ax。而激活值，也就是水平联系，是由参数W_aa决定的，同时每一个时间步都使用相同的参数W_aa。同样的，输出都由参数W_ya决定。（对这里的参数名称做一个解释，比如W_ax，表示这个参数会乘以X来得到a，类似这样感知的理解就好，看图就能够明白了）<br><img src="https://upload-images.jianshu.io/upload_images/8636110-65dafc9207622144.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>注意：最右侧循环图可以理解为左边图的简化版本，但难以理解。</1></p><p>在这种RNN中，当我们预测y<3>时，不仅要使用x<3>的信息，还要使用来自x<1>和x<2>的信息。但这个RNN的一个缺点是它只使用了这个序列中<strong>之前的信息</strong>来做出预测。即当预测y<3>时，它没有用到如x<4>、x<5>、x<6>等等的信息。这就造成了一个问题：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f8a89d0b56d93c18.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>为了判断Teddy是否是人名的一部分，只使用前面的两个单词来预测是远远不够的，即之给定前面的三个单词，不可能确切知道Teddy是否是人名的一部分。</6></5></4></3></2></1></3></3></p><p>因此这个特定结构神经网络的缺点是，它在某一时刻的预测仅使用了从序列中之前时刻的输入信息，并没有使用序列之后地信息。这个问题我们会在Bidirectional RNN(双向RNN)中得到解答。</p><h2 id="Forward-Propagation"><a href="#Forward-Propagation" class="headerlink" title="Forward Propagation"></a>Forward Propagation</h2><p>我们将网络结构更清晰的表示如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-243c146d3b45b0b8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>用等式表示为：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-3628eebbc758f041.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们注意到，式子可以在最后很有规律地表示出其结果。另外，激活函数可以是不同的，比如在RNN中，g1往往选择为tanh，也可以为relu，而g2需要根据我们y的输出值来确定（如果是二分类问题，则选择为sigmoid，如本例的命名实体识别问题；如果是多分类，可以选择softmax等等）。</p><h2 id="Simplified-RNN-notation"><a href="#Simplified-RNN-notation" class="headerlink" title="Simplified RNN notation"></a>Simplified RNN notation</h2><p>在上一小节，我们得到了以下公式：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-dbc63f1e76c12e00.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们将a&lt;\t&gt;的式子简写作：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-a0fe2e3720d0fd9e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>其中，具体的解释如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b37e5421a68fa78a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>将y&lt;\t&gt;写作：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ff02aec2c8ab0538.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>此时，W_y和b_y的下标只有y，这表示会输出什么类型的量，所以W_y是计算y类型的量的权重矩阵；而上面的W_a和b_a表示这些参数是用来计算a类型输出的。</p><h1 id="Back-propagation-through-time"><a href="#Back-propagation-through-time" class="headerlink" title="Back propagation through time"></a>Back propagation through time</h1><p>我们已经学习了循环神经网络的基础结构，在本节，我们将了解反向传播是怎样在循环神经网络中运行的。</p><p>我们已知的前向传播过程：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ea9b7b9a8f14b011.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>接下来，为了计算反向传播，我们还需要一个损失函数。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-cd02087594a1250d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>由于类似二元分类，因此我们选取的损失函数为交叉熵损失函数。</p><p>将损失函数和反向传播表示到网络中：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-123e68b7a83222d7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>其中最为重要的递归操作为从右往左的梯度计算。</p><h1 id="不同类型的RNN"><a href="#不同类型的RNN" class="headerlink" title="不同类型的RNN"></a>不同类型的RNN</h1><p>本文参考Andrej Karpathy的博客: The Unreasonable Effectiveness of Recurrent Neural Networks（对应翻译版：<a href="https://blog.csdn.net/menc15/article/details/78775010" target="_blank" rel="noopener">https://blog.csdn.net/menc15/article/details/78775010</a> ）</p><p>在上一节，我们介绍的RNN结构里Tx和Ty是相等的。但是在实际应用中，Tx和Ty不一定相等。本节会进行介绍。</p><p>常见序列数据：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b447ca3ca31d8374.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="Examples-of-RNN-architectures"><a href="#Examples-of-RNN-architectures" class="headerlink" title="Examples of RNN architectures"></a>Examples of RNN architectures</h2><p>many-to-many(输入输出长度相同), many-to-one(例如情感分类), one-to-one(普通神经网络):<br><img src="https://upload-images.jianshu.io/upload_images/8636110-8bcd5703f21d5141.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>one-to-many(音乐生成), many-to-many(输入输出长度不同，如翻译，用的是encoder-decoder模型):<br><img src="https://upload-images.jianshu.io/upload_images/8636110-453e6eac0e9de534.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>另外还有一个attention模型，之后会讲解。</p><h2 id="Summary-of-RNN"><a href="#Summary-of-RNN" class="headerlink" title="Summary of RNN"></a>Summary of RNN</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-62a3311d3bc8e1bc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="语言模型和序列生成"><a href="#语言模型和序列生成" class="headerlink" title="语言模型和序列生成"></a>语言模型和序列生成</h1><p>一个语言模型能够计算出句子的可能性，如语言识别中，举一个简单的例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d5bcb1c69363f9dc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>因此，语言模型会计算出某个特定句子出现的概率是多少。这个模型是两种系统的基本组成部分，即语音识别系统和机器翻译系统，它能正确输出最接近的句子。而语言模型做的基本工作就是输入一个句子，准确地说是一个文本序列，然后会估计该序列中各个单词出现的可能性。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f3547621968f6bec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="Language-modeling-with-an-RNN"><a href="#Language-modeling-with-an-RNN" class="headerlink" title="Language modeling with an RNN"></a>Language modeling with an RNN</h2><p>首先我们需要一个训练集，这个训练集可能是一个大型的英文语料库，也可能是其他我们需要的语言的语料库。</p><ol><li>第一件事需要进行tokenize，分解文本流为词，或将其转化为序列（比如用one-hot向量表示每一个词）。</li><li>另一件可能要做的事是定义句子的结尾，一般的做法是增加一个额外的标记，叫做<code>&quot;&lt;EOS&gt;&quot;</code>。这样能够让我们清楚一个句子什么时候结束。因此<code>EOS</code>标记可以添加到训练集中每一个句子的结尾。（注意句号可以去掉，或者当做一个单词也计入词典）</li><li>另一件是如果我们的训练集里有一些词并不在词典里（这个词典可能是10,000个常见的英文单词），那么将不在里面的单词用<code>&quot;&lt;UNK&gt;&quot;</code>取代，即用<code>&quot;&lt;UNK&gt;&quot;</code>代替未知词。我们只针对<code>&quot;&lt;UNK&gt;&quot;</code>来建立概率模型。</li><li>下一步，我们要构建一个RNN来构建这些序列的概率模型。</li></ol><h2 id="RNN-model"><a href="#RNN-model" class="headerlink" title="RNN model"></a>RNN model</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-a55c72fd35631fbe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-9266f437645d63ea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>由图上的RNN模型可知，我们首先输入x<1>和a<0>均为零向量，然后开始尝试输出第一个词y_hat<1>。随后，我们将y_hat<1>和a<1>作为计算a<2>的输入，接着通过softmax函数计算出第二个预测词y_hat<2>（这时候，我们所求的是使P(?|Cats)最大的单词，这样依次计算下去…</2></2></1></1></1></0></1></p><p>我们使用的是Softmax函数作为输出层的激活函数，因此选用的loss function为交叉熵损失函数。其中，y_i&lt;\t&gt;为真实的输出，而y_hat_i&lt;\t&gt;则为预测的单词输出，然后再将所有时刻的loss相加，得到最后总的loss function。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ae929432c67ac294.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>假设现在有一个新句子，为了简单起见，假设其含有是三个单词。那么第一层Softmax会计算P(y<1>)，第二层Softmax计算P(y<2>|y<1>)，第三层Softmax计算P(y<3>|y<1>,y<2>)。从而整个句子的概率为：<br>    <img src="https://upload-images.jianshu.io/upload_images/8636110-c191e422a19f7b6c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></2></1></3></1></2></1></p><h1 id="对新序列采样：sampling-novel-sequences"><a href="#对新序列采样：sampling-novel-sequences" class="headerlink" title="对新序列采样：sampling novel sequences"></a>对新序列采样：sampling novel sequences</h1><p>在训练完一个序列模型之后，要想了解这个模型学到了什么，一种非正式的方法就是进行一次新序列采样。</p><h2 id="Sampling-a-sequence-from-a-trained-RNN"><a href="#Sampling-a-sequence-from-a-trained-RNN" class="headerlink" title="Sampling a sequence from a trained RNN"></a>Sampling a sequence from a trained RNN</h2><p>我们记得一个序列模型模拟了任意特定单词序列的概率，而我们要做的是对这个概率分布进行采样，来生成一个新的单词序列。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-54e8c57ca8ba962a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>已知我们训练时所用的RNN模型如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e7571e716d844f83.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>然而为了采样，我们会做一些不同的事情。第一步要做的是对我们想要模型生成的第一个词进行采样，于是我们输入x<1>=0,a<0>=0，因此现在我们的第一个时间步得到的是所有可能的输出，即经过Softmax层后得到的概率。然后根据这个Softmax的分布进行随机采样，使用numpy命令如np.random.choice来对第一个词进行采样；接下来我们进入第二个时间步，把刚刚采样的y<1>传递到下一个位置作为输入，接着Softmax层会预测第二个词；这样依次进行…</1></0></1></p><p>什么时候一个句子算结束呢？一个方法是如果代表句子结尾的标识<code>&quot;&lt;EOS&gt;&quot;</code>在词典中，那么我们可以一直进行采样直到<strong>得到<code>&quot;&lt;EOS&gt;&quot;</code></strong>，这代表我们已经抵达结尾，可以停止随机采样；另一个方法是词典中没有这个标识，那么我们可以决定从20个或100个或其他词中进行采样，直到达到所<strong>设定的时间步</strong>。</p><p>这种方法可能会出现预测出<code>&quot;&lt;UNK&gt;&quot;</code>的情况。如果我们想要避免这种情况，那么可以在<code>&quot;&lt;UNK&gt;&quot;</code>出现时就继续在剩下的词中进行重采样，直到得到一个不是<code>&quot;&lt;UNK&gt;&quot;</code>的单词；当然，如果我们不介意有未知标识的产生，也可以不理会。</p><p>以上就是我们从RNN语言模型中生成一个随机选择的句子。</p><h2 id="Character-level-language-model"><a href="#Character-level-language-model" class="headerlink" title="Character-level language model"></a>Character-level language model</h2><p>在之前我们所建立的都是基于词汇的RNN模型，也就是说，字典中的词都是英语单词。根据我们的实际应用，我们还可以构建一个基于字符的RNN模型。这时，我们的字典中不再是单词，而是常见字符。</p><p>此时输入输出都是单个字符，而不再是单独的词汇。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-78b59a7d11dc5caa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>基于字符的语言模型的优点是我们不必担心会出现未知的标识，比如基于字符的语言模型会把’Mao’视为概率非0的序列，而基于词汇的语言模型，如果’Mao’不在字典中，那么我们只能把它当做未知标识。</p><p>然后基于字符的语言模型的一个明显缺点是我们最后会得到太长的序列。所以基于字符的语言模型在捕捉句子中的依赖关系，也就是句子较前部分如何影响较后部分，不如基于词汇的语言模型，并且它的计算成本也会很大。</p><p>因此在自然语言领域中，大多数应用是使用基于词汇的语言模型。在随着计算能力的提高，在一些特殊情况下，人们也会开始使用基于字符的模型，但这也需要更昂贵的计算成本。</p><p>下面展示一些采样后的结果：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-1884db2f163dac1e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="Vanishing-gradients-with-RNNs"><a href="#Vanishing-gradients-with-RNNs" class="headerlink" title="Vanishing gradients with RNNs"></a>Vanishing gradients with RNNs</h1><p>在前面的学习我们已经了解了RNN是如何工作的，并且知道如何将RNN应用到具体问题上，比如命名实体识别(name entity recognition)、语言模型等等。然后基本的RNN模型有一个很大的问题，也就是<strong>梯度消失</strong>的问题。</p><p>下面解释梯度消失。首先给出两个句子，这两个句子有着长期的依赖，也就是很前面的单词对句子很后面的单词有影响。(cat对应was，cats对应were)</p><ul><li>The cat, which already ate…, was full.</li><li>The cats, which already ate…, were full.<br>但是目前我们见到的<strong>基本RNN模型不擅长捕获这种长期依赖效应</strong>。在之前的讨论，我们知道在训练很深的网络时，我们讨论了梯度消失的问题。如果深度很深，那么从输出y得到的梯度将很难传播回去，很难影响到前面层的权重。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-265dd247b2b9312b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></li></ul><p>而RNN也有着同样的问题，所以其反向传播也比较困难。因为同样的梯度消失的问题，后面层的输出误差很难影响前面层的计算。这就意味着，实际上能难让一个神经网络能够意识到它要记住看到的是单数名词还是复数名词，然后在序列后面生成依赖单复数形式。也就是说，基本的RNN模型里，一个单词只能被其前面的几个单词所影响。这就是RNN的一个缺点。</p><p>如果出现了<strong>梯度爆炸</strong>的情况，一般很容易发现，因为梯度可能会出现如NaN(数值溢出之类的现象。一个解决方法就是进行<strong>梯度修剪</strong>(gradient clipping)，也就是观察我们的梯度向量，如果它大于某个阈值，那么我们缩放梯度向量，保证它不会太大。</p><p>但梯度消失是更难解决的。</p><p>另外复习:</p><ul><li>ReLu激活函数的主要贡献是：<ul><li>解决了梯度消失、爆炸的问题</li><li>计算方便、计算速度快</li><li>加速了网络的训练</li></ul></li><li>BatchNorm: 本质上是解决反向传播过程中的梯度问题。反向传播式子中有w的存在，所以w的大小影响了梯度的消失和爆炸。BatchNorm就是通过对每一层的输出规范化为均值和方差一致的方法，消除了w带来的放大缩小的影响，进而解决梯度消失和爆炸的问题。</li><li>残差结构：解决了梯度消失过快的问题，因此即使构建的网络很深层也不必担心。</li></ul><p>参考博客：<a href="https://blog.csdn.net/qq_25737169/article/details/78847691" target="_blank" rel="noopener">https://blog.csdn.net/qq_25737169/article/details/78847691</a></p><h1 id="GRU-Gated-Recurrent-Unit"><a href="#GRU-Gated-Recurrent-Unit" class="headerlink" title="GRU: Gated Recurrent Unit"></a>GRU: Gated Recurrent Unit</h1><p>参考论文：</p><ol><li>On the properties of neural machine translation: Encoder-decoder approaches, 2014.</li><li>Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling, 2014.</li></ol><p>我们已经学习了基础的RNN模型的运行机制，在本节，我们会学习门控循环单元GPU。它改变了RNN的隐藏层，使其更好地捕捉深层连接，并改善了梯度消失问题。</p><h2 id="RNN-unit"><a href="#RNN-unit" class="headerlink" title="RNN unit"></a>RNN unit</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-33ab1bcbf06b3cb3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="GRU-simplified"><a href="#GRU-simplified" class="headerlink" title="GRU(simplified)"></a>GRU(simplified)</h2><p>GRU设置了一个新的变量C，为记忆细胞(memory cell)，记忆细胞的作用是提供了记忆的能力，比如一只猫是单数还是复数。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-bfb4d77c79ad247d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>在GRU中，c&lt;\t&gt;等于激活值a&lt;\t&gt;。（因为在LSTM中，两个值并不相等，因此为避免混淆，这里采用两种不同的名称表示，即便他们的值是一样的。）</p><p>接下来写出GPU(简化版本)中的关键公式：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-51fc765c5b4d2f63.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>其中，第一个式子表示记忆细胞的候选值，我们用tanh计算出来。</p><p>而GPU的关键思想就是第二个式子，也就是门（用”Γ”代替，另外小写u表示”update”)。为了思考GRU的工作机制，我们思考门的原理。可以看到它是用一个sigmoid函数来计算，对大多数输入来说，sigmoid值在大多数情况下都接近0或者1。</p><p>接下来看第三个式子。如果当Γ_u=1时，说明我们将c&lt;\t&gt;设置为计算的候选值c_tilda&lt;\t&gt;。而如果Γ_u=0，说明我们将c&lt;\t&gt;设置为c&lt;\t-1&gt;，这也说明，Γ_u越小，则前一时刻的状态信息带入越多。而针对我们之前所说的猫吃饱没的例子，我们应该将cat和was之间的所有Γ_u都设置为0，即不进行更新，只设置为旧的值。这样到了was的时候，神经网络还能记得cats的信息。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c121835dcc48c5b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们将GRU图示化，以便更好理解：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-abe4597abfc4bf63.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>而GRU的优点是，通过门来决定，当我们从左到右扫描一个句子的时候，这个时机是应该更新还是不更新记忆细胞。由于sigmoid值很可能取0，因此我们可以通过门的设定来维持记忆细胞的值，即c&lt;\t&gt;=c&lt;\t-1&gt;，而因为Γ_u很接近0，从而不会有梯度消失的问题了。因为Γ_u很接近0，所以c&lt;\t&gt;几乎等于c&lt;\t-1&gt;，而且c&lt;\t&gt;的值也很好地被维持了，即使经过很多很多的时间步。所以这就是缓解梯度消失问题的关键，因此允许神经网络运行在非常庞大的依赖词上，比如cat和was单词，即便它们被很多单词分隔，也仍然能够运行。</p><p>下面补充一点实现的细节。在上面有关记忆细胞候选值的式子里，c&lt;\t&gt;可以是一个向量。如果我们有一个100维的隐藏的激活值，那么c&lt;\t&gt;也是100维的，c_tilda&lt;\t&gt;也是相同的维度，从而Γ_u也是相同的维度。因此说明c&lt;\t&gt;的式子实际上是元素对应的乘积。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b9d520811feecc7e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>而这100维的记忆细胞里，我们只更新需要更新的比特。</p><p>当然，在实际应用中Γ_u不会真的等于0或1，有时候它是0到1的中间值，但这对于直观思考是很方便地。而元素对应的成绩做的是告诉GRU单元哪个记忆细胞的向量维度在每个时间步要做更新，因此我们可以选择保持一些比特不变，而去更新其他的比特。</p><h2 id="Full-GRU"><a href="#Full-GRU" class="headerlink" title="Full GRU"></a>Full GRU</h2><p>针对完整版本的GRU，我们添加了一个变量Γ_r，这个Γ_r告诉我们计算出的c&lt;\t-1&gt;与c&lt;\t&gt;的候选值的相关性有多大。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b4800a75c12cf672.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>为什么需要Γ_r呢？这是因为多年来研究者们试验过很多不同可能的方法来设计这些单元，尝试让神经网络有更深层的连接，尝试产生更大范围的影响，并且解决梯度消失的问题。</p><h2 id="参考其他博客的解释"><a href="#参考其他博客的解释" class="headerlink" title="参考其他博客的解释"></a>参考其他博客的解释</h2><p>为了让自己更加深刻的理解GRU，接下来还参考了以下博文：<a href="https://www.cnblogs.com/jiangxinyang/p/9376021.html" target="_blank" rel="noopener">https://www.cnblogs.com/jiangxinyang/p/9376021.html</a></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-7d87ba39f2e6cd12.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>图中的zt和rt分别表示更新门和重置门。更新门用于控制前一时刻的状态信息被带入到当前状态中的程度，更新门的值越大说明前一时刻的状态信息带入越多。重置门控制前一状态有多少信息被写入到当前的候选集 h_tilda_t 上，重置门越小，前一状态的信息被写入的越少。</p><ol><li><p>GRU的前向传播：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7be385f8b161c998.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p></li><li><p>GRU的训练过程：从前向传播过程中的公式可以看出要学习的参数有Wr、Wz、Wh、Wo。其中前三个参数都是拼接的（因为后先的向量也是拼接的），所以在训练的过程中需要将他们分割出来：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-59fce5507d3623da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>输出层的输入和输出层的输出分别为（注意到输出的激活函数是sigmoid）：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-1fc6bc02fac1c769.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>损失函数（这里用的是平方损失函数）：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-131ec80bce316acc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>接下来求偏导（我没有进行推导）：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f4be19597aa9375b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>其中各中间参数为：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f926c844c1a38601.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>在算出了对各参数的偏导之后，就可以更新参数，依次迭代直到损失收敛。</p></li></ol><h1 id="长短时记忆网络：LSTM"><a href="#长短时记忆网络：LSTM" class="headerlink" title="长短时记忆网络：LSTM"></a>长短时记忆网络：LSTM</h1><p>论文标题：Long shot-term memory, 1997.</p><p>在上一节，我们学习了GRU，它能够让我们在序列中学习非常深的连接(long range connection)。LSTM单元也能做到这一点。</p><p>下面是GRU和LSTM的公式对照。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-25df2c3b3314c65e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>我们注意到，LSTM不再有a&lt;\t&gt;=c&lt;\t&gt;。和GRU一样，有一个更新门，而LSTM的新特性是不只有一个更新门Γ_u控制，而新增加了一个遗忘门Γ_f。然后增加了一个输出门来用于输出a&lt;\t&gt;。</p><p>接下来将LSTM表示为图形：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-557568c6fe786add.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>接下来我们将多个LSTM单元按顺序连接起来，可以发现一件有趣的事：我们会发现在连接的图中有一条线（即所画的红线），这条线显示了只要我们正确地设置了遗忘门和更新门，LSTM是能够很容易地将c<0>的值一直传递到右边，比如c<3>=c<0>。这就是为什么LSTM和GRU非常擅长于长时间记忆某个值，即使经过很长的时间步，依旧能够保持住存在于记忆细胞中的某个值。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-fddaead310ac055e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></0></3></0></p><p>然而，这里介绍的和一般使用的版本不太一样。最常用的版本(LSTM的变体：Recurrent Nets that Time and Count, 2000)是我们的门值不仅取决于a&lt;\t-1&gt;和x&lt;\t&gt;，有时候人们也会偷窥一下c&lt;\t-1&gt;的值，这叫做peephole connection(窥视孔连接)。它的意思是，门值不仅取决于a&lt;\t-1&gt;和x&lt;\t&gt;，也取决于上一个记忆细胞的值。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d09e6eeb0e3993da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>那么我们应该什么时候用GRU，什么时候用LSTM？这里没有统一的准则。尽管我们先介绍了GRU，但在深度学习历史上，LSTM是更早出现的，而GRU是近几年才发明出来的，它可能源于在更加复杂的LSTM模型中做出的简化。研究者们在很多不同问题上尝试了这两种模型，看看在不同问题不同算法中哪个模型更好。</p><p>GRU的优点是更加简单，所以更容易创建一个更大的网络，而且它只有两个门，在计算性上也运行得更快，可以扩大模型的规模；而LSTM更加强大和灵活，因为它有三个门而不是两个。如果我们必须选择一个来使用，那么LSTM应该会作为默认选择来尝试。</p><h2 id="LSTM反向传播推导-未验证"><a href="#LSTM反向传播推导-未验证" class="headerlink" title="LSTM反向传播推导(未验证)"></a>LSTM反向传播推导(未验证)</h2><p>门求偏导：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c4272320bb39158d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-b3d7de5f4728a3a6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>参数求偏导：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-bd0c79d5a5cf075d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-0ad7eaf74f28955d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>最后，计算隐藏状态、记忆状态和输入的偏导数：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-5bc620d0876deeb7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="双向RNN：Bidirectional-RNN-BRNN"><a href="#双向RNN：Bidirectional-RNN-BRNN" class="headerlink" title="双向RNN：Bidirectional RNN(BRNN)"></a>双向RNN：Bidirectional RNN(BRNN)</h1><p>到目前为止我们已经学习了RNN模型的关键构件，但还有两个方法能够让我们构建更好的模型。其中一个方法就是双向RNN模型，这个模型能够让我们在序列的某点处不仅可以获取之前的信息，还可以获取之后的信息；第二个方法是深层RNN，我们将在下一节讲解。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-94465d05da44841f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>为了了解RNN的动机，我们先看一下之前在命名实体识别中见过多次的神经网络。这个网络的问题是，在判断Teddy是否是人名的一部分时，光看句子的前面部分是不够的。为了判断y<3>是0还是1，除了前三个单词，我们还需要更多的信息。所以这是一个单向的RNN。无论里面的单元是GRU、LSTM，这个结论总是成立的，即这是一个单向的RNN神经网络。</3></p><p>下面我们来解释BRNN的工作原理。为了简单，我们用四个输入或者说只有四个单词的句子，这样输入只有x<1>到x<4>。然后我们有四个前向的循环单元，这四个循环单元都有一个当前输入x&lt;\t&gt;，进而得到预测的y_hat&lt;\t&gt;。这是最初的结构：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-207909d44ae745ae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></4></1></p><p>接下来，我们要增加一个反向循环层。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-920edd50c3d7362e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这样，网络就形成了一个无环图(acyclic graph)。给定一个输入序列x<1>到x<4>，这个序列首先计算前向的a<1>，然后计算前向的a<2>，接着a<3>、a<4>。而反向序列从计算a’<4>开始反向进行，计算反向的a’<3>，直到计算完成。再把所有的激活值都计算完成后，就可以得到预测结果了。（这里我们用a<1>表示前向激活值，a’<1>表示后向激活值）</1></1></3></4></4></3></2></1></4></1></p><p>而预测结果的计算公式如下式子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ae40aa5b003d9629.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>通过这种方式，我们的预测不仅利用了过去的信息，也利用了未来的信息。而神经网络里的基础单元不仅仅可以是标准RNN单元，也可以是GRU单元或者LSTM单元。事实上，很多的NLP问题，有LSTM单元的双向RNN模型是用得最多的。所以如果有NLP问题，并且文本句子都是完整的，首先需要标定这些句子，那么一个有LSTM单元的双向RNN模型，既有前向过程也有反向过程，将是不错的首选。</p><p>BPNN不仅能用于基本的RNN结构，也能用于GRU和LSTM。通过这些改变，我们可以用一个RNN或GRU或LSTM构建的模型，并且能够预测任意位置，即使在句子的中间位置，因为模型能够考虑整个句子的信息。而这个<strong>双向RNN网络模型的缺点是我们需要完整的数据序列，才能够预测任意位置</strong>。比如我们需要构建一个语音识别系统，那么BRNN需要考虑整个语音表达。也就是说，我们需要等某个人说完，获取整个语音表达才能处理这段语音，并进一步做语音识别。因此，对于实际的语音识别的应用，通常会有更加复杂的模块，而不仅仅用我们见过的标准BRNN模型。当然，对于很多自然语言处理的应用，如果我们总是可以获取整个句子，那么标准的BRNN模型实际上会很有效。</p><h1 id="深层循环神经网络-Deep-RNNs"><a href="#深层循环神经网络-Deep-RNNs" class="headerlink" title="深层循环神经网络: Deep RNNs"></a>深层循环神经网络: Deep RNNs</h1><p>目前我们学到的不同RNN的版本，每个都可以独当一面。但是要学习非常复杂的函数时，通常我们会把RNN的多个层堆叠在一起构建更深的模型。</p><p>我们知道标准的神经网络结构如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-3639d261d113dffc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>下面给出一个具有三个隐层的RNN网络：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-fd40e2ab6147b4a7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们下面来讲解一下a[2]<3>应该怎么计算。激活值a[2]<3>有两个输入，一个从左边传来， 一个从下边传来，那么计算的时候，用激活函数作用于权重矩阵如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7b6fcea20ef8253a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>另外，Wa^[2]和ba^[2]也用于同一层的计算，同理第一层有参数Wa^[1]和ba^[1]参与计算。对于RNN来说，能够有三层已经很多了，一般我们设置为1或者2。（注意，time step为深度，这个为层数num_layers）</3></3></p><p>当然，有一种结构也会比较常见。即我们将输出去掉，替换成一些深层，而这些层并没有水平连接，只是一个深层网络，然后用来预测y<1>，同样也加深层网络来预测y<2>，等等。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-afb408d18ac35097.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></2></1></p><p>而这些单元既可以是标准的RNN单元，也可以是GRU或者LSTM单元。通常，深层的RNN训练需要很多计算资源，需要很长的时间。</p><h1 id="本周作业"><a href="#本周作业" class="headerlink" title="本周作业"></a>本周作业</h1><p>第一个作业里有关基本RNN和LSTM的反向传播的部分，没有写完，就先放着了，以后有机会再重新填坑。</p><h2 id="Building-your-Recurrent-Neural-Network-Step-by-Step"><a href="#Building-your-Recurrent-Neural-Network-Step-by-Step" class="headerlink" title="Building your Recurrent Neural Network - Step by Step"></a>Building your Recurrent Neural Network - Step by Step</h2><p>Recurrent Neural Networks (RNN) are very effective for Natural Language Processing and other sequence tasks because they have “memory”. They can read inputs x⟨t⟩ (such as words) one at a time, and remember some information/context through the hidden layer activations that get passed from one time-step to the next. This allows a uni-directional RNN to take information from the past to process later inputs. A bidirection RNN can take context from both the past and the future.</p><p>一些符号标记：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7b7f7cdfe6c3e3e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="1-Forward-propagation-for-the-basic-Recurrent-Neural-Network"><a href="#1-Forward-propagation-for-the-basic-Recurrent-Neural-Network" class="headerlink" title="1-Forward propagation for the basic Recurrent Neural Network"></a>1-Forward propagation for the basic Recurrent Neural Network</h3><p>Later this week, you will generate music using an RNN. The basic RNN that you will implement has the structure below. In this example, Tx = Ty.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-560316ebb79f05e5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Here’s how you can implement an RNN:</p><p><strong>Steps</strong>:</p><ol><li>Implement the calculations needed for one time-step of the RNN.</li><li>Implement a loop over $T_x$ time-steps in order to process all the inputs, one at a time. </li></ol><h4 id="1-1-RNN-cell"><a href="#1-1-RNN-cell" class="headerlink" title="1.1- RNN cell"></a>1.1- RNN cell</h4><p>A Recurrent neural network can be seen as the repetition of a single cell. You are first going to implement the computations for a single time-step. The following figure describes the operations for a single time-step of an RNN cell.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-969c576b3584b1ae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-cc03d74c1fe1ab89.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>一个RNN cell处理一个词，而下面代码将其向量化，所以一个RNN cell可以同时处理m个样本中的词。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell_forward</span><span class="params">(xt, a_prev, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a single forward step of the RNN-cell as described in Figure (2)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    xt -- your input data at timestep "t", numpy array of shape (n_x, m). n_x为词向量的长度，m为样本个数</span></span><br><span class="line"><span class="string">    a_prev -- Hidden state at timestep "t-1", numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        ba --  Bias, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a_next -- next hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    yt_pred -- prediction at timestep "t", numpy array of shape (n_y, m)</span></span><br><span class="line"><span class="string">    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Retrieve parameters from "parameters"</span></span><br><span class="line">    Wax = parameters[<span class="string">"Wax"</span>]</span><br><span class="line">    Waa = parameters[<span class="string">"Waa"</span>]</span><br><span class="line">    Wya = parameters[<span class="string">"Wya"</span>]</span><br><span class="line">    ba = parameters[<span class="string">"ba"</span>]</span><br><span class="line">    by = parameters[<span class="string">"by"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute next activation state using the formula given above</span></span><br><span class="line">    a_next = np.tanh(np.dot(Waa,a_prev)+np.dot(Wax,xt)+ba)</span><br><span class="line">    <span class="comment"># compute output of the current cell using the formula given above</span></span><br><span class="line">    yt_pred = softmax(np.dot(Wya,a_next)+by)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># store values you need for backward propagation in cache</span></span><br><span class="line">    cache = (a_next, a_prev, xt, parameters)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a_next, yt_pred, cache</span><br></pre></td></tr></table></figure></p><p>测试：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">xt = np.random.randn(<span class="number">3</span>,<span class="number">10</span>)</span><br><span class="line">a_prev = np.random.randn(<span class="number">5</span>,<span class="number">10</span>)</span><br><span class="line">Waa = np.random.randn(<span class="number">5</span>,<span class="number">5</span>)</span><br><span class="line">Wax = np.random.randn(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">Wya = np.random.randn(<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line">ba = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">by = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">parameters = &#123;<span class="string">"Waa"</span>:Waa, <span class="string">"Wax"</span>: Wax, <span class="string">"Wya"</span>: Wya, <span class="string">"ba"</span>: ba, <span class="string">"by"</span>: by&#125;</span><br><span class="line"></span><br><span class="line">a_next, yt_pred, cache = rnn_cell_forward(xt, a_prev, parameters)</span><br></pre></td></tr></table></figure></p><h4 id="1-2-RNN-forward-pass"><a href="#1-2-RNN-forward-pass" class="headerlink" title="1.2- RNN forward pass"></a>1.2- RNN forward pass</h4><p>You can see an RNN as the repetition of the cell you’ve just built. If your input sequence or data is carried over 10 time steps, then you will copy the RNN cell 10 times. Each cell takes as input the hidden state from the previous cell(a&lt;\t-1&gt;) and the current time-step’s input data(x&lt;\t&gt;). It outputs a hidden state(a&lt;\t&gt;) and a prediciton(y&lt;\t&gt;) for this time-step.</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-ec9cdb2b5803abb7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>Exercise</strong>: Code the forward propagation of the RNN described in Figure(3).</p><p><strong>Instructions</strong>:</p><ol><li>Create a vector of zeros (a) that will store all the hidden states computed by the RNN.</li><li>Initialize the “next” hidden state as a0 (initial hidden state).</li><li>Start looping over each time step, your incremental index is t:<ul><li>Update the “next” hidden state and the cache by running rnn_step_forward</li><li>Store the “next” hidden state in a(t^th position)</li><li>Store the prediction in y</li><li>Add the cache to the list of caches</li></ul></li><li>Return a, y and caches<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span><span class="params">(x, a0, parameters)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Implement the forward propagation of the recurrent neural network described in Figure(3).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        x -- Input data for every time-step, of shape(n_x, m, T_x).</span></span><br><span class="line"><span class="string">        a0 -- Initial hidden state, of shape (n_a, m).</span></span><br><span class="line"><span class="string">        parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                ba --  Bias numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- tuple of values needed for the backward pass, contains (list of caches, x)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialze "caches" which will contain the list of all caches</span></span><br><span class="line">    caches = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of x and Wy</span></span><br><span class="line">    n_x, m, T_x = x.shape</span><br><span class="line">    n_y, n_a = parameters[<span class="string">"Wya"</span>].shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># initialize "a" and "y" with zeros</span></span><br><span class="line">    a = np.zeros((n_a, m, T_x))</span><br><span class="line">    y_pred = np.zeros((n_y, m, T_x))</span><br><span class="line"></span><br><span class="line">    a_next = a0</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T_x):</span><br><span class="line">        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t],a_next, parameters)</span><br><span class="line">        a[:,:,t] = a_next</span><br><span class="line">        y_pred[:,:,t] = yt_pred</span><br><span class="line">        caches.append(cache)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    caches = (caches, x)</span><br><span class="line">    <span class="keyword">return</span> a, y_pred, caches</span><br></pre></td></tr></table></figure></li></ol><p>You’ve successfully built the forward propagation of a recurrent neural network from scratch. This will work well enough for some applications, but it suffers from vanishing gradient problems. So it works best when each output y&lt;\t&gt; can be estimated using mainly “local” context(meaning information from inputs x&lt;\t’&gt;) where t’ is not too far from t.</p><h3 id="2-backward-propagation-for-the-basic-Recurrent-Neural-Network"><a href="#2-backward-propagation-for-the-basic-Recurrent-Neural-Network" class="headerlink" title="2-backward propagation for the basic Recurrent Neural Network"></a>2-backward propagation for the basic Recurrent Neural Network</h3><h4 id="2-1-Basic-RNN-backward-pass"><a href="#2-1-Basic-RNN-backward-pass" class="headerlink" title="2.1- Basic RNN backward pass"></a>2.1- Basic RNN backward pass</h4><p>We start by computing the backward pass for the basic RNN-cell.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-72f75c51fdc14580.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>注意，上图中右边求导数的公式只计算了相对于a_next的梯度，完整的公式需要乘上a_next相对于cost function J的梯度。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell_backward</span><span class="params">(da_next, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the backward pass for the RNN-cell (single time-step).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    da_next -- Gradient of loss with respect to next hidden state</span></span><br><span class="line"><span class="string">    cache -- python dictionary containing useful values (output of rnn_step_forward())</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dx -- Gradients of input data, of shape (n_x, m)</span></span><br><span class="line"><span class="string">                        da_prev -- Gradients of previous hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        dba -- Gradients of bias vector, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Retrieve values from cache</span></span><br><span class="line">    (a_next, a_prev, xt, parameters) = cache</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve values from parameters</span></span><br><span class="line">    Wax = parameters[<span class="string">"Wax"</span>]</span><br><span class="line">    Waa = parameters[<span class="string">"Waa"</span>]</span><br><span class="line">    Wya = parameters[<span class="string">"Wya"</span>]</span><br><span class="line">    ba = parameters[<span class="string">"ba"</span>]</span><br><span class="line">    by = parameters[<span class="string">"by"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the gradient of tanh with respect to a_next</span></span><br><span class="line">    dtanh = (<span class="number">1</span>-a_next*a_next)*da_next</span><br><span class="line">    dxt = np.dot(Wax.T, dtanh)</span><br><span class="line">    dWax = np.dot(dtanh, xt.T)</span><br><span class="line">    da_prev = np.dot(Waa.T, dtanh)</span><br><span class="line">    dWaa = np.dot(dtanh, a_prev.T)</span><br><span class="line">    dba = np.sum(dtanh, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    gradients = &#123;<span class="string">"dxt"</span>: dxt, <span class="string">"da_prev"</span>: da_prev, <span class="string">"dWax"</span>: dWax, <span class="string">"dWaa"</span>: dWaa, <span class="string">"dba"</span>: dba&#125;</span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><p><strong>Backward pass through the RNN</strong></p><p>好难，先占坑。</p><p>Computing the gradients of the cost with respect to a&lt;\t&gt; at every time-step t is useful because it is what helps the gradient backpropagate to the previous RNN-cell To do so, you need to iterate through all the time steps starting at the end, and at each step, you increment the overall dba, dWaa, dWax and you store dx.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span><span class="params">(da, caches)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward pass for a RNN over an entire sequence of input data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    da -- Upstream gradients of all hidden states, of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- tuple containing information from the forward pass (rnn_forward)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dx -- Gradient w.r.t. the input data, numpy-array of shape (n_x, m, T_x)</span></span><br><span class="line"><span class="string">                        da0 -- Gradient w.r.t the initial hidden state, numpy-array of shape (n_a, m)</span></span><br><span class="line"><span class="string">                        dWax -- Gradient w.r.t the input's weight matrix, numpy-array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        dWaa -- Gradient w.r.t the hidden state's weight matrix, numpy-arrayof shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        dba -- Gradient w.r.t the bias, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Retrieve values from the first cache (t=1) of caches (≈2 lines)</span></span><br><span class="line">   </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from da's and x1's shapes (≈2 lines)</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize the gradients with the right sizes (≈6 lines)</span></span><br></pre></td></tr></table></figure><h3 id="3-Forward-propagation-of-Long-Short-Term-Memory-LSTM-network"><a href="#3-Forward-propagation-of-Long-Short-Term-Memory-LSTM-network" class="headerlink" title="3- Forward propagation of Long Short-Term Memory(LSTM) network"></a>3- Forward propagation of Long Short-Term Memory(LSTM) network</h3><p>This following figure shows the operations of an LSTM-cell.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-faa16612756bff66.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Similar to the RNN example above, you will start by implementing the LSTM cell for a single time-step. Then you can iteratively call it from inside a for-loop to have it process an input with T_x time-steps.</p><p><strong>About the gates</strong></p><ul><li><p>Forget gate: For the sake of this illustration, lets assume we are reading words in a piece of text, and want use an LSTM to keep track of grammatical structures, such as whether the subject is singular or plural. If the subject changes from a singular word to a plural word, we need to find a way to get rid of our previously stored memory value of the singular/plural state. In an LSTM, the forget gate lets us do this:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d5af764c972ec343.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>Here,  Wf are weights that govern the forget gate’s behavior. We concatenate [a⟨t−1⟩,x⟨t⟩] and multiply by Wf. The equation above results in a vector  Γ⟨t⟩f with values between 0 and 1. This forget gate vector will be multiplied element-wise by the previous cell state c⟨t−1⟩. So if one of the values of  Γ⟨t⟩fΓf⟨t⟩  is 0 (or close to 0) then it means that the LSTM should remove that piece of information (e.g. the singular subject) in the corresponding component of  c⟨t−1⟩. If one of the values is 1, then it will keep the information.</p></li><li><p>Update gate: Once we forget that the subject being discussed is singular, we need to find a way to update it to reflect that the new subject is now plural. Here is the formula for the update gate:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-58b5ff406da60509.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p></li><li><p>Updating the cell: To update the new subject we need to create a new vector of numbers that we can add to our previous cell state. The equation we use is:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-5eeabc165db7889c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p></li><li><p>Output gate: To decide which outputs we will use, we will use the following two formulas:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-3ae9380d4a619dbc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p></li></ul><h4 id="3-1-LSTM-cell"><a href="#3-1-LSTM-cell" class="headerlink" title="3.1- LSTM cell"></a>3.1- LSTM cell</h4><p>Exercise: Implement the LSTM cell described in the Figure (3).</p><p>Instructions:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-002ba438ea40c34b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>直接按照公式计算即可，注意concat（矩阵拼接）的写法。可以用np.vstack((a,b))，表示竖直拼接。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_cell_forward</span><span class="params">(xt, a_prev, c_prev, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement a single forward step of the LSTM-cell as described in Figure (4)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    xt -- your input data at timestep "t", numpy array of shape (n_x, m).</span></span><br><span class="line"><span class="string">    a_prev -- Hidden state at timestep "t-1", numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">    c_prev -- Memory state at timestep "t-1", numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wi -- Weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bi -- Bias of the save gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wc -- Weight matrix of the first "tanh", numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bc --  Bias of the first "tanh", numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wo -- Weight matrix of the focus gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bo --  Bias of the focus gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">                        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a_next -- next hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    c_next -- next memory state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    yt_pred -- prediction at timestep "t", numpy array of shape (n_y, m)</span></span><br><span class="line"><span class="string">    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilda),</span></span><br><span class="line"><span class="string">          c stands for the memory value</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve parameters from "parameters"</span></span><br><span class="line">    Wf = parameters[<span class="string">"Wf"</span>]</span><br><span class="line">    bf = parameters[<span class="string">"bf"</span>]</span><br><span class="line">    Wi = parameters[<span class="string">"Wi"</span>]</span><br><span class="line">    bi = parameters[<span class="string">"bi"</span>]</span><br><span class="line">    Wc = parameters[<span class="string">"Wc"</span>]</span><br><span class="line">    bc = parameters[<span class="string">"bc"</span>]</span><br><span class="line">    Wo = parameters[<span class="string">"Wo"</span>]</span><br><span class="line">    bo = parameters[<span class="string">"bo"</span>]</span><br><span class="line">    Wy = parameters[<span class="string">"Wy"</span>]</span><br><span class="line">    by = parameters[<span class="string">"by"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of xt and Wy</span></span><br><span class="line">    n_x, m = xt.shape</span><br><span class="line">    n_y, n_a = Wy.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Concatenate a_prev and xt (≈3 lines)</span></span><br><span class="line">    <span class="comment"># concat = np.zeros([n_x+n_a, m])</span></span><br><span class="line">    <span class="comment"># concat[:n_a, :] = a_prev</span></span><br><span class="line">    <span class="comment"># concat[n_a:, :] = xt</span></span><br><span class="line">    concat = np.vstack((a_prev, xt))</span><br><span class="line">    <span class="comment"># Compute values for ft, it, cct, c_next, ot, a_next using the formulas given figure (4) (≈6 lines)</span></span><br><span class="line">    ft = sigmoid(np.dot(Wf, concat) + bf)</span><br><span class="line">    it = sigmoid(np.dot(Wi, concat) + bi)</span><br><span class="line">    cct = np.tanh(np.dot(Wc, concat) + bc)</span><br><span class="line">    c_next = ft*c_prev + it*cct</span><br><span class="line">    ot = sigmoid(np.dot(Wo, concat) + bo)</span><br><span class="line">    a_next = ot * np.tanh(c_next)</span><br><span class="line">    <span class="comment"># Compute prediction of the LSTM cell (≈1 line)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a_next, c_next, yt_pred, cache</span><br></pre></td></tr></table></figure><h4 id="3-2-Forward-pass-for-LSTM"><a href="#3-2-Forward-pass-for-LSTM" class="headerlink" title="3.2- Forward pass for LSTM"></a>3.2- Forward pass for LSTM</h4><p>Now that you have implemented one step of an LSTM, you can now iterate this over this using a for-loop to process a sequence of Tx inputs.</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-53b7e99a279cda10.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_forward</span><span class="params">(x, a0, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the forward propagation of the recurrent neural network using an LSTM-cell described in Figure (3).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- Input data for every time-step, of shape (n_x, m, T_x).</span></span><br><span class="line"><span class="string">    a0 -- Initial hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wi -- Weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bi -- Bias of the save gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wc -- Weight matrix of the first "tanh", numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bc -- Bias of the first "tanh", numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wo -- Weight matrix of the focus gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bo -- Bias of the focus gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">                        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Initialize "caches", which will track the list of all the caches</span></span><br><span class="line">    caches = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of xt and Wy (≈2 lines)</span></span><br><span class="line">    n_x, m, T_x = x.shape</span><br><span class="line">    n_y, n_a = parameters[<span class="string">"Wy"</span>].shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize "a", "c" and "y" with zeros (≈3 lines)</span></span><br><span class="line">    a = np.zeros((n_a, m, T_x))</span><br><span class="line">    c = np.zeros((n_a, m, T_x))</span><br><span class="line">    y = np.zeros((n_y, m, T_x))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize a_next and c_next (≈2 lines)</span></span><br><span class="line">    a_next = a0</span><br><span class="line">    c_next = np.zeros((n_a, m))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop over all time-steps</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T_x):</span><br><span class="line">        <span class="comment"># Update next hidden state, next memory state, compute the prediction, get the cache (≈1 line)</span></span><br><span class="line">        a_next, c_next, yt_pred, cache = lstm_cell_forward(x[:,:,t],a_next,c_next,parameters)</span><br><span class="line">        <span class="comment"># Save the value of the new "next" hidden state in a (≈1 line)</span></span><br><span class="line">        a[:,:,t] = a_next</span><br><span class="line">        <span class="comment"># Save the value of the prediction in y (≈1 line)</span></span><br><span class="line">        y[:,:,t] = yt_pred</span><br><span class="line">        <span class="comment"># Save the value of the next cell state (≈1 line)</span></span><br><span class="line">        c[:,:,t] = c_next</span><br><span class="line">        <span class="comment"># Append the cache into caches (≈1 line)</span></span><br><span class="line">        caches.append(cache)</span><br><span class="line">        </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    caches = (caches, x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a, y, c, caches</span><br></pre></td></tr></table></figure><h3 id="4-Backward-propagation-for-LSTM"><a href="#4-Backward-propagation-for-LSTM" class="headerlink" title="4- Backward propagation for LSTM"></a>4- Backward propagation for LSTM</h3><p>占坑。</p><h2 id="Character-level-language-model-Dinosaurus-land"><a href="#Character-level-language-model-Dinosaurus-land" class="headerlink" title="Character level language model - Dinosaurus land"></a>Character level language model - Dinosaurus land</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;为什么用序列模型：Why-sequence-models&quot;&gt;&lt;a href=&quot;#为什么用序列模型：Why-sequence-models&quot; class=&quot;headerlink&quot; title=&quot;为什么用序列模型：Why sequence models?&quot;&gt;&lt;/a&gt;为
      
    
    </summary>
    
      <category term="深度学习" scheme="https://github.com/DesmonDay/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://github.com/DesmonDay/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>占坑-目标检测</title>
    <link href="https://github.com/DesmonDay/2019/04/23/deep-learningwx/"/>
    <id>https://github.com/DesmonDay/2019/04/23/deep-learningwx/</id>
    <published>2019-04-23T08:54:42.000Z</published>
    <updated>2019-04-23T09:20:22.710Z</updated>
    
    <content type="html"><![CDATA[<p>由于我主要是要了解CV，而不是深入学习。因此为了节省时间，不会再对幻灯片内容做详细的解释。未看完，占坑。</p><h1 id="目标定位：Object-Localization"><a href="#目标定位：Object-Localization" class="headerlink" title="目标定位：Object Localization"></a>目标定位：Object Localization</h1><p><img src="https://upload-images.jianshu.io/upload_images/8636110-450af11aa403610a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>下面对分类定位做详细解释：如果我们单纯的对图片分类，判断是否有行人、车之类的，可以直接通过Softmax层来输出结果；但如果我们还想要定位，比如定位车辆，那么可以让神经网络多输出几个单元，输出一个边框界(bx,by,bh,bw:被检测对象的边框化表示)。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c80a4fba678ec142.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>下面讲解如何确定目标标签y。如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-fa12871a6a4d60a8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>其中，Pc表示图片中是否有我们要检测的对象，而bx/by/bh/bw指明对象位置，而C1/C2/C3告知我们对象的类型。举两个例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-be113d141fe62a9a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>可以看到，左图车存在，所以Pc=1；而右图不存在车，因此Pc=0，剩下的数字皆不必理会。</p><p>接下来定义训练神经网络的损失函数，我们用的是平方损失函数：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-1fff519549149653.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>我们对Pc=y1进行分类讨论。另外，这里用平方误差简化了描述过程，在实际应用中，我们可以对c1/c2/c3和softmax激活函数应用对数损失函数并输出其中一个元素值，通常做法是对边界框坐标应用平方差，对Pc应用逻辑回归函数，甚至采用平方预测误差函数也可以。</p><h1 id="特征点检测"><a href="#特征点检测" class="headerlink" title="特征点检测"></a>特征点检测</h1><h1 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h1><h1 id="卷积的滑动窗口实现"><a href="#卷积的滑动窗口实现" class="headerlink" title="卷积的滑动窗口实现"></a>卷积的滑动窗口实现</h1><h1 id="Bounding-Box预测"><a href="#Bounding-Box预测" class="headerlink" title="Bounding Box预测"></a>Bounding Box预测</h1><h1 id="交并比"><a href="#交并比" class="headerlink" title="交并比"></a>交并比</h1><h1 id="非极大值抑制"><a href="#非极大值抑制" class="headerlink" title="非极大值抑制"></a>非极大值抑制</h1><h1 id="Anchor-Boxes"><a href="#Anchor-Boxes" class="headerlink" title="Anchor Boxes"></a>Anchor Boxes</h1><h1 id="YOLO算法"><a href="#YOLO算法" class="headerlink" title="YOLO算法"></a>YOLO算法</h1><h1 id="RPN网络"><a href="#RPN网络" class="headerlink" title="RPN网络"></a>RPN网络</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;由于我主要是要了解CV，而不是深入学习。因此为了节省时间，不会再对幻灯片内容做详细的解释。未看完，占坑。&lt;/p&gt;
&lt;h1 id=&quot;目标定位：Object-Localization&quot;&gt;&lt;a href=&quot;#目标定位：Object-Localization&quot; class=&quot;hea
      
    
    </summary>
    
      <category term="深度学习" scheme="https://github.com/DesmonDay/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://github.com/DesmonDay/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>第11周-深度卷积神经网络</title>
    <link href="https://github.com/DesmonDay/2019/04/22/deep-learningw11/"/>
    <id>https://github.com/DesmonDay/2019/04/22/deep-learningw11/</id>
    <published>2019-04-22T11:56:53.000Z</published>
    <updated>2019-04-23T08:47:41.584Z</updated>
    
    <content type="html"><![CDATA[<h1 id="为什么要研究实例：Why-look-at-case-studies"><a href="#为什么要研究实例：Why-look-at-case-studies" class="headerlink" title="为什么要研究实例：Why look at case studies?"></a>为什么要研究实例：Why look at case studies?</h1><p>就像我们看别人的代码来学习编程一样，通过研究别人构建有效组件的实例也有利于我们的进步。实际上，在计算机视觉任务中表现良好的神经网络框架，往往也适用于其他任务。</p><p>本周框架：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-3687089c68bac73c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>尽管我的方向是NLP而不是CV，但我觉得学习这些知识也可能给我带来一些启发。</p><h1 id="经典网络：Classic-Network"><a href="#经典网络：Classic-Network" class="headerlink" title="经典网络：Classic Network"></a>经典网络：Classic Network</h1><h2 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h2><p>论文名称：1998, Gradient-based learning applied to document recognition</p><p>LeNet-5针对的是灰度图像，因此图片的大小为32x32x1。实际上，LeNet-5的结构与我们上一篇博客所讲的很像。由于这篇论文是在1998年写成的，当时人们更经常用平均池化，并且不使用padding。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-4e910b02d6352a29.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>随着网络层次的加深，可以发现信道数量一直在增加，而n_H和n_W不断减小。因此在现代的卷积神经网络中，我们会添加padding层。</p><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>论文名称：2012, ImageNet classification with deep convolutional neural network</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-79f60a2ca25b8384.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>另外，AlexNet有以下几个特点：</p><ol><li>与LeNet-5相似，但是网络很大</li><li>AlexNet使用的是ReLu激活函数，而LeNet-5当时用的是sigmoid和tanh</li><li>由于当时的计算能力不强，因此AlexNet是在多个GPU上运行的，即将多个层的运算放置到不同的GPU上</li><li>AlexNet还使用了Local Response Normalization(LRN)，但现在基本不使用了，因此不讲解。</li></ol><p>另外，AlexNet非常大，它具有着一共6000,000个参数。</p><h2 id="VGG-16"><a href="#VGG-16" class="headerlink" title="VGG-16"></a>VGG-16</h2><p>VGG的参数比较少，它是只需要专注于构建卷积层的简单网络。其最大的优点就是简化了网络结构。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-13a8a275d97047b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>可以看到，所有的卷积核参数、池化层参数都是相同的。注意到图中的”x2”,”x3”表示连续做了几次卷积操作。尽管网络结构看起来很深，并且它的参数达到了1.38亿（很大），但由于其理解起来比较简单，因此受到了很多人的青睐。另外，可以注意到图像的高和宽很有规律地减小(224-&gt;112-&gt;56-&gt;28-&gt;14-&gt;7)，而卷积的通道数也有规律地增长(64-&gt;128-&gt;512)，因此也受到了吴老师的称赞。</p><p>尽管现在也有另一个VGG-19，这个网络要更大。但是由于VGG-16的表现与其差不多，因此更多人使用的是VGG-16。</p><p>阅读论文推荐顺序：AlexNet-&gt;VGG-&gt;LeNet-5</p><h1 id="残差网络：Residual-Networks"><a href="#残差网络：Residual-Networks" class="headerlink" title="残差网络：Residual Networks"></a>残差网络：Residual Networks</h1><p>论文名称：2015，Deep residual networks for image recognition</p><p>ResNet是由残差块(residual block)构建的，下面介绍一下残差块。</p><p>在我们一般的神经网络计算中，假设我们从a[l]计算到a[l+2]，一般经过下面的几个阶段：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d9c1196cdbfa4e2c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>可以看到，在得到输入a[l]后，需要经过LINEAR-&gt;ReLU得到下一层输入a[l+1]，再继续同样的操作得到a[l+2]。而这一条主路径在残差网络中有所变化。</p><p>我们会将a[l]复制到神经网络的深层，在ReLu非线性激活前加上a[l]，我们把这条路称为”short cut/skip connection”。因此a[l]插入的时机是在LINEAR之后，ReLU之前。图示表示如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-1be201d72a334145.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>如图，右下角的新的a[l+2]的式子就代表了一个残差块。在实际操作中，a[l]可以跳过一层或者好几层，从而将信息传递到神经网络的更深层。ResNet的发明者发现使用残差块能够训练更深的神经网络。所以构建一个ResNet网络就是通过将很多这样的残差块堆积在一起，形成一个深层神经网络。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-1ed377ce8f44da79.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>如上图所示，该神经网络是由5个残差块连接在一起构成的残差网络。</p><p>如果我们使用标准优化算法训练一个普通的神经网络(plain network)，没有残差块，凭借经验，我们会发现随着网络深度的加深，训练错误会减小，然后再增大。而理论上，随着网络深度的加深，训练错误应该越小。因此对于普通神经网络来说，深度越深，用优化算法越难训练，因此训练错误越多。在使用ResNet，随着网络加深，训练错误一直在减少。<strong>这种方式有助于解决梯度消失和梯度爆炸问题，让我们在训练更深网络的同时，又能保持良好的性能。</strong></p><h2 id="为什么ResNet表现好"><a href="#为什么ResNet表现好" class="headerlink" title="为什么ResNet表现好"></a>为什么ResNet表现好</h2><p>下面给出一个例子来解释ResNet表现好的原因，至少可以说明，如何在构建更深层次的ResNet网络的同时，还不降低他们在训练集上的效率。通常来讲，网络在训练集上表现好，才能在hold-out交叉验证集上或dev/test set有好的表现。</p><p>在训练普通的神经网络时，我们发现网络越深，它在训练集上的表现越差，因此我们往往不使用太深的网络。但这一原则在训练ResNet的时候并不适用。下面看例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-63e9024edcaf03ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>如图，我们给Big NN添加了两层，并且增加了一个残差块。此时的输出a[l+2]为残差块的输出。我们注意到，如果我们使用L2正则化或者权重衰减，它会压缩W[l+2]的值。如果X[l+2]=0, b=0，那么这几项就没有了，因此g(a[l]) = a[l]。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-35fcbb0c593197eb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>因此这意味着，即使给神经网络增加了这两层，它的效率也并不逊色于更简单的神经网络，因为学习identity function对该网络来说并不难。不论是把残差块添加到神经网络的中间还是末端位置，都不会影响网络的表现。吴老师认为，残差网络表现好的原因是<strong>这些残差块能够很容易地学习恒等函数</strong>(相比之下，普通深层网络是难以学习的），我们能确定网络性能不会受到影响，甚至可以提高我们的网络性能。</p><p>除此之外，关于残差网络的另一个值得探讨的细节是<strong>假设z[l+2]和a[l]具有相同的维度</strong>。因此在ResNet中使用了很多的卷积，使得a[l]的维度等于z[l+2]的维度。</p><p>接下来看一下网络对比的例子。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-9578fd8c334562e7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>可以看到，ResNet的卷积核基本都是3x3相同的，保证了z[l+2]和a[l]的维度相同。</p><h1 id="Network-in-Network-and-1x1-convolutions"><a href="#Network-in-Network-and-1x1-convolutions" class="headerlink" title="Network in Network and 1x1 convolutions"></a>Network in Network and 1x1 convolutions</h1><p>论文名称：Network in Network, 2013</p><p>在架构内容设计(designing content architectures)方面，其中一个比较有帮助的想法是使用1x1卷积。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-6a549d4dbec3f148.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>我们看到，对于6x6x1平面而言，1x1卷积只是将原图像的每个数字乘以卷积核内的数字而已，因此没有起到作用。而对于一张x6x32的图片来说，使用1x1过滤器进行卷积效果更好。具体来说，1x1卷积核的作用是遍历这36个单元格，计算左图中32个数字和过滤器中32个数字的乘积，然后应用ReLU非线性函数，图例如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-aac996651ccbf765.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>即将数字对应相乘后再相加，再应用到ReLU上。</p><p>所以1x1卷积从根本上理解，这32个单元都应用了一个全连接神经网络。而全连接层的作用是输入32个数字和过滤器数量，标记为n_C[l+1]，在36个单元上重复此过程，因此输出结果为6x6x#filters。这个方法通常被称为1x1卷积，有时也称为Network in Network。</p><p>下面给一个Network in Network的应用。假设我们有一个28x28x192的输入层，我们可以使用池化层压缩它的高度和宽度，而如果信道数量很大，我们可以用1x1卷积来缩小信道数量的大小。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-fbc5aefdd70c01d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如果我们想保持信道数量不变，也是可行的：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ee93a77f7c64cfd1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>当然，想要增加信道数量也可以。</p><p>因此，通过1x1卷积的简单操作，我们可以压缩或者保持输入层中的信道数量，甚至是增加信道数量。在下一节，我们将讲述1x1卷积是如何帮助构建Inception网络的。</p><h1 id="谷歌Inception网络"><a href="#谷歌Inception网络" class="headerlink" title="谷歌Inception网络"></a>谷歌Inception网络</h1><p>论文名称：Going deeper with convolutions, 2014</p><p>构建卷积层时，我们要决定卷积核的大小究竟是1x3/3x3/5↓，或者要不要添加池化层。而Inception网络的作用是代替你来做决定，尽管这样做使得网络结构变得复杂，但表现却非常好。</p><h2 id="Inception核心模块"><a href="#Inception核心模块" class="headerlink" title="Inception核心模块"></a>Inception核心模块</h2><p>Inception网络的核心模块如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f7157e63d390b856.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>其基本思想是，Inception网络不需要人为决定使用哪个卷积核或是否需要池化，而是由网络自行确定这些参数，你可以给网络添加这些参数的所有可能值，然后把这些输出连接起来，让网络自己学习它需要什么样的参数、采用哪些卷积核组合。</p><h2 id="The-problem-of-computational-cost"><a href="#The-problem-of-computational-cost" class="headerlink" title="The problem of computational cost"></a>The problem of computational cost</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-e46e27fa822065a1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>乘法运算的总次数为每个输出值所需的乘法运算次数(5x5x192)乘以输出值个数(28x28x32)，结果等于1.2亿。即使在现代，用计算机来进行1.2亿次乘法运算，其成本也相当高。</p><h2 id="Using-1x1-convolution"><a href="#Using-1x1-convolution" class="headerlink" title="Using 1x1 convolution"></a>Using 1x1 convolution</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-85df96549d033170.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-82ee668d12ee5fb7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们要做的是把左边这个大的输入层压缩成较小的中间层(bottleneck layer)，再用卷积核恢复原来的输出大小。接下来看看计算成本。</p><p>首先，第一个卷积层的乘法次数为输出值个数(28x28x16)乘以每个输出值所需乘法运算次数(192)，相乘结果为240万；对于第二个卷积层的乘法次数为输出值个数(28x28x32)乘以每个输出值所需乘法运算次数(5x5x16)，相乘结果为1千万。因此总的计算成本为原来的十分之一，即1240万。所需的加法次数与乘法次数类似，因此只统计了乘法运算的次数。</p><p>总结：如果我们在构建神经网络层的时候，不想决定池化层是使用1x1、3x3、还是5x5的过滤器，那么inception模块可以让我们应用各种类型的过滤器，再将它们连接起来。事实证明，只要合理构建瓶颈层，我们既可以显著缩小表示层规模，又不会降低网络性能，从而节省了大量计算。</p><h2 id="完整结构"><a href="#完整结构" class="headerlink" title="完整结构"></a>完整结构</h2><p>在前面的几节，我们已经知道了Inception网络的基础模块。在本视频中，我们将学习如何将这些模块组合起来，构建我们的Inception网络。</p><p>一个Inception模块：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-933d7f4a64642dbd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Inception网络所做的就是将多个Inception模块组合起来：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-59c8869b6c28ee2c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>我们可以发现其中有很多Inception模块，另外网络中有一些额外的最大化池来改变维度中的高和宽。因此我们实际上就是用多个Inception模块在不同位置进行的组合。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-85a4a092cabb2646.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Inception网络还有一些额外的分支。而这些分支所做的就是通过隐藏层来做输出，即通过一些全连接层，然后使用一个softmax来预测输出结果的标签。它确保了即使是隐藏单元和中间层，它们也参与了特征计算，可以预测图片的分类。这个特点在Inception网络中起到一种调整的作用，也能防止网络发生过拟合。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-266e0c1258429a9f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>最后总结，如果我们理解了Inception模块，就能理解Inception网络，其实质就是将Inception模块一环接一环，最后组成网络。自从Inception模块诞生以来，经过研究者们的不断发展，衍生出许多新的版本。所以当我们在看一些比较新的Inception算法的论文时，发现人们使用这些新版本的算法效果也一样很好，比如Inception V2、V3以及V4，还有版本引入了跳跃连接(ResNet)的方法，也有特别好的效果。但所有的变体都建立在同一种基础的思想上，就是把许多Inception模块通过某种方式连接在一起。</p><p>接下来，我们会讲解如何真正使用这些算法来构建自己的计算机视觉系统。</p><h1 id="使用开源的实现方案-Using-open-source-implementations"><a href="#使用开源的实现方案-Using-open-source-implementations" class="headerlink" title="使用开源的实现方案: Using open-source implementations"></a>使用开源的实现方案: Using open-source implementations</h1><p>到目前我们已经学习了几个非常有效的神经网络和ConvNet架构。接下来会分享几条如何使用它们的实用性建议，首先从使用开放源码的实现开始。</p><p>事实证明很多神经网络复杂细致，因而难以复制，因为一些参数调整的细节问题，例如学习率衰减，会影响性能。幸运的是，很多深度学习者会将自己的成果开源，放在<strong>Github</strong>上。因此如果我们看到一篇研究论文想应用其成果，我们通常会在网络上寻找一个开源的实现，这比自己实现要好得多。</p><p>因此一个常用的工作流程是，选择一个喜欢的架构，接着寻找一个开源实现，从Github下载，再进行调整。</p><h1 id="迁移学习"><a href="#迁移学习" class="headerlink" title="迁移学习"></a>迁移学习</h1><p>如果你要做一个计算机视觉的应用，相比于从头训练权重，或者说从随机初始化权重开始，如果我们能下载别人已经训练好的网络结构的权重，我们通常能够进展比较快。用这个预训练模型转移到我们感兴趣的任务上，也就是用迁移学习把公共数据集的知识迁移到我们自己的问题上。</p><h2 id="数据集很小"><a href="#数据集很小" class="headerlink" title="数据集很小"></a>数据集很小</h2><p>举例子。假设我们要对猫图片分类，类别有Tiger、Misty和Neither。但我们没有那么多的猫图片，也就是我们的数据集很小。吴老师建议我们从网上下载一些神经网络开源的实现，不仅下载代码，同时也把权重下载下来。</p><p>比如对于ImageNet数据集，一共有1000个类别，因此在大多数网络结构最后都有一个Softmax分类器要预测1000种类别。这时候我们可以把最后一层去掉，而创建自己的Softmax单元。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ddb921702cff581f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>如图，我们通常将前面已经预训练好的权重保持不变(freeze)，而只训练Softmax层的权重。通过使用其他人预训练的权重，我们可能得到很好的性能，即使我们的数据集很小。幸运的是，很多深度学习框架都支持这种操作，我们可以设置<strong>trainableParameter=0</strong>或者<strong>freeze=1</strong>的参数，保证前面预训练好的权重不参与训练，即允许我们指定是否训练特顶层的权重。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-60d4ae8655494447.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>另一个加速训练的技巧是我们可以先计算出Softmax前一层的输出或激活值，将它们保存在硬盘中（因为前面部分不变），再运用Softmax函数进行预测。</p><h2 id="数据集更大"><a href="#数据集更大" class="headerlink" title="数据集更大"></a>数据集更大</h2><p>根据经验，如果我们有一个更大的标记好的数据集，在这种情况下，我们可以冻结更少的层，然后训练后面的层。即我们的规律是，如果我们的数据集越大，那么需要冻结的层数越少，能够训练的层数也越多。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-08325eec9599fb15.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>当然，如果我们的数据集很大，那么可以用开源的网络和它的权重作为初始化，然后训练整个网络（当然，输出层可能要根据自己的需要进行改变）。</p><h1 id="数据增强-Data-Augmentation"><a href="#数据增强-Data-Augmentation" class="headerlink" title="数据增强: Data Augmentation"></a>数据增强: Data Augmentation</h1><p>在实践中，更多的数据对大多数计算机视觉任务都有帮助。而不像其他领域，有时候得到充足的数据，但是效果却不怎么样。在现代，计算机视觉的主要问题就是没有办法得到充足的数据，因此这就意味着我们在训练计算机视觉模型时，数据增强可能会起到作用。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-621c4a01351e88a1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-5fa4d3bb2f7e35d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>（color shifting主要针对光线照明之类的情况）</p><h2 id="Implementing-distortion-during-training"><a href="#Implementing-distortion-during-training" class="headerlink" title="Implementing distortion during training"></a>Implementing distortion during training</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-14646dba1095e556.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="计算机视觉现状"><a href="#计算机视觉现状" class="headerlink" title="计算机视觉现状"></a>计算机视觉现状</h1><p><img src="https://upload-images.jianshu.io/upload_images/8636110-62695a5a31fe9f1d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>通常不用于实际生产，而只用于竞赛或者baseline测试：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-9bdf1d2dddfaa040.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-82542b2f348fda8a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="本周作业"><a href="#本周作业" class="headerlink" title="本周作业"></a>本周作业</h1><h2 id="Keras-Tutorial-the-Happy-House"><a href="#Keras-Tutorial-the-Happy-House" class="headerlink" title="Keras Tutorial - the Happy House"></a>Keras Tutorial - the Happy House</h2><p>Keras is more restrictive than the lower-level frameworks, so there are some very complex models that you can implement in TensorFlow but not (without more difficulty) in Keras. That being said, Keras will work fine for many common models.</p><p>导入包：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#import tensorflow as tf</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> image</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> layer_utils</span><br><span class="line"><span class="keyword">from</span> keras.utils.data_utils <span class="keyword">import</span> get_file</span><br><span class="line"><span class="keyword">from</span> keras.applications.imagenet_utils <span class="keyword">import</span> preprocess_input</span><br><span class="line"><span class="keyword">import</span> pydot</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> SVG</span><br><span class="line"><span class="keyword">from</span> keras.utils.vis_utils <span class="keyword">import</span> model_to_dot</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> plot_model</span><br><span class="line"><span class="keyword">from</span> kt_utils <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</span><br><span class="line">K.set_image_data_format(<span class="string">'channels_last'</span>)</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.pyplot <span class="keyword">import</span> imshow</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure></p><h3 id="1-The-Happy-House"><a href="#1-The-Happy-House" class="headerlink" title="1- The Happy House"></a>1- The Happy House</h3><p>For your next vacation, you decided to spend a week with five of your friends from school. It is a very convenient house with many things to do nearby. But the most important benefit is that everybody has commited to be happy when they are in the house. So anyone wanting to enter the house must prove their current state of happiness.</p><p>As a deep learning expert, to make sure the “Happy” rule is strictly applied, you are going to build an algorithm which that uses pictures from the front door camera to check if the person is happy or not. The door should open only if the person is happy.</p><p>You have gathered pictures of your friends and yourself, taken by the front-door camera. The dataset is labbeled.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b53bd750e48c3398.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Run the following code to normalize the dataset and learn about its shapes.<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize image vectors</span></span><br><span class="line">X_train = X_train_orig/<span class="number">255.</span></span><br><span class="line">X_test = X_test_orig/<span class="number">255.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Reshape</span></span><br><span class="line">Y_train = Y_train_orig.T</span><br><span class="line">Y_test = Y_test_orig.T</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"number of training examples = "</span> + str(X_train.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"number of test examples = "</span> + str(X_test.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X_train shape: "</span> + str(X_train.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Y_train shape: "</span> + str(Y_train.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X_test shape: "</span> + str(X_test.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Y_test shape: "</span> + str(Y_test.shape))</span><br></pre></td></tr></table></figure></p><p><strong>Details of the “Happy” dataset</strong>:</p><ul><li>Images are of shape (64,64,3)</li><li>Training: 600 pictures</li><li>Test: 150 pictures</li></ul><h3 id="2-Building-a-model-in-Keras"><a href="#2-Building-a-model-in-Keras" class="headerlink" title="2- Building a model in Keras"></a>2- Building a model in Keras</h3><p>Keras is very good for rapid prototyping. In just a short time you will be able to build a model that achieves outstanding results.</p><p>Here is an example of a model in Keras:<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(input_shape)</span>:</span></span><br><span class="line">    <span class="comment"># Define the input placeholder as a tensor with shape input_shape.</span></span><br><span class="line">    X_input = Input(input_shape)</span><br><span class="line">    <span class="comment"># Zero-Padding: pads the border of X_input with zeroes</span></span><br><span class="line">    X = ZeroPadding2D((<span class="number">3</span>,<span class="number">3</span>))(X_input)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># CONV -&gt; BN -&gt; RELU Block applied to X</span></span><br><span class="line">    X = Conv2D(<span class="number">32</span>,(<span class="number">7</span>,<span class="number">7</span>),strides=(<span class="number">1</span>,<span class="number">1</span>),name=<span class="string">'conv0'</span>)(X)</span><br><span class="line">    X = BatchNormalization(axis=<span class="number">3</span>, name=<span class="string">'bn0'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># MAXPOOL</span></span><br><span class="line">    X = MaxPooling2D((<span class="number">2</span>,<span class="number">2</span>),name=<span class="string">'max_pool'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># FLATTEN X (means convert it to a vector) + FULLYCONNECTED</span></span><br><span class="line">    X = Flatten()(X)</span><br><span class="line">    X = Dense(<span class="number">1</span>,activation=<span class="string">'sigmoid'</span>,name=<span class="string">'fc'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create model. This creates your Keras model instance, you'll use this instance to train/test the model.</span></span><br><span class="line">    model = Model(inputs=X_input, outputs=X, name=<span class="string">'HappyModel'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure></p><p>Note that Keras uses a different convention with variable names than we’ve previously used with numpy and TensorFlow. In particular, rather than creating and assigning a new variable on each step of forward propagation such as <code>X</code>, <code>Z1</code>, <code>A1</code>, <code>Z2</code>, <code>A2</code>, etc. for the computations for the different layers, in Keras code each line above just reassigns <code>X</code> to a new value using <code>X = ...</code>. In other words, during each step of forward propagation, we are just writing the latest value in the commputation into the same variable <code>X</code>. The only exception was <code>X_input</code>, which we kept separate and did not overwrite, since we needed it at the end to create the Keras model instance (<code>model = Model(inputs = X_input, ...)</code> above). </p><p><strong>Exercise</strong>: Implement a <code>HappyModel()</code>. This assignment is more open-ended than most. We suggest that you start by implementing a model using the architecture we suggest, and run through the rest of this assignment using that as your initial model. But after that, come back and take initiative to try out other model architectures. For example, you might take inspiration from the model above, but then vary the network architecture and hyperparameters however you wish. You can also use other functions such as <code>AveragePooling2D()</code>, <code>GlobalMaxPooling2D()</code>, <code>Dropout()</code>. </p><p><strong>Note</strong>: You have to be careful with your data’s shapes. Use what you’ve learned in the videos to make sure your convolutional, pooling and fully-connected layers are adapted to the volumes you’re applying it to.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">HappyModel</span><span class="params">(input_shape)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the HappyModel.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    input_shape -- shape of the images of the dataset</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a Model() instance in Keras</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Feel free to use the suggested outline in the text above to get started, and run through the whole</span></span><br><span class="line">    <span class="comment"># exercise (including the later portions of this notebook) once. The come back also try out other</span></span><br><span class="line">    <span class="comment"># network architectures as well. </span></span><br><span class="line">    X_input = Input(shape=input_shape)</span><br><span class="line">    X = ZeroPadding2D(padding=(<span class="number">1</span>, <span class="number">1</span>))(X_input)</span><br><span class="line">    X = Conv2D(<span class="number">8</span>, kernel_size=(<span class="number">3</span>,<span class="number">3</span>), strides=(<span class="number">1</span>,<span class="number">1</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis=<span class="number">3</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    X = MaxPooling2D(pool_size=(<span class="number">2</span>,<span class="number">2</span>), strides=(<span class="number">2</span>,<span class="number">2</span>), padding=<span class="string">'valid'</span>)(X)</span><br><span class="line"></span><br><span class="line">    X = ZeroPadding2D(padding=(<span class="number">1</span>, <span class="number">1</span>))(X)</span><br><span class="line">    X = Conv2D(<span class="number">16</span>, kernel_size=(<span class="number">3</span>,<span class="number">3</span>), strides=(<span class="number">1</span>,<span class="number">1</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis=<span class="number">3</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    X = MaxPooling2D(pool_size=(<span class="number">2</span>,<span class="number">2</span>), strides=(<span class="number">2</span>,<span class="number">2</span>), padding=<span class="string">'valid'</span>)(X)</span><br><span class="line"></span><br><span class="line">    X = ZeroPadding2D(padding=(<span class="number">1</span>, <span class="number">1</span>))(X)</span><br><span class="line">    X = Conv2D(<span class="number">32</span>, kernel_size=(<span class="number">3</span>,<span class="number">3</span>), strides=(<span class="number">1</span>,<span class="number">1</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis=<span class="number">3</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    X = MaxPooling2D(pool_size=(<span class="number">2</span>,<span class="number">2</span>), strides=(<span class="number">2</span>,<span class="number">2</span>), padding=<span class="string">'valid'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># FC</span></span><br><span class="line">    X = Flatten()(X)</span><br><span class="line">    Y = Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)(X)</span><br><span class="line">    </span><br><span class="line">    model = Model(inputs = X_input, outputs = Y, name=<span class="string">'HappyModel'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><p>You have now built a function to describe your model. To train and test this model, there are four steps in Keras:</p><ol><li>Create the model by calling the function above</li><li>Compile the model by calling <code>model.compile(optimizer = &quot;...&quot;, loss = &quot;...&quot;, metrics = [&quot;accuracy&quot;])</code></li><li>Train the model on train data by calling <code>model.fit(x = ..., y = ..., epochs = ..., batch_size = ...)</code></li><li>Test the model on test data by calling <code>model.evaluate(x = ..., y = ...)</code></li></ol><p>If you want to know more about <code>model.compile()</code>, <code>model.fit()</code>, <code>model.evaluate()</code> and their arguments, refer to the official <a href="https://keras.io/models/model/" target="_blank" rel="noopener">Keras documentation</a>.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ### (1 line)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: create the model.</span></span><br><span class="line">happyModel = HappyModel((<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: compile the model to configure the learning process. </span></span><br><span class="line">happyModel.compile(optimizer=keras.optimizers.Adam(lr=<span class="number">0.001</span>,beta_1=<span class="number">0.9</span>,beta_2=<span class="number">0.999</span>,epsilon=<span class="number">1e-08</span>,decay=<span class="number">0.0</span>),loss=<span class="string">'binary_crossentropy'</span>,metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3: train the model</span></span><br><span class="line">happyModel.fit(x=X_train, y=Y_train, batch_size=<span class="number">16</span>, epochs=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 4: test/evaluate the model.</span></span><br><span class="line">preds = happyModel.evaluate(x=X_test,y=Y_test)</span><br><span class="line"></span><br><span class="line">print(preds)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Loss = "</span> + str(preds[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Test Accuracy = "</span> + str(preds[<span class="number">1</span>]))</span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br></pre></td></tr></table></figure><p>测试结果输出为：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-a27eadbcae8ea2db.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="3-Conclusion"><a href="#3-Conclusion" class="headerlink" title="3- Conclusion"></a>3- Conclusion</h3><p><strong>What we would like you to remember from this assignment:</strong></p><ul><li>Keras is a tool we recommend for rapid prototyping. It allows you to quickly try out different model architectures. Are there any applications of deep learning to your daily life that you’d like to implement using Keras?</li><li>Remember how to code a model in Keras and the four steps leading to the evaluation of your model on the test set. Create-&gt;Compile-&gt;Fit/Train-&gt;Evaluate/Test.</li></ul><h3 id="Test-with-your-own-image-Optional"><a href="#Test-with-your-own-image-Optional" class="headerlink" title="Test with your own image (Optional)"></a>Test with your own image (Optional)</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### START CODE HERE ###</span></span><br><span class="line">img_path = <span class="string">'images/my_image.jpg'</span></span><br><span class="line"><span class="comment">### END CODE HERE ###</span></span><br><span class="line">img = image.load_img(img_path, target_size=(<span class="number">64</span>, <span class="number">64</span>))</span><br><span class="line">imshow(img)</span><br><span class="line"></span><br><span class="line">x = image.img_to_array(img)</span><br><span class="line">x = np.expand_dims(x, axis=<span class="number">0</span>)</span><br><span class="line">x = preprocess_input(x)</span><br><span class="line"></span><br><span class="line">print(happyModel.predict(x))</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line"><span class="comment">### 5- Other useful functions in Keras (Optional)</span></span><br><span class="line">Two other basic features of Keras that yo<span class="string">u'll find useful are:</span></span><br><span class="line"><span class="string">- `model.summary()`: prints the details of your layers in a table with the sizes of its inputs/outputs</span></span><br><span class="line"><span class="string">- `plot_model()`: plots your graph in a nice layout. You can even save it as ".png" using SVG() if you'</span>d like to share it on social media ;). It <span class="keyword">is</span> saved <span class="keyword">in</span> <span class="string">"File"</span> then <span class="string">"Open..."</span> <span class="keyword">in</span> the upper bar of the notebook.</span><br><span class="line">![image.png](https://upload-images.jianshu.io/upload_images/8636110-7783b4f931ab61f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)</span><br><span class="line">![image.png](https://upload-images.jianshu.io/upload_images/8636110-041ce06a21b93545.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)</span><br><span class="line"></span><br><span class="line">可以通过这个过程来巩固自己的卷积维数计算。</span><br><span class="line"></span><br><span class="line"><span class="comment">## Residual Network</span></span><br><span class="line"></span><br><span class="line">**In this assignment, you will:**</span><br><span class="line">- Implement the basic building blocks of ResNets. </span><br><span class="line">- Put together these building blocks to implement <span class="keyword">and</span> train a state-of-the-art neural network <span class="keyword">for</span> image classification.</span><br><span class="line"></span><br><span class="line">导入包:</span><br><span class="line">``` py</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model, load_model</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> image</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> layer_utils</span><br><span class="line"><span class="keyword">from</span> keras.utils.data_utils <span class="keyword">import</span> get_file</span><br><span class="line"><span class="keyword">from</span> keras.applications.imagenet_utils <span class="keyword">import</span> preprocess_input</span><br><span class="line"><span class="keyword">import</span> pydot</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> SVG</span><br><span class="line"><span class="keyword">from</span> keras.utils.vis_utils <span class="keyword">import</span> model_to_dot</span><br><span class="line"><span class="keyword">from</span> keras.utils <span class="keyword">import</span> plot_model</span><br><span class="line"><span class="keyword">from</span> resnets_utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> keras.initializers <span class="keyword">import</span> glorot_uniform</span><br><span class="line"><span class="keyword">import</span> scipy.misc</span><br><span class="line"><span class="keyword">from</span> matplotlib.pyplot <span class="keyword">import</span> imshow</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</span><br><span class="line">K.set_image_data_format(<span class="string">'channels_last'</span>)</span><br><span class="line">K.set_learning_phase(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="1-The-problem-of-very-deep-neural-networks"><a href="#1-The-problem-of-very-deep-neural-networks" class="headerlink" title="1- The problem of very deep neural networks"></a>1- The problem of very deep neural networks</h3><p>Last week, you built your first convolutional neural network. In recent years, neural networks have become deeper, with state-of-the-art networks going from just a few layers (e.g., AlexNet) to over a hundred layers.</p><p>The main benefit of a very deep network is that it can represent very complex functions. It can also learn features at many different levels of abstraction, from edges (at the lower layers) to very complex features (at the deeper layers). However, using a deeper network doesn’t always help. A huge barrier to training them is <strong>vanishing gradients</strong>: very deep networks often have a gradient signal that goes to zero quickly, thus making gradient descent unbearably slow. More specifically, during gradient descent, as you backprop from the final layer back to the first layer, you are multiplying by the weight matrix on each step, and thus the gradient can decrease exponentially quickly to zero (or, in rare cases, grow exponentially quickly and “explode” to take very large values).</p><p>During training, you might therefore see the magnitude (or norm) of the gradient for the earlier layers descrease to zero very rapidly as training proceeds:</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-02b0b5e0ea98a2c3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>You are now going to solve this problem by building a Residual Network!</p><h3 id="2-Building-a-Residual-Network"><a href="#2-Building-a-Residual-Network" class="headerlink" title="2- Building a Residual Network"></a>2- Building a Residual Network</h3><p>In ResNets, a “shortcut” or a “skip connection” allows the gradient to be directly backpropagated to earlier layers:</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-24b9c326de7a32ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>The image on the left shows the “main path” through the network. The image on the right adds a shortcut to the main path. By stacking these ResNet blocks on top of each other, you can form a very deep network.</p><p>We also saw in lecture that having ResNet blocks with the shortcut also makes it very easy for one of the blocks to learn an identity function. This means that you can stack on additional ResNet blocks with little risk of harming training set performance. (There is also some evidence that the ease of learning an identity function—even more than skip connections helping with vanishing gradients—accounts for ResNets’ remarkable performance.)</p><p>Two main types of blocks are used in a ResNet, depending mainly on whether the input/output dimensions are same or different. You are going to implement both of them.</p><h4 id="2-1-The-identity-block"><a href="#2-1-The-identity-block" class="headerlink" title="2.1- The identity block"></a>2.1- The identity block</h4><p>The identity block is the standard block used in ResNets, and corresponds to the case where the input activation (say a[l]) has the same dimension as the output activation (say a[l+2]).<br><img src="https://upload-images.jianshu.io/upload_images/8636110-bfed90c554f6fd81.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>In this exercise, you’ll actually implement a slightly more powerful version of this identity block, in which the skip connection “skips over” 3 hidden layers rather than 2 layers. It looks like this:</p><p>Here’re the individual steps.</p><p>First component of main path:</p><ul><li>The first CONV2D has $F_1$ filters of shape (1,1) and a stride of (1,1). Its padding is “valid” and its name should be <code>conv_name_base + &#39;2a&#39;</code>. Use 0 as the seed for the random initialization. </li><li>The first BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2a&#39;</code>.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li></ul><p>Second component of main path:</p><ul><li>The second CONV2D has $F_2$ filters of shape $(f,f)$ and a stride of (1,1). Its padding is “same” and its name should be <code>conv_name_base + &#39;2b&#39;</code>. Use 0 as the seed for the random initialization. </li><li>The second BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2b&#39;</code>.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li></ul><p>Third component of main path:</p><ul><li>The third CONV2D has $F_3$ filters of shape (1,1) and a stride of (1,1). Its padding is “valid” and its name should be <code>conv_name_base + &#39;2c&#39;</code>. Use 0 as the seed for the random initialization. </li><li>The third BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2c&#39;</code>. Note that there is no ReLU activation function in this component. </li></ul><p>Final step: </p><ul><li>The shortcut and the input are added together.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">identity_block</span><span class="params">(X, f, filters, stage, block)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the identity block as defined in Figure 4</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    f -- integer, specifying the shape of the middle CONV's window for the main path</span></span><br><span class="line"><span class="string">    filters -- python list of integers, defining the number of filters in the CONV layers of the main path</span></span><br><span class="line"><span class="string">    stage -- integer, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    block -- string/character, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># defining name basis</span></span><br><span class="line">    conv_name_base = <span class="string">'res'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    bn_name_base = <span class="string">'bn'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve Filters</span></span><br><span class="line">    F1, F2, F3 = filters</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Save the input value. You'll need this later to add back to the main path. </span></span><br><span class="line">    X_shortcut = X</span><br><span class="line"></span><br><span class="line">    <span class="comment"># First component of main path</span></span><br><span class="line">    <span class="comment"># 这个的参数filters代表卷积核的数量。</span></span><br><span class="line">    X = Conv2D(filters=F1, kernel_size=(<span class="number">1</span>,<span class="number">1</span>), strides=(<span class="number">1</span>,<span class="number">1</span>), padding=<span class="string">'valid'</span>, name=conv_name_base+<span class="string">'2a'</span>, kernel_initializer=glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis=<span class="number">3</span>, name=bn_name_base+<span class="string">'2a'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Second component of main path (≈3 lines)</span></span><br><span class="line">    X = Conv2D(filters=F2, kernel_size=(f,f), strides=(<span class="number">1</span>,<span class="number">1</span>), padding=<span class="string">'same'</span>, name=conv_name_base+<span class="string">'2b'</span>, kernel_initializer=glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis=<span class="number">3</span>, name=bn_name_base+<span class="string">'2b'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Third component of main path (≈2 lines)</span></span><br><span class="line">    X = Conv2D(filters=F3, kernel_size=(<span class="number">1</span>,<span class="number">1</span>), strides=(<span class="number">1</span>,<span class="number">1</span>), padding=<span class="string">'valid'</span>, name=conv_name_base+<span class="string">'2c'</span>, kernel_initializer=glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis=<span class="number">3</span>, name=bn_name_base+<span class="string">'2c'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)</span></span><br><span class="line">    X = Add()([X, X_shortcut])</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br></pre></td></tr></table></figure><p>测试：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tf.reset_default_graph()</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    A_prev = tf.placeholder(<span class="string">"float"</span>,[<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">6</span>])</span><br><span class="line">    X = np.random.randn(<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">6</span>)</span><br><span class="line">    A = identity_block(A_prev, f = <span class="number">2</span>, filters=[<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>], stage=<span class="number">1</span>, block=<span class="string">'a'</span>)</span><br><span class="line">    sess.run(sess.global_variables_initializer())</span><br><span class="line">    out = sess.run([A], feed_dict=&#123;A_prev:X, K.learning_phase():<span class="number">0</span>&#125;)</span><br><span class="line">    print(<span class="string">"out="</span> + str(out[<span class="number">0</span>][<span class="number">1</span>][<span class="number">1</span>][<span class="number">0</span>]))</span><br></pre></td></tr></table></figure></p><h3 id="2-2-The-convolutional-block"><a href="#2-2-The-convolutional-block" class="headerlink" title="2.2- The convolutional block"></a>2.2- The convolutional block</h3><p>You’ve implemented the ResNet identity block. Next, the ResNet “convolutional block” is the other type of block. You can use this type of block when the input and output dimensions don’t match up. The difference with the identity block is that there is a CONV2D layer in the shortcut path:</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-6a0e5278100d6218.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>The CONV2D layer in the shortcut path is used to resize the input  xx  to a different dimension, so that the dimensions match up in the final addition needed to add the shortcut value back to the main path. (This plays a similar role as the matrix  WsWs  discussed in lecture.) For example, to reduce the activation dimensions’s height and width by a factor of 2, you can use a 1x1 convolution with a stride of 2. The CONV2D layer on the shortcut path does not use any non-linear activation function. Its main role is to just apply a (learned) linear function that reduces the dimension of the input, so that the dimensions match up for the later addition step.</p><p>The details of the convolutional block are as follows.</p><p>First component of main path:</p><ul><li>The first CONV2D has $F_1$ filters of shape (1,1) and a stride of (s,s). Its padding is “valid” and its name should be <code>conv_name_base + &#39;2a&#39;</code>. </li><li>The first BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2a&#39;</code>.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li></ul><p>Second component of main path:</p><ul><li>The second CONV2D has $F_2$ filters of (f,f) and a stride of (1,1). Its padding is “same” and it’s name should be <code>conv_name_base + &#39;2b&#39;</code>.</li><li>The second BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2b&#39;</code>.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li></ul><p>Third component of main path:</p><ul><li>The third CONV2D has $F_3$ filters of (1,1) and a stride of (1,1). Its padding is “valid” and it’s name should be <code>conv_name_base + &#39;2c&#39;</code>.</li><li>The third BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;2c&#39;</code>. Note that there is no ReLU activation function in this component. </li></ul><p>Shortcut path:</p><ul><li>The CONV2D has $F_3$ filters of shape (1,1) and a stride of (s,s). Its padding is “valid” and its name should be <code>conv_name_base + &#39;1&#39;</code>.</li><li>The BatchNorm is normalizing the channels axis.  Its name should be <code>bn_name_base + &#39;1&#39;</code>. </li></ul><p>Final step: </p><ul><li>The shortcut and the main path values are added together.</li><li>Then apply the ReLU activation function. This has no name and no hyperparameters. </li></ul><p><strong>Exercise</strong>: Implement the convolutional block. We have implemented the first component of the main path; you should implement the rest. As before, always use 0 as the seed for the random initialization, to ensure consistency with our grader.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convolutional_block</span><span class="params">(X, f, filters, stage, block, s=<span class="number">2</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the identity block as defined in Figure 4</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    f -- integer, specifying the shape of the middle CONV's window for the main path</span></span><br><span class="line"><span class="string">    filters -- python list of integers, defining the number of filters in the CONV layers of the main path</span></span><br><span class="line"><span class="string">    stage -- integer, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    block -- string/character, used to name the layers, depending on their position in the network</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># defining name basis</span></span><br><span class="line">    conv_name_base = <span class="string">'res'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    bn_name_base = <span class="string">'bn'</span> + str(stage) + block + <span class="string">'_branch'</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve Filters</span></span><br><span class="line">    F1, F2, F3 = filters</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Save the input value. You'll need this later to add back to the main path. </span></span><br><span class="line">    X_shortcut = X</span><br><span class="line"></span><br><span class="line">    <span class="comment">##### MAIN PATH #####</span></span><br><span class="line">    <span class="comment"># First component of main path</span></span><br><span class="line">    X = Conv2D(filters = F1, kernel_size = (<span class="number">1</span>, <span class="number">1</span>), strides = (s,s), padding = <span class="string">'valid'</span>, name = conv_name_base + <span class="string">'2a'</span>, kernel_initializer = glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis = <span class="number">3</span>, name = bn_name_base + <span class="string">'2a'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Second component of main path (≈3 lines)</span></span><br><span class="line">    X = Conv2D(filters=F2, kernel_size=(f,f), strides=(<span class="number">1</span>,<span class="number">1</span>), padding=<span class="string">'same'</span>, name=conv_name_base+<span class="string">'2b'</span>, kernel_initializer=glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis=<span class="number">3</span>,name=bn_name_base+<span class="string">'2b'</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Third component of main path (≈2 lines)</span></span><br><span class="line">    X = Conv2D(filters=F3, kernel_size=(<span class="number">1</span>,<span class="number">1</span>), strides=(<span class="number">1</span>,<span class="number">1</span>), padding=<span class="string">'valid'</span>, name=conv_name_base+<span class="string">'2c'</span>, kernel_initializer=glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis=<span class="number">3</span>, name=bn_name_base+<span class="string">'2c'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment">##### SHORTCUT PATH #### (≈2 lines)</span></span><br><span class="line">    X_shortcut = Conv2D(filters=F3, kernel_size=(<span class="number">1</span>,<span class="number">1</span>), strides=(s,s), padding=<span class="string">'valid'</span>, name=conv_name_base+<span class="string">'1'</span>, kernel_initializer=glorot_uniform(seed=<span class="number">0</span>))(X_shortcut)</span><br><span class="line">    X_shortcut = BatchNormalization(axis=<span class="number">3</span>, name=bn_name_base+<span class="string">'1'</span>)(X_shortcut)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)</span></span><br><span class="line">    X = Add()([X, X_shortcut])</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X</span><br></pre></td></tr></table></figure><h3 id="3-Building-your-first-ResNet-model-50-layers"><a href="#3-Building-your-first-ResNet-model-50-layers" class="headerlink" title="3- Building your first ResNet model(50 layers)"></a>3- Building your first ResNet model(50 layers)</h3><p>You now have the necessary blocks to build a very deep ResNet. The following figure describes in detail the architecture of this neural network. “ID BLOCK” in the diagram stands for “Identity block,” and “ID BLOCK x3” means you should stack 3 identity blocks together.</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-72cfe8a4efaeeada.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>The details of this ResNet-50 model are:</p><ul><li>Zero-padding pads the input with a pad of (3,3)</li><li>Stage 1:<ul><li>The 2D Convolution has 64 filters of shape (7,7) and uses a stride of (2,2). Its name is “conv1”.</li><li>BatchNorm is applied to the channels axis of the input.</li><li>MaxPooling uses a (3,3) window and a (2,2) stride.</li></ul></li><li>Stage 2:<ul><li>The convolutional block uses three set of filters of size [64,64,256], “f” is 3, “s” is 1 and the block is “a”.</li><li>The 2 identity blocks use three set of filters of size [64,64,256], “f” is 3 and the blocks are “b” and “c”.</li></ul></li><li>Stage 3:<ul><li>The convolutional block uses three set of filters of size [128,128,512], “f” is 3, “s” is 2 and the block is “a”.</li><li>The 3 identity blocks use three set of filters of size [128,128,512], “f” is 3 and the blocks are “b”, “c” and “d”.</li></ul></li><li>Stage 4:<ul><li>The convolutional block uses three set of filters of size [256, 256, 1024], “f” is 3, “s” is 2 and the block is “a”.</li><li>The 5 identity blocks use three set of filters of size [256, 256, 1024], “f” is 3 and the blocks are “b”, “c”, “d”, “e” and “f”.</li></ul></li><li>Stage 5:<ul><li>The convolutional block uses three set of filters of size [512, 512, 2048], “f” is 3, “s” is 2 and the block is “a”.</li><li>The 2 identity blocks use three set of filters of size [256, 256, 2048], “f” is 3 and the blocks are “b” and “c”.</li></ul></li><li>The 2D Average Pooling uses a window of shape (2,2) and its name is “avg_pool”.</li><li>The flatten doesn’t have any hyperparameters or name.</li><li>The Fully Connected (Dense) layer reduces its input to the number of classes using a softmax activation. Its name should be <code>&#39;fc&#39; + str(classes)</code>.</li></ul><p><strong>Exercise</strong>: Implement the ResNet with 50 layers described in the figure above. We have implemented Stages 1 and 2. Please implement the rest. (The syntax for implementing Stages 3-5 should be quite similar to that of Stage 2.) Make sure you follow the naming convention in the text above. </p><p>You’ll need to use this function: </p><ul><li>Average pooling <a href="https://keras.io/layers/pooling/#averagepooling2d" target="_blank" rel="noopener">see reference</a></li></ul><p>Here’re some other functions we used in the code below:</p><ul><li>Conv2D: <a href="https://keras.io/layers/convolutional/#conv2d" target="_blank" rel="noopener">See reference</a></li><li>BatchNorm: <a href="https://keras.io/layers/normalization/#batchnormalization" target="_blank" rel="noopener">See reference</a> (axis: Integer, the axis that should be normalized (typically the features axis))</li><li>Zero padding: <a href="https://keras.io/layers/convolutional/#zeropadding2d" target="_blank" rel="noopener">See reference</a></li><li>Max pooling: <a href="https://keras.io/layers/pooling/#maxpooling2d" target="_blank" rel="noopener">See reference</a></li><li>Fully conected layer: <a href="https://keras.io/layers/core/#dense" target="_blank" rel="noopener">See reference</a></li><li>Addition: <a href="https://keras.io/layers/merge/#add" target="_blank" rel="noopener">See reference</a></li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ResNet50</span><span class="params">(input_shape=<span class="params">(<span class="number">64</span>,<span class="number">64</span>,<span class="number">3</span>)</span>, classes=<span class="number">6</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implementation of the popular ResNet50 the following architecture:</span></span><br><span class="line"><span class="string">    CONV2D -&gt; BATCHNORM -&gt; RELU -&gt; MAXPOOL -&gt; CONVBLOCK -&gt; IDBLOCK*2 -&gt; CONVBLOCK -&gt; IDBLOCK*3</span></span><br><span class="line"><span class="string">    -&gt; CONVBLOCK -&gt; IDBLOCK*5 -&gt; CONVBLOCK -&gt; IDBLOCK*2 -&gt; AVGPOOL -&gt; TOPLAYER</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    input_shape -- shape of the images of the dataset</span></span><br><span class="line"><span class="string">    classes -- integer, number of classes</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a Model() instance in Keras</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Define the input as a tensor with shape input_shape</span></span><br><span class="line">    X_input = Input(input_shape)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Zero-Padding</span></span><br><span class="line">    X = ZeroPadding2D((<span class="number">3</span>, <span class="number">3</span>))(X_input)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 1</span></span><br><span class="line">    X = Conv2D(<span class="number">64</span>,kernel_size=(<span class="number">7</span>,<span class="number">7</span>),strides=(<span class="number">2</span>,<span class="number">2</span>),name=<span class="string">"conv1"</span>,kernel_initializer=glorot_uniform(seed=<span class="number">0</span>))(X)</span><br><span class="line">    X = BatchNormalization(axis3, name=<span class="string">"bn_conv1"</span>)(X)</span><br><span class="line">    X = Activation(<span class="string">'relu'</span>)(X)</span><br><span class="line">    X = MaxPooling2D((<span class="number">3</span>,<span class="number">3</span>), strides=(<span class="number">2</span>,<span class="number">2</span>))(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 2</span></span><br><span class="line">    X = convolutional_block(X, f=<span class="number">3</span>, filters=[<span class="number">64</span>,<span class="number">64</span>,<span class="number">256</span>], stage=<span class="number">2</span>, block=<span class="string">'a'</span>, s=<span class="number">1</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">64</span>,<span class="number">64</span>,<span class="number">256</span>], stage=<span class="number">2</span>, block=<span class="string">'b'</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">64</span>,<span class="number">64</span>,<span class="number">256</span>], stage=<span class="number">2</span>, block=<span class="string">'c'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 3</span></span><br><span class="line">    X = convolutional_block(X, f=<span class="number">3</span>, filters=[<span class="number">128</span>,<span class="number">128</span>,<span class="number">512</span>], stage=<span class="number">3</span>, block=<span class="string">'a'</span>, s=<span class="number">2</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">128</span>,<span class="number">128</span>,<span class="number">512</span>], block=<span class="string">'b'</span>, stage=<span class="number">3</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">128</span>,<span class="number">128</span>,<span class="number">512</span>], block=<span class="string">'c'</span>, stage=<span class="number">3</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">128</span>,<span class="number">128</span>,<span class="number">512</span>], block=<span class="string">'d'</span>, stage=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 4</span></span><br><span class="line">    X = convolutional_block(X, f=<span class="number">3</span>, filters=[<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], s=<span class="number">2</span>, block=<span class="string">'a'</span>, stage=<span class="number">4</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], block=<span class="string">'b'</span>, stage=<span class="number">4</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], block=<span class="string">'c'</span>, stage=<span class="number">4</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], block=<span class="string">'d'</span>, stage=<span class="number">4</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], block=<span class="string">'e'</span>, stage=<span class="number">4</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">1024</span>], block=<span class="string">'f'</span>, stage=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stage 5</span></span><br><span class="line">    X = convolutional_block(X, f=<span class="number">3</span>, filters=[<span class="number">512</span>,<span class="number">512</span>,<span class="number">2048</span>], s=<span class="number">2</span>, block=<span class="string">'a'</span>, stage=<span class="number">5</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">2048</span>], block=<span class="string">'b'</span>, stage=<span class="number">5</span>)</span><br><span class="line">    X = identity_block(X, <span class="number">3</span>, [<span class="number">256</span>,<span class="number">256</span>,<span class="number">2048</span>], block=<span class="string">'c'</span>, stage=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># AVGPOOL</span></span><br><span class="line">    X = AveragePooling2D(pool_size=(<span class="number">2</span>,<span class="number">2</span>),name=<span class="string">'avg_pool'</span>)(X)</span><br></pre></td></tr></table></figure><p>接下来是之前一样的四个步骤：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = ResNet50(input_shape = (<span class="number">64</span>, <span class="number">64</span>, <span class="number">3</span>), classes = <span class="number">6</span>)</span><br><span class="line">model.compile(optimizer=<span class="string">'adam'</span>, loss=<span class="string">'categorical_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">model.fit(X_train, Y_train, epochs = <span class="number">20</span>, batch_size = <span class="number">32</span>)</span><br><span class="line">preds = model.evaluate(X_test, Y_test)</span><br></pre></td></tr></table></figure></p><p>ResNet50 is a powerful model for image classification when it is trained for an adequate number of iterations. We hope you can use what you’ve learnt and apply it to your own classification problem to perform state-of-the-art accuracy.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;为什么要研究实例：Why-look-at-case-studies&quot;&gt;&lt;a href=&quot;#为什么要研究实例：Why-look-at-case-studies&quot; class=&quot;headerlink&quot; title=&quot;为什么要研究实例：Why look at case 
      
    
    </summary>
    
      <category term="深度学习" scheme="https://github.com/DesmonDay/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://github.com/DesmonDay/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>第10周-卷积神经网络</title>
    <link href="https://github.com/DesmonDay/2019/04/21/deep-learningw10/"/>
    <id>https://github.com/DesmonDay/2019/04/21/deep-learningw10/</id>
    <published>2019-04-20T18:27:39.000Z</published>
    <updated>2020-02-19T05:13:19.391Z</updated>
    
    <content type="html"><![CDATA[<p>接下来的四周为计算机视觉——卷积神经网络的内容。</p><h1 id="计算机视觉"><a href="#计算机视觉" class="headerlink" title="计算机视觉"></a>计算机视觉</h1><p>常见的计算机视觉问题包括图像分类、目标检测、神经网络实现的图片风格迁移等等。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-6da3b6dc31c4aac5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在应用计算机视觉时，我们面临的一个挑战是数据的输入可能会非常大，以图片输入为例：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-2132ecef254524fe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>可以看到，如果我们采用的是64x64的图片，那么输入大小为12288，但如果是相对高清的图片，输入的大小可以达到3million。如果按照我们之前所讲全连接的神经网络来做，所需要的参数大小将会非常巨大，并且对内存的要求也很高。为了解决这种情况，我们使用的神经网络实际上为卷积神经网络。</p><h1 id="卷积运算：边缘检测示例"><a href="#卷积运算：边缘检测示例" class="headerlink" title="卷积运算：边缘检测示例"></a>卷积运算：边缘检测示例</h1><p>卷积运算是卷积神经网络的最基本的组成部分，我们使用边缘检测作为入门样例，了解卷积是如何进行计算的。</p><p>在进行图像识别的时候，我们会进行边缘检测，示例如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-24c5a8a203e46666.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如何在图像中检测这些边缘？下面举一个例子：我们给出一个6x6的灰度图像（因此只有一个颜色通道），也就是6x6x1的矩阵。为了检测图像中的垂直边缘，我们可以构造一个3x3的矩阵称为过滤器（又称卷积核，一般为3x3矩阵），再将图像矩阵与这个过滤矩阵做卷积运算。这个卷积运算的输出为4x4的矩阵。具体如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e5d257063b772320.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>可以看到，我们按顺序移动蓝色方块，再将方块中的数字与卷积核进行计算，计算的方法即对应元素相乘后再相加，得到的结果再写入结果矩阵对应的位置中。因此，在左上角的蓝色方块中，我们的计算为3x1+1x1+2x1+0x0+5x0+7x0+1x)-1+8x(-1)+2x(-1)=-5，其他结果也一样通过这种方式获得。</p><p>卷积运算在python中为conv_forward，在tensorflow中为tf.nn.conv2d，在Keras框架中为Conv2D。几乎所有的编程框架都有提供一些函数来实现卷积运算。</p><p>用简单例子解释为何这种运算可以这样计算：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-4d3c64a43489e39c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>可以发现，我们的图像中间有着一个明显的垂直直线，这条垂直线是从黑到白的过滤线。当用一个3x3过滤器进行卷积运算时，这个3x3过滤器可视为左边有明亮像素，中间有过渡(0)，右边有深色像素的图例。通过卷积运算后，我们的矩阵对应的图像，在中间有段明亮的区域，这可以对应检查到这个6x6图像中间的垂直边缘。（这里的维数有些不正确，即检测到的边缘过粗，这是因为在此例中的图片过小，当我们使用的是1000x1000的图像，会发现其能很好地检测图像中的垂直边缘。）通过这种卷积运算，我们可以发现图像中的垂直边缘。</p><h1 id="更多边缘检测内容"><a href="#更多边缘检测内容" class="headerlink" title="更多边缘检测内容"></a>更多边缘检测内容</h1><p>在本节中，我们会学习如何区分正边和负边（即由亮到暗与由暗到亮的区别），也就是边缘的过渡。我们也可以了解到其他类型的边缘检测以及如何实现这些算法。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-dc8899bc8d386eb3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>对比发现，上图为由亮到暗的过渡，而下图为由暗到亮的过渡。也就是说，中间的这个3x3卷积核能够帮助我们区分正边和负边。</p><h2 id="Vertical-and-Horizontal-Edge-Detection"><a href="#Vertical-and-Horizontal-Edge-Detection" class="headerlink" title="Vertical and Horizontal Edge Detection"></a>Vertical and Horizontal Edge Detection</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-b8a90a6aa25f7d62.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>左边的卷积核针对垂直边缘，而右边的卷积核针对水平边缘。另外，我们可以通过上图的一个例子来验证水平边缘的检测正确性。在右边用橙色框出来的10，表明其左边为亮，右边为暗，对应着原图像的上面过渡部分，其他的值也可以这么对应分析。</p><p>总而言之，通过使用不同的滤波器，我们可以找出垂直的或者水平的边缘。但事实上，对于这个3x3的卷积核来说，我们只使用了其中一种数字组合。在计算机视觉的文献中，曾争论过怎样的数字组合猜是最好的。<br>1、Sobel过滤器，它的优点在于增加了中间一行元素的权重，这使得结果的鲁棒性会更高一些。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-6bc7d0ef4bca22c2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>2、 Scharr过滤器，它有着和之前完全不同的特性，但实际上也是一钟垂直边缘检测，如果将其旋转90度，可以得到对应水平边缘检测。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-2fa4f586983b3583.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>随着<strong>深度学习</strong>的发展，当我们真正想去检测出复杂图像的边缘，也许我们不需要使用研究者选择的数字，而是可以<strong>把这些数字当成参数</strong>，通过<strong>后向传播</strong>算法来得到对应值。相比垂直和水平边缘，这种方法也可以检验包括其他方向的边缘。将卷积核的所有数字设置为参数，通过数据反馈，让神经网络自动学习，我们会发现神经网络可以学习一些低级的特征，比如边缘的特征。构成这些计算的基础是卷积运算，因此使得反向传播算法能够让神经网络学习任何它所需要的3x3过滤器，并在整幅图片上应用它，输出它所检测的特征。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-8f5daa4b3c5c6794.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="Padding"><a href="#Padding" class="headerlink" title="Padding"></a>Padding</h1><p>在卷积神经网络中，一个基本的卷积操作就是padding。</p><p>在之前我们在做卷积运算示例中，我们的输出矩阵为4x4维度，这是因为我们使用的过滤器在原图片上只可能有4x4种可能的位置。对应的，如果我们有nxn的图像，而过滤器为fxf，那么输出结果的维度为(n-f+1)x(n-f+1)。这样做有两个缺点，一是每次做卷积操作，我们的图像会缩小，比如从6x6到4x4，如果再多几次卷积运算，那么我们的图像就会变得很小了；二是如果我们注意到角落边的像素，这个像素点只被一个输出使用，因为它只位于一个3x3区域的一角，但如果是在中间的像素点，那么会有很多3x3区域重叠。因此那些在角落或者边缘区域的像素点在输出中采用较少，意味着我们丢掉了图像边缘位置的许多信息。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-51dbad653021f2cb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>为了解决这两个问题，一是输出缩小，二是图像边缘的大部分信息丢失，我们可以在卷积操作之前对图像进行填充。例如，对上述图像进行填充，由6x6变为了8x8，那么我们得到的输出和原始图像一样，都是6x6的图像。通常，我们用进行填充。如果p是填充的数量，那么在本例中，p=padding=1，那么输出就变为了(n+2p-f+1)x(n+2p-f+1)。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-45c74100fa75f632.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>通过填充，位于角落或图像边缘的信息发挥作用较小的缺点就被削弱了。如果我们想再增加像素填充，则可以得到p=2之类的填充后的图像，如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ebe1f304259697e7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="Valid-and-Same-convolutions"><a href="#Valid-and-Same-convolutions" class="headerlink" title="Valid and Same convolutions"></a>Valid and Same convolutions</h2><p>至于填充多少像素，通常有两个选择，分别称为Valid卷积和Same卷积。Valid卷积意味着没有padding；而另一个Same卷积，这个方法意味着我们填充图像后，输出大小和原图像的大小是一样的，具体的计算过程见下图：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e7d237be92bbae53.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>另外注意到，在计算机视觉的惯例中，<strong>f通常是奇数</strong>。原因一是奇数才可以保证对称的填充；二是计算机视觉通常需要一个中心位置，便于指出过滤器的位置。</p><h1 id="卷积步长：Strided-convolutions"><a href="#卷积步长：Strided-convolutions" class="headerlink" title="卷积步长：Strided convolutions"></a>卷积步长：Strided convolutions</h1><p>卷积步长是另一个构建卷积神经网络的基本操作，下面我们举例解释卷积步长的含义。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-8979cfce84796e30.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>在本例中，我们设置Stride=2。先计算第一个位置的输出：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-0022fc160f42e86e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>接下来，要计算下一个位置的输出。由于卷积步长为2，因此我们不像之前一样将3x3区域往右移动一位，而是移动两位进行计算，如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d8b67dbf0004c314.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>继续右移两个单位：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-a17259e83c0465e4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>当我们要移动到下一行的时候，我们的步长也是2，因此下一个位置如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-84bdc3ef5cc59e88.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>等等等。最后得到的输出为：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-8871e610b35fba21.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>因此输入输出的维度由以下公式决定：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e9601faf7c8e6030.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>注意到，如果商不是整数，在这种情况下我们可以向下取整。这个原则实现的方式是，你只在蓝框完全包括在图像或填充完的图像内部时，才对它进行运算。如果有的蓝框移动到了图像外部，那么我们不对其进行运算。也就是说，我们的3x3过滤器必须处于图像中或者填充之后的图像区域内，因此要向下取整。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d66f4cdd25a87444.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="Summary-of-convolutions"><a href="#Summary-of-convolutions" class="headerlink" title="Summary of convolutions"></a>Summary of convolutions</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-af5878b975293364.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="Technical-note-on-cross-correlation-vs-convolution"><a href="#Technical-note-on-cross-correlation-vs-convolution" class="headerlink" title="Technical note on cross-correlation vs. convolution"></a>Technical note on cross-correlation vs. convolution</h2><p>这里讲解一个关于互相关和卷积的技术性建议，这不会影响到我们构建卷积神经网络的方式。如果我们看的是一本典型的数学教科书，那么卷积的定义是做元素乘积求和，实际上还有一个步骤是我们首先要做的，也就是在把这个6x6矩阵和3x3的过滤器卷积前，首先将3x3的过滤器沿水平和垂直轴翻转（先顺时针旋转90度，再水平翻转），用得到的矩阵来做元素相乘求和的操作。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-aff45991e5b60ae2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>从技术上讲，这个操作被称为互相关。在深度学习领域中，有很多人把它叫做卷积运算，但我们通常不需要用到翻转的步骤。事实证明在信号处理货某些数学分支中，卷积的定义包含翻转，使得卷积运算符拥有(A*B)*C=A*(B*C)的结合律性质。这对于一些信号处理应用来说很好，但对深度神经网络而言并不重要，因此我们忽略了这个双重镜像操作，从而简化代码。</p><p>综上所述，我们学习了如何进行卷积、如何使用填充、如何在卷积中选择步长，但我们目前为止使用的是关于矩阵的卷积。接下来会讲解如何对立体进行卷积。</p><h1 id="三维卷积：Convolutions-volumes"><a href="#三维卷积：Convolutions-volumes" class="headerlink" title="三维卷积：Convolutions volumes"></a>三维卷积：Convolutions volumes</h1><p>本节讲解如何在三维立体上进行卷积运算。假设我们想要检测RGB彩色图像的特征，即具有三个颜色通道。因此其维度为6x6x3，因此过滤器也需要是3x3x3的维度：（高、宽、通道个数）<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7c18e494b51a7d40.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>接下来研究背后的细节。其实实际的计算过程与二维的也很类似，我们将过滤器当做一个正方体，放置到原三维图像上，然后将这27个数字对应相乘和相加，填入输出的对应位置即可：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-3abb380561e76211.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>再将立方体往右移一个单位，得到下一位，等等等，直到到达最后一位。</p><p>那么这个过滤器的作用是什么？举个例子，这个过滤器是3x3x3的，如果我们想检测图像红色通道的垂直边缘，而不关心其他通道，那么可以将三个过滤器分别设置如下后，再进行堆叠：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-4099faae498d0270.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>当然，如果我们想检测所有通道的垂直边缘，则可以使用这样的过滤器。因此，参数的不同选择，可以得到不同的过滤器。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-a0ac14e4a54ed0e3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>按照计算机视觉的惯例，当你的输入有特定的高宽和通道数时，我们的过滤器可以有不同的高，不同的宽，但是通道数必须相同。现在，我们了解了如何对立方体卷积，那么，如果我们想要同时检测垂直边缘和水平边缘，以及其他方向的边缘应该怎么做？换句话说，想同时使用多个过滤器，应该怎么办？</p><p>假设我们同时使用水平过滤器和垂直过滤器，过程如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c27afbd37be9239b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>简单来说，我们得到的输出也成为了一个三维立体，这样就是同时两用了多个过滤器。</p><p>下面对维度进行总结：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d0b784321e478e03.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>即输入的彩色图像为nxnxn_c，过滤器为fxfxn_c，这里n_c为通道数目，那么输出的维度为(n-f+1)x(n-f+1)xn_c’，这里n_c’指所应用的过滤器数目，即输出的通道数等于我们要检测的特征数。比如对前面的同时使用水平过滤器和垂直过滤器来说，n_c’=2。另外，这个式子默认我们没有使用padding。</p><p>对于这里的符号，n_c在学术文献中被称为通道(channel)或者深度(depth)，在视频中统一称为通道。</p><h1 id="单层卷积网络：One-layer-of-a-convolutional-network"><a href="#单层卷积网络：One-layer-of-a-convolutional-network" class="headerlink" title="单层卷积网络：One layer of a convolutional network"></a>单层卷积网络：One layer of a convolutional network</h1><p>本节讲的是如何构建卷积神经网络的卷积层。下面看一个例子。</p><p>这个例子与前面的使用多个过滤器例子相同。输入6x6x3的图像，再通过一个卷积核，我们将得到的输出加上参数b，再通过ReLu激活函数，同样得到4x4的矩阵。再将两个输出堆叠起来，从而得到4x4x2的输出，这便是卷积神经网络的一层。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-280ea14a37187b24.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>将上述过程映射到标准神经网络中，可以解释为：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-bfc3ed93735c4ff3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>即将原始输入图像当做z[0]，也就是X，而卷积核为W[1]那么卷积的操作类似于”W[1]a[0]”，之后我们的卷积加偏置值也类似原有的”W[1]a[0]+b”，即Z，最后应用非线性函数得到4x4矩阵。通过这个过程，我们可以得到卷积神经网络中的一层。因为我们有2个过滤器，因此我们得到了4x4x2的输出；如果有10个过滤器，那么得到的就是4x4x10的输出。</p><p>接下来举例计算一层中的参数数目：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-4fa87e9a9864a40a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>如图，对应的参数数目为10x(3x3x3+1)=280个参数。我们注意到，不论输入的图片有多大，无论是1000x1000，还是5000x5000，我们的参数仍然是280个，可以用这些过滤器来检测水平特征、垂直特征和其他特征。<strong>即使这些图片很大，参数却很少，这就是卷积神经网络的一个特征，叫做“避免过拟合(less prone to over fitting)”。</strong>现在我们知道了如何提取10个特征，将其应用到大图片上，而参数数量固定不变。</p><h2 id="Summary-of-notation"><a href="#Summary-of-notation" class="headerlink" title="Summary of notation"></a>Summary of notation</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-835c64616af3c9e1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>注意，这个标记并没有在深度学习的文献中得到统一。上述标记的输入和输出对应一层卷积神经网络的输入输出，另外，输出的高和宽的计算方式也列在了右侧，即向下取整的那个式子。之后通过练习进行熟悉即可。</p><h1 id="简单卷积神经网络示例"><a href="#简单卷积神经网络示例" class="headerlink" title="简单卷积神经网络示例"></a>简单卷积神经网络示例</h1><p>假设我们有一张图片，想要做图片识别，比如分类问题。假设其大小为39x39x3，第一层的filter为3x3x3，对应的步长为1，padding为0，并且设filter有10个。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-99be17573b85d6a0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>上述卷积神经网络一共通过了3个卷积层的处理。在最后得到的7x7x40的输出后，我们一共获得了194个特征，通过将这些特征平滑化，即映射为向量后，再通过logistic函数或者Softmax函数，得到最后的分类结果。</p><p>设计卷积神经网络时，确定上述的超参数是一件麻烦的事，比如决定过滤器的大小(filter size)、步幅(stride)、padding、使用多少个过滤器等等。另外在本节课要记住的是，随着神经网络计算深度不断加深，通常开始时的图像要大一些，高和宽随着深度加深而不断减小，而信道数量则在增加。</p><p>一个典型的卷积网络通常有三层，<strong>包括卷积层(Convolution)、池化层(Pooling)，以及全连接层(Fully connected</strong>。虽然仅用卷积层也有可能构建出很好的神经网络，大部分的神经网络架构师依然会添加池化层和全连接层。幸运的是，池化层和全连接层要比卷积层更容易设计。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-2271c5e0b5e7e42e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="池化层：Pooling-Layer"><a href="#池化层：Pooling-Layer" class="headerlink" title="池化层：Pooling Layer"></a>池化层：Pooling Layer</h1><p>除了卷积层，卷积网络也经常使用池化层来缩减模型的大小，提高计算速度，同时提高所提取特征的鲁棒性。</p><h2 id="Max-Pooling-最大化池"><a href="#Max-Pooling-最大化池" class="headerlink" title="Max Pooling: 最大化池"></a>Max Pooling: 最大化池</h2><p>最大化池的示例如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-231d3cf598eae81f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>通过选取2x2区域内的最大值，映射到一个2x2的矩阵当中。而最大化池的超参数包括filter的大小，本文中为2，以及步幅stride的大小，本文为2。</p><p>对最大化池的直观解释：我们可以把右边的4x4区域看作是某些特征的集合，那么数字大意味着可能提取了一些特定特征。比如左上区域为9的整个特征可能是一个猫眼探测器。因此最大化池操作的功能就是只要在任何一个象限内提取到某个特征，它都会保留在最大化池的输出中。因此最大化运算的实际作用是如果在过滤器中提取到某个特征，那么保留其最大值；如果某个象限没有提取到特征，那么其中的最大值也还是会很小。而需要承认的是，人们使用最大化池的主要原因是此方法在很多实验中表现很好。另外最大化池有趣的一点是，它有一组超参数，但是并没有参数需要学习。一旦确定了f和s，那么就固定了。</p><p>另外，对于最大化池的输出，之前卷积输出的公式也适用于最大化池。以一个3x3的过滤器为例：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-21b37e65798b205e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>本例针对的是二维的输入，如果输入是三维的，那么就分信道执行，对每个信道执行相同的最大化操作。以上例为具体例子，如果我们的输入信道为n_c，那么输出为3x3xn_c。</p><h2 id="Average-Pooling-平均池化"><a href="#Average-Pooling-平均池化" class="headerlink" title="Average Pooling: 平均池化"></a>Average Pooling: 平均池化</h2><p>平均池化选取的不是区域的最大值，而是平均值，不过这种池化方法并不常用。当然，例外的是对于很深的深层神经网络来说，我们可以用平均池化来分解规模为7x7x1000的网络表示层，在整个空间求平均值，得到1x1x1000的输出。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-4677b61194940de7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>在做pooling时，往往很少用到超参数padding。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-289b4ce9132362e5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>另外，注意到pooling仅仅是提取静态属性，因此pooling中没有参数需要学习。而只需要设置超参数，这些超参数的值可以是人为设置的，也可以是通过交叉验证来设置。</p><h1 id="卷积神经网络示例"><a href="#卷积神经网络示例" class="headerlink" title="卷积神经网络示例"></a>卷积神经网络示例</h1><p>下面举一个手写数字识别的常见例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-74c0755a088a3f93.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>可以看到，上述神经网络经历了以下阶段：CONV1-&gt;POOL1-&gt;CONV2-&gt;POOL2-&gt;FC3-&gt;FC4-&gt;Softmax。注意到，实际上计算层数时，将CONV和POOL算作一层，因为POOL中没有参数需要学习，因此不作为单独一层计算。另外可以发现一个规律，随着神经网络越深，发现n_H和n_W越来越小，而通道数量n_C则逐渐变大，这也是卷积网络常见的模式。另一个常用的卷积网络模式如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-01db157ba4575af2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>另外，可以注意到卷积网络有很多超参数。一个原则是：不要自己设置超参数，而是查看文献中别人采用了哪些超参数，选择一个在别人任务中效果很好的架构，它也可能使用于你的任务。</p><p>接下来讲一讲激活值的维数、大小和参数的数量，可以手动计算一下。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-290ddcd54592a0bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>我们注意到以下几点：第一，输入层和池化层没有参数；第二，卷积层的参数相对较少，而更多的参数都存在于神经网络的全连接层；第三，发现随着神经网络的加深，激活值会逐渐减少。如果激活值下降太快，也会影响网络性能。许多卷积网络都具有这些属性和性质。</p><p>总结，一个卷积神经网络的基本模块包括卷积层、池化层和全连接层。许多计算机视觉研究在探索如何把这些基本模块整合起来，构建高效的神经网络。根据经验，找到整合基本构造模块最好的方法就是大量阅读别人的案例。</p><h1 id="为什么选择卷积"><a href="#为什么选择卷积" class="headerlink" title="为什么选择卷积"></a>为什么选择卷积</h1><h2 id="参数共享和稀疏连接"><a href="#参数共享和稀疏连接" class="headerlink" title="参数共享和稀疏连接"></a>参数共享和稀疏连接</h2><p>和只用全连接层相比，卷积层的两个主要优势在于<strong>参数共享和稀疏连接</strong>。举个例子，假设对于下面的输入和输出：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f80e3c253904cc19.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>如果我们只使用全连接层，那么需要的参数大小为3072x4704，约为1400万个参数；而使用卷积，我们的参数大小只需要(5x5+1)x6=156。</p><p>卷积网络参数少有两个原因，一是参数共享，即我们可以在图片的不同区域中使用同样的参数，以便提取特征：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-af95572a5883cc17.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>原因二是使用了稀疏连接。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ecd1fa2389d4ad7d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>给出一个具体的解释：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7a65120579fbb54b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>从上图中，我们发现输出的最左上角上的0，只与36个输入特征的9个相连接，而与其他像素值无关。这就是稀疏连接的概念。</p><p>神经网络通过这两种机制来减少参数，使得我们可以用更小的训练集来训练，从而预防过拟合。另外，卷积神经网络善于捕捉平移不变(translation invariance)。通过观察发现，向右移动两个像素，图片中的猫依然清晰可见。这是因为神经网络的卷积结构使得即使移动几个像素，该图片仍然具有非常相似的特征，应该属于相同的输出标记。这就是卷积网络在计算机视觉任务中表现良好的原因。</p><h2 id="整合"><a href="#整合" class="headerlink" title="整合"></a>整合</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-8c14793776391c7a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="本周作业"><a href="#本周作业" class="headerlink" title="本周作业"></a>本周作业</h1><h2 id="Convolutional-Neural-Networks-Step-by-Step"><a href="#Convolutional-Neural-Networks-Step-by-Step" class="headerlink" title="Convolutional Neural Networks: Step by Step"></a>Convolutional Neural Networks: Step by Step</h2><h3 id="1-Packages"><a href="#1-Packages" class="headerlink" title="1- Packages"></a>1- Packages</h3><p>导入包：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><h3 id="2-Outline-of-the-Assignment"><a href="#2-Outline-of-the-Assignment" class="headerlink" title="2- Outline of the Assignment"></a>2- Outline of the Assignment</h3><p>You will be implementing the building blocks of a convolutional neural network! Each function you will implement will have detailed instructions that will walk you through the steps needed:</p><ul><li>Convolution functions, including:<ul><li>Zero Padding</li><li>Convolve window</li><li>Convolution forward</li><li>Convolution backward </li></ul></li><li>Pooling functions, including:<ul><li>Pooling forward</li><li>Create mask </li><li>Distribute value</li><li>Pooling backward </li></ul></li></ul><p>This notebook will ask you to implement these functions from scratch in numpy. In the next notebook, you will use the TensorFlow equivalents of these functions to build the following model:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7048e9ad0ba4f08f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>Note that for every forward function, there is its corresponding backward equivalent. Hence, at every step of your forward module you will store some parameters in a cache. These parameters are used to compute gradients during backpropagation.</p><h3 id="3-Convolutional-Neural-Networks"><a href="#3-Convolutional-Neural-Networks" class="headerlink" title="3- Convolutional Neural Networks"></a>3- Convolutional Neural Networks</h3><p>Although programming frameworks make convolutions easy to use, they remain one of the hardest concepts to understand in Deep Learning. A convolution layer transforms an input volume into an output volume of different size, as shown below.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-282eb6ec56a20d04.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>In this part, you will build every step of the convolution layer. You will first implement two helper functions: one for zero padding and the other for computing the convolution function itself.</p><h4 id="3-1-Zero-Padding"><a href="#3-1-Zero-Padding" class="headerlink" title="3.1- Zero-Padding"></a>3.1- Zero-Padding</h4><p>Zero-padding adds zeros around the border of an image:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-a6d4953e936eab7f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><strong>The main benefits of padding are the following:</strong></p><ul><li>It allows you to use a CONV layer without necessarily shrinking(收缩) the height and width of the volumes. This is important for building deeper networks, since otherwise the height/width would shrink as you go to deeper layers. An important special case is the “same” convolution, in which the height/width is exactly preserved after one layer.</li><li>It helps us keep more of the information at the border of an image. Without padding, very few values at the next layer would be affected by pixels as the edges of an image.</li></ul><p><strong>Exercise</strong>: Implement the following function, which pads all the images of a batch of examples X with zeros. <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html" target="_blank" rel="noopener">Use np.pad</a>. Note if you want to pad the array “a” of shape (5,5,5,5,5) with <code>pad = 1</code> for the 2nd dimension, <code>pad = 3</code> for the 4th dimension and <code>pad = 0</code> for the rest, you would do:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = np.pad(a, ((<span class="number">0</span>,<span class="number">0</span>), (<span class="number">1</span>,<span class="number">1</span>), (<span class="number">0</span>,<span class="number">0</span>), (<span class="number">3</span>,<span class="number">3</span>), (<span class="number">0</span>,<span class="number">0</span>)), <span class="string">'constant'</span>, constant_values = (..,..))</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">zero_pad</span><span class="params">(X, pad)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, </span></span><br><span class="line"><span class="string">    as illustrated in Figure 1.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images</span></span><br><span class="line"><span class="string">    pad -- integer, amount of padding around each image on vertical and horizontal dimensions</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    X_pad = np.pad(X, ((<span class="number">0</span>,<span class="number">0</span>),(pad,pad),(pad,pad),(<span class="number">0</span>,<span class="number">0</span>)), <span class="string">'constant'</span>, constant_values=(<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> X_pad</span><br></pre></td></tr></table></figure><p>示例输出：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-bf19d4ad2bac87e7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="3-2-Single-step-of-convolution"><a href="#3-2-Single-step-of-convolution" class="headerlink" title="3.2- Single step of convolution"></a>3.2- Single step of convolution</h4><p>In this part, implement a single step of convolution, in which you apply the filter to a single position of the input. This will be used to build a convolutional unit, which:</p><ul><li>Takes an input volume</li><li>Applies a filter at every position of the input</li><li>Outputs another volume(usually of different size)</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/8636110-0c33896407f617d7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>In a computer vision application, each value in the matrix on the left corresponds to a single pixel value, and we convolve a 3x3 filter with the image by multiplying its values element-wise with the original matrix, then summing them up. In this first step of the exercise, you will implement a single step of convolution, corresponding to applying a filter to just one of the positions to get a single real-valued output.</p><p>Later in this notebook, you’ll apply this function to multiple positions of the input to implement the full convolutional operation.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_single_step</span><span class="params">(a_slice_prev, W, b)</span>:</span> <span class="comment">#注意这里的a_slice_prev，需要和W对应维度</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation </span></span><br><span class="line"><span class="string">    of the previous layer.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)</span></span><br><span class="line"><span class="string">    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)</span></span><br><span class="line"><span class="string">    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    <span class="comment"># Element-wise product between a_slice and W. Add bias.</span></span><br><span class="line">    s = a_slice_prev * W + b</span><br><span class="line">    <span class="comment"># Sum over all entries of the volume s</span></span><br><span class="line">    Z = np.sum(s)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Z</span><br></pre></td></tr></table></figure><h4 id="3-3-Convolutional-Neural-Networks-Forward-pass"><a href="#3-3-Convolutional-Neural-Networks-Forward-pass" class="headerlink" title="3.3- Convolutional Neural Networks - Forward pass"></a>3.3- Convolutional Neural Networks - Forward pass</h4><p>In the forward pass, you will take many filters and convolve them on the input. Each ‘convolution’ gives you a 2D matrix output. You will then stack these outputs to get a 3D volume.</p><p><strong>Exercise</strong>: Implement the function below to convolve the filters W on an input activation A_prev. This function takes as input A_prev, the activations output by the previous layer (for a batch of m inputs), F filters/weights denoted by W, and a bias vector denoted by b, where each filter has its own (single) bias. Finally you also have access to the hyperparameters dictionary which contains the stride and the padding.</p><p><strong>Hint</strong>:</p><ol><li>To select a 2x2 slice at the upper left corner of a matrix “a_prev” (shape (5,5,3)), you would do:<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a_slice_prev = a_prev[<span class="number">0</span>:<span class="number">2</span>,<span class="number">0</span>:<span class="number">2</span>,:]</span><br></pre></td></tr></table></figure></li></ol><p>This will be useful when you will define a_slice_prev below, using the start/end indexes you will define.</p><ol><li>To define a_slice you will need to first define its corners vert_start, vert_end, horiz_start and horiz_end. This figure may be helpful for you to find how each of the corner can be defined using h, w, f and s in the code below.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-535f5da4758566ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></li></ol><p><strong>Reminder</strong>: The formulas relating the output shape of the convolution to the input shape is:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-cd8e9f6b48eee975.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>For this exercise, we won’t worry about vectorization, and will just implement everything with for-loops.</p><p>对于此部分，需要注意的是vert_start/vert_end/horiz_start/horiz_end的计算方式。这里我一开始是没有考虑到stride的，大错特错！<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_forward</span><span class="params">(A_prev, W, b, hyperparameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the forward propagation for a convolution function</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)</span></span><br><span class="line"><span class="string">    b -- Biases, numpy array of shape (1, 1, 1, n_C)</span></span><br><span class="line"><span class="string">    hparameters -- python dictionary containing "stride" and "pad"</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    cache -- cache of values needed for the conv_backward() function</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from A_prev's shape (≈1 line)  </span></span><br><span class="line">    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve dimensions from W's shape (≈1 line)</span></span><br><span class="line">    (f, f, n_C_prev, n_C) = W.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve information from "hparameters" (≈2 lines)</span></span><br><span class="line">    stride = hparameters[<span class="string">"stride"</span>]</span><br><span class="line">    pad = hparameters[<span class="string">"pad"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor. (≈2 lines)</span></span><br><span class="line">    n_H = int((n_H_prev-f+<span class="number">2</span>*pad)/stride) + <span class="number">1</span></span><br><span class="line">    n_W = int((n_W_prev-f+<span class="number">2</span>*pad)/stride) + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize the output volume Z with zeros. (≈1 line)</span></span><br><span class="line">    Z = np.zeros((m, n_H, n_W, n_C))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create A_prev_pad by padding A_prev</span></span><br><span class="line">    A_prev_pad = zero_pad(A_prev, pad)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        a_prev_pad = A_prev_pad[i]</span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(n_H):</span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> range(n_W):</span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> range(n_C):</span><br><span class="line">                    <span class="comment"># Find the corners of the current "slice" (≈4 lines) 这里要注意！</span></span><br><span class="line">                    vert_start = h * stride</span><br><span class="line">                    vert_end = vert_start + f</span><br><span class="line">                    horiz_start = w * stride</span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)</span></span><br><span class="line">                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈1 line)</span></span><br><span class="line">                    Z[i,h,w,c] = conv_single_step(a_slice_prev, W[:,:,:,c], b[:,:,:,c])</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Making sure your output shape is correct</span></span><br><span class="line">    <span class="keyword">assert</span>(Z.shape == (m, n_H, n_W, n_C))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Save information in "cache" for the backprop</span></span><br><span class="line">    cache = (A_prev, W, b, hparameters)</span><br></pre></td></tr></table></figure></p><p>Finally, CONV layer should also contain an activation, in which case we would add the following line of code:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Convolve the window to get back one output neuron</span></span><br><span class="line">Z[i, h, w, c] = ...</span><br><span class="line"><span class="comment"># Apply activation</span></span><br><span class="line">A[i, h, w, c] = activation(Z[i, h, w, c])</span><br></pre></td></tr></table></figure></p><h3 id="4-Pooling-layer"><a href="#4-Pooling-layer" class="headerlink" title="4- Pooling layer"></a>4- Pooling layer</h3><p>The pooling (POOL) layer reduces the height and width of the input. It helps reduce computation, as well as helps make feature detectors more invariant to its position in the input. The two types of pooling layers are:</p><ul><li>Max-pooling layer: slides an (f,f) window over the input and stores the max value of the window in the output.</li><li>Average-pooling layer: slides an (f,f) window over the input and stores the average value of the window in the output.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-8e12bb6926cbe828.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></li></ul><p>These pooling layers have no parameters for backpropagation to train. However, they have hyperparameters such as the window size f. This specifies the height and width of the fxf window you would compute a max or average over.</p><h4 id="4-1-Forward-Pooling"><a href="#4-1-Forward-Pooling" class="headerlink" title="4.1- Forward Pooling"></a>4.1- Forward Pooling</h4><p>Now, you are going to implement MAX-POOL and AVG-POOL, in the same function.<br><strong>Reminder</strong>: As there’s no padding, the formulas binding the output shape of the pooling to the input shape is:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ac2760d7166ca719.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool_forward</span><span class="params">(A_prev, hparameters, mode=<span class="string">"max"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the forward pass of the pooling layer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    hparameters -- python dictionary containing "f" and "stride"</span></span><br><span class="line"><span class="string">    mode -- the pooling mode you would like to use, defined as a string ("max" or "average")</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve dimensions from the input shape</span></span><br><span class="line">    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve hyperparameters from "hparameters"</span></span><br><span class="line">    f = hparameters[<span class="string">"f"</span>]</span><br><span class="line">    stride = hparameters[<span class="string">"stride"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define the dimensions of the output</span></span><br><span class="line">    n_H = int(<span class="number">1</span> + (n_H_prev - f) / stride)</span><br><span class="line">    n_W = int(<span class="number">1</span> + (n_W_prev - f) / stride)</span><br><span class="line">    n_C = n_C_prev</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize output matrix A</span></span><br><span class="line">    A = np.zeros((m, n_H, n_W, n_C))      </span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(n_H):</span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> range(n_W):</span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> range(n_C):</span><br><span class="line">                    <span class="comment"># Find the corners of the current "slice" (≈4 lines)</span></span><br><span class="line">                    vert_start = h * stride</span><br><span class="line">                    vert_end = vert_start + f</span><br><span class="line">                    horiz_start = w * stride</span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)</span></span><br><span class="line">                    a_prev_slice = A_prev[i,vert_start:vert_end,horiz_start:horiz_end, c]</span><br><span class="line">                    <span class="comment"># Compute the pooling operation on the slice. Use an if statment to differentiate the modes. Use np.max/np.mean.</span></span><br><span class="line">                    <span class="keyword">if</span> mode == <span class="string">"max"</span>:</span><br><span class="line">                        A[i, h, w, c] = np.max(a_prev_slice)</span><br><span class="line">                    <span class="keyword">elif</span> mode == <span class="string">"average"</span>:</span><br><span class="line">                        A[i, h, w, c] = np.mean(a_prev_slice)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Store the input and hparameters in "cache" for pool_backward()</span></span><br><span class="line">    cache = (A_prev, hparameters)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Making sure your output shape is correct</span></span><br><span class="line">    <span class="keyword">assert</span>(A.shape == (m, n_H, n_W, n_C))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure></p><h3 id="5-Backpropagation-in-convolutional-neural-networks"><a href="#5-Backpropagation-in-convolutional-neural-networks" class="headerlink" title="5- Backpropagation in convolutional neural networks"></a>5- Backpropagation in convolutional neural networks</h3><p>In modern deep learning frameworks, you only have to implement the forward pass, and the framework takes care of the backward pass, so most deep learning engineers don’t need to bother with the details of the backward pass. The backward pass for convolutional networks is complicated. If you wish however, you can work through this optional portion of the notebook to get a sense of what backprop in a convolutional network looks like.</p><p>When in an earlier course you implemented a simple (fully connected) neural network, you used backpropagation to compute the derivatives with respect to the cost to update the parameters. Similarly, in convolutional neural networks you can to calculate the derivatives with respect to the cost in order to update the parameters. The backprop equations are not trivial and we did not derive them in lecture, but we briefly presented them below.</p><h4 id="5-1-Convolutional-layer-backward-pass"><a href="#5-1-Convolutional-layer-backward-pass" class="headerlink" title="5.1- Convolutional layer backward pass"></a>5.1- Convolutional layer backward pass</h4><p>Let’s start by implementing the backward pass for a CONV layer.</p><h4 id="5-1-1-Computing-dA"><a href="#5-1-1-Computing-dA" class="headerlink" title="5.1.1- Computing dA:"></a>5.1.1- Computing dA:</h4><p>This is the formula for computing dA with respect to the cost for a certain filter  Wc and a given training example:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-2e088e8094180b32.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Where Wc is a filter and dZ_hw is a scalar corresponding to the gradient of the cost with respect to the output of the conv layer Z at the hth row and wth column (corresponding to the dot product taken at the ith stride left and jth stride down). Note that at each time, we multiply the the same filter  WcWc  by a different dZ when updating dA. We do so mainly because when computing the forward propagation, each filter is dotted and summed by a different a_slice. Therefore when computing the backprop for dA, we are just adding the gradients of all the a_slices.</p><p>In code, inside the appropriate for-loops, this formula translates into:<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]</span><br></pre></td></tr></table></figure></p><h4 id="5-1-2-Computing-dW"><a href="#5-1-2-Computing-dW" class="headerlink" title="5.1.2- Computing dW:"></a>5.1.2- Computing dW:</h4><p>This is the formula for computing dWc( dWc is the derivative of one filter) with respect to the loss:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-5d3173e6db1e5e5d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>Where a_slice corresponds to the slice which was used to generate the acitivation  Z_ij. Hence, this ends up giving us the gradient for W with respect to that slice. Since it is the same W, we will just add up all such gradients to get dW.</p><p>In code, inside the appropriate for-loops, this formula translates into:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dW[:,:,:,c] += a_slice * dZ[i, h, w, c]</span><br></pre></td></tr></table></figure></p><h4 id="5-1-3-Computing-db"><a href="#5-1-3-Computing-db" class="headerlink" title="5.1.3- Computing db:"></a>5.1.3- Computing db:</h4><p>This is the formula for computing db with respect to the cost for a certain filter W_c:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-29a302c85af0188d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>As you have previously seen in basic neural networks, db is computed by summing $dZ$. In this case, you are just summing over all the gradients of the conv output (Z) with respect to the cost. </p><p>In code, inside the appropriate for-loops, this formula translates into:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db[:,:,:,c] += dZ[i, h, w, c]</span><br></pre></td></tr></table></figure></p><p><strong>Exercise</strong>: Implement the <code>conv_backward</code> function below. You should sum over all the training examples, filters, heights, and widths. You should then compute the derivatives using formulas 1, 2 and 3 above.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_backward</span><span class="params">(dZ, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for a convolution function</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)</span></span><br><span class="line"><span class="string">    cache -- cache of values needed for the conv_backward(), output of conv_forward()</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),</span></span><br><span class="line"><span class="string">               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)</span></span><br><span class="line"><span class="string">    dW -- gradient of the cost with respect to the weights of the conv layer (W)</span></span><br><span class="line"><span class="string">          numpy array of shape (f, f, n_C_prev, n_C)</span></span><br><span class="line"><span class="string">    db -- gradient of the cost with respect to the biases of the conv layer (b)</span></span><br><span class="line"><span class="string">          numpy array of shape (1, 1, 1, n_C)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve information from "cache"</span></span><br><span class="line">    (A_prev, W, b, hparameters) = cache</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from A_prev's shape</span></span><br><span class="line">    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from W's shape</span></span><br><span class="line">    (f, f, n_C_prev, n_C) = W.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve information from "hparameters"</span></span><br><span class="line">    stride = hparameters[<span class="string">"stride"</span>]</span><br><span class="line">    pad = hparameters[<span class="string">"pad"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from dZ's shape</span></span><br><span class="line">    (m, n_H, n_W, n_C) = dZ.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize dA_prev, dW, db with the correct shapes</span></span><br><span class="line">    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                         </span><br><span class="line">    dW = np.zeros((f, f, n_C_prev, n_C))</span><br><span class="line">    db = np.zeros((<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,n_C))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Pad A_prev and dA_prev</span></span><br><span class="line">    A_prev_pad = zero_pad(A_prev, pad)</span><br><span class="line">    dA_prev_pad = zero_pad(dA_prev, pad)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                       <span class="comment"># loop over the training examples</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># select ith training example from A_prev_pad and dA_prev_pad</span></span><br><span class="line">        a_prev_pad = A_prev_pad[i,:,:,:]</span><br><span class="line">        da_prev_pad = dA_prev_pad[i,:,:,:]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(n_H):                   <span class="comment"># loop over vertical axis of the output volume</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> range(n_W):               <span class="comment"># loop over horizontal axis of the output volume</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> range(n_C):           <span class="comment"># loop over the channels of the output volume</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Find the corners of the current "slice"</span></span><br><span class="line">                    vert_start = h * stride</span><br><span class="line">                    vert_end = vert_start + f </span><br><span class="line">                    horiz_start = w * stride</span><br><span class="line">                    horiz_end = horiz_start + f </span><br><span class="line"></span><br><span class="line">                    <span class="comment"># Use the corners to define the slice from a_prev_pad</span></span><br><span class="line">                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end,:]</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># Update gradients for the window and the filter's parameters using the code formulas given above</span></span><br><span class="line">                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]</span><br><span class="line">                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]</span><br><span class="line">                    db[:,:,:,c] += dZ[i, h, w, c]</span><br><span class="line">                    </span><br><span class="line">        <span class="comment"># Set the ith training example's dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])</span></span><br><span class="line">        dA_prev_pad[i] = da_prev_pad[pad:-pad,pad:-pad,:]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Making sure your output shape is correct</span></span><br><span class="line">    <span class="keyword">assert</span>(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure></p><h3 id="5-2-Pooling-layer-backward-pass"><a href="#5-2-Pooling-layer-backward-pass" class="headerlink" title="5.2- Pooling layer - backward pass"></a>5.2- Pooling layer - backward pass</h3><p>Next, let’s implement the backward pass for the pooling layer, starting with the MAX-POOL layer. Even though a pooling layer has no parameters for backprop to update, you still need to backpropagation the gradient through the pooling layer in order to compute gradients for layers that came before the pooling layer.</p><h4 id="5-2-1-Max-pooling-backward-pass"><a href="#5-2-1-Max-pooling-backward-pass" class="headerlink" title="5.2.1- Max pooling - backward pass"></a>5.2.1- Max pooling - backward pass</h4><p>Before jumping into the backpropagation of the pooling layer, you are going to build a helper function called create_mask_from_window() which does the following:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-677db6a2620243b0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>As you can see, this function creates a “mask” matrix which keeps track of where the maximum of the matrix is. True (1) indicates the position of the maximum in X, the other entries are False (0). You’ll see later that the backward pass for average pooling will be similar to this but using a different mask.</p><p><strong>Exercise</strong>: Implement <code>create_mask_from_window()</code>. This function will be helpful for pooling backward.<br>Hints:</p><ul><li><a href="">np.max()</a> may be helpful. It computes the maximum of an array.</li><li><p>If you have a matrix X and a scalar x: <code>A = (X == x)</code> will return a matrix A of the same size as X such that:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A[i,j] = True if X[i,j] = x</span><br><span class="line">A[i,j] = False if X[i,j] != x</span><br></pre></td></tr></table></figure></li><li><p>Here, you don’t need to consider cases where there are several maxima in a matrix.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_mask_from_window</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates a mask from an input matrix x, to identify the max entry of x.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- Array of shape (f, f)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    mask = (x == np.max(x))</span><br><span class="line">    <span class="keyword">return</span> mask</span><br></pre></td></tr></table></figure></li></ul><p>Why do we keep track of the position of the max? It’s because this is the input value that ultimately influenced the output, and therefore the cost. Backprop is computing gradients with respect to the cost, so anything that influences the ultimate cost should have a non-zero gradient. So, backprop will “propagate” the gradient back to this particular input value that had influenced the cost.</p><h4 id="5-2-2-Average-pooling-backward-pass"><a href="#5-2-2-Average-pooling-backward-pass" class="headerlink" title="5.2.2- Average pooling - backward pass"></a>5.2.2- Average pooling - backward pass</h4><p>In max pooling, for each input window, all the “influence” on the output came from a single input value—the max. In average pooling, every element of the input window has equal influence on the output. So to implement backprop, you will now implement a helper function that reflects this.</p><p>For example if we did average pooling in the forward pass using a 2x2 filter, then the mask you’ll use for the backward pass will look like:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-27ae8fc6c24d23b1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>This implies that each position in the dZ matrix contributes equally to output because in the forward pass, we took an average.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distribute_value</span><span class="params">(dz, shape)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Distributes the input value in the matrix of dimension shape</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dz -- input scalar</span></span><br><span class="line"><span class="string">    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a -- Array of size (n_H, n_W) for which we distributed the value of dz</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from shape (≈1 line)</span></span><br><span class="line">    (n_H, n_W) = shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute the value to distribute on the matrix (≈1 line)</span></span><br><span class="line">    average = dz / (n_H * n_W)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create a matrix where every entry is the "average" value (≈1 line)</span></span><br><span class="line">    a = np.full(shape, average)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> a</span><br></pre></td></tr></table></figure><p>np.full(shape, value)函数可以将array的每个值初始化为value，且array的维度设置为shape。</p><h4 id="5-2-3-Putting-it-together-Pooling-backward"><a href="#5-2-3-Putting-it-together-Pooling-backward" class="headerlink" title="5.2.3- Putting it together: Pooling backward"></a>5.2.3- Putting it together: Pooling backward</h4><p>You now have everything you need to compute backward propagation on a pooling layer.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pool_backward</span><span class="params">(dA, cache, mode = <span class="string">"max"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the backward pass of the pooling layer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A</span></span><br><span class="line"><span class="string">    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters </span></span><br><span class="line"><span class="string">    mode -- the pooling mode you would like to use, defined as a string ("max" or "average")</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve information from cache (≈1 line)</span></span><br><span class="line">    (A_prev, hparameters) = cache</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve hyperparameters from "hparameters" (≈2 lines)</span></span><br><span class="line">    stride = hparameters[<span class="string">'stride'</span>]</span><br><span class="line">    f = hparameters[<span class="string">'f'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)</span></span><br><span class="line">    m, n_H_prev, n_W_prev, n_C_prev = np.shape(A_prev)</span><br><span class="line">    m, n_H, n_W, n_C = np.shape(dA)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize dA_prev with zeros (≈1 line)</span></span><br><span class="line">    dA_prev = np.zeros(np.shape(A_prev))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                       <span class="comment"># loop over the training examples</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># select training example from A_prev (≈1 line)</span></span><br><span class="line">        a_prev = A_prev[i]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> range(n_H):                   <span class="comment"># loop on the vertical axis</span></span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> range(n_W):               <span class="comment"># loop on the horizontal axis</span></span><br><span class="line">                <span class="keyword">for</span> c <span class="keyword">in</span> range(n_C):           <span class="comment"># loop over the channels (depth)</span></span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Find the corners of the current "slice" (≈4 lines)</span></span><br><span class="line">                    vert_start = h * stride</span><br><span class="line">                    vert_end = vert_start + f</span><br><span class="line">                    horiz_start = w * stride</span><br><span class="line">                    horiz_end = horiz_start + f</span><br><span class="line">                    </span><br><span class="line">                    <span class="comment"># Compute the backward propagation in both modes.</span></span><br><span class="line">                    <span class="keyword">if</span> mode == <span class="string">"max"</span>:</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment"># Use the corners and "c" to define the current slice from a_prev (≈1 line)</span></span><br><span class="line">                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment"># Create the mask from a_prev_slice (≈1 line)</span></span><br><span class="line">                        mask = create_mask_from_window(a_prev_slice)</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment"># Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)</span></span><br><span class="line">                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += np.multiply(mask, dA[i, h, w, c])</span><br><span class="line">                        </span><br><span class="line">                    <span class="keyword">elif</span> mode == <span class="string">"average"</span>:</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment"># Get the value a from dA (≈1 line)</span></span><br><span class="line">                        da = dA[i, h, w, c]</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment"># Define the shape of the filter as fxf (≈1 line)</span></span><br><span class="line">                        shape = (f, f)</span><br><span class="line">                        </span><br><span class="line">                        <span class="comment"># Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (≈1 line)</span></span><br><span class="line">                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)</span><br><span class="line">                        </span><br><span class="line">    <span class="comment">### END CODE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Making sure your output shape is correct</span></span><br><span class="line">    <span class="keyword">assert</span>(dA_prev.shape == A_prev.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev</span><br></pre></td></tr></table></figure><h2 id="Convolutional-Neural-Networks-Application"><a href="#Convolutional-Neural-Networks-Application" class="headerlink" title="Convolutional Neural Networks: Application"></a>Convolutional Neural Networks: Application</h2><h3 id="1-0-Tensorflow-model"><a href="#1-0-Tensorflow-model" class="headerlink" title="1.0- Tensorflow model"></a>1.0- Tensorflow model</h3><p>导入包：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.framework <span class="keyword">import</span> ops</span><br><span class="line"><span class="keyword">from</span> cnn_utils <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><p>导入数据：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Loading the data (signs)</span></span><br><span class="line">X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()</span><br></pre></td></tr></table></figure></p><p>As a reminder, the SIGNS dataset is a collection of 6 signs representing numbers from 0 to 5.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c4705e853cb55280.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>The next cell will show you an example of a labelled image in the dataset. Feel free to change the value of index below and re-run to see different examples.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of a picture</span></span><br><span class="line">index = <span class="number">6</span></span><br><span class="line">plt.imshow(X_train_orig[index])</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"y = "</span> + str(np.squeeze(Y_train_orig[:, index])))</span><br></pre></td></tr></table></figure></p><p>In Course 2, you had built a fully-connected network for this dataset. But since this is an image dataset, it is more natural to apply a ConvNet to it.</p><p>To get started, let’s examine the shapes of your data.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">X_train = X_train_orig/<span class="number">255.</span></span><br><span class="line">X_test = X_test_orig/<span class="number">255.</span></span><br><span class="line">Y_train = convert_to_one_hot(Y_train_orig, <span class="number">6</span>).T</span><br><span class="line">Y_test = convert_to_one_hot(Y_test_orig, <span class="number">6</span>).T</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"number of training examples = "</span> + str(X_train.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"number of test examples = "</span> + str(X_test.shape[<span class="number">0</span>]))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X_train shape: "</span> + str(X_train.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Y_train shape: "</span> + str(Y_train.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"X_test shape: "</span> + str(X_test.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Y_test shape: "</span> + str(Y_test.shape))</span><br><span class="line">conv_layers = &#123;&#125;</span><br></pre></td></tr></table></figure></p><h3 id="1-1-Create-placeholders"><a href="#1-1-Create-placeholders" class="headerlink" title="1.1- Create placeholders"></a>1.1- Create placeholders</h3><p>TensorFlow requires that you create placeholders for the input data that will be fed into the model when running the session.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_placeholders</span><span class="params">(n_H0, n_W0, n_C0, n_y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates the placeholders for the tensorflow session.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    n_H0 -- scalar, height of an input image</span></span><br><span class="line"><span class="string">    n_W0 -- scalar, width of an input image</span></span><br><span class="line"><span class="string">    n_C0 -- scalar, number of channels of the input</span></span><br><span class="line"><span class="string">    n_y -- scalar, number of classes</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype "float"</span></span><br><span class="line"><span class="string">    Y -- placeholder for the input labels, of shape [None, n_y] and dtype "float"</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈2 lines)</span></span><br><span class="line">    X = tf.placeholder(shape=[<span class="keyword">None</span>,n_H0,n_W0,n_C0],dtype=<span class="string">"float"</span>)</span><br><span class="line">    Y = tf.placeholder(shape=[<span class="keyword">None</span>,n_y],dtype=<span class="string">"float"</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X, Y</span><br></pre></td></tr></table></figure></p><h3 id="1-2-Initialize-parameters"><a href="#1-2-Initialize-parameters" class="headerlink" title="1.2- Initialize parameters"></a>1.2- Initialize parameters</h3><p>You will initialize weights/filters W1 and W2 using tf.contrib.layers.xavier_initializer(seed = 0). You don’t need to worry about bias variables as you will soon see that TensorFlow functions take care of the bias. Note also that you will only initialize the weights/filters for the conv2d functions. TensorFlow initializes the layers for the fully connected part automatically. We will talk more about that later in this assignment.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Initializes weight parameters to build a neural network with tensorflow. The shapes are:</span></span><br><span class="line"><span class="string">                        W1 : [4, 4, 3, 8]</span></span><br><span class="line"><span class="string">                        W2 : [2, 2, 8, 16]</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- a dictionary of tensors containing W1, W2</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    tf.set_random_seed(<span class="number">1</span>)                              <span class="comment"># so that your "random" numbers match ours</span></span><br><span class="line">        </span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 2 lines of code)</span></span><br><span class="line">    W1 = tf.get_variable(name=<span class="string">"W1"</span>, shape=[<span class="number">4</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">8</span>], initializer=tf.contrib.layers.xavier_initializer(seed=<span class="number">0</span>))</span><br><span class="line">    W2 = tf.get_variable(name=<span class="string">"W2"</span>, shape=[<span class="number">2</span>,<span class="number">2</span>,<span class="number">8</span>,<span class="number">16</span>], initializer=tf.contrib.layers.xavier_initializer(seed=<span class="number">0</span>))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p><strong>注意</strong>tf.Variable和tf.get_variable的区别：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.Variable(initial_value=None, trainable=True, collections=None, validate_shape=True, caching_device=None, name=None, variable_def=None, dtype=None, expected_shape=None, import_scope=None)</span><br><span class="line"></span><br><span class="line">tf.get_variable(name, shape=None, dtype=None, initializer=None, regularizer=None, trainable=True, collections=None, caching_device=None, partitioner=None, validate_shape=True, custom_getter=None)</span><br></pre></td></tr></table></figure></p><h3 id="1-3-Forward-propagation"><a href="#1-3-Forward-propagation" class="headerlink" title="1.3- Forward propagation"></a>1.3- Forward propagation</h3><p>In TensorFlow, there are built-in functions that carry out the convolution steps for you.</p><ul><li><strong>tf.nn.conv2d(X,W1, strides = [1,s,s,1], padding = ‘SAME’):</strong> given an input X and a group of filters W1, this function convolves W1’s filters on X. The third input ([1,f,f,1]) represents the strides for each dimension of the input (m, n_H_prev, n_W_prev, n_C_prev). You can read the full documentation <a href="https://www.tensorflow.org/api_docs/python/tf/nn/conv2d" target="_blank" rel="noopener">here</a></li><li><strong>tf.nn.max_pool(A, ksize = [1,f,f,1], strides = [1,s,s,1], padding = ‘SAME’):</strong> given an input A, this function uses a window of size (f, f) and strides of size (s, s) to carry out max pooling over each window. You can read the full documentation <a href="https://www.tensorflow.org/api_docs/python/tf/nn/max_pool" target="_blank" rel="noopener">here</a></li><li><strong>tf.nn.relu(Z1):</strong> computes the elementwise ReLU of Z1 (which can be any shape). You can read the full documentation <a href="https://www.tensorflow.org/api_docs/python/tf/nn/relu" target="_blank" rel="noopener">here.</a></li><li><strong>tf.contrib.layers.flatten(P)</strong>: given an input P, this function flattens each example into a 1D vector it while maintaining the batch-size. It returns a flattened tensor with shape [batch_size, k]. You can read the full documentation <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/flatten" target="_blank" rel="noopener">here.</a></li><li><strong>tf.contrib.layers.fully_connected(F, num_outputs):</strong> given a the flattened input F, it returns the output computed using a fully connected layer. You can read the full documentation <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected" target="_blank" rel="noopener">here.</a></li></ul><p>In the last function above (<code>tf.contrib.layers.fully_connected</code>), the fully connected layer automatically initializes weights in the graph and keeps on training them as you train the model. Hence, you did not need to initialize those weights when initializing the parameters. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the forward propagation for the model:</span></span><br><span class="line"><span class="string">    CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset placeholder, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "W2"</span></span><br><span class="line"><span class="string">                  the shapes are given in initialize_parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z3 -- the output of the last LINEAR unit</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Retrieve the parameters from the dictionary "parameters" </span></span><br><span class="line">    W1 = parameters[<span class="string">'W1'</span>]</span><br><span class="line">    W2 = parameters[<span class="string">'W2'</span>]</span><br><span class="line"></span><br><span class="line">    Z1 = tf.nn.conv2d(X,W1,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">'SAME'</span>)</span><br><span class="line">    A1 = tf.nn.relu(Z1)</span><br><span class="line">    P1 = tf.nn.max_pool(A1,ksize=[<span class="number">1</span>,<span class="number">8</span>,<span class="number">8</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">8</span>,<span class="number">8</span>,<span class="number">1</span>],padding=<span class="string">'SAME'</span>)</span><br><span class="line">    Z2 = tf.nn.conv2d(P1,W2,strides=[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>],padding=<span class="string">'SAME'</span>)</span><br><span class="line">    A2 = tf.nn.relu(Z2)</span><br><span class="line">    P2 = tf.nn.max_pool(A2,ksize=[<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>],strides=[<span class="number">1</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">1</span>],padding=<span class="string">'SAME'</span>)</span><br><span class="line">    P2 = tf.contrib.layers.flatter(P2)</span><br><span class="line">    Z2 = tf.contrib.layers.fully_connected(P2,<span class="number">6</span>,activation_fn=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure><h3 id="1-4-Compute-cost"><a href="#1-4-Compute-cost" class="headerlink" title="1.4- Compute cost"></a>1.4- Compute cost</h3><p>Implement the compute cost function below. You might find these two functions helpful:</p><ul><li>tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y): computes the softmax entropy loss. This function both computes the softmax activation function as well as the resulting loss. You can check the full documentation here.</li><li>tf.reduce_mean: computes the mean of elements across dimensions of a tensor. Use this to sum the losses over all the examples to get the overall cost. You can check the full documentation here.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(Z3, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the cost</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector placeholder, same shape as Z3</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost - Tensor of the cost function</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z3,labels=Y))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><h2 id="1-4-Model"><a href="#1-4-Model" class="headerlink" title="1.4- Model"></a>1.4- Model</h2><p>Finally you will merge the helper functions you implemented above to build a model. You will train it on the SIGNS dataset.</p><p>You have implemented random_mini_batches() in the Optimization programming assignment of course 2. Remember that this function returns a list of mini-batches.</p><p>The model below should:</p><ul><li>create placeholders</li><li>initialize parameters</li><li>forward propagate</li><li>compute the cost</li><li>create an optimizer</li></ul><p>Finally you will create a session and run a for loop for num_epochs, get the mini-batches, and then for each mini-batch you will optimize the function.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_test, Y_test, learning_rate = <span class="number">0.009</span>, num_epochs = <span class="number">100</span>, minibatch_size = <span class="number">64</span>, print_cost = True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a three-layer ConvNet in Tensorflow:</span></span><br><span class="line"><span class="string">    CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X_train -- training set, of shape (None, 64, 64, 3)</span></span><br><span class="line"><span class="string">    Y_train -- test set, of shape (None, n_y = 6)</span></span><br><span class="line"><span class="string">    X_test -- training set, of shape (None, 64, 64, 3)</span></span><br><span class="line"><span class="string">    Y_test -- test set, of shape (None, n_y = 6)</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the optimization</span></span><br><span class="line"><span class="string">    num_epochs -- number of epochs of the optimization loop</span></span><br><span class="line"><span class="string">    minibatch_size -- size of a minibatch</span></span><br><span class="line"><span class="string">    print_cost -- True to print the cost every 100 epochs</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    train_accuracy -- real number, accuracy on the train set (X_train)</span></span><br><span class="line"><span class="string">    test_accuracy -- real number, testing accuracy on the test set (X_test)</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    ops.reset_default_graph()</span><br><span class="line">    tf.set_random_seed(<span class="number">1</span>)</span><br><span class="line">    seed = <span class="number">3</span></span><br><span class="line">    (m, n_H0, n_W0, n_C0) = X_train.shape</span><br><span class="line">    n_y = Y_train.shape[<span class="number">1</span>]</span><br><span class="line">    costs = []</span><br><span class="line"></span><br><span class="line">    X, Y = create_placeholders(n_H0,n_W0,n_C0,n_y)</span><br><span class="line">    parameters = initialize_parameters()</span><br><span class="line">    Z3 = forward_propagation(X,parameters)</span><br><span class="line">    cost = compute_cost(Z3)</span><br><span class="line">    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        sess.run(init)</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">            minibatch_cost = <span class="number">0</span></span><br><span class="line">            num_minibatches = ini(m / minibatch_size)</span><br><span class="line">            seed = seed + <span class="number">1</span></span><br><span class="line">            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> minibatch <span class="keyword">in</span> minibatches:</span><br><span class="line">                (minibatch_X, minibatch_Y) = minibatch</span><br><span class="line">                temp_cost, _ = sess.run([cost, optimizer], feed_dict=&#123;X:minibatch_X,Y:minibatch_Y&#125;)</span><br><span class="line">        <span class="keyword">if</span> print_cost == <span class="keyword">True</span> <span class="keyword">and</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">print</span> (<span class="string">"Cost after epoch %i: %f"</span> % (epoch, minibatch_cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost == <span class="keyword">True</span> <span class="keyword">and</span> epoch % <span class="number">1</span> == <span class="number">0</span>:</span><br><span class="line">                costs.append(minibatch_cost)</span><br><span class="line"></span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    predict_op = tf.argmax(Z3, <span class="number">1</span>)</span><br><span class="line">    correct_prediction = tf.equal(predict_op, tf.argmax(Y, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>))</span><br><span class="line">    print(accuracy)</span><br><span class="line">    train_accuracy = accuracy.eval(&#123;X: X_train, Y: Y_train&#125;)</span><br><span class="line">    test_accuracy = accuracy.eval(&#123;X: X_test, Y: Y_test&#125;)</span><br><span class="line">    print(<span class="string">"Train Accuracy:"</span>, train_accuracy)</span><br><span class="line">    print(<span class="string">"Test Accuracy:"</span>, test_accuracy)</span><br><span class="line">                </span><br><span class="line">    <span class="keyword">return</span> train_accuracy, test_accuracy, parameters</span><br></pre></td></tr></table></figure><p>如果将之前的max_pool的ksize和strides都改为3x3，结果会得到优化，经过某次训练得到结果：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c77ae96f74c7b4d1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>显然，结果过拟合了，即结果具有高方差。解决此问题的方法是，要么使用正则化，要么加大训练集的数量。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;接下来的四周为计算机视觉——卷积神经网络的内容。&lt;/p&gt;
&lt;h1 id=&quot;计算机视觉&quot;&gt;&lt;a href=&quot;#计算机视觉&quot; class=&quot;headerlink&quot; title=&quot;计算机视觉&quot;&gt;&lt;/a&gt;计算机视觉&lt;/h1&gt;&lt;p&gt;常见的计算机视觉问题包括图像分类、目标检测、神经网络
      
    
    </summary>
    
      <category term="深度学习" scheme="https://github.com/DesmonDay/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://github.com/DesmonDay/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>第9周-机器学习ML策略(2)</title>
    <link href="https://github.com/DesmonDay/2019/04/19/deep-learningw9/"/>
    <id>https://github.com/DesmonDay/2019/04/19/deep-learningw9/</id>
    <published>2019-04-19T07:11:05.000Z</published>
    <updated>2019-04-20T18:18:49.595Z</updated>
    
    <content type="html"><![CDATA[<h1 id="进行误差分析：Carrying-out-error-analysis"><a href="#进行误差分析：Carrying-out-error-analysis" class="headerlink" title="进行误差分析：Carrying out error analysis"></a>进行误差分析：Carrying out error analysis</h1><p>误差分析在我们实际进行机器学习项目中是非常重要的一步，它有利于提高我们的模型表现，使我们的算法逼近人类水平。</p><h2 id="Look-at-dev-examples-to-evaluate-ideas"><a href="#Look-at-dev-examples-to-evaluate-ideas" class="headerlink" title="Look at dev examples to evaluate ideas"></a>Look at dev examples to evaluate ideas</h2><p>假设我们有这样的一个例子，我们要做一个猫分类的分类器，但是其准确率只有90%，也就是错误率为10%。在分析了分类错误的例子后，发现图片将一些狗狗也分类成了猫，因此，我们是需要花上数月训练出一个新的狗分类模型，还是通过其他方法呢？<br><img src="https://upload-images.jianshu.io/upload_images/8636110-0e2f83c5f04414c5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>误差分析流程：（尽管有些人会鄙视人工统计做法，但这种误差分析的流程可以节省时间来确定哪些是最有希望的方向，某个方向是否有价值等等）</p><ul><li>Get ~100 mislabeled dev set examples.</li><li>Count up how many are dogs.</li></ul><p>分析结果：如果只有5%的出错例子是狗，那么我们训练的狗分类器实际上作用不大，而只是将误差从10%下降到9.5%，但至少改善了我们算法的性能上限。再假设我们发现50%的图片是狗的照片，那么我们花时间去解决狗的图片会比较值得，因为我们的误差降到了5%。</p><h2 id="Evaluate-multiple-ideas-in-parallel"><a href="#Evaluate-multiple-ideas-in-parallel" class="headerlink" title="Evaluate multiple ideas in parallel"></a>Evaluate multiple ideas in parallel</h2><p>假设我们有针对猫分类器的多种改善算法性能的想法，吴老师用的是表格来表示人工过一遍我们要评估的想法。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-caf8cb8796b21c6a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在这个人工统计过程中可能会遇到其他的印象因素，比如因为图片加了滤镜导致分了错误，那么也可以再添加一列进行统计。这样的统计方式能够快速地<strong>帮助你确定算法优化的方向</strong>。通过计算不同错误标记类型占总数的百分比，可以帮助我们发现哪些问题需要优先解决，或者给你构思新优化方向的灵感。</p><p>在做误差分析时，有时我们会注意到验证集当中有些样本被错误标记了，要怎么做呢？请看下一节。</p><h1 id="清楚标注错误的数据：Cleaning-up-incorrectly-labeled-data"><a href="#清楚标注错误的数据：Cleaning-up-incorrectly-labeled-data" class="headerlink" title="清楚标注错误的数据：Cleaning up incorrectly labeled data"></a>清楚标注错误的数据：Cleaning up incorrectly labeled data</h1><p>在观察数据时，当发现一些数据的标签是错误的，是否需要花时间对标签进行修正呢？</p><p>首先我们可以观察训练集，如果发现这些错误是随机的，那么可以不予理会（只要数据集足够大，那么实际误差也会比较小），当然修正它也完全可以：DL algoritms are quite robust to <strong>random errors</strong> in the training set. 即深度学习算法对训练集中的随机误差是十分鲁棒的。但对于系统误差而言，深度学习算法则并不鲁棒。即如果我们始终将白色的狗标记为猫，那么我们的分类器就会一直学习错误的结果。</p><p>如果我们在验证集或者测试集上发现标记错误的样例，应该怎么做呢？我们也可以做误差分析，计算错误标记的例子数量。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-da04376f26606d58.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>从表格中知道，一共有6%的错误标记例子，那么是否需要或者值得花时间对这些标记进行修正呢？吴恩达老师的建议是，如果这些例子验证影响了分类器在验证集上的表现，那么就需要对其进行修正；反正，则不应该花费宝贵的时间进行人工修正。</p><p>举一个例子进行解释：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-4a79da8bd8e032f3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>对于左边的例子，我们的总体错误率为10%，而因为标记错误导致的错误率为0.6%，那么当务之急不是去修改标记，而是针对9.6%去进行算法的改善；对于右边的例子，总体错误率为2%，而因为标记错误导致的错误率为0.6%，此时修改错误标记的样本则变得很有必要。另外需要记住的是，设置验证集的目的是为了帮助我们在分类器A和B之间进行选择。</p><p>在对验证集/测试集进行标记的修正时，我们需要注意以下几点：</p><ol><li>Apply same process to your dev and test sets tot make sure they continue to come from the same distribution.（我们在验证集上设置目标，并且希望推广到测试集上进行目标的推广，因此两个数据集的分布需要一致。所以如果我们要对验证集进行样例修正，那么也需要对测试集这么做。）</li><li>Consider examining examples your algorithm got right as well as ones it got wrong.（我们不仅需要检查算法出错的例子，也需要再次检查它做对的例子，因为这些例子也可能是标记错误的，否则是不公平的。不过这种做法通常不采用，因此可能有98%做对，而2%做错，而检查98%的例子所花的时间会很多。）</li><li>Train and dev/test data may come from slightly different distributions.（如果我们只决定修正验证集和测试集中的标签，因为它们通常比训练集小，那么可能会在数据集的分布上有细微的差别。）</li></ol><p>因此，亲自检查数据也是很有用的，能够帮助我们确定要优秀尝试哪些方法或者想法。</p><h1 id="快速搭建你的第一个系统，并进行迭代"><a href="#快速搭建你的第一个系统，并进行迭代" class="headerlink" title="快速搭建你的第一个系统，并进行迭代"></a>快速搭建你的第一个系统，并进行迭代</h1><p>在这一节中，吴恩达老师讲解了快速搭建系统并且迭代这一步的重要性。他举了语音识别的例子，比如我们需要搭建一个语音识别例子，但是我们不清楚应该在哪个方向进行改进。可能的方向如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-acd65f5c48cb9d3c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>因此，吴恩达老师建议我们，先不要想着在什么方向上下手，而是迅速开始搭建系统进行训练和迭代，再根据实际的算法表现来选定方向。整个步骤流程如下：</p><ol><li>Set up dev/test set and metric.</li><li>Build initial system quickly.</li><li>Use Bias/Variance analysis &amp; Error analysis to prioritize next steps.通过这一步找出最有希望的方向。</li></ol><p>如果我们在某个应用领域上很有经验，那么上述建议的适用程度则低一些。当在这个应用领域上有很多学术文献，这些文献处理的问题和我们要解决的几乎完全相同时，比如人脸识别有很多学术文献，那么我们完全可以从现有文献出发，一开始就搭建复杂的系统。</p><p>很多时候，大多数团队要么搭建了过于复杂的系统，要么搭建了过于简单的系统。因此，如果我们希望将机器学习算法应用到新的应用程序里，我们的目标是弄出能用的系统，而不是发明全新的机器学习算法，那么我们可以快速搭建你的第一个系统，并进行迭代，用他们的算法表现来做误差分析之类的，根据分析结果确定下一步优先要做的方向。</p><h1 id="在不同的分布上进行训练和测试"><a href="#在不同的分布上进行训练和测试" class="headerlink" title="在不同的分布上进行训练和测试"></a>在不同的分布上进行训练和测试</h1><p>深度学习算法对训练数据有着很大的需求，当我们收集到足够多带标签的数据构成训练集时，算法效果最好。这也导致很多团队用尽一切办法去收集大量数据放置到训练集中，即便这些数据与验证集、测试集来自不同的分布。在深度学习时代，越来越多的团队使用来自和验证集、测试集分布不同的数据来训练。这里有一些做法可以用来处理训练集和测试集存在差异的情况。</p><p>再次取猫分类的例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7aafc877af5260c4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>然而，来自手机app的图片只有10,000张，来自网页的图片有200,000张。而我们又不愿意直接用10,000张图片来训练，因为这样训练集的规模就太小了。而网页的图片很多，但是它又不是我们原本的分布。我们有以下两个选择：</p><ol><li><p>将所有图片随机打乱后，再划分训练集、验证集和测试集。比如设置训练集为205,000张图片，验证集为2,500张图片，测试集为2,500张图片。然而，由于我们的目的是设置验证集用来不断逼近我们的目标，但是实际上，验证集中有2381张来自网页的图片，而只有119张来自手机。也就是说，如果这样设计，那么我们会花大量时间用来优化来自网页下载的图片（不符合我们的目的）。</p></li><li><p>选择二是训练集一共有200,000张来自网页的图和5,000张来自手机的图片，而验证集和测试集都是2,500张来自手机的图片。这样划分数据的好处是我们现在瞄准的目标就是想要处理的目标，即我们的验证集包含的图片都来自手机，这就是我们真正关心的图片分布。因此我们试着搭建一个学习系统，让系统在处理手机上传图片分布时效果良好。当然，这样做的缺点是训练集和验证集/测试集的分布不同，但事实证明，这样的划分方法能够长期给你带来更好的系统性能。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-a2f3061afcf667dc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p></li></ol><p>接下来看一下语音识别的例子。我们应该如何收集数据来训练产品语言识别模块呢？最后的数据集划分方法类似上面的方法2。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-5753e279568bb523.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="Bias-and-Variance-with-mismatched-data-distributions"><a href="#Bias-and-Variance-with-mismatched-data-distributions" class="headerlink" title="Bias and Variance with mismatched data distributions"></a>Bias and Variance with mismatched data distributions</h1><p>估计学习算法的偏差和方差可以帮助我们确定接下来应该优化做的方向，但是，当你的训练集和验证集、测试集的分布不同时，分析偏差和方差的方式可能不同。</p><p>我们继续用猫分类器为例。我们可能会遇上这样的情况：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-96f55fa26dd3d316.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>一般情况下，我们会认为这个算法的方差过大，而采取减小方差的方法。但是，如果训练集和验证集的分布不同，即算法因为没有看过验证集相似的数据而表现不佳，那么这可能就不是单纯的减小方差的问题。问了判断到底是否应该减小方差，我们可以设置一个training-dev set: Same distribution as training set, but not used for training.</p><p>一个针对training-dev set的解释。如图，数据分为了4个部分，其中training-dev set不参与训练。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-85af1351aa5ff193.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>情景一：</p><ul><li>Training error:1%</li><li>Training-dev error: 9%</li><li>Dev error: 10%<br>这种情况下，发现尽管训练集和Training-dev集的分布相同，但是算法表现并不好，可以判断存在方差问题。</li></ul><p>情景二：</p><ul><li>Training error: 1%</li><li>Training-dev error: 1.5%</li><li>Dev error: 10%<br>这种情况下，我们的方差问题其实很小，因此这实际上是数据不匹配问题，因为我们的算法没有直接在Training-dev集和Dev集上训练，而且两个数据集分布不同，但是算法在前者表现好，而后者表现差，说明是数据不匹配即分布不同导致的问题。</li></ul><p>情景三：</p><ul><li>Training error: 10%</li><li>Training-dev error: 11%</li><li>Dev error: 12%<br>注意我们的人类水平即贝叶斯误差估计是0%，而这个情景很明显存在偏差问题，存在可避免偏差问题。</li></ul><p>情景四：</p><ul><li>Training error: 10%</li><li>Training-dev error: 11%</li><li>Dev error: 20%<br>这个情景存在着两个问题：第一，<strong>可避免偏差</strong>相当高，因为我们在训练集上也没有做好；第二，<strong>数据不匹配</strong>问题，可以看到Training-dev集数据集和dev数据集的表现相差很大。</li></ul><h2 id="Bias-variance-on-mismatched-training-and-dev-test-sets"><a href="#Bias-variance-on-mismatched-training-and-dev-test-sets" class="headerlink" title="Bias/variance on mismatched training and dev/test sets"></a>Bias/variance on mismatched training and dev/test sets</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-f92f4e0d14f6090e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>对图上内容做详细解释：通过training set error和human level error，我们可以得知avoidable bias；通过training set error和training-dev set error，我们可以得知方差variance；通过training-dev set error和dev error，我们可以知道数据不匹配情况；通过dev error和test error，我们可以知道算法过拟合的程度。</p><p>我们观察到，上述误差是一个递增的过程，但也可能出现向下图右侧的情况：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-bf2ebd879b1f7cf5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>这主要是有时候，我们的验证集/测试集分布比我们应用实际处理的数据要容易得多，那么误差真的可能如上图那样下降。</p><h2 id="More-general-formulation"><a href="#More-general-formulation" class="headerlink" title="More general formulation"></a>More general formulation</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-440a23bb73685536.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>通常情况下，我们可以完成上述的一个表格，其中左边为训练数据，而右边为测试/验证数据，可以发现他们的分布不同。通过计算红框中不同的差值，我们可以判断算法的偏差、方差和数据不匹配的问题。当然，我们也可以将表格填满，从而发现人类表现在左边比在右边要更好，可以发现更多的数据特性。</p><p>我们在之前就讲过如何降低偏差、降低方差的方法，那么，如何处理数据不匹配的问题呢？</p><h1 id="定位数据不匹配：Addressing-data-mismatch"><a href="#定位数据不匹配：Addressing-data-mismatch" class="headerlink" title="定位数据不匹配：Addressing data mismatch"></a>定位数据不匹配：Addressing data mismatch</h1><p>如果我们的训练集来自和验证集/测试集不同的分布，而误差分析显示我们存在数据不匹配的问题，我们应该怎么做呢？</p><p>一般我们会这么做，而这种方法通常可以解决很多问题。</p><ul><li>Carry out manual error analysis to try to understand difference between traning and dev/test sets. 通常我们需要观察一下验证集的样本，弄清楚训练集和验证集的不同。就汽车语音识别系统而言，我们可能发现验证集的数据存在很多汽车噪音，这就是验证集和训练集的差异之一。当我们了解了验证集误差的性质时，我们就知道验证集和训练集不同，因此就会尝试做下一步。</li><li>Make training data more similar; or collect more data similar to dev/test sets. 比如，当我们发现车辆背景噪声是主要的误差来源，那么我们可以模拟车辆噪声数据；或者发现很难识别街道数据时，则下意识地添加数字的语音信息。（人工合成数据）</li></ul><h2 id="Artificial-data-synthesis"><a href="#Artificial-data-synthesis" class="headerlink" title="Artificial data synthesis"></a>Artificial data synthesis</h2><p>给出一个简单的音频合成例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7fab9a09060765cd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>通过人工合成数据，我们可以很方便地得到更多的数据，而不需要自己收集数据。所以，当我们的误差分析显示我们应该让自己的数据听起来像在车里录的，那么我们可以人工合成这样的音频，再给机器学习算法进行训练。</p><p>但是，这种人工合成数据有一个潜在问题。比如，我们在安静的背景里录得10000个小时音频数据，而只录了一个小时车辆背景噪音，我们可以尝试将车辆噪声重复10000次叠加到音频里，但是我们的学习算法会对这1小时汽车噪声过拟合。并且，如果我们只录了1个小时的车辆背景噪声，那么实际上我们可能只模拟了全部数据空间的一小部分，从汽车噪音的很小的子集中合成数据。对于我们人耳来说，这可能听起来和其他任意一小时车辆噪声是一样的，但是神经网络则会对这一小时汽车噪声造成过拟合。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-0f225f4c2619ef3e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>因此，为了学习算法有更好的性能，可能有10000个小时的汽车背景噪声数据是更好的，但是可能对于人耳而言，我们可能会觉得自己得到的数据其实都是类似的。</p><p>再举一个汽车识别的例子。我们的学习算法可能对人工合成出的车集合产生过拟合的情况，因为它们只是所有可能出现的集合中很小的一个集合。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c60ffb7b9921f087.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总而言之，如果认为存在数据不匹配问题，那么我们可以进行误差分析，或者看看训练集和验证集，试图了解和找出这两个数据分布的不同之处。之后再看看是否有办法手机更多类似验证集的数据进行训练。我们提高了人工合成数据的方法，这个方法确实有效。在语音识别中，吴老师说已经看到了人工数据合成显著提升了已经很好的语音识别系统的表现，因此这是可行的。但是在使用人工数据合成时，一定要谨慎，记住我们有可能从整个的可能空间中只选了很小的一部分去模拟数据。</p><p>上述就是如何处理数据不匹配的问题。</p><h1 id="迁移学习：Transfer-learning"><a href="#迁移学习：Transfer-learning" class="headerlink" title="迁移学习：Transfer learning"></a>迁移学习：Transfer learning</h1><p>深度学习最强大的理念之一是，有时候我们可以从一个任务中习得知识，再将这些知识应用到另一个独立的任务中。这叫做迁移学习。比如我们的神经网络已经学会分类猫，那么学得的经验知识也可以帮助我们识别x射线扫描图。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-8bb1cf8c2d14663b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>举一个例子。假设我们用图上的神经网络进行了图像识别，识别如猫狗之类的动物，然后想用这个神经网络迁移学习其他任务的知识，如放射线诊断。那么我们可以把神经网络最后一层拿掉，然后随机初始化最后一层的权重，将目标更换为我们的放射线诊断，然后再重新训练。<em>根据经验，如果我们的新数据集很小，那么我们可以只更新最后一层和倒数第二层的权重，而前面的权重保持不变；如果我们的数据集足够，那么也可以对所有层的权重进行更新。</em></p><p>因此，在图像识别数据的初期训练阶段，我们称为<strong>预训练(pre-training)</strong>，因为我们在用图像数据来预训练神经网络的权重；之后，如果我们想重新更新权重，然后在放射科数据上训练，这个过程称为<strong>微调(fine tuning)</strong>。（预训练就是指预先训练的一个模型或者指预先训练模型的过程；微调就是指将预训练过的模型作用于自己的数据集，并使参数适应自己数据集的过程。）</p><p>更通俗的解释：</p><ul><li>你需要搭建一个网络模型来完成一个特定的图像分类的任务。首先，你需要随机初始化参数，然后开始训练网络，不断调整直到网络的损失越来越小。在训练的过程中，一开始初始化的参数会不断变化。当你觉得结果很满意的时候，你就可以将训练模型的参数保存下来，以便训练好的模型可以在下次执行类似任务时获得较好的结果。这个过程就是<strong>pre-training</strong>。</li><li>之后，你又接收到一个类似的图像分类的任务。这时候，你可以直接使用之前保存下来的模型的参数来作为这一任务的初始化参数（已经学习了很多低层次特征），然后在训练的过程中，依据结果不断进行一些修改。此时我们使用的是一个pre-trained模型，而训练过程称为fine tuning。</li></ul><p>为什么这样做有效果呢？对于低层次特征，比如边缘检测、曲线检测、阳性物体检测，从非常大的图像识别数据库中学得这些知识，可能有助于让我们的学习算法在放射科诊断中做得更好，因为算法学到了很多结构信息的知识，如线条、点、曲线等，从而更好的进行学习。</p><p>再假设我们已经训练出一个语音识别系统，输入为音频片段，输出为听写文本。然后接下来我们想要完成触发词的检测，即之前所提到过的可以启动智能家居的一些出发词，那么我们可能需要去掉神经网络的最后一层，然后加入新的输出结点，当然有时候我们甚至往神经网络中加入几个新的层，然后把唤醒词检测问题的标签Y放入进行训练。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e9c10274268d9c66.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>那么，迁移学习什么时候起作用呢？迁移学习起作用的场合时，<em>对于迁移来源问题你有着很多数据，但迁移目标问题的数据则没有多少。(反过来则不适用)</em>例如，图像识别任务中我们有10,000,000个样本，在神经网络的前面几层可以学习到很多知识，但对于放射任务，我们只有100个样本，因此我们可以迁移从图像识别训练中学到的知识，帮我们加强放射科识别任务的性能；再例如，我们已经通过10,000个小时的数据中学到了很多人类声音的特征，但对于触发词检测，我们只有1个小时的数据，因此在这种情况下，预先学到很多人类声音的特征、人类语言的组成部分等等，能够帮助我们开发出一个很好的触发词检测系统，尽管我们的数据集相对较小。</p><h2 id="When-transfer-learning-makes-sense"><a href="#When-transfer-learning-makes-sense" class="headerlink" title="When transfer learning makes sense"></a>When transfer learning makes sense</h2><ul><li>Task A and B have the same input x.</li><li>You have a lot more data for Task A than Task B.</li><li>Low level features from A coule be helpful for learning B.</li></ul><h1 id="多任务学习：Multi-task-learning"><a href="#多任务学习：Multi-task-learning" class="headerlink" title="多任务学习：Multi-task learning"></a>多任务学习：Multi-task learning</h1><p>在迁移学习中，我们的学习步骤是串行的，我们从任务A中学习然后再迁移到任务B。在多任务学习中，我们同时开始学习，试图让神经网络同时做多个事情，然后希望这里的每个任务能够帮到其他所有任务。</p><p>举一个例子。假设我们在研发无人驾驶车辆，那么我们的无人驾驶车可能需要同时检测不同的物体，比如检测行人、车辆、停车标志、交通灯等。如下图图像。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f7d0feb15c87bfda.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>假设图像是输入，那么我们就要输出一个向量，检测的物体越多，向量的维数越大。简单假设为检测4个物品，如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-48d7b9aaea951a7f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如果从整体来看训练集，那么整个训练集的输出则为(4,m)的矩阵。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ecae26faf18b065e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>因此接下来我们要做的就是训练一个神经网络来预测y值，从下图的神经网络输出层可以看到，4个结点分别对应了4个被检测的对象。当然，我们也需要设置cost function。可以看到，我们这里不仅要对所有的样本求和，还需要对4个维度的各自的loss求和，这是因为我们执行的是多任务学习，每个维度都有可能存在；与Softmax回归相比，Softmax回归只支持单标签，而多任务学习中一张图片可能有多个标签。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-10627ebe2a722670.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>另外，尽管有的时候一些图片所给出的标签不明确，如可能输出中含<code>?</code>：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-67cca08106a794cc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>多任务学习仍然可以进行，这个时候我们在计算loss function的时候只要考虑输出为0或1的值即可。</p><h2 id="When-multi-task-learning-makes-sense"><a href="#When-multi-task-learning-makes-sense" class="headerlink" title="When multi-task learning makes sense"></a>When multi-task learning makes sense</h2><ul><li>Training on a set of tasks that could be benefit from having shared lower-level features.(共用低层次特征)</li><li>Usually: Amount of data you have for each task is quite similar.(数据量)</li><li>Can train a big enough neural network to do well on all the tasks.</li></ul><p>多任务学习的替代方法是为每个任务训练一个单独的神经网络。另外，多任务学习会降低性能的唯一情况，和训练单个神经网络相比性能更低的情况，就是我们的神经网络还不够大。如果可以训练一个足够大的神经网络，那么多任务学习肯定不会或者很少会降低性能，并且我们都希望它可以提升性能，至少比单独训练神经网络来单独完成各个任务的性能要好。</p><p>在实践中，多任务学习的使用频率要低于迁移学习。大多数迁移学习的应用都是因为我们需要解决的问题所拥有的训练数据很少，因此我们会找到一个数据很多的相关问题来预先学习，再将知识迁移到这个新问题上。但是多任务学习则比较少见，可能计算机视觉是一个例外。在物体检测（最显著的例外情况）中，我们看到更多使用多任务学习的应用，即用一个神经网络来尝试检测很多物体，比分别训练不同的神经网络来检测物体更好。总的来说，目前迁移学习的使用频率更高，但两者都可以成为我们的强力工具。</p><h1 id="什么是端到端的深度学习：end-to-end-deep-learning"><a href="#什么是端到端的深度学习：end-to-end-deep-learning" class="headerlink" title="什么是端到端的深度学习：end-to-end deep learning"></a>什么是端到端的深度学习：end-to-end deep learning</h1><p>深度学习中最令人振奋的动态之一就是端到端深度学习的兴起。简而言之，以前有一些数据处理系统或者学习系统，它们需要多个阶段的处理，而端到端深度学习就是忽略所有这些不同的阶段，用单个神经网络来代替它。</p><p>举一个语音识别的例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-1be5178435b2445a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>可以看到，传统的流水线方法包含多个阶段，包括人工提取特征、学习音位、构成单词等，而端到端深度学习只需要将数据集输入后，通过深层神经网络就可以得到我们的听写文本(transcripts)，因此优势很明显。然而，端到端深度学习的一个缺点是，训练的时候需要大量的数据，因此在具有大数据集的前提下，采用端到端的效果会很好；而当数据集较小时，传统流水线方法的效果也很好；当数据量适中时，可以采用中间件方法，即我们可能输入的是音频，跳过特征提取步骤，尝试从神经网络输出音位(phoneme)，这也算是向端到端学习迈出的一小步。</p><p>再举一个人脸识别的例子，是一个人脸门禁系统。这个系统的输入应该是相机的照片，迄今为止最好的方法应该是多步的：首先要运行一个软件来检测人脸，放大图像的部分，并通过裁剪使人脸居中，再将图片作为神经网络的输入，来学习出该人的身份。（研究人员发现，比起一步到位，把这个问题分解成两个更简单的步骤表现得更好）<br><img src="https://upload-images.jianshu.io/upload_images/8636110-6d9fb8ff32662cb5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>这里注意，训练第二步时，我们输入两张图片，然后神经网络所做的是将输入的两张图进行比较，判断是否是同一个人。即假设我们有10,000个员工，那么就将每个员工图片与进入者的图片进行比对即可。</p><p>为什么这里两步法更好呢？</p><ol><li>我们要解决的两个问题，每个问题实际上很简单</li><li>两个子任务的训练数据都很多。具体来说，有很多数据可以用于人脸识别训练，任务就是观察一张图，找出人脸所在的位置并框出来。对于任务二，业务领先的公司也拥有着数亿图像用于训练，通过观察两张图片，判断照片中认得身份来确定是否为同一个人。相比之下，如果我们想一步到位，即端到端学习，但是我们实际的数据对(x,y)的量要少得多，其中x是门禁系统拍摄的图像，y是人的身份，因为我们没有足够多的数据去解决这个端到端学习问题。</li></ol><p>再来看几个例子。以机器翻译为例，传统的机器翻译也有着一个复杂的流水线过程，而端到端深度学习在机器翻译的表现很好：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-53641c212c796c66.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>从一个手的骨骼判断儿童的年龄：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-9161b2a38a34ab0d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>由于没有足够的数据来用端到端方式来训练词任务，因此相比之下，多步方法的效果更好。</p><p><strong>总结</strong>：端到端深度学习系统是可行的，它的表现可以很好，可以简化系统架构，让我们不需要搭建那么多手工设计的单独组件，但这一方法并不是每次都能成功。</p><h1 id="是否使用端到端学习"><a href="#是否使用端到端学习" class="headerlink" title="是否使用端到端学习"></a>是否使用端到端学习</h1><h2 id="Pros-and-cons-of-end-to-end-deep-learning"><a href="#Pros-and-cons-of-end-to-end-deep-learning" class="headerlink" title="Pros and cons of end-to-end deep learning"></a>Pros and cons of end-to-end deep learning</h2><p>Pros:</p><ul><li>Let the data speak. (Enough data)</li><li>Less hand-designing of components needed.</li></ul><p>Cons:</p><ul><li>May need large amount of data.</li><li>Excludes potentially useful hand-designed components.(数据来源：original data, hand-designed data在小数据集上很有用)</li></ul><h2 id="Applying-end-to-end-deep-learning"><a href="#Applying-end-to-end-deep-learning" class="headerlink" title="Applying end-to-end deep learning"></a>Applying end-to-end deep learning</h2><p>Key question: Do you have sufficient data to learn a function of the complexity needed to map x to y?</p><p>举一个更复杂的例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-29dc6049430c326a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>可以看到，这不是一个端到端学习的例子，相反，它做到了以下两点：</p><ol><li>Use DL to learn individual components.</li><li>Carefully choose X-&gt;Y depending on what tasks you can get data for.</li></ol><p>相反的，要通过观察图像直接判断方向盘转向，即端到端学习，我认为在自动驾驶领域里基本是不太可能的一件事。</p><h1 id="本周作业"><a href="#本周作业" class="headerlink" title="本周作业"></a>本周作业</h1><p>下面粘贴做错的题：<br>1、答案是C，而我选了AB。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-9ede204ad6b600ff.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>2、答案是AD，而我选了A。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-121041c86d1c8ba2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;进行误差分析：Carrying-out-error-analysis&quot;&gt;&lt;a href=&quot;#进行误差分析：Carrying-out-error-analysis&quot; class=&quot;headerlink&quot; title=&quot;进行误差分析：Carrying out erro
      
    
    </summary>
    
      <category term="深度学习" scheme="https://github.com/DesmonDay/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://github.com/DesmonDay/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>第8周-机器学习ML策略(1)</title>
    <link href="https://github.com/DesmonDay/2019/04/18/deep-learningw8/"/>
    <id>https://github.com/DesmonDay/2019/04/18/deep-learningw8/</id>
    <published>2019-04-18T05:12:56.000Z</published>
    <updated>2019-04-19T07:07:18.726Z</updated>
    
    <content type="html"><![CDATA[<p>接下来两周内容为第三课——结构化机器学习项目。</p><h1 id="为什么是机器学习策略"><a href="#为什么是机器学习策略" class="headerlink" title="为什么是机器学习策略"></a>为什么是机器学习策略</h1><p>在我们进行机器学习的时候，我们需要达到想要的结果，可能会采取以下措施：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-dd1137950af28f05.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>然而，我们需要一个有效且快速的方法，来判断哪些想法是靠谱的，甚至可以提出新的想法，判断哪些是值得一试的想法，哪些是可以舍弃的想法。因此这一周学习的是有关机器学习的策略，以便于我们之后的学习，提高效率，使得我们的深度学习系统更加实用。</p><h1 id="正交化-Orthogonalization"><a href="#正交化-Orthogonalization" class="headerlink" title="正交化-Orthogonalization"></a>正交化-Orthogonalization</h1><p>搭建机器学习系统的挑战之一是我们可以尝试和改变的东西太多了，比如说许多超参数可以调整。而对于一些高效率的机器学习专家，他们思维清晰，对于要调整什么来达到某个效果，非常清楚。这个过程称为正交化。</p><p>接下来给一个TV tuning的例子。电路设计师通过精确的电路设计，使得每个旋钮能够对应一个具体的功能，比如改变图像的高度、宽度、方向等等。在这种情况下，正交化指的是电视设计师设计旋钮，使得每个旋钮都只能调整一个性质，使得调整电视图像更为容易。</p><p>再给出一个Car的例子。比如我们有方向盘、油门和刹车，分别控制方向和速度，因此也使得车子的驾驶更加容易。尽管我们可能也可以通过一个工具控制多种功能来达成目的，但这种做法比一个工具控制单一功能要难实现得多。因此对于正交化，我们可以想出一个维度来控制方向，另一个维度来控制速度，然而如果用一个控制装置同时控制方向和速度，那就难以让我们的车子按照自己想要的方式驾驶。所以单独控制更为方便。</p><p>在设计机器学习项目时，我们需要依次遵循以下四个原则：</p><ul><li>Fit training set well on cost function: 对于这一原则，适用的旋钮有更大更深的神经网络，或者修改优化算法为Adam，以及其他只影响训练集表现得方法。</li><li>Fit dev set well on cost function: 对于这一原则，如果我们的算法在训练集表现很好，但在验证集表现不佳，说明产生了过拟合，我们可以采用Regularization，或者是添加更多的训练集数据。</li><li>Fit test set well on cost function: 如果算法在训练集和验证集表现良好，但是在测试集表现不佳，我们可以采用更大的验证集。</li><li>Performs well in real world: 如果算法在测试集表现不好，可能有两个原因，一是我们的验证集分布与实际不相同，所以需要改变验证集；二是我们的成本函数设计得不正确。</li></ul><p>对于Early stop的策略，由于在采用的时候会同时改变算法在训练集和验证集上的表现，即它是非正交化的，因此吴老师往往不会采用early stop的策略。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-2b791e231b8c3150.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="单一数字评估指标-Single-number-evaluation-metric"><a href="#单一数字评估指标-Single-number-evaluation-metric" class="headerlink" title="单一数字评估指标-Single number evaluation metric"></a>单一数字评估指标-Single number evaluation metric</h1><p>我们在进行机器学习项目的时候，如果能够为我们的问题设置一个单实数评估指标，那么我们的进展会快很多。举一个简单的例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-bab6b2b5bdafbea3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>比如我们对某个分类器运用查准率和查全率作为我们的评估指标，但是如果两个分类器在不同的评估指标上有着不同的表现，我们就无法区分哪个分类器更好。</p><p>查准率(Precision)与查全率(Recall)具体含义：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-2da8f4640e480e97.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>因此我们找到一个新的评估指标，可以结合查准率和查全率，最经典的评估指标为F1 score，其公式称为”Harmonic mean”(查准率P和查全率R的调和平均数)。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c6e191f180b0bada.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>通常一个优秀的团队所实施的机器学习项目，一般有一个定义明确的dev set，再加上单个的实数评估指标，使得我们能够快速判断分类器的优劣，并且加快我们的迭代速度。</p><p>再举一个例子，假设我们开发一个Cat app，用来服务不同地区。表中表示的我们算法在不同地区中表示的误差率。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-0c0331814ce34bdf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>那么多的误差数据，显然我们很难从中挑选出表现最好的算法。因此吴老师建议，除了计算分类器在四个不同的地理区域上的表现，同时也要计算平均值，从而方便地区分哪个算法表现得更好，从而方便地提高我们的决策能力。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7a12da8c65c34360.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="满足和优化指标：Satisficing-and-optimizing-metrics"><a href="#满足和优化指标：Satisficing-and-optimizing-metrics" class="headerlink" title="满足和优化指标：Satisficing and optimizing metrics"></a>满足和优化指标：Satisficing and optimizing metrics</h1><p>实际上，要把我们所有需要考虑的情况都集中到一个实数评测指标，是一件不容易的事。在这种情况下，满足和优化指标时很有用的。举一个猫分类的例子，假设我们十分重视分类准确率，可以是F1 score，也可以是其他评测准确率的指标。但同时我们也关注运行时间，因此我们可以将准确率和运行时间结合起来。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-625dab251a4e39fd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>将单纯的线性加权求和来表示两个指标显得过于简单。因此我们可能想要有一个分类，能够最大限度提高准确率，但同时满足我们的运行时间在某个限度内，因此在这个例子中，我们需要optimizing准确率，而需要satisficing运行时间。</p><p>更一般地，如果我们在乎N个指标，我们希望其中一个能够尽可能地好，而其余的N-1个则要满足某个阈值即可，如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-339d71b25751dfb3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>举另一个例子，假设我们正在构建一个用于检测wake words(trigger words)的系统，就像语音控制设备，比如Okay Google用来唤醒设备，或者Hey Siri来唤醒Siri。那么我们会在乎触发字检测系统的准确性，即当有人说出其中一个触发词时，有多大的概率能够唤醒设备；我们也会在意假阳性的数量，即没有人说触发词时，设备被随机唤醒的概率有多大。</p><p>在这种情况下，结合两个指标的合理方式是最优化准确性，并且比如保证24小时内最多只有1次假阳性的限度。因此在本例中，准确性是optimizing，假阳性数量为satisficing。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-a7b5ef61b107f0c5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>总结，当我们需要顾及多个指标的时候，比如有一个优化指标是我们想尽可能优化的，还有一个或多个需要满足一定阈值的指标，我们现在就可以有一个完全自动的方法，能够在观察不同的结果后迅速选出表现最好的算法。</p><h1 id="训练-验证-测试集分布：Train-dev-test-distributions"><a href="#训练-验证-测试集分布：Train-dev-test-distributions" class="headerlink" title="训练/验证/测试集分布：Train/dev/test distributions"></a>训练/验证/测试集分布：Train/dev/test distributions</h1><p>划分训练集、验证集和测试集的方式会大大影响了我们在建立机器学习应用方面取得进展的速度。（dev set: development set, hold out cross validation set)</p><p>我们应该如何设立验证集合测试集呢？比如一个猫分类的例子，我们有来自以下不同地区的数据集，而如果我们像下图一样简单的划分前四个为dev set，后四个为test set，那么这样的划分结果将会非常糟糕，因为dev set和test set来自不同的分布。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-568cb607e2d038b6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>实际上，我们应该保证dev set和test set来自同一个分布。</p><p>我们需要记住的是，设立我们的验证集，再加上一个单实数评估指标，就像设下目标告诉团队，这就是我们想要达到的目标。迅速地用验证集和单指标来尝试不同的算法和参数，从而针对验证集做评估指标的优化。然而针对上图的例子，我们的团队可能会花上数月（无用功）在验证集上进行迭代优化，最后发现我们最终在测试集上的数据与验证集的数据分布不同，因此明显结果也会不令人满意。为了避免这种情况，我们需要做的是将所有的输入<strong>重新随机洗牌</strong>，将所有数据混在一起，从而达到同分布的目的。</p><p>我们应该做到的是：Choose a dev set and test set to reflect data you expect to get in the future and cnosider important to do well on. 如果我们得到了新的数据，也需要随机地将它们分配到dev set和test set中，保证数据集的分布相同。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-5bc58fc11faa8d9a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="验证集和测试集的大小"><a href="#验证集和测试集的大小" class="headerlink" title="验证集和测试集的大小"></a>验证集和测试集的大小</h1><p>Old way of splitting data:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-712702343306b5c5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Modern way(big data era):<br><img src="https://upload-images.jianshu.io/upload_images/8636110-34505e70b26d37af.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Guiline:</p><ul><li><strong>Size of dev set</strong>: Set your dev set to be big enough to detect difference in algorithm/models you’re trying out.</li><li><strong>Size of test set</strong>: Set your test set to be big enough to give high confidence in the overall performance of your system.</li><li>Test set helps evaluate the performance of the final classifier which could be less 30% of the whole data set.</li></ul><h1 id="When-to-change-dev-test-sets-and-metrics"><a href="#When-to-change-dev-test-sets-and-metrics" class="headerlink" title="When to change dev/test sets and metrics"></a>When to change dev/test sets and metrics</h1><h2 id="修改评估指标"><a href="#修改评估指标" class="headerlink" title="修改评估指标"></a>修改评估指标</h2><p>举个例子：假设我们有一个猫图片分类应用，最开始我们的指标选定的是分类错误率。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c62de8e8e640ecdb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>如上例，单从评估指标来看，A算法表现得更好。但是A算法会推送一些色情图像，因此尽管它的分类错误率低，但它所推送的图片不符合要求，因此实际上算法B更符合我们的要求，因为它不会把色情图片推送为猫。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-7e4cb57ed6736360.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>这种情况说明我们应该更换评估指标，或者改变dev set或者test set了。因为这个错误率评估指标对色情图片和非色情图片一视同仁，但我们不希望分类器不会错误标记色情图片。</p><p>因此修改评估指标的一个方法是添加权重项，修改如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f0fcf28879a058e3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>可以看到我们给色情图片的权重更大。为了使用这种方法，我们需要将验证集和测试集过一遍，标记出所有的色情图片，以使用这种方法。因此，如果对现有的评估指标不满意，我们可以尝试去修改评估指标的定义。</p><p>因此我们可以把机器学习<strong>分为两步</strong>(正交化)，第一步是设置目标：Discuss how to define a metric to evaluate classifiers；第二步是如何精准逼近目标：worry separately about how to do well on this metric. 可能的方法是修改成本函数或者评估指标。</p><h2 id="修改验证集-测试集"><a href="#修改验证集-测试集" class="headerlink" title="修改验证集/测试集"></a>修改验证集/测试集</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-7b78e87c2f3cecdb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如果我们的dev/test set都是高清图片，而用户图片均是比较低质量的图片，那么就要考虑修改验证集/测试集，使得我们的数据能够更好地反映实际需要处理好的数据。</p><p>总体方针：If doing well on your metric + dev/test set does not correspond to doing well on your application, change your metric and/or dev/test set.</p><h1 id="为什么和人类水平比较"><a href="#为什么和人类水平比较" class="headerlink" title="为什么和人类水平比较"></a>为什么和人类水平比较</h1><p><img src="https://upload-images.jianshu.io/upload_images/8636110-455e91616bb13b72.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>性能无法超过理论最优误差（贝叶斯最优误差），即我们无法通过更多的训练使得我们的模型表现优于上限。我们经常会发现，机器学习的发展非常快，直到我们超越人类表现之前一直很快，但当超越人类的表现时，进展会变慢。两个原因：一是人类水平在很多任务中离贝叶斯最优误差不远；二是只要你的表现比人类的表现差，那就有一定的工具来提升表现，一旦超越人类表现，就难以找到合适的工具了。</p><p>人类在很多任务上表现优秀。只要我们的机器学习算法表现得比人类水平差，那么就可以：</p><ol><li>Get labeled data from humans.</li><li>Gain insight from manual error analysis: Why did a person get this right?</li><li>Better analysis of bias/variance.</li></ol><p>而一旦算法的表现超过人类，上述三种方法都不适用了，因此说提升算法表现的难度也加大了。</p><h1 id="可避免偏差：Avoidable-bias"><a href="#可避免偏差：Avoidable-bias" class="headerlink" title="可避免偏差：Avoidable bias"></a>可避免偏差：Avoidable bias</h1><p><img src="https://upload-images.jianshu.io/upload_images/8636110-3e6e63b5762ceffb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>In this case, the human level error as a proxy for Bayes error since humans are good to identify images/ If you want to improve the performance of the traning set but you can’t do better than the Bayes error otherwise the training set is overfitting. By knowing the Bayes error, it is easier to focus on whether bias or variance avoidance tactic will improve the performance of the model.</p><ul><li><p>Scenario A: There is a 7% gap between the performance of the training set and the human level error. It means that the algorithm isn’t fitting well with the training set since the target is around 1%. To reslve the issue, we use <strong>bias reduction technique</strong> such as training a bigger neural network or running the training set longer.</p></li><li><p>Secnario B: The training set is doing good since there is only a 0.5% difference with the human level error. <strong>The diffenrence between the training set and the human level error is called avoidable bias.</strong> The focus here is to reduce the variance since the difference between the training error and the development error is 2%. To resolve the issue, we use <strong>variance reduction</strong> technique such as regularization or have a bigger training set.</p></li></ul><h1 id="理解人类表现的含义：Understanding-human-level-performance"><a href="#理解人类表现的含义：Understanding-human-level-performance" class="headerlink" title="理解人类表现的含义：Understanding human level performance"></a>理解人类表现的含义：Understanding human level performance</h1><p>前提：人类水平误差可以用来估计贝叶斯最优误差。</p><p>给定下面医学图像分类的例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-53f08b46770b01b2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>如何定义人类水平误差？首先根据定义，贝叶斯最优误差必须在0.5%以下，因此在这个背景下，最优误差在0.5%以下，因此我们将人类水平定义为0.5%。</p><p>现在，为了发表论文或者部署系统，也许人类水平错误率的定义可以不一样，比如我们可以使用1%，只要超越了一个普通医生的表现，那么也意味着系统已经达到实用水准了，可以有部署价值。如果我们的目标是超越单个人类，那么选取1%是合理的。但如果我们的目标是替代贝叶斯最优误差，那么选取0.5%的错误率定义才合适。</p><p>为了了解为什么这个很重要，让我们看一个误差分析的例子。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c9019d736a44e507.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>分析：</p><ul><li>Scenario A: In this case, the choice of human-level performance doesn’t have an impact. The avoidable bias is between 4%-4.5% and the variance is 1%. Therefore, the focus should be on <strong>bias reduction</strong> technique.</li><li>Scenario B: In this case, the choice of human-level performance doesn’t have an impact. The avoidable bias is between 0%-0.5% and the variance is 4%. Therefore, the focus should be on <strong>variance reduction</strong> technique.</li><li>Scenario C: In this case, the estimate for Bayes error has to be 0.5% since you can’t go lower that the human-level performance otherwise the training set is overfitting. Also, the avoidable bias is 0.2% and the variance is 0.1%. Therefore, the focus should be on bias reduction technique.</li></ul><p>Summary of bias/variance with human-level performance:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-3594f186dae899c1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ul><li>Human-level error - proxy for Bayes error</li><li>If the difference between human-level error and the training error is bigger than the difference between the training error and the development error. The focus should be on <strong>bias reduction</strong> technique.</li><li>If the difference between training error and the development error is bigger than the difference between the human-level error and the training error. The focus should be on <strong>variance reduction</strong> technique.</li></ul><p>一旦我们的机器学习算法表现超过人类时，那么要取得进步也将会变得困难。</p><h1 id="超过人类表现：Surpassing-human-level-performance"><a href="#超过人类表现：Surpassing-human-level-performance" class="headerlink" title="超过人类表现：Surpassing human-level performance"></a>超过人类表现：Surpassing human-level performance</h1><p>很多团队会因为机器在特定的识别分类任务中超越了人类水平而激动不已。首先给出下面的一个例子。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d7b756fe701f09d1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ul><li>Scenario A: In this case, the Bayes error is 0.5%, therefore the available bias is 0.1% et the variance is 0.2%.</li><li>Scenario B: In this case, there is not enough information to know if bias reduction or variance reduction has to be done on the algorithm. It doesn’t mean that the model cannot be improve, it means that the conventional ways to know if bias reduction or variance reduction are not working in this case.</li></ul><p>There are many problems where machine learning significantly surpass human-level performance, especially with <strong>structured data</strong>(数据库):</p><ul><li>Online advertising</li><li>Prodct recommendations</li><li>Logistics(predicint transit time)</li><li>Loan approvals(判断可否批准贷款)</li></ul><p>这些都是结构化数据，而非自然感知的任务。人类在自然感知任务上表现很好，如视觉、语音识别等，而对计算机而言则比较难。而对于结构化数据，计算机能够看到的数据比人类多，因此其表现也会高于人类，能够更敏锐地识别出数据中的统计规律。</p><h1 id="改善模型表现：Improving-your-model-performance"><a href="#改善模型表现：Improving-your-model-performance" class="headerlink" title="改善模型表现：Improving your model performance"></a>改善模型表现：Improving your model performance</h1><p><strong>The two fundamental assumptions of supervised learning</strong>:</p><ol><li>Have a low avoidable bias which means that the training set fits well.(Train bigger network or train longer time.)</li><li>Have a low or acceptable variance which means that the training set performance generalizes well to the development set and test set.(Regularization or get more train data.)</li></ol><p><strong>Reduce (avoidable)bias and variance</strong><br><img src="https://upload-images.jianshu.io/upload_images/8636110-5a74281157a48056.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="本周作业"><a href="#本周作业" class="headerlink" title="本周作业"></a>本周作业</h1><p>这周作业都是选择题，这里贴一下我做错的题目：</p><ol><li><p>这题的答案是False。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-11994aee8df96272.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>答案解释：Adding this data to the training set will change the training set distributon. However, it is not a problem to have different training and dev distribution. On the contrary, it would be <strong>very problemanic to have different dev and test set distribution.</strong>（实际上，验证集和测试集的分布不同导致的问题更严重，而训练集分布与dev/test集的分布不同并没有造成很大问题）</p></li><li><p>这题的答案是多选，选择AD，而我只选择了D。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-8c0a2a950a374604.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p></li><li><p>这题正确答案是A，而我选择了D。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-37ed0a7d64c4bf34.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>参考别人的解释：1000张照片对应前面的数据总量是不足一提的，隔着好几个数量级，而这个新物种的紧急程度所占的比重，远大于新物种数据量在总数据量中所占的比重，因此当你的评价指标无法正确评估算法时，最好定义一个新的评估指标。（类似于猫分类中的色情图片）</p></li><li><p>这题应该选ABD，我只选了AB，主要是没有正确理解D选项的意义。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-97402839ce624327.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>答案解释：D选项之所以对，是因为它强调的是，如果我的这个模型需要两周的时间完成一次训练，那么我们可以迭代的速度就很慢，无法尝试更多的参数或其他想法之类的。</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;接下来两周内容为第三课——结构化机器学习项目。&lt;/p&gt;
&lt;h1 id=&quot;为什么是机器学习策略&quot;&gt;&lt;a href=&quot;#为什么是机器学习策略&quot; class=&quot;headerlink&quot; title=&quot;为什么是机器学习策略&quot;&gt;&lt;/a&gt;为什么是机器学习策略&lt;/h1&gt;&lt;p&gt;在我们进行机器
      
    
    </summary>
    
      <category term="深度学习" scheme="https://github.com/DesmonDay/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://github.com/DesmonDay/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>第7周-超参数调试、Batch正则化和程序框架</title>
    <link href="https://github.com/DesmonDay/2019/04/17/deep-learningw7/"/>
    <id>https://github.com/DesmonDay/2019/04/17/deep-learningw7/</id>
    <published>2019-04-17T06:48:08.000Z</published>
    <updated>2019-04-24T03:42:53.587Z</updated>
    
    <content type="html"><![CDATA[<h1 id="调试处理-Tuning-process"><a href="#调试处理-Tuning-process" class="headerlink" title="调试处理-Tuning process"></a>调试处理-Tuning process</h1><p>在深度学习中，我们需要调整的超参数有学习率alpha、Momentum的参数beta、Adam优化算法的参数beta1/beta2/epsilon、神经网络层数、每一层隐藏单元个数、衰退率learning rate decay、mini-batch的大小等等。在这些超参数中，一些超参数比其他的要重要，其中学习率是最重要的超参数。图中，红色为1，橙色为2，紫色为3，Adam的参数则通常为默认值。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-614ac5dedfb62fba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>策略一： Try random values, but don’t use a grid.</strong> 通常，我们可能会使用网格(grid)搜索，但这种方法仅适用于超参数较少的情况。当训练深度神经网络时，我们不使用网格搜索，而是设置随机值。有时我们能难预知哪些超参数更重要，因为我们搜索的超参数可能有很多个，因此采取随机取值而不是网格取值表明你探究了更多重要超参数的潜在值。</p><p><strong>策略二： Coarse to fine(从粗糙到惊喜)</strong> 形象化例子如下，现在粗糙的网格中随机搜索，再在结果比较优的几个取值周围进行更精细地随机选取。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-3c3aa61662ca95ca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="为超参数选择合适的范围"><a href="#为超参数选择合适的范围" class="headerlink" title="为超参数选择合适的范围"></a>为超参数选择合适的范围</h1><p>在上一节中我们知道了在超参数范围中随机取值可以提高我们的搜索效率。但随机取值并不是在有效值范围内的随机均匀取值，而是选择合适的标尺用于探究这些超参数。</p><p>对于可以随机均匀取值的超参数，如隐藏层单元个数，神经网络层数等：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-dd45dffc58d6b230.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>而对于有些超参数则不适合使用随机均匀取值。比如学习率，我们觉得最小取值为0.0001，最大取值为1。显然，90%的搜索会集中在0.0001到0.1之间，但在0.1到1却只有10%的可能。因此，我们可以采取另一种搜索策略。如图，设置几个固定点为0.0001,0.001，0.01,0.1和1，在这些范围内再进行随机均匀取值。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d0364e61dc42e640.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>用python表示为：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">r = <span class="number">-4</span> * np.random.rand() <span class="comment"># 那么r属于[-4,0]</span></span><br><span class="line">alpha = pow(<span class="number">10</span>, r) <span class="comment"># 那么alpha属于[10^(-4), 1]</span></span><br></pre></td></tr></table></figure></p><p>另一个比较棘手的超参数调参例子是beta，其是用来计算指数的加权平均值。假设我们认定beta是0.9到0.999中的某个值。我们需要注意的是，beta取值0.9类似于与计算10天的温度平均值，取值0.999相当于在1000个值中取平均。因此我们在0.9到0.999中取值，就不适合用线性搜索，即不可在此区间随机均匀取值。</p><p>因此最好的方法是<strong>考虑1-beta</strong>，其取值为0.1到0.001。然后再应用学习率的取值方法，有r取值在[-3,-1]，再设置1-beta=10^r，从而得到beta。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e71d62ea9e02eaa7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>为什么不可以使用线性取值呢？这是因为，当beta越接近1时，其所得结果的灵敏度会变化，即使beta只有微小的变化。因此当beta在0.9到0.9005之间取值，我们的结果几乎不会变化；但beta在0.999(1000个温度数据)到0.9995(2000个温度数据)之间取值，则会对我们的算法产生巨大影响。</p><p>因此，我们需要在超参数选择中做出正确的scale decision。</p><h1 id="超参数训练的实践：Panda-vs-Caviar"><a href="#超参数训练的实践：Panda-vs-Caviar" class="headerlink" title="超参数训练的实践：Panda vs Caviar"></a>超参数训练的实践：Panda vs Caviar</h1><p>到目前为止，我们已经听了许多关于如何搜索最优超参数的内容，在结束该讨论之前，我们讲讲如何组织超参数搜索过程。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-537d7048bfd60f4c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如今的深度学习已经应用到许多不同的领域，某个应用领域的超参数设定，有可能通用与另一领域，不同的应用领域出现相互交融。比如，吴老师说，他曾经看到过计算机视觉领域中涌现的巧妙方法，比如Confonets或ResNets，它们还成功应用于语音识别。</p><p>深度学习领域中，发展很好的一点是不同应用领域的人们会阅读越来多其它研究深度学习领域的文章，跨领域寻找灵感。</p><p>就超参数设定而言，即使我们只研究一个问题，比如逻辑学，如果我们已经找到一组很好的参数设置，并继续发展算法。或许在几个月的过程中，观察到数据会逐渐改变，而这些改变使得我们原来的超参数设定不再好用。因此我们需要重新测试或评估我们的超参数(Re-test hyperparameters occasionally)，至少每隔几个月一次，以确保对数值依然满意。</p><p>最后，关于如何搜索超参数的问题，有两种重要的思路。一个是babysitting one model，即每天根据模型的表，对该模型进行不同参数的调整（如学习率），这通常是因为我们没有足够的计算能力；一个是Training many models in parallel，同时训练多种模型，从中选择表现最优的模型，用这种方式我们可以试验许多不同的参数设置，从中选择最好的。</p><p>上面两种方法就好像熊猫和鱼卵的对比，而这主要是由于我们的计算资源来决定的。</p><h1 id="Batch-Norm-——-感觉还不太懂，需要回看"><a href="#Batch-Norm-——-感觉还不太懂，需要回看" class="headerlink" title="Batch Norm —— 感觉还不太懂，需要回看"></a>Batch Norm —— 感觉还不太懂，需要回看</h1><p>机器学习领域有个很重要的假设：IID独立同分布假设，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。那Batch Norm的作用是什么呢？Batch Norm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。</p><h2 id="正则化网络的激活函数-——-Normalizing-activations-in-a-network"><a href="#正则化网络的激活函数-——-Normalizing-activations-in-a-network" class="headerlink" title="正则化网络的激活函数 —— Normalizing activations in a network"></a>正则化网络的激活函数 —— Normalizing activations in a network</h2><p>在之前的课程中我们学到过归一化输入特征对于训练神经网络参数W和b的速度提升有很大帮助，如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d9a7d88b3ab44cd3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>那么这就产生了对于每一层隐藏层的输入是否要归一化的问题。对于有些学者而言，有着是归一化Z还是A的讨论，这里吴老师默认第一选择是归一化Z。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-8994a5d740f75b29.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>Implementing Batch Norm</strong> 假设我们有隐藏单元值Z[1]到Z[m]，这里简化了原有的符号表示。Batch Norm使得归一化不仅适用于训练的输入，也能适用于隐藏层的输入。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e7de36e5dad0a21c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在图中，iteration内我们首先计算了平均值mu和方差，并且计算了我们原有归一化后的Z值。但是由于我们并不希望每一个隐藏层都具有相同的平均值和方差，因此添加了两个超参数gamma和beta来调整对应的平均值和方差。计算得到结果后，我们使用新的Z值而不是原来的Z值来参与训练。</p><h2 id="将Batch-Norm拟合进神经网络"><a href="#将Batch-Norm拟合进神经网络" class="headerlink" title="将Batch Norm拟合进神经网络"></a>将Batch Norm拟合进神经网络</h2><p>接下来我们将Batch Norm拟合进神经网络中，简单的图示过程如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-775b2e5b1e351d96.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>因此我们可以得到整个神经网络的参数为：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-fb4bb4712854d8ca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>对于beta[l]和gamma[l]，我们也可以使用梯度下降的方法来对其进行更新，注意这里的beta与优化算法中的beta是两个完全不同的参数。</p><p>在实际应用深度学习框架时，我们往往不需要实现Batch Norm的细节，比如Tensorflow中，可以直接使用tf.nn.batch_normalization来实现BN。</p><p>实际中，Batch Norm经常与Mini-batch一同使用，简单的图示过程如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-24152afe1d99a49e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这里有一个需要注意的细节。我们在Batch Norm中的参数为W[l],b[l],beta[l]和gamma[l]。在原先的实现中，我们计算Z[l]=W[l]a[l-1]+b[l]，但在实施了Batch Norm之后，b[l]都会被减去，因为我们在减去平均值时就相当于将b[l]消去了。因此在使用Batch归一化时，我们可以将b[l]简单地设置为常数0，而不需要对其进行更新。另外注意参数的维度即可。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e520559dfb8398ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>接下来讲解整个过程：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-05624bba3926d5d2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>注意到，db实际上不用再计算了。另外，Batch Norm也适用于其他的优化算法，如Adam等。</p><h2 id="为什么Batch-Norm奏效？"><a href="#为什么Batch-Norm奏效？" class="headerlink" title="为什么Batch Norm奏效？"></a>为什么Batch Norm奏效？</h2><p>一个原因就是我们之前在归一化输入特征时讲到的，通过归一化所有的输入特征值，以获得类似范围的值，可以加快学习速度。</p><p>另一个原因就是考虑到covariate shift的问题，这个问题是指如果我们有一个从X到Y的映射函数，当X的分布发生改变时，那么这个函数也要变化。<br>对于深度学习这种包含很多隐层的网络结构，在训练过程中，因为各层参数不停在变化，所以每个隐层都会面临covariate shift的问题，也就是在训练过程中，隐层的输入分布老是变来变去，这就是所谓的“Internal Covariate Shift”，Internal指的是深层网络的隐层，是发生在网络内部的事情，而不是covariate shift问题只发生在输入层。因此Batch Norm可以确保，<strong>无论输入的数据如何变化，输入的均值和方差保持不变</strong>。</p><p>Batch Norm减少了输入值改变带来的问题，它使得这些值变得更稳定，即使输入分布改变了一些，那么归一化后它改变的程度也刽很多。它所做的是当前层的输入改变时，使得后层需要适应的程度减少了。这就意味着减弱了前层参数的作用与后层参数的作用之间的联系，使得网络每一层都可以自己学习，而稍稍独立于其它层，也有利于加速整个网络的学习。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-5cecdac3b73ab75f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Batch Norm还有一个作用，它有<strong>轻微的正则化效果</strong>，将Batch Norm应用于Mini-batch上，因为是在mini-batch上计算均值和方差，而不是在整个数据集上，因此可以存在一点噪声，而这些噪声的作用和dropout类似，dropout是在每个隐藏层的激活值上增加了噪音，通过一定的概率使得隐藏单元激活或者失活；另一个轻微但非直观的效果是，如果我们应用了较大的mini-batch，如512而不是64，我们减少了噪音，因此减少了正则化效果。这也是dropout的一个奇怪的性质，就是应用较大的mini-batch可以减少正则化效果。</p><p>一般来说，我们不会把Batch Norm当做正则化方式，而是把它当做将归一化隐藏层并且加速学习的一种方式。</p><h2 id="Batch-Norm-at-test-time"><a href="#Batch-Norm-at-test-time" class="headerlink" title="Batch Norm at test time"></a>Batch Norm at test time</h2><p>Batch Nrom将数据以mini-batch的形式进行处理，但在测试时，我们可能需要对每个样本逐一处理（预测）。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7e8c3f45eb2fc31c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>回想最开始，我们是通过以上等式执行Batch Norm。在一个mini-batch中，将所有的Z(i)值求和计算均值，计算方差后再计算z_norm(i)，最后再次调整z_norm得到z_tilda。注意，用于计算的均值和方差是在整个mini-batch上计算的，但在测试时，我们不可能将一个mini-batch的样本同时处理，因此需要用其他方式得到均值和方差，并且假设我们只有一个样本的话，一个样本的均值和方差没有意义。因此实际上，为了将我们的神经网络运用于测试，需要单独估算均值和方差。在典型的Batch Norm运用中，我们需要用一个<strong>指数加权平均</strong>来估算，这个平均值涵盖了所有的mini-batch。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-98d15e5d90f5b2e3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>假设我们在<strong>训练集</strong>上有多个mini-batch，通过在每个mini-batch上计算当前隐藏层的均值mu和方差，我们得到了每一层的均值和方差的不同数值（以mini-batch来变化），因此我们可以像之前计算温度一样计算得到均值和方差的指数加权平均值。最后在测试时，使用均值和方差的指数加权平均来求z_norm，再使用我们在神经网络训练过程中得到的beta和gamma参数来计算我们的测试样本的z_tilda值。</p><p>关于Batch Norm更详细的知识解释可看：<a href="https://www.cnblogs.com/guoyaohua/p/8724433.html" target="_blank" rel="noopener">https://www.cnblogs.com/guoyaohua/p/8724433.html</a></p><h1 id="Softmax回归"><a href="#Softmax回归" class="headerlink" title="Softmax回归"></a>Softmax回归</h1><p>在之前我们所讲到的分类都是二元分类，接下来讲解与多元分类相关的Softmax回归。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-bbb70262d7bbfa50.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们用大写字母C来表示输入会被分入的类别总个数，如上图一共有4类，即0,1,2,3。我们要用神经网络来进行多元分类，希望有输出层的神经元个数来告知我们这4种类型中每一个的概率有多大。（为什么这里输出层单元可以有这样的对应关系呢？我不明白）<br><img src="https://upload-images.jianshu.io/upload_images/8636110-50c507c273d8ca17.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>要做到多个概率的输入，需要用到Softmax函数。与sigmoid和relu激活函数的输入和输出不同（这两者的输入输出都是一个实值），Softmax的输入可以是一个向量。由下图可知，我们的Z[L]的维度为(4,1)，而得到的输出a[L]的维度也是(4,1)。并且，计算时首先算出Z[L]每个元素的指数幂，随后再进行整体归一化，得到对应的概率值，而这个概率值也就是我们想要的结果。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-09b94965cfeb2353.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在图中右边也给了一个简单的计算例子，即算出来Z[L] = [5 2 -1 3].T，通过计算指数幂得到t = [148.4, 7.4, 0.4, 20.1].T，总和为176.3，从而计算得到概率输出为a[L] = [0.842, 0.042, 0.002, 0.114].T。</p><p>接下来举了没有隐藏层的神经网络结合Softmax的例子方便理解。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-2973d044b8e02939.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>可以看到，尽管没有输出层，Softmax还是可以学习到线性分界，那么结合隐藏层的话，就可以得到更复杂的非线性分界了。</p><h1 id="训练一个Softmax分类器"><a href="#训练一个Softmax分类器" class="headerlink" title="训练一个Softmax分类器"></a>训练一个Softmax分类器</h1><h2 id="Understanding-softmax"><a href="#Understanding-softmax" class="headerlink" title="Understanding softmax"></a>Understanding softmax</h2><p>用临时变量t进行归一化，之后计算得到对应的概率。hard max会观察Z的值，然后直接在最大的元素上设置输出为1，其他的为0；而Softmax则使得Z到概率之间的映射更为温和。而Softmax回归实际上是Logistic回归的扩展。当C=2，我们可以得到输出层的两个概率，而由于我们实际上不需要两个概率，只要得到其中一个值就可以知道另一个，因此Logistic的输出实际上只有一个。因此我们可以说softmax回归将logistic回归推广到了两种分类以上。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-92f9e7e0c5bf6fca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="Loss-function"><a href="#Loss-function" class="headerlink" title="Loss function"></a>Loss function</h2><p>假设我们的ground truth label是cat，即y = [0 1 0 0].T，而我们训练得到的a[L] = y_hat = [0.3 0.2 0.1 0.4].T，这实际上不是好的结果。那么我们需要一个loss function来衡量误差。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-0106142b36f0b484.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>概括来说，损失函数所做的就是找到训练集中的真实类别，然后试图使该类别相应的概率尽可能地高。左边显示的是单个样本的loss function，而右边以W,b为参数的则是整个数据集的loss function。</p><h2 id="Gradient-descent-with-softmax"><a href="#Gradient-descent-with-softmax" class="headerlink" title="Gradient descent with softmax"></a>Gradient descent with softmax</h2><p>由于之后可以用深度学习框架来做作业了，吴恩达老师没有讲具体的求导过程…</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-80661870a8174346.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>其实，最关键的就是求得a关于z的导数，这里分为了两种情况：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7deaa194aa1f6222.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-1c1ce0c9393e4a41.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>得到了上述之后，再求Loss函数关于a的导数，相乘即可，从而就有了Loss函数关于z的导数。</p><h1 id="深度学习框架"><a href="#深度学习框架" class="headerlink" title="深度学习框架"></a>深度学习框架</h1><p><img src="https://upload-images.jianshu.io/upload_images/8636110-dd2f0508b990f685.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>还需补上Pytorch，这个也是必须掌握的。所以我要掌握的有Tensorflow和Pytorch。</p><h1 id="Tensorflow"><a href="#Tensorflow" class="headerlink" title="Tensorflow"></a>Tensorflow</h1><p>这里吴恩达老师给了一些Tensorflow基本结构的例子。</p><p>首先，我们设置cost function为J(w) = w^2 - l0*w + 25，而我们希望求得使得J(w)最小化的w（显然w=5时J最小），简单的tensorflow程序如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d7f70c6b0f4c858f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>注意到cost函数可以由注释部分写为下方比较简单的形式。而tf.train.GradientDescentOptimizer中的参数为学习率，只有当run学习函数的时候，w才会变化。再经过1000次迭代后，注意到输出的w为4.9999886，这与w=5非常接近。</p><p>如果我们希望加入训练数据，比如在这个二次方程中，希望将方程的系数作为输入的数据，可以用tf.placeholder来完成。示例代码如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-9e6d230b9de67024.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>注意，我们使用feed_dict参数传入我们的训练数据。</p><p>另外，有个可以注意的地方：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e97f4875e64ea430.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>通常我们写程序的时候，采取右边的with方式来写，这种写法有利于在执行内循环出错时的内存释放。</p><p>Tensorflow程序的核心是计算损失函数，然后Tensorflow会自动求出导数，以及如何最小化损失。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-6fe151d952572db2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>这个损失函数的作用就是让TensorFlow建立计算图，计算图所做的事情如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-408e2209b83d8c8f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>而Tensorflow的优点是，通过用这个计算图基本实现前向传播，而且内置了所有必要的反向函数，因此我们在使用内置函数计算前向传播时，它可以自动地计算反向传播。（Tensorflow计算图用的是运算符作为结点）</p><h1 id="本周作业"><a href="#本周作业" class="headerlink" title="本周作业"></a>本周作业</h1><h2 id="1-Exploring-the-Tensorflow-Library"><a href="#1-Exploring-the-Tensorflow-Library" class="headerlink" title="1- Exploring the Tensorflow Library"></a>1- Exploring the Tensorflow Library</h2><p>Import packages:<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.python.framework <span class="keyword">import</span> ops</span><br><span class="line"><span class="keyword">from</span> tf_utils <span class="keyword">import</span> load_dataset, random_mini_batches, convert_to_one_hot, predict</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p><p>首先给出一个简单的Loss function的例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e94150daadfbe46c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">y_hat = tf.constant(<span class="number">36</span>, name=<span class="string">"y_hat"</span>)</span><br><span class="line">y = tf.constant(<span class="number">39</span>, name=<span class="string">"y"</span>)</span><br><span class="line"></span><br><span class="line">loss = tf.Variable((y-y_hat)**<span class="number">2</span>, name=<span class="string">"loss"</span>)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(loss))</span><br></pre></td></tr></table></figure><p>Writing and running programs in TensorFlow has the following steps:</p><ol><li>Create Tensors (variables) that are not yet executed/evaluated.</li><li>Write operations between those Tensors.</li><li>Initialize your Tensors.</li><li>Create a Session.</li><li>Run the Session. This will run the operations you’d written above.</li></ol><p>接下来看一个例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-def65b969b402de8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>我们没有得到20的结果，而是得到了一个tensor的介绍：You got a tensor saying that the result is a tensor that does not have the shape attribute, and is of type “int32”. 我们所做的只是将其放入了计算图，但并没有开始运算。为了能够使这两个数字相乘，我们需要创建会话并且运行它。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-5022cfcfc1a94396.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>Summarize: remember to initialize your variables, create a session and run the operations inside the session.</strong></p><p>接下来，我们需要知道placeholders。A placeholder is an object whose value you can specify only later. To specify values for a placeholder, you can pass in values by using a “feed dictionary” (feed_dict variable). Below, we created a placeholder for x. This allows us to pass in a number later when we run the session.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-0b3589e4d39947fe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Here’s what’s happening: When you specify the operations needed for a computation, you are telling TensorFlow how to construct a computation graph. The computation graph can have some placeholders whose values you will specify only later. Finally, when you run the session, you are telling TensorFlow to execute the computation graph.</p><h3 id="1-1-Linear-function"><a href="#1-1-Linear-function" class="headerlink" title="1.1- Linear function"></a>1.1- Linear function</h3><p>Lets start this programming exercise by computing the following equation: Y=WX+bY, where W and  X are random matrices and b is a random vector.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_function</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a linear function: </span></span><br><span class="line"><span class="string">            Initializes W to be a random tensor of shape (4,3)</span></span><br><span class="line"><span class="string">            Initializes X to be a random tensor of shape (3,1)</span></span><br><span class="line"><span class="string">            Initializes b to be a random tensor of shape (4,1)</span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    result -- runs the session for Y = WX + b </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    <span class="comment">### START CODE HERE ### (4 lines of code)</span></span><br><span class="line">    X = tf.constant(np.random.randn(<span class="number">3</span>,<span class="number">1</span>), name=<span class="string">"X"</span>)</span><br><span class="line">    W = tf.constant(np.random.randn(<span class="number">4</span>,<span class="number">3</span>), name=<span class="string">"W"</span>)</span><br><span class="line">    b = tf.constant(np.random.randn(<span class="number">4</span>,<span class="number">1</span>), name=<span class="string">"b"</span>)</span><br><span class="line">    Y = tf.add(tf.matmul(W,X),b)</span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create the session using tf.Session() and run it with sess.run(...) on the variable you want to calculate</span></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    result = sess.run(Y)</span><br><span class="line">    <span class="comment">### END CODE HERE ### </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># close the session </span></span><br><span class="line">    sess.close()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><h3 id="1-2-Computing-the-sigmoid"><a href="#1-2-Computing-the-sigmoid" class="headerlink" title="1.2- Computing the sigmoid"></a>1.2- Computing the sigmoid</h3><p>tf.placeholder的参数意义：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tf.placeholder(</span><br><span class="line">    dtype,</span><br><span class="line">    shape=None,</span><br><span class="line">    name=None</span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><p>Tensorflow offers a variety of commonly used neural network functions like tf.sigmoid and tf.softmax. For this exercise lets compute the sigmoid function of an input. You will do this exercise using a placeholder variable x. When running the session, you should use the feed dictionary to pass in the input z. In this exercise, you will have to (i) create a placeholder x, (ii) define the operations needed to compute the sigmoid using tf.sigmoid, and then (iii) run the session.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the sigmoid of z</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    z -- input value, scalar or vector</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    results -- the sigmoid of z</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### ( approx. 4 lines of code)</span></span><br><span class="line">    <span class="comment"># Create a placeholder for x. Name it 'x'.</span></span><br><span class="line">    x = tf.placeholder(tf.float32, name=<span class="string">"x"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute sigmoid(x)</span></span><br><span class="line">    sigmoid = tf.sigmoid(x)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create a session, and run it. Please use the method 2 explained above. </span></span><br><span class="line">    <span class="comment"># You should use a feed_dict to pass z's value to x. </span></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        <span class="comment"># Run session and call the output "result"</span></span><br><span class="line">        result = sess.run(sigmoid, feed_dict=&#123;x:z&#125;)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure><p><strong>Summarize</strong>:</p><ol><li>Create placeholders.</li><li>Specify the computation graph corresponding to operations you want to compute.</li><li>Create the session.</li><li>Run the session, using a feed dictionary if necessary to specify placeholder variables’ values.</li></ol><h3 id="1-3-Computing-the-Cost"><a href="#1-3-Computing-the-Cost" class="headerlink" title="1.3- Computing the Cost"></a>1.3- Computing the Cost</h3><p>You can also use a built-in function to compute the cost of your neural network. So instead of needing to write code to compute this as a function of a[2]_i and y(i) for i=1…m:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-0db81609e5d5461e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这里用的一个函数是tf.nn.sigmoid_cross_entropy_with_logits，其中tf.nn.sigmoid_cross_entropy_with_logits(logits = …,  labels = …)，另外可以看到下面注释的notes中写着：What we’ve been calling “z” and “y” in this class are respectively called “logits” and “labels” in the TensorFlow documentation. So logits will feed into z, and labels into y. </p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cost</span><span class="params">(logits, labels)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the cost using the sigmoid cross entropy</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    logits -- vector containing z, output of the last linear unit (before the final sigmoid activation)</span></span><br><span class="line"><span class="string">    labels -- vector of labels y (1 or 0) </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Note: What we've been calling "z" and "y" in this class are respectively called "logits" and "labels" </span></span><br><span class="line"><span class="string">    in the TensorFlow documentation. So logits will feed into z, and labels into y. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost -- runs the session of the cost (formula (2))</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create the placeholders for "logits" (z) and "labels" (y) (approx. 2 lines)</span></span><br><span class="line">    z = tf.placeholder(tf.float32, name=<span class="string">"z"</span>)</span><br><span class="line">    y = tf.placeholder(tf.float32, name=<span class="string">"y"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Use the loss function (approx. 1 line)</span></span><br><span class="line">    cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=z,labels=y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create a session (approx. 1 line). See method 1 above.</span></span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Run the session (approx. 1 line).</span></span><br><span class="line">    cost = sess.run(cost, feed_dict=&#123;z:logits, y:labels&#125;)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Close the session (approx. 1 line). See method 1 above.</span></span><br><span class="line">    sess.close()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><h3 id="1-4-Using-One-Hot-encodings"><a href="#1-4-Using-One-Hot-encodings" class="headerlink" title="1.4- Using One Hot encodings"></a>1.4- Using One Hot encodings</h3><p>Many times in deep learning you will have a y vector with numbers ranging from 0 to C-1, where C is the number of classes. If C is for example 4, then you might have the following y vector which you will need to convert as follows:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-26496561ada5c4f3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>This is called a “one hot” encoding, because in the converted representation exactly one element of each column is “hot” (meaning set to 1). To do this conversion in numpy, you might have to write a few lines of code. In tensorflow, you can use one line of code:</p><ul><li>tf.one_hot(indices, depth, axis)</li></ul><p>注意，axis=0是按列编码，即与上图中类似，而axis=1则按照行编码。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_hot_matrix</span><span class="params">(labels, C)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates a matrix where the i-th row corresponds to the ith class number and the jth column</span></span><br><span class="line"><span class="string">                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) </span></span><br><span class="line"><span class="string">                     will be 1. </span></span><br><span class="line"><span class="string">                     </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    labels -- vector containing the labels </span></span><br><span class="line"><span class="string">    C -- number of classes, the depth of the one hot dimension</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    one_hot -- one hot matrix</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line)</span></span><br><span class="line">    C = tf.constant(C,name=<span class="string">"C"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Use tf.one_hot, be careful with the axis (approx. 1 line)</span></span><br><span class="line">    one_hot_matrix = tf.one_hot(indices=labels,depth=C,axis=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create the session (approx. 1 line)</span></span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Run the session (approx. 1 line)</span></span><br><span class="line">    one_hot = sess.run(one_hot_matrix)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Close the session (approx. 1 line). See method 1 above.</span></span><br><span class="line">    sess.close()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> one_hot</span><br></pre></td></tr></table></figure><h3 id="1-5-Initialize-with-zeros-and-ones"><a href="#1-5-Initialize-with-zeros-and-ones" class="headerlink" title="1.5- Initialize with zeros and ones"></a>1.5- Initialize with zeros and ones</h3><p>Now you will learn how to initialize a vector of zeros and ones. The function you will be calling is tf.ones(). To initialize with zeros you could use tf.zeros() instead. These functions take in a shape and return an array of dimension shape full of zeros and ones respectively.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ones</span><span class="params">(shape)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates an array of ones of dimension shape</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    shape -- shape of the array you want to create</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    ones -- array containing only ones</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create "ones" tensor using tf.ones(...). (approx. 1 line)</span></span><br><span class="line">    ones = tf.ones(shape)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create the session (approx. 1 line)</span></span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Run the session to compute 'ones' (approx. 1 line)</span></span><br><span class="line">    ones = sess.run(ones)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Close the session (approx. 1 line). See method 1 above.</span></span><br><span class="line">    sess.close()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> ones</span><br></pre></td></tr></table></figure><h2 id="2-Building-your-first-neural-network-in-tensorflow"><a href="#2-Building-your-first-neural-network-in-tensorflow" class="headerlink" title="2- Building your first neural network in tensorflow"></a>2- Building your first neural network in tensorflow</h2><p>In this part of the assignment you will build a neural network using tensorflow. Remember that there are two parts to implement a tensorflow model:</p><ul><li>Create the computation graph</li><li>Run the graph</li></ul><h3 id="2-0-Problem-statement-SIGNS-Dataset"><a href="#2-0-Problem-statement-SIGNS-Dataset" class="headerlink" title="2.0- Problem statement: SIGNS Dataset"></a>2.0- Problem statement: SIGNS Dataset</h3><p>One afternoon, with some friends we decided to teach our computers to decipher sign language. We spent a few hours taking pictures in front of a white wall and came up with the following dataset. It’s now your job to build an algorithm that would facilitate communications from a speech-impaired person to someone who doesn’t understand sign language.</p><ul><li><strong>Training set</strong>: 1080 pictures (64 by 64 pixels) of signs representing numbers from 0 to 5 (180 pictures per number).</li><li><strong>Test set</strong>: 120 pictures (64 by 64 pixels) of signs representing numbers from 0 to 5 (20 pictures per number).</li></ul><p>Note that this is a subset of the SIGNS dataset. The complete dataset contains many more signs. Here are examples for each number, and how an explanation of how we represent the labels. These are the original pictures, before we lowered the image resolutoion to 64 by 64 pixels.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-926f06281619b2fb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Run the following code to load the dataset.<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()</span><br></pre></td></tr></table></figure></p><p>Change the index below and run the cell to visualize some examples in the dataset.<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example of a picture</span></span><br><span class="line">index = <span class="number">0</span></span><br><span class="line">plt.imshow(X_train_orig[index])</span><br><span class="line">print(<span class="string">"y = "</span> + str(np.squeeze(Y_train_orig[:,index])))</span><br></pre></td></tr></table></figure></p><p>As usual you flatten the image dataset, then normalize it by dividing by 255. On top of that, you will convert each label to a one-hot vector as shown in Figure 1. Run the cell below to do so.<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Flatten the traing and test images</span></span><br><span class="line">X_train_flatten = X_train_orig.reshape(X_train_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line">X_test_flatten = X_test_orig.reshape(X_test_orig.shape[<span class="number">0</span>], <span class="number">-1</span>).T</span><br><span class="line"><span class="comment"># Normalize the image vectors</span></span><br><span class="line">X_train = X_train_flatten / <span class="number">255.</span></span><br><span class="line">X_test = X_test_flatten / <span class="number">255.</span></span><br><span class="line"><span class="comment"># Convert training and test labels to one hot matrixs</span></span><br><span class="line">Y_train = convert_to_one_hot(Y_train_orig, <span class="number">6</span>)</span><br><span class="line">Y_test = convert_to_one_hot(Y_test_orig, <span class="number">6</span>)</span><br></pre></td></tr></table></figure></p><p>Your goal is to build an algorithm capable of recognizing a sign with high accuracy. To do so, you are going to build a tensorflow model that is almost the same as one you have previously built in numpy for cat recognition (but now using a softmax output). It is a great occasion to compare your numpy implementation to the tensorflow one.</p><p>The model is LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SOFTMAX. The SIGMOID output layer has been converted to a SOFTMAX. A SOFTMAX layer generalizes SIGMOID to when there are more than two classes.</p><h3 id="2-1-Create-placeholders"><a href="#2-1-Create-placeholders" class="headerlink" title="2.1- Create placeholders"></a>2.1- Create placeholders</h3><p>Your first task is to create placeholders for X and Y. This will allow you to later pass your training data in when you run your session.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_placeholders</span><span class="params">(n_x, n_y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates the placeholders for the tensorflow session.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)</span></span><br><span class="line"><span class="string">    n_y -- scalar, number of classes (from 0 to 5, so -&gt; 6)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X -- placeholder for the data input, of shape [n_x, None] and dtype "float"</span></span><br><span class="line"><span class="string">    Y -- placeholder for the input labels, of shape [n_y, None] and dtype "float"</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Tips:</span></span><br><span class="line"><span class="string">    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.</span></span><br><span class="line"><span class="string">      In fact, the number of examples during test/train is different.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">    X = tf.placeholder(dtype=<span class="string">"float"</span>,shape=(n_x,<span class="keyword">None</span>))</span><br><span class="line">    Y = tf.placeholder(dtype=<span class="string">"float"</span>,shape=(n_y,<span class="keyword">None</span>))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X, Y</span><br></pre></td></tr></table></figure><h3 id="2-2-Initializing-the-parameters"><a href="#2-2-Initializing-the-parameters" class="headerlink" title="2.2- Initializing the parameters"></a>2.2- Initializing the parameters</h3><p>Your second task is to initialize the parameters in tensorflow. </p><p><strong>Exercise</strong>: Implement the function below to initialize the parameters in tensorflow. You are going use Xavier Initialization for weights and Zero Initialization for biases. The shapes are given below. As an example, to help you, for W1 and b1 you could use:</p><ul><li>W1 = tf.get_variable(“W1”, [25,12288], initializer = tf.contrib.layers.xavier_initializer(seed = 1)) (tf.contrib这一方法在Tensorflow2.0会被取消)</li><li>b1 = tf.get_variable(“b1”, [25,1], initializer = tf.zeros_initializer())</li></ul><p>先要写一下tf.get_variable的各个参数名和含义：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">tf.get_variable(</span><br><span class="line">    name, <span class="comment"># 新变量或现有变量的名称。</span></span><br><span class="line">    shape=<span class="keyword">None</span>, <span class="comment"># 新变量或现有变量的形状。</span></span><br><span class="line">    dtype=<span class="keyword">None</span>, <span class="comment"># 新变量或现有变量的类型（默认为DT_FLOAT）。</span></span><br><span class="line">    initializer=<span class="keyword">None</span>, <span class="comment"># 如果创建了则用它来初始化变量。</span></span><br><span class="line">    regularizer=<span class="keyword">None</span>, <span class="comment"># </span></span><br><span class="line">    trainable=<span class="keyword">True</span>, <span class="comment"># 如果为True，还将变量添加到图形集合GraphKeys.TRAINABLE_VARIABLES</span></span><br><span class="line">    collections=<span class="keyword">None</span>, <span class="comment"># 要将变量添加到的图表集合列表</span></span><br><span class="line">    caching_device=<span class="keyword">None</span>, <span class="comment"># 可选的设备字符串或函数，描述变量应被缓存以供读取的位置。</span></span><br><span class="line">    partitioner=<span class="keyword">None</span>, <span class="comment"># 可选callable，接受完全定义的TensorShape和要创建的Variable的dtype，并返回每个轴的分区列表</span></span><br><span class="line">    validate_shape=<span class="keyword">True</span>, <span class="comment"># 如果为False，则允许使用未知形状的值初始化变量。</span></span><br><span class="line">    use_resource=<span class="keyword">None</span>, <span class="comment"># 如果为False，则创建常规变量。如果为true，则使用定义良好的语义创建实验性ResourceVariable。</span></span><br><span class="line">    custom_getter=<span class="keyword">None</span>,</span><br><span class="line">    constraint=<span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Initializes parameters to build a neural network with tensorflow. The shapes are:</span></span><br><span class="line"><span class="string">                        W1 : [25, 12288]</span></span><br><span class="line"><span class="string">                        b1 : [25, 1]</span></span><br><span class="line"><span class="string">                        W2 : [12, 25]</span></span><br><span class="line"><span class="string">                        b2 : [12, 1]</span></span><br><span class="line"><span class="string">                        W3 : [6, 12]</span></span><br><span class="line"><span class="string">                        b3 : [6, 1]</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    tf.set_random_seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 6 lines of code)</span></span><br><span class="line">    W1 = tf.get_variable(<span class="string">"W1"</span>, shape=(<span class="number">25</span>,<span class="number">12288</span>), initializer=tf.contrib.layers.xavier_initializer(seed=<span class="number">1</span>))</span><br><span class="line">    b1 = tf.get_variable(<span class="string">"b1"</span>, shape=(<span class="number">25</span>,<span class="number">1</span>), initializer=tf.zeros_initializer())</span><br><span class="line">    W2 = tf.get_variable(<span class="string">"W2"</span>, shape=(<span class="number">12</span>,<span class="number">25</span>), initializer=tf.contrib.layers.xavier_initializer(seed=<span class="number">1</span>))</span><br><span class="line">    b2 = tf.get_variable(<span class="string">"b2"</span>, shape=(<span class="number">12</span>,<span class="number">1</span>), initializer=tf.zeros_initializer())</span><br><span class="line">    W3 = tf.get_variable(<span class="string">"W3"</span>, shape=(<span class="number">6</span>,<span class="number">12</span>), initializer=tf.contrib.layers.xavier_initializer(seed=<span class="number">1</span>))</span><br><span class="line">    b3 = tf.get_variable(<span class="string">"b3"</span>, shape=(<span class="number">6</span>,<span class="number">1</span>), initializer=tf.zeros_initializer())</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2,</span><br><span class="line">                  <span class="string">"W3"</span>: W3,</span><br><span class="line">                  <span class="string">"b3"</span>: b3&#125;</span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>At this moment, the parameters haven’t been evaluated yet.</p><h3 id="2-3-Forward-propagation-in-tensorflow"><a href="#2-3-Forward-propagation-in-tensorflow" class="headerlink" title="2.3- Forward propagation in tensorflow"></a>2.3- Forward propagation in tensorflow</h3><p>You will now implement the forward propagation module in tensorflow. The function will take in a dictionary of parameters and it will complete the forward pass. The functions you will be using are:</p><ul><li>tf.add(…,…) to do an addition</li><li>tf.matmul(…,…) to do a matrix multiplication</li><li>tf.nn.relu(…) to apply the ReLU activation</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the forward propagation for the model: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SOFTMAX</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset placeholder, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3"</span></span><br><span class="line"><span class="string">                  the shapes are given in initialize_parameters</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z3 -- the output of the last LINEAR unit</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve the parameters from the dictionary "parameters" </span></span><br><span class="line">    W1 = parameters[<span class="string">'W1'</span>]</span><br><span class="line">    b1 = parameters[<span class="string">'b1'</span>]</span><br><span class="line">    W2 = parameters[<span class="string">'W2'</span>]</span><br><span class="line">    b2 = parameters[<span class="string">'b2'</span>]</span><br><span class="line">    W3 = parameters[<span class="string">'W3'</span>]</span><br><span class="line">    b3 = parameters[<span class="string">'b3'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:</span></span><br><span class="line">    Z1 = tf.add(tf.matmul(W1,X),b1)                                          <span class="comment"># Z1 = np.dot(W1, X) + b1</span></span><br><span class="line">    A1 = tf.nn.relu(Z1)                                              <span class="comment"># A1 = relu(Z1)</span></span><br><span class="line">    Z2 = tf.add(tf.matmul(W2,A1),b2)                                              <span class="comment"># Z2 = np.dot(W2, a1) + b2</span></span><br><span class="line">    A2 = tf.nn.relu(Z2)                                             <span class="comment"># A2 = relu(Z2)</span></span><br><span class="line">    Z3 = tf.add(tf.matmul(W3,A2),b3)                                              <span class="comment"># Z3 = np.dot(W3,Z2) + b3</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z3</span><br></pre></td></tr></table></figure><p>You may have noticed that the forward propagation doesn’t output any cache. You will understand why below, when we get to brackpropagation.</p><h3 id="2-4-Compute-cost"><a href="#2-4-Compute-cost" class="headerlink" title="2.4- Compute cost"></a>2.4- Compute cost</h3><p>As seen before, it is very easy to compute the cost using:</p><pre><code>- tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = ..., labels = ...))</code></pre><p><strong>Question</strong>: Implement the cost function below.</p><ul><li>It is important to know that the “logits” and “labels” inputs of tf.nn.softmax_cross_entropy_with_logits are expected to be of shape (number of examples, num_classes). We have thus transposed Z3 and Y for you.</li><li>Besides, tf.reduce_mean basically does the summation over the examples. 注意！</li></ul><p>这里要注意tf.nn.softmax_cross_entropy_with_logits，其参数含义如下：</p><ul><li>logits: 神经网络最后一层的输出，如果有batch的话，它的大小就是[batchsize，num_classes]，单样本的话，大小就是num_classes.</li><li>labels: 实际的标签，大小同上。</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(Z3, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the cost</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector placeholder, same shape as Z3</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost - Tensor of the cost function</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)</span></span><br><span class="line">    logits = tf.transpose(Z3)</span><br><span class="line">    labels = tf.transpose(Y)</span><br><span class="line"></span><br><span class="line">    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><h3 id="2-5-Backward-propagation-amp-parameter-update"><a href="#2-5-Backward-propagation-amp-parameter-update" class="headerlink" title="2.5- Backward propagation &amp; parameter update"></a>2.5- Backward propagation &amp; parameter update</h3><p>This is where you become grateful to programming frameworks. All the backpropagation and the parameters update is taken care of in 1 line of code. It is very easy to incorporate this line in the model.</p><p>After you compute the cost function. You will create an “optimizer” object. You have to call this object along with the cost when running the tf.session. When called, it will perform an optimization on the given cost with the chosen method and learning rate.</p><p>For instance, for gradient descent the optimizer would be:</p><ul><li>optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)</li></ul><p>To make the optimization you would do:</p><ul><li>_, c = sess.run([optimizer, cost], feed_dict={X:minibatch_X, Y:minibatch_Y})</li></ul><p>This computes the backpropagation by passing through the tensorflow graph in the reverse order. From cost to inputs.</p><p><strong>Note</strong> When coding, we often use _ as a “throwaway” variable to store values that we won’t need to use later. Here, _ takes on the evaluated value of optimizer, which we don’t need (and c takes the value of the cost variable).</p><h3 id="2-6-Building-the-model"><a href="#2-6-Building-the-model" class="headerlink" title="2.6- Building the model"></a>2.6- Building the model</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X_train, Y_train, X_test, Y_test, learning_rate=<span class="number">0.0001</span>, num_epoches=<span class="number">1500</span>, minibatch_size=<span class="number">32</span>, print_cost=True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a three-layer tensorflow neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SOFTMAX.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)</span></span><br><span class="line"><span class="string">    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)</span></span><br><span class="line"><span class="string">    X_test -- training set, of shape (input size = 12288, number of training examples = 120)</span></span><br><span class="line"><span class="string">    Y_test -- test set, of shape (output size = 6, number of test examples = 120)</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the optimization</span></span><br><span class="line"><span class="string">    num_epochs -- number of epochs of the optimization loop</span></span><br><span class="line"><span class="string">    minibatch_size -- size of a minibatch</span></span><br><span class="line"><span class="string">    print_cost -- True to print the cost every 100 epochs</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    ops.reset_default_graph() <span class="comment"># tensorflow在生产环境下，需要将default graph 重新初始化，以保证内存中没有其他的Graph，或者说我们需要在每个session之后清理相应的Graph。</span></span><br><span class="line">    tf.set_random_seed(<span class="number">1</span>)</span><br><span class="line">    seed = <span class="number">3</span></span><br><span class="line">    (n_x, m) = X_train.shape</span><br><span class="line">    n_y = Y_train.shape[<span class="number">0</span>]</span><br><span class="line">    costs = []</span><br><span class="line"></span><br><span class="line">    X, Y = create_placeholders(n_x,, n_y)</span><br><span class="line"></span><br><span class="line">    parameters = initialize_parameters()</span><br><span class="line"></span><br><span class="line">    Z3 = forward_propagation(X, parameters)</span><br><span class="line"></span><br><span class="line">    cost = compute_cost(Z3, Y)</span><br><span class="line"></span><br><span class="line">    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)</span><br><span class="line"></span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        sess.run(init)</span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">            epoch_cost = <span class="number">0</span></span><br><span class="line">            num_minibatches = int(m / minibatch_size)</span><br><span class="line">            seed += <span class="number">1</span></span><br><span class="line">            minibatches = random_mini_batches(X_train, Y_train, minibatch_size)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> minibatch <span class="keyword">in</span> minibatches:</span><br><span class="line">                (minibatch_X, minibatch_Y) = minibatch</span><br><span class="line">                _, minibatch_cost = sess.run([optimizer, cost], feed_dict=&#123;X:minibatch_X,Y:minibatch_Y&#125;) <span class="comment"># 列表表示optimizer和cost同时计算</span></span><br><span class="line">                epoch_cost += minibatch_cost / num_minibatches</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> print_cost == <span class="keyword">True</span> <span class="keyword">and</span> epoch % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">print</span> (<span class="string">"Cost after epoch %i: %f"</span> % (epoch, epoch_cost))</span><br><span class="line">            <span class="keyword">if</span> print_cost == <span class="keyword">True</span> <span class="keyword">and</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">                costs.append(epoch_cost)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># plot the cost</span></span><br><span class="line">        plt.plot(np.squeeze(costs))</span><br><span class="line">        plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">        plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">        plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">        plt.show()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># lets save the parameters in a variable</span></span><br><span class="line">        parameters = sess.run(parameters)</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"Parameters have been trained!"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate the correct predictions</span></span><br><span class="line">        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Calculate accuracy on the test set</span></span><br><span class="line">        accuracy = tf.reduce_mean(tf.cast(correct_prediction, <span class="string">"float"</span>)) <span class="comment"># tf.cast转变数据格式</span></span><br><span class="line"></span><br><span class="line">        print(<span class="string">"Train Accuracy:"</span>, accuracy.eval(&#123;X:X_train, Y:Y_train&#125;))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line"></span><br><span class="line">parameters = model(X_train, Y_train, X_test, Y_test)</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line">结果：</span><br><span class="line">![image.png](https://upload-images.jianshu.io/upload_images/8636110-52fb43d5d3768581.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)</span><br><span class="line"></span><br><span class="line">可以看到，结果有些过拟合了。</span><br><span class="line"></span><br><span class="line">补充函数细节：</span><br><span class="line"><span class="number">1.</span> tf.equal(A, B)是对比这两个矩阵或者向量的相等的元素，如果是相等的那就返回<span class="keyword">True</span>，反正返回<span class="keyword">False</span>，返回的值的矩阵维度和A是一样的</span><br><span class="line">。</span><br><span class="line"><span class="number">2.</span> tf.cast转换数据格式，如tf.cast(correct_prediction, <span class="string">"float"</span>)，其中correct_prediction本为bool格式，现在转换为float格式。</span><br><span class="line"><span class="number">3.</span> tf.reduce_mean(A,axis=<span class="number">0</span>) <span class="comment">#求平均，其中axis=0是按列求平均，axis=1按行求</span></span><br><span class="line"><span class="number">4.</span> accuracy.eval(&#123;X:X_train, Y:Y_train&#125;的写法类似于：</span><br><span class="line">``` py</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(accuracy, feed_dict=&#123;X:X_train,Y:Y_train&#125;)</span><br></pre></td></tr></table></figure><p><strong>Insights</strong>:</p><ul><li>Your model seems big enough to fit the training set well. However, given the difference between train and test accuracy, you could try to add L2 or dropout regularization to reduce overfitting.</li><li>Think about the session as a block of code to train the model. Each time you run the session on a minibatch, it trains the parameters. In total you have run the session a large number of times (1500 epochs) until you obtained well trained parameters.</li></ul><h3 id="2-7-Reduce-overfitting"><a href="#2-7-Reduce-overfitting" class="headerlink" title="2.7- Reduce overfitting"></a>2.7- Reduce overfitting</h3><p>Add dropout, keep_prob=0.9, Result:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e6fde280ea117e2f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>发现效果很差，可能的原因是本身神经网络层数就少，而且隐藏层结点个数也比较少，因此不适合使用dropout。</p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p><strong>What you should remember</strong>:</p><ul><li>Tensorflow is a programming framework used in deep learning</li><li>The two main object classes in tensorflow are Tensors and Operators.</li><li>When you code in tensorflow you have to take the following steps:<ul><li>Create a graph containing Tensors (Variables, Placeholders …) and Operations (tf.matmul, tf.add, …)</li><li>Create a session</li><li>Initialize the session</li><li>Run the session to execute the graph</li></ul></li><li>You can execute the graph multiple times as you’ve seen in model()</li><li>The backpropagation and optimization is automatically done when running the session on the “optimizer” object.</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;调试处理-Tuning-process&quot;&gt;&lt;a href=&quot;#调试处理-Tuning-process&quot; class=&quot;headerlink&quot; title=&quot;调试处理-Tuning process&quot;&gt;&lt;/a&gt;调试处理-Tuning process&lt;/h1&gt;&lt;p&gt;在深
      
    
    </summary>
    
      <category term="深度学习" scheme="https://github.com/DesmonDay/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://github.com/DesmonDay/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>第6周-优化算法</title>
    <link href="https://github.com/DesmonDay/2019/04/16/deep-learningw6/"/>
    <id>https://github.com/DesmonDay/2019/04/16/deep-learningw6/</id>
    <published>2019-04-16T08:34:21.000Z</published>
    <updated>2019-04-17T04:55:12.575Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Mini-batch梯度下降"><a href="#Mini-batch梯度下降" class="headerlink" title="Mini-batch梯度下降"></a>Mini-batch梯度下降</h1><p>如果针对整个数据集进行梯度下降，如果数据集很大，那么速度就会很慢。因此，我们可以对训练集中的一部分进行梯度下降。因此可以将训练集分为多个mini-batch，针对每个mini-batch进行梯度下降。从图中，我们看到Batch vs Mini-Batch，这里对比的就是我们原本的针对整个训练集进行梯度下降，以及选取mini-batch进行梯度下降的两种算法的对比。另外，针对每个mini-batch，其标记符号为X^{1}…X^{t}.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b90b350915d8f58d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Mini-batch梯度下降的实现过程：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-48eeaa69726e431c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>从图中可知，我们在一次for循环中遍历所有的mini-batch样本，每次的计算仅针对X^{t}和Y^{t}来进行，整个的for循环称为一次epoch。如果我们要进行多次epoch，可以再添加一个针对epoch的for循环。</p><h1 id="理解Mini-batch梯度下降"><a href="#理解Mini-batch梯度下降" class="headerlink" title="理解Mini-batch梯度下降"></a>理解Mini-batch梯度下降</h1><p><img src="https://upload-images.jianshu.io/upload_images/8636110-2fb5165da85d56d2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>因此我们需要决定mini-batch的大小，一般有如下三种情况：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7a03ffbba9277896.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>三种选择下的不同梯度下降情况如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-551f71d8ab370f62.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-09c7eea411372cee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ol><li>随机梯度下降：失去了向量化的速度优势，且梯度来回震荡剧烈；</li><li>Mini-Batch: 学习速度最快，并且不需要处理整个数据集；</li><li>Batch: 每一次迭代时间过长，需要处理整个数据集。</li></ol><p>那么如何选择合适的位于1与m之间的mini-batch大小呢？</p><ol><li>当数据集较小，如m&lt;=2000，则使用Batch梯度下降</li><li>当数据集较大，通常选择：64,128,256,512，1024等等（2的次数）（通常这里需要多加尝试，选择合适的值）</li><li>确保mini batch的大小在可用内存大小的范围内。</li></ol><h1 id="指数加权平均"><a href="#指数加权平均" class="headerlink" title="指数加权平均"></a>指数加权平均</h1><p>首先，我们需要知道平均数求法，即比如我们现在有100天的温度值，要求这100天的平均温度值，那么我们可以直接利用公式进行计算：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-21e01bea913eb11f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>何为指数加权平均，如何计算指数加权平均，下面给了一个关于温度的例子，可以看到得到的红线相比原来的蓝点要平滑许多。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-4d85231e7f47c235.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>改变$\beta$的作用，导致我们得到的指数加权平均数的不同：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c0c6d59de3f109ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>可以看到，当$\beta$越大，得到的曲线越平缓，但是却不能很好反映温度的变化；$\beta$越小，则曲线越接近温度的变化，但这样子得到的噪声或异常值也会有很多。因此，合适的$\beta$值，也会影响着算法的表现。</p><h1 id="理解指数加权平均"><a href="#理解指数加权平均" class="headerlink" title="理解指数加权平均"></a>理解指数加权平均</h1><p>指数加权平均是几种优化算法的关键步骤，而其实质上是将每日温度与指数衰减函数相乘，然后求和。形象化的图形解释如下图：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-4a5ff2b6a6d30d45.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>另外，按照粗略的计算，当$\beta=0.9$时，相当于计算了10天的平均温度，当$\beta=0.98$时，相当于计算了50天的平均温度。</p><p>实际的实现：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d4bb770fc95e9dc4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>可以看到实现很简单，并且内存占用也很少，因此其效率也很高。尽管这不是最精确的计算，因为最精确的计算时直接计算温度的平均值，这往往会得到更好的预测，但这种做法的缺点是如果保存所有最近的温度数据和过去的天数温度，这会占用很多内存。因此在深度学习中往往使用指数加权平均数来计算。</p><h1 id="指数加权平均的偏差修正-Bias-correction-in-exponentially-weighted-average"><a href="#指数加权平均的偏差修正-Bias-correction-in-exponentially-weighted-average" class="headerlink" title="指数加权平均的偏差修正-Bias correction in exponentially weighted average"></a>指数加权平均的偏差修正-Bias correction in exponentially weighted average</h1><p>有一种偏差修正的技巧，能够使我们通过指数加权平均计算的平均数更加准确。如下图，当$\beta=0.9$时，我们得到红线；当$\beta=0.98$时，如果我们直接使用上面所写的原公式计算，即$v_t = \beta v_{t-1} + (1-\beta)\theta_t$，我们实际上得到的是紫线，而其实更合理的是绿色线。可以看到紫色曲线的起点较低，因此我们需要处理这种情况。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-859ebd4c5aa72ba2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>举原公式的一个例子，我们设置$v_0$初值为0，所以计算$v_1$时，由于$v_0=0$，而且我们第一天的温度为40的话，那么计算出来的$v_1$实际上很小，不符合实际情况，继续计算下去的结果也均会偏小。因此我们可以有方法来让估测更好，特别是在估测初期。我们将估计的值不再表示为$v_t$，而是表示为$\frac{v_t}{1-\beta ^t}$。因此当t比较小时，分母较小，我们可以消除偏差；当t比较大时，分母接近1，则还原为原式。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-befc97b423e2f524.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="动量梯度下降-Gradient-descent-with-momentum"><a href="#动量梯度下降-Gradient-descent-with-momentum" class="headerlink" title="动量梯度下降-Gradient descent with momentum"></a>动量梯度下降-Gradient descent with momentum</h1><p>有一种叫做Momentum的优化算法，它的运行速度总是比标准的梯度下降算法要快。其简单的思想就是，计算梯度的指数加权平均数，并利用该梯度来更新权重。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-985686082610c848.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>假设我们要对上图的成本函数J做梯度下降，其中红点为最优点，蓝点为我们的起点。如果我们采用梯度下降法，那么整个过程就如下图的蓝线，其频繁的上下波动明显减慢了梯度下降法的速度。为了避免大幅度的波动（紫线），我们也需要采用一个较小的学习率。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-61f572480ad43f6a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>因此，我们可以采用动量梯度下降的方法，其对应的步骤如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-dbd732b1528e2151.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>我们可以形象地把整个过程当做一个从山上滚下来的球的加速运动，其中，以$V_{db}$的计算式为例，$d_b$起到了加速度的作用，而原来的$V_{db}$则表示速度，$\beta$则表现为摩擦力，保证了球不会无限加速下去。在这个例子中，球获得了动量，因此也称为动量梯度下降。</p><p>由上图算法步骤，我们可知超参数包括学习率$\alpha$和指数加权参数$\beta$，而$\beta$最常用的值为0.9（在梯度下降中则表示平均了前十次迭代的梯度，另外0.9是很棒的鲁棒数）。在实际中，我们在使用梯度下降或M<br>omentum的时候，通常不需要考虑偏差修正。另外，$db$和$dW$最开始初始化为零向量。另外，$v_{dW}=\beta v_{dw}+(1-\beta)dW$可以写成另一种算法$v_{dW}=\beta v_{dw}+dW$，即去掉了$1-\beta$。但大多数情况下，原有的表达形式更方便，效果也更好一点。</p><h1 id="RMSprop-Root-Mean-Square-Prop"><a href="#RMSprop-Root-Mean-Square-Prop" class="headerlink" title="RMSprop - Root Mean Square Prop"></a>RMSprop - Root Mean Square Prop</h1><p>RMSprop也是可以加速梯度下降的一种优化算法。我们在上一节就看到了，普通的梯度下降在横轴方向推进，但在纵轴方向上则来回摆动。因此，我们会希望梯度的下降在纵轴的速度比较慢，而横轴则比较快，如绿线所示。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-172f808eac7f2cb0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这一部分与动量梯度下降有所不同。具体如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-2ff21025939023a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>可以看到，在梯度更新部分，后半个式子乘的是$(dW)^2$（这也是叫做Square的原因）。随后，更新参数时减去的是学习率乘以$\frac{dW}{\sqrt(S_{dW})}$，因此有Root Mean Square的说法。使用RMSprop进行更新，我们得到的梯度下降速度在纵轴方向上摆动较小，在横轴方向则持续推进。另外，我们也可以选取较大的学习率，并且也不会影响到纵轴的速度。</p><p>需要注意的是，由于在下一节会将Momentum和RMSprop结合起来，所以为了区分两个相同的超参数$\beta$，因此我们将RMSprop的超参数写作$\beta_2$。同时，为了避免分母为0，即$\sqrt(S_{dW})$接近0，因此我们会加一个小小的$\epsilon$来使得数据稳定。通常取$\epsilon$为10^{-8}.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-431dfd3e84bd8190.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>总结：RMSprop与Momentum相似的一点是可以消除梯度下降中的摆动，同时也包括mini-batch梯度下降，并且RMSprop允许我们使用一个更大的学习率$\alpha$。从而加快算法的学习速度。</p><p>一个趣事：RMSprop最开始不是在学术论文上提出的，而是Jeff在Coursera课程上提出的。</p><h1 id="Adam优化算法"><a href="#Adam优化算法" class="headerlink" title="Adam优化算法"></a>Adam优化算法</h1><p>RMSprop和Adam优化算法可以使用于不同的神经网络结构，而Adam算法是将Momentum和RMSprop结合起来的算法。算法过程如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-630ba6bad3fbc27b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>接下来对上述算法流程的一个解释：首先初始化四个变量。在每一次的迭代中，计算当前mini-batch的梯度dW和db，然后计算Momentum和RMSprop各自需要的量（注意指数加权平均的\beta表示不一样）。之后，再对两者将进行偏差矫正，之后使用更新公式将Momentum和RMSprop结合起来，从而得到了Adam优化算法。</p><p>在此算法中有许多超参数：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-89256312e13395ec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>对于后面三个，我们通常直接使用缺省值，而真正需要调整的是学习率$\alpha$。</p><p>为什么叫做Adam，其全称为Adaptive Moment Estimation.因此$\beta_1$用来计算dW微分，称为first moment，而$\beta_2$用来计算平方数的指数加权平均数，称为second moment。</p><h1 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h1><p>加快梯度下降的一个方法是随时间增长而慢慢地减少学习率，我们称为学习率衰减(decay)。对于下图，蓝线指我们使用mini-batch但是固定的学习率，因此最后我们可能不断地再最优点迭代但很难达到最优区域；而针对绿线，我们首先可以采取较大的学习率，到了接近收敛的时候，则采用较小的学习率，以便于接近最优点。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-0f0fa5c4ca3507ed.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>接下来讲讲如何实现学习率衰减。</p><p>方法：首先理解epoch的含义，一次epoch是指遍历一次训练集全集，形象化表示如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-31c17b1d12db0402.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>而学习率衰减的公式及一个简单的例子如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-6217f59e9c3c3ea4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>可以看到，这里包括了两个需要调整的超参数：$\alpha_0$和衰减率decay_rate，从而实现了学习率随着epoch的增大而不断较小的目的。</p><p>其他方法：如指数衰减。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-651b92262bcf2019.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>或者其他三种形式的衰减：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-5a0ca18edc26ef32.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>另外，也有一些人采用手动调整学习率的方法。在下周，我们会学习如何对大量的超参数进行高效搜索的方法。</p><h1 id="局部最优的问题"><a href="#局部最优的问题" class="headerlink" title="局部最优的问题"></a>局部最优的问题</h1><p>在深度学习研究早期，人们总是担心优化算法会困在极差的局部最优中。但随着深度学习理论的不断发展，我们对局部最优有了更深的理解。</p><p>当我们创建一个神经网络，通常梯度为0的点并不是这个图中的局部最优点。实际上，成本函数的零梯度点通常是鞍点(saddle points)。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b63d6a4c12dc670f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>但一个具有高维空间的成本函数，如果梯度为0，那么在每个方向它可能是凸函数，也可能是凹函数。比如，对于一个20000维的空间，如果要得到局部最优，这个可能性是比较小的，而我们更可能是遇到鞍点，如下图右图。因此我们从深度学习的历史可以学习到，我们在低维空间学习到的经验，并不适用于高维空间的情况。（低维空间容易遇到局部最优点，但高维空间更多的是鞍点）</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-3605ee1a34e68d5f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Problems of plateaus: 平稳段：导数长期接近0。对于水平平面，我们需要很长的一段时间才能达到平稳段的点，再由于左边或右边的随机扰动，最后才可以走出平稳段。因此我们可以知道，我们在高维空间里是不太可能困在局部最优中的，但另一个问题是我们可能需要较长的时间才能走出平稳段。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7ddff38de2eb2b55.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="本周作业"><a href="#本周作业" class="headerlink" title="本周作业"></a>本周作业</h1><p>代入包：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> opt_utils <span class="keyword">import</span> load_params_and_grads, initialize_parameters, forward_propagation, backward_propagation</span><br><span class="line"><span class="keyword">from</span> opt_utils <span class="keyword">import</span> compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">7.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br></pre></td></tr></table></figure></p><h2 id="1-Gradient-Descent"><a href="#1-Gradient-Descent" class="headerlink" title="1- Gradient Descent"></a>1- Gradient Descent</h2><p>一个简单的exercise:<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_with_gd</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using one step of gradient descent</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters to be updated:</span></span><br><span class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients to update each parameters:</span></span><br><span class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></span><br><span class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural networks</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Update rule for each parameter</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L): <span class="comment"># l从0开始记起，因此for循环中记为l+1</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 2 lines)</span></span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">'dW'</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">'db'</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure></p><p>接下来，展示一个区分Batch和SGD的代码样例：</p><ul><li><p><strong>(Batch) Gradient Descent</strong>:</p>  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X = data_input</span><br><span class="line">Y = labels</span><br><span class="line">parameters = initialize_parameters(layers_dims)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">    <span class="comment"># Forward propagation</span></span><br><span class="line">    a, caches = forward_propagation(X, parameters)</span><br><span class="line">    <span class="comment"># Compute cost.</span></span><br><span class="line">    cost = compute_cost(a, Y)</span><br><span class="line">    <span class="comment"># Backward propagation.</span></span><br><span class="line">    grads = backward_propagation(a, caches, parameters)</span><br><span class="line">    <span class="comment"># Update parameters.</span></span><br><span class="line">    parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure></li><li><p><strong>Stochastic Gradient Descent</strong>:</p>  <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">X = data_input</span><br><span class="line">Y = labels</span><br><span class="line">parameters = initialize_parameters(layers_dims)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>, m):</span><br><span class="line">        <span class="comment"># Forward propagation</span></span><br><span class="line">        a, caches = forward_propagation(X[:,j], parameters)</span><br><span class="line">        <span class="comment"># Compute cost</span></span><br><span class="line">        cost = compute_cost(a, Y[:,j])</span><br><span class="line">        <span class="comment"># Backward propagation</span></span><br><span class="line">        grads = backward_propagation(a, caches, parameters)</span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads)</span><br></pre></td></tr></table></figure></li></ul><p>可以看到实现随机梯度下降需要三个for循环：</p><ul><li>Over the number of iterations</li><li>Over the m training examples</li><li>Over the layers (to update all parameters, from (W[1],b[1]) to (W[L],b[L]))</li></ul><p><strong>What we should remember</strong>:</p><ul><li>The difference between gradient descent, mini-batch gradient descent and stochastic gradient descent is the number of examples you use to perform one update step.</li><li>You have to tune a learning rate hyperparameter α.</li><li>With a well-turned mini-batch size, usually it outperforms either gradient descent or stochastic gradient descent (particularly when the training set is large).</li></ul><h2 id="2-Mini-Batch-Gradient-Descent"><a href="#2-Mini-Batch-Gradient-Descent" class="headerlink" title="2- Mini-Batch Gradient Descent"></a>2- Mini-Batch Gradient Descent</h2><p>实现Mini-Batch梯度下降包括两步，一是对训练样本进行随机打乱，二是划分mini-batches。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-904369f041ec7b05.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-5fa4deb2997c6ad9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_mini_batches</span><span class="params">(X, Y, mini_batch_size=<span class="number">64</span>, seed=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates a list of random minibatches from (X, Y)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    mini_batch_size -- size of the mini-batches, integer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    mini_batches = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 1: Shuffle (X,Y)</span></span><br><span class="line">    permutation = list(np.random.permutation(m)) <span class="comment"># 将[0,m)序列打乱</span></span><br><span class="line">    shuffled_X = X[:, permutation]</span><br><span class="line">    shuffled_Y = Y[:, permutation].reshape((<span class="number">1</span>,m))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 2: Partition (Shuffled_X, shuffled_Y) Minus the end case.</span></span><br><span class="line">    num_complete_minibatches = math.floor(m / mini_batch_size) <span class="comment">#number of mini batches of size mini_batch_size in your partitionning</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>, num_complete_minibatches):</span><br><span class="line">        mini_batch_X = shuffled_X[:, mini_batch_size*k : mini_batch_size*(k+<span class="number">1</span>)]</span><br><span class="line">        mini_batch_Y = shuffle_Y[:, mini_batch_size*k : mini_batch_size*(k+<span class="number">1</span>)].reshape((<span class="number">1</span>,<span class="number">-1</span>))</span><br><span class="line">        mini_batch = (mini_batdch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Handling the end case</span></span><br><span class="line">    <span class="keyword">if</span> m % mini_batch_size != <span class="number">0</span>:</span><br><span class="line">        mini_batch_X = shuffled_X[:, mini_batch_size*num_complete_minibatches : m]</span><br><span class="line">        mini_batch_Y = shuffled_Y[:, mini_batch_size*num_complete_minibatches : m].reshape((<span class="number">1</span>,<span class="number">-1</span>))</span><br><span class="line">        mini_batch = (mini_batch_X, mini_batch_Y)</span><br><span class="line">        mini_batches.append(mini_batch)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> mini_batches</span><br></pre></td></tr></table></figure><p><strong>What we should remember</strong>:</p><ul><li>Shuffling and Partitioning are the two steps required to build mini-batches</li><li>Powers of two are often chosen to be the mini-batch size, e.g., 16,32,64,128.</li></ul><h2 id="3-Momentum"><a href="#3-Momentum" class="headerlink" title="3- Momentum"></a>3- Momentum</h2><p>Because mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will “oscillate” toward convergence. Using momentum can reduce these oscillations.</p><p>Momentum takes into account the past gradients to smooth out the update. We will store the ‘direction’ of the previous gradients in the variable v. Formally, this will be the exponentially weighted average of the gradient on previous steps. </p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_velocity</span><span class="params">(parameters)</span>:</span> <span class="comment"># 只是初始化</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Initializes the velocity as a python dictionary with:</span></span><br><span class="line"><span class="string">                - keys: "dW1", "db1", ..., "dWL", "dbL" </span></span><br><span class="line"><span class="string">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters.</span></span><br><span class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    v -- python dictionary containing the current velocity.</span></span><br><span class="line"><span class="string">                    v['dW' + str(l)] = velocity of dWl</span></span><br><span class="line"><span class="string">                    v['db' + str(l)] = velocity of dbl</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    L = len(parameters) // <span class="number">2</span></span><br><span class="line">    v = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)].shape[<span class="number">0</span>], parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)].shape[<span class="number">1</span>]))</span><br><span class="line">        v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">"b"</span>+str(l+<span class="number">1</span>)].shape[<span class="number">0</span>], parameters[<span class="string">'b'</span>+str(l+<span class="number">1</span>)].shape[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">return</span> v</span><br></pre></td></tr></table></figure><p>接下来，实现具体的Momentum算法：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-550ee71c4d175b2c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_with_momentum</span><span class="params">(parameters, grads, v, beta, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using Momentum</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients for each parameters:</span></span><br><span class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></span><br><span class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></span><br><span class="line"><span class="string">    v -- python dictionary containing the current velocity:</span></span><br><span class="line"><span class="string">                    v['dW' + str(l)] = ...</span></span><br><span class="line"><span class="string">                    v['db' + str(l)] = ...</span></span><br><span class="line"><span class="string">    beta -- the momentum hyperparameter, scalar</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    v -- python dictionary containing your updated velocities</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    L = len(parameters) // <span class="number">2</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = beta * v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] + (<span class="number">1</span>-beta) * grads[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = beta * v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] + (<span class="number">1</span>-beta) * grads[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l+<span class="number">1</span>)] =  parameters[<span class="string">'W'</span> + str(l+<span class="number">1</span>)] - learning_rate * v[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">'b'</span> + str(l+<span class="number">1</span>)] - learning_rate * v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">    <span class="keyword">return</span> parameters, v</span><br></pre></td></tr></table></figure></p><p><strong>Note</strong>:</p><ul><li>The velocity is initialized with zeros. So the algorithm will take a few iterations to “build up” velocity and start to take bigger steps.</li><li>If β = 0, then this just becomes standard gradient descent without momentum.</li></ul><p><strong>How do we choose β</strong>?</p><ul><li>The larger the momentum β is, the smoother the update because the more we take the past gradients into account. But if β is too big, it could also smooth out the updates too much.</li><li>Common values for β range from 0.8 to 0.999. If you don’t feel inclined to tune this,  β=0.9 is often a reasonable default.</li><li>Tuning the optimal β for your model might need trying several values to see what works best in term of reducing the value of the cost function J.</li></ul><p>**What we should remember”:</p><ul><li>Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent.</li><li>You have to tune a momentum hyperparameter β and a learning rate α.</li></ul><h2 id="4-Adam"><a href="#4-Adam" class="headerlink" title="4- Adam"></a>4- Adam</h2><p>Adam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp (described in lecture) and Momentum.</p><p><strong>How does Adam word?</strong></p><ol><li>It calculates an exponentially weighted average of past gradients, and stores it in variables v (before bias correction) and  v_corrected  (with bias correction).</li><li>It calculates an exponentially weighted average of the squares of the past gradients, and stores it in variables s (before bias correction) and s_corrected(with bias correction).</li><li>It updates parameters in a direction based on combining information from “1” and “2”.</li></ol><p><img src="https://upload-images.jianshu.io/upload_images/8636110-438631276f2ef3a1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_adam</span><span class="params">(parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Initializes v and s as two python dictionaries with:</span></span><br><span class="line"><span class="string">                - keys: "dW1", "db1", ..., "dWL", "dbL" </span></span><br><span class="line"><span class="string">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters.</span></span><br><span class="line"><span class="string">                    parameters["W" + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters["b" + str(l)] = bl</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns: </span></span><br><span class="line"><span class="string">    v -- python dictionary that will contain the exponentially weighted average of the gradient.</span></span><br><span class="line"><span class="string">                    v["dW" + str(l)] = ...</span></span><br><span class="line"><span class="string">                    v["db" + str(l)] = ...</span></span><br><span class="line"><span class="string">    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.</span></span><br><span class="line"><span class="string">                    s["dW" + str(l)] = ...</span></span><br><span class="line"><span class="string">                    s["db" + str(l)] = ...</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(parameters) // <span class="number">2</span> <span class="comment"># number of layers in the neural networks</span></span><br><span class="line">    v = &#123;&#125;</span><br><span class="line">    s = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        v[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)].shape[<span class="number">0</span>],parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)].shape[<span class="number">1</span>]))</span><br><span class="line">        v[<span class="string">"db"</span>+str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">"b"</span>+str(l+<span class="number">1</span>)].shape[<span class="number">0</span>],parameters[<span class="string">"b"</span>+str(l+<span class="number">1</span>)].shape[<span class="number">1</span>]))</span><br><span class="line">        s[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)].shape[<span class="number">0</span>],parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)].shape[<span class="number">1</span>]))</span><br><span class="line">        s[<span class="string">"db"</span>+str(l+<span class="number">1</span>)] = np.zeros((parameters[<span class="string">"b"</span>+str(l+<span class="number">1</span>)].shape[<span class="number">0</span>],parameters[<span class="string">"b"</span>+str(l+<span class="number">1</span>)].shape[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">return</span> v, s</span><br></pre></td></tr></table></figure><p>接下来，更新参数：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters_with_adam</span><span class="params">(parameters, grads, v, s, t, learning_rate=<span class="number">0.01</span>, beta1=<span class="number">0.9</span>, beta2=<span class="number">0.999</span>, epsilon=<span class="number">1e-8</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using Adam</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    parameters['W' + str(l)] = Wl</span></span><br><span class="line"><span class="string">                    parameters['b' + str(l)] = bl</span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients for each parameters:</span></span><br><span class="line"><span class="string">                    grads['dW' + str(l)] = dWl</span></span><br><span class="line"><span class="string">                    grads['db' + str(l)] = dbl</span></span><br><span class="line"><span class="string">    v -- Adam variable, moving average of the first gradient, python dictionary</span></span><br><span class="line"><span class="string">    s -- Adam variable, moving average of the squared gradient, python dictionary</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar.</span></span><br><span class="line"><span class="string">    beta1 -- Exponential decay hyperparameter for the first moment estimates </span></span><br><span class="line"><span class="string">    beta2 -- Exponential decay hyperparameter for the second moment estimates </span></span><br><span class="line"><span class="string">    epsilon -- hyperparameter preventing division by zero in Adam updates</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    v -- Adam variable, moving average of the first gradient, python dictionary</span></span><br><span class="line"><span class="string">    s -- Adam variable, moving average of the squared gradient, python dictionary</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(parameters) // <span class="number">2</span></span><br><span class="line">    v_corrected = &#123;&#125;</span><br><span class="line">    s_corrected = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        v[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)] = beta1 * v[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)] + (<span class="number">1</span>-beta1) * grads[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line">        v[<span class="string">"db"</span>+str(l+<span class="number">1</span>)] = beta1 * v[<span class="string">"db"</span>+str(l+<span class="number">1</span>)] + (<span class="number">1</span>-beta1) * grads[<span class="string">"db"</span> + str(l+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># bias-correction</span></span><br><span class="line">        v_corrected[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = v[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] / (<span class="number">1</span>-pow(beta1, t))</span><br><span class="line">        v_corrected[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = v[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] / (<span class="number">1</span>-pow(beta1, t))</span><br><span class="line"></span><br><span class="line">        s[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)] = beta2 * s[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)] + (<span class="number">1</span>-beta2) * pow(grads[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)],<span class="number">2</span>)</span><br><span class="line">        s[<span class="string">"db"</span>+str(l+<span class="number">1</span>)] = beta2 * s[<span class="string">"db"</span>+str(l+<span class="number">1</span>)] + (<span class="number">1</span>-beta2) * pow(grads[<span class="string">"db"</span> + str(l+<span class="number">1</span>)],<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># bias-correction</span></span><br><span class="line">        s_corrected[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = s[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] / (<span class="number">1</span>-pow(beta2, t))</span><br><span class="line">        s_corrected[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = s[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] / (<span class="number">1</span>-pow(beta2, t))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># update W and b</span></span><br><span class="line">        parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"W"</span> + str(l+<span class="number">1</span>)] - learning_rate * v_corrected[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)] / np.sqrt(s_corrected[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)] + epsilon)</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] - learning_rate * v_corrected[<span class="string">"db"</span>+str(l+<span class="number">1</span>)] / np.sqrt(s_corrected[<span class="string">"db"</span>+str(l+<span class="number">1</span>)] + epsilon)</span><br><span class="line">    <span class="keyword">return</span> parameters, v, s</span><br></pre></td></tr></table></figure></p><h2 id="5-Model-with-different-optimization-algorithm"><a href="#5-Model-with-different-optimization-algorithm" class="headerlink" title="5- Model with different optimization algorithm"></a>5- Model with different optimization algorithm</h2><p>Lets use the following “moons” dataset to test the different optimization methods. (The dataset is named “moons” because the data from each of the two classes looks a bit like a crescent-shaped moon.)<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_X, train_Y = load_dataset()</span><br></pre></td></tr></table></figure></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-5118dd90e09c52ea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, Y, layers_dims, optimizer, learning_rate=<span class="number">0.007</span>, mini_batch_size=<span class="number">64</span>, beta=<span class="number">0.9</span>, beta1=<span class="number">0.9</span>, beta2=<span class="number">0.999</span>, epsilon=<span class="number">1e-8</span>, num_epochs = <span class="number">10000</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    3-layer neural network model which can be run in different optimizer modes.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    layers_dims -- python list, containing the size of each layer</span></span><br><span class="line"><span class="string">    learning_rate -- the learning rate, scalar.</span></span><br><span class="line"><span class="string">    mini_batch_size -- the size of a mini batch</span></span><br><span class="line"><span class="string">    beta -- Momentum hyperparameter</span></span><br><span class="line"><span class="string">    beta1 -- Exponential decay hyperparameter for the past gradients estimates </span></span><br><span class="line"><span class="string">    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates </span></span><br><span class="line"><span class="string">    epsilon -- hyperparameter preventing division by zero in Adam updates</span></span><br><span class="line"><span class="string">    num_epochs -- number of epochs</span></span><br><span class="line"><span class="string">    print_cost -- True to print the cost every 1000 epochs</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    L = len(layers_dims) <span class="comment"># number of layers in the neural networks</span></span><br><span class="line">    costs = []</span><br><span class="line">    t = <span class="number">0</span></span><br><span class="line">    seed = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize parameters</span></span><br><span class="line">    parameters = initialize_parameters(layers_dims)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize the optimizer</span></span><br><span class="line">    <span class="keyword">if</span> optimizer == <span class="string">"gd"</span>:</span><br><span class="line">        <span class="keyword">pass</span> <span class="comment"># no initialization required for gradient descent</span></span><br><span class="line">    <span class="keyword">elif</span> optimizer == <span class="string">"momentum"</span>:</span><br><span class="line">        v = initialize_velocity(parameters)</span><br><span class="line">    <span class="keyword">elif</span> optimizer == <span class="string">"adam"</span>:</span><br><span class="line">        v, s = initialize_adam(parameters)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Optimization loop</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="comment"># Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch</span></span><br><span class="line">        seed = seed + <span class="number">1</span></span><br><span class="line">        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> minibatch <span class="keyword">in</span> minibatches:</span><br><span class="line">            <span class="comment"># Select a mini_batch</span></span><br><span class="line">            (minibatch_X, minibatch_Y) = minibatch</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Forward propagation</span></span><br><span class="line">            a3, caches = forward_propagation(mini_batch_X, parameters)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute cost</span></span><br><span class="line">            cost = compute_cost(a3, mini_batch_Y)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Backward propagation</span></span><br><span class="line">            grads = backward_propagation(minibatch_X, mini_batch_Y, caches)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Update parameters</span></span><br><span class="line">            <span class="keyword">if</span> optimizer == <span class="string">"gd"</span>：</span><br><span class="line">                parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">            <span class="keyword">elif</span> optimizer == <span class="string">"momentum"</span>:</span><br><span class="line">                parameters = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)</span><br><span class="line">            <span class="keyword">elif</span> optimizer == <span class="string">"adam"</span>:</span><br><span class="line">                t = t + <span class="number">1</span> <span class="comment"># Adam counter</span></span><br><span class="line">                parameters = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2, epsilon)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Print the cost every 1000 epoch</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after epoch %i: %f"</span> %(i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'epochs (per 100)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate = "</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure></p><h3 id="5-1-Mini-batch-Gradient-descent"><a href="#5-1-Mini-batch-Gradient-descent" class="headerlink" title="5.1- Mini-batch Gradient descent"></a>5.1- Mini-batch Gradient descent</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train 3-layer model</span></span><br><span class="line">layers_dims = [train_X.shape[<span class="number">0</span>], <span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">parameters = model(train_X, train_Y, layers_dims, optimizer = <span class="string">"gd"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = predict(train_X, train_Y, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot decision boundary</span></span><br><span class="line">plt.title(<span class="string">"Model with Gradient Descent optimization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">2.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><p>结果：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-8444151dd32c3ed0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-60376c25400e0caf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="5-2-Mini-batch-gradient-descent-with-momentum"><a href="#5-2-Mini-batch-gradient-descent-with-momentum" class="headerlink" title="5.2- Mini-batch gradient descent with momentum"></a>5.2- Mini-batch gradient descent with momentum</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train 3-layer model</span></span><br><span class="line">layers_dims = [train_X.shape[<span class="number">0</span>], <span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">parameters = model(train_X, train_Y, layers_dims, beta = <span class="number">0.9</span>, optimizer = <span class="string">"momentum"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = predict(train_X, train_Y, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot decision boundary</span></span><br><span class="line">plt.title(<span class="string">"Model with Momentum optimization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">2.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><p>结果：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-6be70187abe0d1f2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-659ccbba10be0431.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Because this example is relatively simple, the gains from using momemtum are small; but for more complex problems you might see bigger gains.</p><h3 id="5-3-Mini-batch-with-Adam-model"><a href="#5-3-Mini-batch-with-Adam-model" class="headerlink" title="5.3- Mini-batch with Adam model"></a>5.3- Mini-batch with Adam model</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train 3-layer model</span></span><br><span class="line">layers_dims = [train_X.shape[<span class="number">0</span>], <span class="number">5</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">parameters = model(train_X, train_Y, layers_dims, optimizer = <span class="string">"adam"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Predict</span></span><br><span class="line">predictions = predict(train_X, train_Y, parameters)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot decision boundary</span></span><br><span class="line">plt.title(<span class="string">"Model with Adam optimization"</span>)</span><br><span class="line">axes = plt.gca()</span><br><span class="line">axes.set_xlim([<span class="number">-1.5</span>,<span class="number">2.5</span>])</span><br><span class="line">axes.set_ylim([<span class="number">-1</span>,<span class="number">1.5</span>])</span><br><span class="line">plot_decision_boundary(<span class="keyword">lambda</span> x: predict_dec(parameters, x.T), train_X, train_Y)</span><br></pre></td></tr></table></figure><p>结果：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-af530967c5a62d74.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-d3d841aee105446b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="5-4-Summary"><a href="#5-4-Summary" class="headerlink" title="5.4- Summary"></a>5.4- Summary</h3><p><img src="https://upload-images.jianshu.io/upload_images/8636110-2292db4d01a01927.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Momentum usually helps, but given the small learning rate and the simplistic dataset, its impact is almost negligeable.Also, the huge oscillations you see in the cost come from the fact that some minibatches are more difficult than others for the optimization algorithm.</p><p>Adam on the other hand, clearly outperforms mini-batch gradient descent and Momentum. If you run the model for more epochs on this simple dataset, all three methods will lead to very good results. However, you’ve seen that Adam converges a lot faster.</p><p>Some advantages of Adam include:</p><ul><li>Relatively low memory requirements (though higher than gradient descent and gradient descent with momentum)</li><li>Usually works well even with little tuning of hyperparameters(except α)</li></ul><p>Adam论文链接：<a href="https://arxiv.org/pdf/1412.6980.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1412.6980.pdf</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Mini-batch梯度下降&quot;&gt;&lt;a href=&quot;#Mini-batch梯度下降&quot; class=&quot;headerlink&quot; title=&quot;Mini-batch梯度下降&quot;&gt;&lt;/a&gt;Mini-batch梯度下降&lt;/h1&gt;&lt;p&gt;如果针对整个数据集进行梯度下降，如果数据集很
      
    
    </summary>
    
      <category term="深度学习" scheme="https://github.com/DesmonDay/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://github.com/DesmonDay/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Word Embedding教程</title>
    <link href="https://github.com/DesmonDay/2019/04/15/NLP%E5%AD%A6%E4%B9%A01/"/>
    <id>https://github.com/DesmonDay/2019/04/15/NLP学习1/</id>
    <published>2019-04-15T07:26:13.000Z</published>
    <updated>2019-04-15T12:49:09.194Z</updated>
    
    <content type="html"><![CDATA[<p>参考李理的免费书籍《深度学习理论与实战：提高篇》进行学习，这里记录一些比较重要的笔记。注：本博客记录仅用于个人学习。原博客地址：<a href="https://fancyerii.github.io/2019/03/14/dl-book/" target="_blank" rel="noopener">https://fancyerii.github.io/2019/03/14/dl-book/</a> </p><h1 id="词的表示方法"><a href="#词的表示方法" class="headerlink" title="词的表示方法"></a>词的表示方法</h1><p>不同于更底层的图像和声音信号，语言是高度抽象的离散符号系统。为了能够使用神经网络来解决NLP任务，几乎所有的深度学习模型都会在第一步把离散的符号变成向量。我们希望把一个词映射到“语义”空间的一个点，使得相似的词的距离较近而不相似的较远。为此，通常用向量来表示一个点，该向量称为词向量。</p><h1 id="one-hot向量"><a href="#one-hot向量" class="headerlink" title="one-hot向量"></a>one-hot向量</h1><p>最简单的表示方法为one-hot。假设我们的词典大小为4，即一共有4个词（实际上为成千上万）。则词典中的每个词对应一个下标，每个词都用长度为4的向量表示，只有对应的下标为1，其余为0。如下图，第一个词是[1,0,0,0]，而第三个词是[0,0,1,0]。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-228b8c0619b4b24b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>然而，one-hot向量表示的问题是不满足我们的期望——相似的词的距离较近而不相似的较远。对于one-hot向量来说，相同的词距离是0，而不同的词距离是1，显然不正确。比如apple和pear的距离显然要比apple和cat的距离要近，但在one-hot表示里apple和其他所有词的距离都是1。</p><p>另外，one-hot也是一个高维的稀疏向量。而我们希望用一个低维的稠密向量来表示一个词，并且希望每一维都是表示某种语义，比如第一维代表水果（假设），那么apple和pear在这一维的值比较大，而cat的值比较小。这样apple和pear的距离就比cat和apple的距离要近。</p><h1 id="神经网络语言模型"><a href="#神经网络语言模型" class="headerlink" title="神经网络语言模型"></a>神经网络语言模型</h1><p>如何学习到比较好的词向量？最早的词向量可以追溯到神经网络语言模型。首先需要了解语言模型的概念和传统的基于统计得N-gram语言模型。</p><p>给定词序列$w_1,…,w_k$，语言模型会计算这个序列的概率，根据条件概率的定义，我们可以把联合概率分解为如下的条件概率：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-4e42aec1a0f47495.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>实际的语言模型很难考虑特别长的历史，通常我们会限定当前词的概率值依赖于之前的N-1个词，这就是所谓的N-Gram语言模型：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-2e89b69d1f069aa0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>在实际的应用中N的取值通常是2-5。</p><p>我们通常用困惑度（Perplexity）来衡量语言模型的好坏，其基本思想是：给测试集的句子赋予较高的概率值的语言模型较好，当语言模型训练完之后，测试集中的句子都是正常的句子，那么训练好的模型就是在测试集上的概率越高越好，公式如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-2b6cbebe6bffad2b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>由公式可知，句子概率越大，语言模型越好，困惑度越小。</p><p>而在语言模型的训练中，通常采用Perplexity的对数表达形式：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-0a55e734cde036d7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>N-gram语言模型可以通过最大似然方法来估计参数，假设$C(w_{k-2}w_{k-1}w_k)$表示3个词$w_{k-2}w_{k-1}w_{k}$连续出现在一起的次数，类似的$C(w_{k-2}w_{k-1})$表示两个词$w_{k-2}w_{k-1}$连续出现在一起的次数，那么：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e656b9ff23603a06.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>最大似然估计的最大问题是数据的稀疏性，如果3个词没有在训练数据中一起出现过，那么概率为0，但不在训练数据里出现不代表它不是合理的句子。实际我们一般会使用Discount和Backoff等平滑方法来改进最大似然估计。Discount的意思就是把一些高频N-gram的概率分配给从没有出现过的N-gram，Backoff是指如果N-gram没有出现过，那我们就使用(N-1)-gram来估计。比如Katz平滑方法公式如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-257a612981f9daf5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>其中，$d$是折扣系数，$\alpha (w_{k-1},w_{k-2})$是回退系数，而$C’$是一个阈值。当$C(w_{k-2}w_{k-1}w_k)$的出现频次高于阈值，则折扣系数为1；当其低于阈值时，则对其概率用$d$进行打折，而剩下的一部分概率则经过回退分配到那些没有在训练数据中出现过的$w_{k-2}w_{k-1}w_k$中。</p><p>接下来参考(<a href="http://blog.pluskid.org/?p=361" target="_blank" rel="noopener">http://blog.pluskid.org/?p=361</a>) ：折扣系数$d$是由我们定的，在Katz平滑中是根据出现次数$r$来选择系数$d$的。对于$r\in [1,C’]$，令$d_r = \frac{(r+1)n_{r+1}}{rn_r}$，其中$n_r$表示出现次数为$r$的trigram的个数，$n_{r+1}$类推。</p><p>而回退系数则要通过概率和等于1这样的一个等式来计算，具体来说，我们有<br><img src="https://upload-images.jianshu.io/upload_images/8636110-73141b42e44f8685.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>于是，<br><img src="https://upload-images.jianshu.io/upload_images/8636110-763777484ee23424.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>N-gram语言模型有两个比较大的问题：</p><ol><li>N不能太大，否则需要存储的N-gram太多，因此它无法考虑长距离的依赖。比如”I grew up in France… I speak fluent_”，我们想猜测fluent后面哪个词的可能性大。如果只看”speak fluent”，那么French, English和Chinese的概率都是一样大，但是通过前面的”I grew up in Frence”，我们可以知道French的概率要大得多。这个问题会通过之后的RNN/LSTM/GRU等模型来进行一定程度的解决。</li><li>泛化能力差，因为它完全基于词的共现。比如训练数据中有“我在北京”，但没有“我在上海”，那么$p(上海|在)$的概率就会比$p(北京|在)$小很多。但实际上上海和北京作为地名，都可以出现在“在”的后面。这个问题和one-hot问题类似，原因在于我们把北京和上海当成了完全不同的东西，但我们希望它们是类似的。</li></ol><p>通常，把一个词表示成一个低维稠密的向量能够解决这个问题，通过上下文，模型能够知道北京和上海经常出现在相似的上下文里，因此模型能用相似的向量来表示这两个不同的词。神经网络表示如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-832759817fc69b74.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这个模型的输入是当前要预测的词，比如用前两个词预测当前词$w_t$。模型首先用lookup table把一个词变成一个向量，然后把这两个词的向量拼接成一个大的向量，输入神经网络，最后使用softmax输出预测每个词的概率。</p><p>Lookup table等价于one-hot向量乘以Embedding矩阵。假设我们有3个词，词向量的维度是5维，那么Embedding矩阵就是(3, 5)的矩阵，比如：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-daa42fea9634ffc9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>这个矩阵的每一行表示一个词的词向量，那么我们要获得第二个词的词向量，可用如下矩阵乘法来提取：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e8c63668351c9f38.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>但是这样的实现并不高效，我们只需要”复制”第二行就可以了，因此大部分深度学习框架都提供了Lookup table的操作，用于从一个矩阵中提取某一行或者某一列。这个Embedding矩阵不是固定的，它也是神经网络的参数之一。通过语言模型的学习，我们就可以得到这个Embedding矩阵，从而得到词向量。</p><h1 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h1><p>我们可以使用语言模型（或者其他任务比如机器翻译）来获得词向量，但是语言模型的训练很慢（机器翻译则更慢，而且还需要监督的标注数据），因此说词向量是这些任务的副产品。而Mikolov等人提出Word2Vec的方法，其就是直接用于训练词向量，而且速度更快。</p><p>Word2Vec的基本思想就是分布假设(distributional hypothesis)：如果两个词的上下文相似，那么这两个词的语义就相似。上下文有很多粒度，比如文档的粒度，也就是一个词的上下文是所有与它出现在同一个文档中的词，或者是较细的粒度，比如当前词前后固定大小的窗口。如下图所示，written的上下文是前后两个词，即”Potter is by J.K.”这4个词。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-56a32e835303f660.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>除了我们即将介绍的Word2Vec，还有很多其它方法也可以利用上述假设学习词向量。所有通过Distributional假设学习到的(向量)表示都叫做Distributional表示(Representation)。</p><p>注意，还有一个很像的属于叫分布表示(distributed representation)。它指的是用稠密的低维向量来表示一个词的语义，即把语义“分散”到不同的维度上。与之相对的是one-hot表示，它的语义集中在高维的稀疏的某一维上。</p><p>这里省略Word2Vec的介绍，因为我有其他的补充材料可查看，就不多写了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;参考李理的免费书籍《深度学习理论与实战：提高篇》进行学习，这里记录一些比较重要的笔记。注：本博客记录仅用于个人学习。原博客地址：&lt;a href=&quot;https://fancyerii.github.io/2019/03/14/dl-book/&quot; target=&quot;_blank&quot;
      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/DesmonDay/categories/NLP/"/>
    
    
      <category term="python" scheme="https://github.com/DesmonDay/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>第5周-深度学习的实用层面</title>
    <link href="https://github.com/DesmonDay/2019/04/14/deep-learningw5/"/>
    <id>https://github.com/DesmonDay/2019/04/14/deep-learningw5/</id>
    <published>2019-04-14T06:05:22.000Z</published>
    <updated>2019-04-14T06:37:14.420Z</updated>
    
    <content type="html"><![CDATA[<h1 id="训练-验证-测试集"><a href="#训练-验证-测试集" class="headerlink" title="训练/验证/测试集"></a>训练/验证/测试集</h1><p>在实际构建和训练深层神经网络的时候，我们往往要确定一些超参数，如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-713247709c45087d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在实际应用中，对数据集进行划分为训练集、验证集、测试集可以加速神经网络的集成，也可以更有效地衡量算法的偏差和方差，从而帮助我们更高效地选择合适的方法来优化算法。图中展示了在数据样本较少时，我们将其划分比例为70%-30%或60%-20%-20%；当大数据时，则验证集和测试集的比例往往会很小。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-75b51a7281b1f656.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>当然，我们有时候会面临训练集和测试集分布不均衡的情况。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-efce84cda9a9e303.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如上图举例，我们的训练集来自网页上的图片，可能都是很高清的，而测试集和验证集的图片都来自用户自己拍摄的，很显然图片的分布就不同。面对这种情况，我们需要遵循的一个准则就是：Make sure dev and test set come from same distribution.</p><p>另外，实际应用中也可以不需要测试集，只使用训练集和验证集（有些人可能就会称作训练集和测试集）。</p><h1 id="偏差-方差-Bias-Variance"><a href="#偏差-方差-Bias-Variance" class="headerlink" title="偏差/方差 - Bias/Variance"></a>偏差/方差 - Bias/Variance</h1><p>我们可以通过训练集和验证集上的误差来确定算法的偏差和方差的高低情况，再根据具体的情况，如高偏差高方差、低偏差高方差等来判断下一步应该如何进行算法的优化。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ba80c1f989a721b5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>下图的猫分类的偏差方差衡量的前提有两个；1. 基本误差很小，即Optimal(Bayes) error很小；2. train和dev set的分布相同。然而，我们首先查看训练集误差，若误差大，说明高偏差，反之为低偏差；再看验证集误差，若误差大，则高方差，反之为低方差。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-bb4f07d38e066e76.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>接着，吴老师又举了一个高偏差和高方差的具体例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f396c27fd87c080f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们可以看到，由于这个分类器基本是线性拟合，所以其拟合程度低，说明为高偏差；但同时，它也有着过拟合的部分，如图的两个曲折部分，因此也具有高方差。在高维数据分类中，这种情况非常常见：有些区域偏差高，有些区域方差高。</p><h1 id="机器学习基础-Basic-Recipie-for-ML"><a href="#机器学习基础-Basic-Recipie-for-ML" class="headerlink" title="机器学习基础 - Basic Recipie for ML"></a>机器学习基础 - Basic Recipie for ML</h1><p>一般，我们会先查看是否High bias(training set performance)? 如果是，则有以下方法来进行调整：训练更深更大的神经网络、训练更长时间、或者修改我们的神经网络结构。最后达到至少能够拟合训练数据的目的。</p><p>接着，我们查看是否Hign variance(dev set performance)? 如果是，则有以下方法：获得更多的数据、正则化、修改神经网络结构。</p><p>当偏差和方差都比较小时，则我们的工作也基本完成。在以前，我们会特别注重”Bias Variance trade-off”，但是如今的深度学习并不需要特别看重这个问题，因为我们其实可以再减少偏差的同时不损害方差（利用正则化之类的方法）。</p><h1 id="正则化-Regularization"><a href="#正则化-Regularization" class="headerlink" title="正则化 - Regularization"></a>正则化 - Regularization</h1><p>当我们出现过拟合的结果，则高方差，那么我们第一个想到的方法应该是正则化。另一个方法则是得到更多的数据，但这往往比较难。而正则化通常可以避免过拟合。</p><p>以Logistic回归为例，我们可以在图中看到L2-范数和L1-范数的写法，其中lambda为正则化参数，而L2-范数相比L1-范数而言更为常用。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-784b3faf89a9b5ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>对神经网络而言，我们不再称为L2-范数，而称为Frobenius Norm。因为这里其范数的计算方式为一个矩阵中所有元素的平方和。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-3e53a9bda80463ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>另外，我们可以注意到，由于我们的cost函数发生变化，则多了正则项，那么dW的计算方式也有了变化。具体计算公式如图所示，并且我们可以发现，加了正则化项后，我们的参数W也是往减少的趋势走，我们称其为”Weight Decay”。</p><h2 id="为什么正则化可以减少过拟合"><a href="#为什么正则化可以减少过拟合" class="headerlink" title="为什么正则化可以减少过拟合"></a>为什么正则化可以减少过拟合</h2><p>这里吴老师给了两个比较直观的解释。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-a15998e3aef2b7cb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如图，当我们将正则化参数lambda设置得足够大，那么W会接近0，从而使得很多隐藏单元的影响变小了，因此网络结构也变得简单，从而可以减少过拟合。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-38ecfcc88978ab40.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>又如上图，如果我们的lambda增大，那么W会较小，从而Z也会减小。如果Z的范围小，则tanh激活函数近似于线性函数，从而使得神经网络类似于线性神经网络，因此可以减小过拟合。</p><h2 id="Dropout-Regularization"><a href="#Dropout-Regularization" class="headerlink" title="Dropout Regularization"></a>Dropout Regularization</h2><p>此部分参考总结：<a href="https://blog.csdn.net/u012328159/article/details/80210363" target="_blank" rel="noopener">https://blog.csdn.net/u012328159/article/details/80210363</a></p><p>为了防止过拟合的问题，我们最常使用的手段就是L2正则化，即在代价函数后面加一个L2正则项。dropout正则化是Srivastava在2014年提出来的：Dropout: A Simple Way to Prevent Neural Networks from Overfitting。Dropout的思想其实非常简单粗暴：对于网络的每一层，随机的丢弃一些单元。如下图所示:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b620859b7502897a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如何实现正则化，吴老师举了Inverted dropout的例子来进行阐述：从技术实现方面来看下dropout正则项，这里最重要的一个参数就是keep_prob，称作保留概率（同样，1−keep_prob1−keep_prob则为丢弃概率），比如某一层的 keep_prob=0.8，则意味着某一层随机的保留80%的神经单元（也即有20%的单元被丢弃）。通常实现dropout regularization的技术称为 inverted dropout，假设对于第三层，则inverted dropout的具体实现为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">d3 = np.random.rand(a3.shape[<span class="number">0</span>],a3.shape[<span class="number">1</span>]) &lt; keep_prob</span><br><span class="line">a3 = np.multiply(a3, d3)</span><br><span class="line">a3 = a3 / keep_prob</span><br><span class="line">z4 = np.dot(w4, a3) + b4</span><br></pre></td></tr></table></figure></p><p>对于上述第三行代码的解释：因为有1−keep_prob的单元失活了，这样a3的期望值也就减少了1−keep_prob，所以我们要用a3/keep_prob，这样a3的期望值不变。这就是inverted dropout。</p><p>我们用两个动态图来演示下dropout的过程（素材来自ng的deep learning课）：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ec4d349d9efbcfad.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-a1c3c77880d41203.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>另外要注意的是，我们在测试阶段的时候是不使用Dropout的。</p><h2 id="理解Dropout"><a href="#理解Dropout" class="headerlink" title="理解Dropout"></a>理解Dropout</h2><p>为什么Dropout能够减小过拟合，吴老师给出了两个比较直观的解释：</p><ul><li>正是因为在每一层随机地丢弃了一些单元，所以相当于训练出来的网络要比原有的网络小得多，这在一定程度上解释了避免过拟合的问题。</li><li>如下图所示的一个简单单层网络，因为每一个特征都有可能被丢弃，所以整个网络不会偏向于某一个特征（把某特征的权重的值赋的很大），会把每一个特征的权重都赋的很小，这就有点类似于L2正则化了，能够起到减轻过拟合的作用。 （压缩权重）</li></ul><p>另外，不同层的keep_prob可以设置得不同，这就有些类似于正则化中的参数lambda。</p><p>简单总结：如果你担心某些层比其他层更容易发生过拟合，那么可以把某些层的keep_prob设置得低一些，而缺点是为了使用交叉验证，我们要搜索更多的超参数；另一种方案则是在一些层上应用dropout，而一些层则不用。dropout在计算机视觉领域用得很频繁，因为我们往往足够的数据，因此经常容易发生过拟合，那么dropout就是必然会使用的。另外需要记住，dropout是正则化中的一种方法，它可以帮助预防过拟合。不过，dropout的使用使得我们的cost function不再明确，因此我们的一个做法就是，先去掉dropout函数，画出cost function图确保是递减的，然后再使用dropout。</p><h2 id="其他正则化方法"><a href="#其他正则化方法" class="headerlink" title="其他正则化方法"></a>其他正则化方法</h2><ol><li>Data augmentation-数据增强</li></ol><p>如果我们无法获得更多的数据，以图片为例，我们可以增加其他方向不同的图片，或者随机翻转和裁剪图片，额外生成假数据。尽管这种做法不如增加一组新图片，因为存在一些冗余，但是这样做也节省了获取更多猫咪图片的花费；对于数字，我们也可以随机旋转或扭转数字来扩增数据。如以下例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-91b91c7f61140ea1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><ol><li>Early stopping</li></ol><p>我们可以画出迭代过程中的训练误差和在验证集上的误差，通过观察验证误差，来提前结束迭代。这样子得到的参数W不至于过大，因此也减少了过拟合。但是early-stopping的一个主要缺点就是我们无法独立地处理优化cost function和防止过拟合这两个过程，因为提前停止梯度下降，我们实际上也停止了cost funciton的优化，因此我们实际上是用一种方法来同时解决两个问题。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-8db9fe9dc325e0bf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们也可以选择不同的正则化参数lambda，但这意味着更多的训练和尝试，计算代价会很大。因此Early stopping就不需要尝试那么多的lambda。</p><h1 id="标准化输入-Normalize-input"><a href="#标准化输入-Normalize-input" class="headerlink" title="标准化输入-Normalize input"></a>标准化输入-Normalize input</h1><p>训练神经网络时，提高训练速度的方法之一是对输入进行归一化。假设我们有一个训练集,它有两个输入特征,所以输入特征x是二维的,这是数据集的散点图。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-12733db941f0f44f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>归一化需要两个步骤：零均值化、归一化方差。</p><p>第一步：零均值化。subtract out or to zero out the mean 计算出u即x(i)的均值。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-1f523947cb70ca6e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>随后x = x-u。即通过移动训练集，直到其为零均值化。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b8ff7f5e486c6d33.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>第二步：归一化方差。</p><p>如上图所示，特征1的方差比特征2的要大很多。计算出方差sigma^2，如下图：</p><p>上一步我们已经完成了零均值化，接下来将所有数据都除以向量sigma^2.最后数据分布如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-1cf9df8abbc940c3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>最后注意，如果我们用了此种方法来对训练集的特征进行归一化，那么对于测试集也要使用同样的u和sigma做归一化，而不是在训练集和测试集上分别评估出不同的u和sigma。</p><p><strong>疑惑</strong>：为什么是除以方差，而不是除以标准差？</p><h2 id="为什么我们要对输入进行标准化"><a href="#为什么我们要对输入进行标准化" class="headerlink" title="为什么我们要对输入进行标准化"></a>为什么我们要对输入进行标准化</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-4a2ffb823b61cb8a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>标准化后，cost function更容易优化，更快达到迭代目标，即前提是特征的范围基本一致，而不是相差很大。</p><h1 id="梯度消失与梯度爆炸-Vanishing-exploding-gradients"><a href="#梯度消失与梯度爆炸-Vanishing-exploding-gradients" class="headerlink" title="梯度消失与梯度爆炸-Vanishing/exploding gradients"></a>梯度消失与梯度爆炸-Vanishing/exploding gradients</h1><p>在本节中，周老师举了一个较为简单的例子来解释梯度爆炸和梯度消失的问题。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-d9e550e711f9d0c8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>为了简单起见，每个隐藏层的结点数都为2，并且设置激活函数为线性激活函数，且b=0。在例子中可以看到，如果我们将W设置得稍微比I(全1矩阵)大一些，最后得到的预测结果y呈指数型增长，即出现指数爆炸；而若是W比I稍微小一些，得到的结果也是指数级下降，即出现梯度消失。这个问题长久以来都是深层神经网络发展的一个阻力。</p><h1 id="深层神经网络的权重初始化"><a href="#深层神经网络的权重初始化" class="headerlink" title="深层神经网络的权重初始化"></a>深层神经网络的权重初始化</h1><p>我们想出了一个不完整的解决方案，有助于我们为神经网络更谨慎的选择随机初始化参数。</p><p>一般而言，为了预防Z过大或过小，随着n越大，w_i应该越小，则最合理的方法是设置w_i = 1/n，其中n表示输入神经元的特征数。因此我们实际上一般写作：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-a429a52e392636b9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>其中n^{l-1}即为我们输入到第l层神经单元的数目。</p><p>通常，relu激活函数的W_i应该设置为2/n比较合适，而tanh函数的见下图：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-5ab0a9239083c607.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们可以给上述参数再增加一个乘法参数，但上述方差参数的调优优先级并不高。通过上述的方法，我们确实降低的梯度爆炸和梯度消失问题。</p><h1 id="梯度的数值逼近-Numerical-approximation-of-gradients"><a href="#梯度的数值逼近-Numerical-approximation-of-gradients" class="headerlink" title="梯度的数值逼近-Numerical approximation of gradients"></a>梯度的数值逼近-Numerical approximation of gradients</h1><p>为了实现梯度检验，我们首先说说如何对计算梯度做数值逼近。吴老师在这节主要是利用了斜率的概念来计算导数，用实际的计算例子来验证梯度计算得是否正确，如下图：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d7d6a7c99eff4439.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们在检验的时候用如下公式更加准确：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-75636009046c847a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="梯度检验"><a href="#梯度检验" class="headerlink" title="梯度检验"></a>梯度检验</h1><p>梯度检验能够帮我们节省很多时间，帮我们发现在反向传播过程中的bug，接下来我们看看如何利用它来调试或检验backprop的实施是否正确。</p><p>假设网络中含有下列参数，W1和b1…Wl和bl。为了执行梯度检验，首先我们将所有参数转换成一个巨大的向量数据，将矩阵W转换成向量之后，做连接运算，得到一个巨型向量theta。我们的代价函数J是所有W和b的函数，因此我们得到了J(theta)。接着，我们可以同样把求得的dW1和db1…dWl和dbl转换成一个新的向量，用他们作为dtheta，它与theta具有相同维度。因此现在的问题是，dtheta和代价函数J的梯度有什么关系？<br><img src="https://upload-images.jianshu.io/upload_images/8636110-05f969f5800bcde9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>接下来就是实施梯度检验的过程，英语中简称为”grad check”。首先我们要清楚J是超参数theta的一个函数，不论theta的维度是多少，为了实施梯度检验，我们要做的是循环执行，对每个i也就是对每个theta的组成元素计算dtheta_approx(i)的值，我们要使用的是双边误差的公式：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-4bfc7d49becd3bc0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>从上节课的梯度数值逼近，我们知道dtheta_approx(i)的值应该逼近dtheta(i)的值，而dtheta(i)即为代价函数的偏导数。通过对每个i执行这个运算，我们可以得到两个向量，即dtheta的逼近值dtheta_approx和dtheta本身。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b4eba04454e0a5ee.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>利用上式计算两个向量之间的距离，再利用右边的值大小判断是否正确。在实施神经网络的时候，我们经常要执行foreprop和backprop，如果我们发现梯度检验有一个较大的值，那么我们就可以怀疑存在bug，因此需要进行调试。</p><h2 id="关于梯度检验实现的提示"><a href="#关于梯度检验实现的提示" class="headerlink" title="关于梯度检验实现的提示"></a>关于梯度检验实现的提示</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-360fe300127922bf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>一点解释：</p><ol><li>不要在训练的时候进行梯度检验。</li><li>如果算法在梯度检验的时候发生了错误，我们要查看不同的可能导致错误的组成部分，比如dbl或者dWl。</li><li>注意正则化，如果我们在成本函数中加了正则项，那么在梯度检验求梯度的时候也要求正则项的梯度。</li><li>不要与Dropout一块使用，因为Dropout会随机地使一些隐藏单元不起作用。</li><li>只有当W和b接近0时，梯度下降的实施是正确的，因此我们在随机初始化过程中运行梯度检验，再训练网络。</li></ol><h1 id="本周作业"><a href="#本周作业" class="headerlink" title="本周作业"></a>本周作业</h1><h2 id="Initialization"><a href="#Initialization" class="headerlink" title="Initialization"></a>Initialization</h2><p>A well chosen intialization can:</p><pre><code>- Speed up the convergence of gradient descent- Increase the odds of gradient descent converging to a lower training (and generalization) error</code></pre><p>Import packages and the planar dataset:<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">from</span> init_utils <span class="keyword">import</span> sigmoid, relu, compute_loss, forward_propagation, backward_propagation</span><br><span class="line"><span class="keyword">from</span> init_utils <span class="keyword">import</span> update_parameters, predict, load_dataset, plot_decision_boundary, predict_dec</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">7.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># load image dataset: blue/red dots in circles</span></span><br><span class="line">train_X, train_Y, test_X, test_Y = load_dataset()</span><br></pre></td></tr></table></figure></p><h3 id="1-Neural-Network-model"><a href="#1-Neural-Network-model" class="headerlink" title="1-Neural Network model"></a>1-Neural Network model</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, Y, learning_rate = <span class="number">0.01</span>, num_iterations = <span class="number">15000</span>, print_cost = True, initialization = <span class="string">"he"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate for gradient descent </span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations to run gradient descent</span></span><br><span class="line"><span class="string">    print_cost -- if True, print the cost every 1000 iterations</span></span><br><span class="line"><span class="string">    initialization -- flag to choose which initialization to use ("zeros","random" or "he")</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">        </span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = [] <span class="comment"># to keep track of the loss</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>] <span class="comment"># number of examples</span></span><br><span class="line">    layers_dims = [X.shape[<span class="number">0</span>], <span class="number">10</span>, <span class="number">5</span>, <span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters dictionary.</span></span><br><span class="line">    <span class="keyword">if</span> initialization == <span class="string">"zeros"</span>:</span><br><span class="line">        parameters = initialize_parameters_zeros(layers_dims)</span><br><span class="line">    <span class="keyword">elif</span> initialization == <span class="string">"random"</span>:</span><br><span class="line">        parameters = initialize_parameters_random(layers_dims)</span><br><span class="line">    <span class="keyword">elif</span> initialization == <span class="string">"he"</span>:</span><br><span class="line">        parameters = initialize_parameters_he(layers_dims)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line">        a3, cache = forward_propagation(X, parameters)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Loss</span></span><br><span class="line">        cost = compute_loss(a3, Y)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backward propagation.</span></span><br><span class="line">        grads = backward_propagation(X, Y, cache)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the loss every 1000 iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration &#123;&#125;: &#123;&#125;"</span>.format(i, cost))</span><br><span class="line">            costs.append(cost)</span><br><span class="line">            </span><br><span class="line">    <span class="comment"># plot the loss</span></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per hundreds)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="2-Zero-initialization"><a href="#2-Zero-initialization" class="headerlink" title="2-Zero initialization"></a>2-Zero initialization</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_zeros</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    layer_dims -- python array (list) containing the size of each layer.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (layers_dims[1], 1)</span></span><br><span class="line"><span class="string">                    ...</span></span><br><span class="line"><span class="string">                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])</span></span><br><span class="line"><span class="string">                    bL -- bias vector of shape (layers_dims[L], 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layers_dims)            <span class="comment"># number of layers in the network</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.zeros((layers_dims[l],layers_dims[l<span class="number">-1</span>]))</span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layers_dims[l], <span class="number">1</span>))</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>Run the following code to train your model on 15,000 iterations using zeros initialization.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y, initialization = <span class="string">"zeros"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the train set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure><p>Result:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-67175d9b586dbce4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-5d79d87b5d330532.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>The model is predicting 0 for every example.</p><p>In general, initializing all the weights to zero results in the network failing to break symmetry. This means that every neuron in each layer will learn the same thing, and you might as well be training a neural network with  n[l]=1  for every layer, and the network is no more powerful than a linear classifier such as logistic regression.</p><p><strong>Remember</strong>:</p><ul><li>The weights W[l] should be initialized randomly to break symmetry.</li><li>It is however okay to initialize the biases b[l] to zeros. Symmetry is still broken so long as W[l] is initialized randomly.</li></ul><h3 id="3-Random-initialization"><a href="#3-Random-initialization" class="headerlink" title="3- Random initialization"></a>3- Random initialization</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_random</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    layer_dims -- python array (list) containing the size of each layer.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (layers_dims[1], 1)</span></span><br><span class="line"><span class="string">                    ...</span></span><br><span class="line"><span class="string">                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])</span></span><br><span class="line"><span class="string">                    bL -- bias vector of shape (layers_dims[L], 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">3</span>)               <span class="comment"># This seed makes sure your "random" numbers will be the as ours</span></span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layers_dims)            <span class="comment"># integer representing the number of layers</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layers_dims[l],layers_dims[l<span class="number">-1</span>])*<span class="number">10</span></span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layers_dims[l],<span class="number">1</span>))</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>Result:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-2731639cbe8165e1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-fc5fe0ffe08338a4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong> Observations </strong></p><ul><li>The cost starts very high. This is because with large random-valued weights, the last activation (sigmoid) outputs results that are very close to 0 or 1 for some examples, and when it gets that example wrong it incurs a very high loss for that example. Indeed, when  log(a[3])=log(0), the loss goes to infinity.</li><li>Poor initialization can lead to vanishing/exploding gradients, which also slows down the optimization algorithm.</li><li>If you train this network longer you will see better results, but initializing with overly large random numbers slows down the optimization.</li></ul><p><strong> In summary: </strong></p><ul><li>Initializing weights to very large random values does not work well.</li><li>Hopefully intializing with small random values does better. The important question is: how small should be these random values be? Lets find out in the next part!</li></ul><h3 id="4-He-initialization"><a href="#4-He-initialization" class="headerlink" title="4- He initialization"></a>4- He initialization</h3><p>Finally, try “He Initialization”; this is named for the first author of He et al., 2015.</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-98f736c820e171c2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_he</span><span class="params">(layers_dims)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    layer_dims -- python array (list) containing the size of each layer.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (layers_dims[1], 1)</span></span><br><span class="line"><span class="string">                    ...</span></span><br><span class="line"><span class="string">                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])</span></span><br><span class="line"><span class="string">                    bL -- bias vector of shape (layers_dims[L], 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">3</span>)</span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layers_dims) - <span class="number">1</span> <span class="comment"># integer representing the number of layers</span></span><br><span class="line">     </span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L + <span class="number">1</span>):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layers_dims[l],layers_dims[l<span class="number">-1</span>])*np.sqrt(<span class="number">2</span>/layers_dims[l<span class="number">-1</span>])</span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layers_dims[l],<span class="number">1</span>))</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>Result:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-305e6f7d962816f1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-ccc49b3765571c43.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong> Observation </strong></p><ul><li>The model with He initialization separates the blue and the red dots very well in a small number of iterations.</li></ul><h3 id="5-Conclusions"><a href="#5-Conclusions" class="headerlink" title="5- Conclusions"></a>5- Conclusions</h3><p><img src="https://upload-images.jianshu.io/upload_images/8636110-f61a6518bbd38752.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>What we need to remember:</p><ul><li>Different initializations lead to different results.</li><li>Random initialization is used to break symmetry and make sure different hidden units can learn different things.</li><li>Don’t intialize to values that are too large.</li><li>He initialization works well for networks with ReLU activations.</li></ul><h2 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h2><p>import packages:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># import packages</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> reg_utils <span class="keyword">import</span> sigmoid, relu, plot_decision_boundary, initialize_parameters, load_2D_dataset, predict_dec</span><br><span class="line"><span class="keyword">from</span> reg_utils <span class="keyword">import</span> compute_cost, predict, forward_propagation, backward_propagation, update_parameters</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> sklearn.datasets</span><br><span class="line"><span class="keyword">import</span> scipy.io</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">7.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br></pre></td></tr></table></figure></p><p><strong> Problem Statement </strong> You have just been hired as an AI expert by the French Football Corporation. They would like you to recommend positions where France’s goal keeper should kick the ball so that the French team’s players can then hit it with their head.</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-55d3ce441f5f8b6f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>They give you the following 2D dataset from France’s past 10 games.<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_X, train_Y, test_X, test_Y = load_2D_dataset()</span><br></pre></td></tr></table></figure></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-3bc560f1523d3b04.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Each dot corresponds to a position on the football field where a football player has hit the ball with his/her head after the French goal keeper has shot the ball from the left side of the football field.</p><ul><li>If the dot is blue, it means the French player managed to hit the ball with his/her head</li><li>If the dot is red, it means the other team’s player hit the ball with their head</li></ul><p><strong>Your goal</strong>: Use a deep learning model to find the positions on the field where the goalkeeper should kick the ball.</p><p><strong>Analysis of the dataset</strong>: This dataset is a little noisy, but it looks like a diagonal line separating the upper left half (blue) from the lower right half (red) would work well. </p><p>You will first try a non-regularized model. Then you’ll learn how to regularize it and decide which model you will choose to solve the French Football Corporation’s problem. </p><h3 id="1-Non-regularized-model"><a href="#1-Non-regularized-model" class="headerlink" title="1- Non-regularized model"></a>1- Non-regularized model</h3><p>You will use the following neural network (already implemented for you below). This model can be used: </p><ul><li>in <em>regularization mode</em> — by setting the <code>lambd</code> input to a non-zero value. We use “<code>lambd</code>“ instead of “<code>lambda</code>“ because “<code>lambda</code>“ is a reserved keyword in Python. </li><li>in <em>dropout mode</em> — by setting the <code>keep_prob</code> to a value less than one</li></ul><p>You will first try the model without any regularization. Then, you will implement:</p><ul><li><em>L2 regularization</em> — functions: “<code>compute_cost_with_regularization()</code>“ and “<code>backward_propagation_with_regularization()</code>“</li><li><em>Dropout</em> — functions: “<code>forward_propagation_with_dropout()</code>“ and “<code>backward_propagation_with_dropout()</code>“</li></ul><p>In each part, you will run this model with the correct inputs so that it calls the functions you’ve implemented. Take a look at the code below to familiarize yourself with the model.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, Y, learning_rate = <span class="number">0.3</span>, num_iterations = <span class="number">30000</span>, print_cost = True, lambd = <span class="number">0</span>, keep_prob = <span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the optimization</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    print_cost -- If True, print the cost every 10000 iterations</span></span><br><span class="line"><span class="string">    lambd -- regularization hyperparameter, scalar</span></span><br><span class="line"><span class="string">    keep_prob - probability of keeping a neuron active during drop-out, scalar.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learned by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">        </span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = []                            <span class="comment"># to keep track of the cost</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]                        <span class="comment"># number of examples</span></span><br><span class="line">    layers_dims = [X.shape[<span class="number">0</span>], <span class="number">20</span>, <span class="number">3</span>, <span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters dictionary.</span></span><br><span class="line">    parameters = initialize_parameters(layers_dims)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop (gradient descent)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line">        <span class="keyword">if</span> keep_prob == <span class="number">1</span>: <span class="comment"># 无dropout</span></span><br><span class="line">            a3, cache = forward_propagation(X, parameters)</span><br><span class="line">        <span class="keyword">elif</span> keep_prob &lt; <span class="number">1</span>:</span><br><span class="line">            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Cost function</span></span><br><span class="line">        <span class="keyword">if</span> lambd == <span class="number">0</span>: <span class="comment"># 无L2正则</span></span><br><span class="line">            cost = compute_cost(a3, Y)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># Backward propagation.</span></span><br><span class="line">        <span class="keyword">assert</span>(lambd==<span class="number">0</span> <span class="keyword">or</span> keep_prob==<span class="number">1</span>)    <span class="comment"># it is possible to use both L2 regularization and dropout, </span></span><br><span class="line">                                            <span class="comment"># but this assignment will only explore one at a time</span></span><br><span class="line">        <span class="keyword">if</span> lambd == <span class="number">0</span> <span class="keyword">and</span> keep_prob == <span class="number">1</span>:</span><br><span class="line">            grads = backward_propagation(X, Y, cache)</span><br><span class="line">        <span class="keyword">elif</span> lambd != <span class="number">0</span>:</span><br><span class="line">            grads = backward_propagation_with_regularization(X, Y, cache, lambd)</span><br><span class="line">        <span class="keyword">elif</span> keep_prob &lt; <span class="number">1</span>:</span><br><span class="line">            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Update parameters.</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Print the loss every 10000 iterations</span></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration &#123;&#125;: &#123;&#125;"</span>.format(i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (x1,000)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><p>Let’s train the model without any regularization, and observe the accuracy on the train/test sets.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the training set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure><p><img src="https://upload-images.jianshu.io/upload_images/8636110-3cd473ea1ea143ca.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-b551f869daf57480.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>The non-regularized model is obviously overfitting the training set. It is fitting the noisy points! Lets now look at two techniques to reduce overfitting.</p><h3 id="2-L2-Regularization"><a href="#2-L2-Regularization" class="headerlink" title="2- L2 Regularization"></a>2- L2 Regularization</h3><p><img src="https://upload-images.jianshu.io/upload_images/8636110-c2875ba8ba446530.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost_with_regularization</span><span class="params">(A3, Y, parameters, lambd)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the cost function with L2 regularization. See formula (2) above.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector, of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing parameters of the model</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost - value of the regularized loss function (formula (2))</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    W3 = parameters[<span class="string">"W3"</span>]</span><br><span class="line">    </span><br><span class="line">    cross_entropy_cost = compute_cost(A3, Y) <span class="comment"># This gives you the cross-entropy part of the cost</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    L2_regularization_cost = (np.sum(np.square(W1))+np.sum(np.square(W2))+np.sum(np.square(W3)))*(lambd/(<span class="number">2</span>*m))</span><br><span class="line">    <span class="comment">### END CODER HERE ###</span></span><br><span class="line">    </span><br><span class="line">    cost = cross_entropy_cost + L2_regularization_cost</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation_with_regularization</span><span class="params">(X, Y, cache, lambd)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the backward propagation of our baseline model to which we added an L2 regularization.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset, of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector, of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    cache -- cache output from forward_propagation()</span></span><br><span class="line"><span class="string">    lambd -- regularization hyperparameter, scalar</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    dW3 = <span class="number">1.</span>/m * np.dot(dZ3, A2.T) + W3*(lambd/m)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    db3 = <span class="number">1.</span>/m * np.sum(dZ3, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; <span class="number">0</span>))</span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    dW2 = <span class="number">1.</span>/m * np.dot(dZ2, A1.T) + W2*(lambd/m)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    db2 = <span class="number">1.</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; <span class="number">0</span>))</span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    dW1 = <span class="number">1.</span>/m * np.dot(dZ1, X.T) + W1*(lambd/m)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    db1 = <span class="number">1.</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;<span class="string">"dZ3"</span>: dZ3, <span class="string">"dW3"</span>: dW3, <span class="string">"db3"</span>: db3,<span class="string">"dA2"</span>: dA2,</span><br><span class="line">                 <span class="string">"dZ2"</span>: dZ2, <span class="string">"dW2"</span>: dW2, <span class="string">"db2"</span>: db2, <span class="string">"dA1"</span>: dA1, </span><br><span class="line">                 <span class="string">"dZ1"</span>: dZ1, <span class="string">"dW1"</span>: dW1, <span class="string">"db1"</span>: db1&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><p>Train and test:<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y, lambd = <span class="number">0.7</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the train set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure></p><p>Result:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-1caff43cb140270a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-83ff2ceab15f3524.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>Observations</strong>:</p><ul><li>The value of $\lambda$ is a hyperparameter that you can tune using a dev set.</li><li>L2 regularization makes your decision boundary smoother. If $\lambda$ is too large, it is also possible to “oversmooth”, resulting in a model with high bias.</li></ul><p><strong>What is L2-regularization actually doing?</strong>:</p><p>L2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes. </p><p><strong>What you should rememeber</strong> —— the implications</p><ul><li>The cost computation: A regularization term is added to the cost</li><li>The backpropagation function: There are extra terms in the gradients with respect to weight matrices</li><li>Weights end up smaller (“weight decay”): Weights are pushed to smaller values.</li></ul><h3 id="3-Dropout"><a href="#3-Dropout" class="headerlink" title="3- Dropout"></a>3- Dropout</h3><p>Finally, dropout is a widely used regularization technique that is specific to deep learning. <strong>It randomly shuts down some neurons in each iteration</strong>. </p><p>When you shut some neurons down, you actually modify your model. The idea behind drop-out is that at each iteration, you train a different model that uses only a subset of your neurons. With dropout, your neurons thus become less sensitive to the activation of one other specific neuron, because that other neuron might be shut down at any time.</p><h4 id="3-1-Forward-propagation"><a href="#3-1-Forward-propagation" class="headerlink" title="3.1- Forward propagation"></a>3.1- Forward propagation</h4><p>注意随机函数用的是np.random.rand，获得0到1之间的随机数；而不是randn。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation_with_dropout</span><span class="params">(X, parameters, keep_prob = <span class="number">0.5</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the forward propagation: LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset, of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (20, 2)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (20, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (3, 20)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (3, 1)</span></span><br><span class="line"><span class="string">                    W3 -- weight matrix of shape (1, 3)</span></span><br><span class="line"><span class="string">                    b3 -- bias vector of shape (1, 1)</span></span><br><span class="line"><span class="string">    keep_prob - probability of keeping a neuron active during drop-out, scalar</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A3 -- last activation value, output of the forward propagation, of shape (1,1)</span></span><br><span class="line"><span class="string">    cache -- tuple, information stored for computing the backward propagation</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># retrieve parameters</span></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line">    W3 = parameters[<span class="string">"W3"</span>]</span><br><span class="line">    b3 = parameters[<span class="string">"b3"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span></span><br><span class="line">    Z1 = np.dot(W1, X) + b1</span><br><span class="line">    A1 = relu(Z1)</span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 4 lines)         # Steps 1-4 below correspond to the Steps 1-4 described above. </span></span><br><span class="line">    D1 = np.random.rand(A1.shape[<span class="number">0</span>],A1.shape[<span class="number">1</span>])                                         <span class="comment"># Step 1: initialize matrix D1 = np.random.rand(..., ...)</span></span><br><span class="line">    D1 = D1 &lt; keep_prob                                         <span class="comment"># Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)</span></span><br><span class="line">    A1 = A1 * D1                                        <span class="comment"># Step 3: shut down some neurons of A1</span></span><br><span class="line">    A1 = A1 / keep_prob                                         <span class="comment"># Step 4: scale the value of neurons that haven't been shut down</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    Z2 = np.dot(W2, A1) + b2</span><br><span class="line">    A2 = relu(Z2)</span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 4 lines)</span></span><br><span class="line">    D2 = np.random.rand(A2.shape[<span class="number">0</span>],A2.shape[<span class="number">1</span>])                                         <span class="comment"># Step 1: initialize matrix D2 = np.random.rand(..., ...)</span></span><br><span class="line">    D2 = D2 &lt; keep_prob                                      <span class="comment"># Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)</span></span><br><span class="line">    A2 = A2 * D2                                        <span class="comment"># Step 3: shut down some neurons of A2</span></span><br><span class="line">    A2 = A2 / keep_prob                                         <span class="comment"># Step 4: scale the value of neurons that haven't been shut down</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    Z3 = np.dot(W3, A2) + b3</span><br><span class="line">    A3 = sigmoid(Z3)</span><br><span class="line">    </span><br><span class="line">    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> A3, cache</span><br></pre></td></tr></table></figure></p><h4 id="3-2-Backward-propagation-with-dropout"><a href="#3-2-Backward-propagation-with-dropout" class="headerlink" title="3.2- Backward propagation with dropout"></a>3.2- Backward propagation with dropout</h4><p>Instruction: Backpropagation with dropout is actually quite easy. You will have to carry out 2 Steps:</p><ol><li>You had previously shut down some neurons during forward propagation, by applying a mask $D^{[1]}$ to <code>A1</code>. In backpropagation, you will have to shut down the same neurons, by reapplying the same mask $D^{[1]}$ to <code>dA1</code>. </li><li>During forward propagation, you had divided <code>A1</code> by <code>keep_prob</code>. In backpropagation, you’ll therefore have to divide <code>dA1</code> by <code>keep_prob</code> again (the calculus interpretation is that if $A^{[1]}$ is scaled by <code>keep_prob</code>, then its derivative $dA^{[1]}$ is also scaled by the same <code>keep_prob</code>).</li></ol><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation_with_dropout</span><span class="params">(X, Y, cache, keep_prob)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the backward propagation of our baseline model to which we added dropout.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input dataset, of shape (2, number of examples)</span></span><br><span class="line"><span class="string">    Y -- "true" labels vector, of shape (output size, number of examples)</span></span><br><span class="line"><span class="string">    cache -- cache output from forward_propagation_with_dropout()</span></span><br><span class="line"><span class="string">    keep_prob - probability of keeping a neuron active during drop-out, scalar</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache</span><br><span class="line">    </span><br><span class="line">    dZ3 = A3 - Y</span><br><span class="line">    dW3 = <span class="number">1.</span>/m * np.dot(dZ3, A2.T)</span><br><span class="line">    db3 = <span class="number">1.</span>/m * np.sum(dZ3, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    dA2 = np.dot(W3.T, dZ3)</span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    dA2 = dA2 * D2              <span class="comment"># Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation</span></span><br><span class="line">    dA2 = dA2 / keep_prob              <span class="comment"># Step 2: Scale the value of neurons that haven't been shut down</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    dZ2 = np.multiply(dA2, np.int64(A2 &gt; <span class="number">0</span>))</span><br><span class="line">    dW2 = <span class="number">1.</span>/m * np.dot(dZ2, A1.T)</span><br><span class="line">    db2 = <span class="number">1.</span>/m * np.sum(dZ2, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    dA1 = np.dot(W2.T, dZ2)</span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    dA1 = dA1 * D1              <span class="comment"># Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation</span></span><br><span class="line">    dA1 = dA1 / keep_prob              <span class="comment"># Step 2: Scale the value of neurons that haven't been shut down</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    dZ1 = np.multiply(dA1, np.int64(A1 &gt; <span class="number">0</span>))</span><br><span class="line">    dW1 = <span class="number">1.</span>/m * np.dot(dZ1, X.T)</span><br><span class="line">    db1 = <span class="number">1.</span>/m * np.sum(dZ1, axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)</span><br><span class="line">    </span><br><span class="line">    gradients = &#123;<span class="string">"dZ3"</span>: dZ3, <span class="string">"dW3"</span>: dW3, <span class="string">"db3"</span>: db3,<span class="string">"dA2"</span>: dA2,</span><br><span class="line">                 <span class="string">"dZ2"</span>: dZ2, <span class="string">"dW2"</span>: dW2, <span class="string">"db2"</span>: db2, <span class="string">"dA1"</span>: dA1, </span><br><span class="line">                 <span class="string">"dZ1"</span>: dZ1, <span class="string">"dW1"</span>: dW1, <span class="string">"db1"</span>: db1&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><p>Train and test:<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">parameters = model(train_X, train_Y, keep_prob = <span class="number">0.86</span>, learning_rate = <span class="number">0.3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the train set:"</span>)</span><br><span class="line">predictions_train = predict(train_X, train_Y, parameters)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"On the test set:"</span>)</span><br><span class="line">predictions_test = predict(test_X, test_Y, parameters)</span><br></pre></td></tr></table></figure></p><p>Result:<img src="https://upload-images.jianshu.io/upload_images/8636110-375b4aa1cbb89014.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-bcd49e6ed1b6b2d1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>What you should remember about dropout:</strong></p><ul><li>Dropout is a regularization technique.</li><li>You only use dropout during training. Don’t use dropout (randomly eliminate nodes) during test time.</li><li>Apply dropout both during forward and backward propagation.</li><li>During training time, divide each dropout layer by keep_prob to keep the same expected value for the activations. For example, if keep_prob is 0.5, then we will on average shut down half the nodes, so the output will be scaled by 0.5 since only the remaining half are contributing to the solution. Dividing by 0.5 is equivalent to multiplying by 2. Hence, the output now has the same expected value. You can check that this works even when keep_prob is other values than 0.5. </li></ul><h3 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h3><p><img src="https://upload-images.jianshu.io/upload_images/8636110-75bf68c731402015.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Note that regularization hurts training set performance! This is because it limits the ability of the network to overfit to the training set. But since it ultimately gives better test accuracy, it is helping your system.</p><p><strong>What we need to remember</strong>:</p><ul><li>Regularization will help you reduce overfitting.</li><li>Regularization will drive your weights to lower values.</li><li>L2 regularization and Dropout are two very effective regularization techniques.</li></ul><h2 id="Gradient-Checking"><a href="#Gradient-Checking" class="headerlink" title="Gradient Checking"></a>Gradient Checking</h2><p>import packages:<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> testCases <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> gc_utils <span class="keyword">import</span> sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector</span><br></pre></td></tr></table></figure></p><p><strong>1) How does gradient checking work?</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-16af127a400268af.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>2) 1-dimensional gradient checking</strong><br><img src="https://upload-images.jianshu.io/upload_images/8636110-89a1e5d5f434b40b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward_propagation</span><span class="params">(x, theta)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the linear forward propagation (compute J) presented in Figure 1 (J(theta) = theta * x)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- a real-valued input</span></span><br><span class="line"><span class="string">    theta -- our parameter, a real number as well</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    J -- the value of function J, computed using the formula J(theta) = theta * x</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    J = theta * x</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> J</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagation</span><span class="params">(x, theta)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the derivative of J with respect to theta (see Figure 1).</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- a real-valued input</span></span><br><span class="line"><span class="string">    theta -- our parameter, a real number as well</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dtheta -- the gradient of the cost with respect to theta</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    dtheta = x</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dtheta</span><br></pre></td></tr></table></figure></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-f71a8784ae28956f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_check</span><span class="params">(x, theta, epsilon = <span class="number">1e-7</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation presented in Figure 1.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- a real-valued input</span></span><br><span class="line"><span class="string">    theta -- our parameter, a real number as well</span></span><br><span class="line"><span class="string">    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    difference -- difference (2) between the approximated gradient and the backward propagation gradient</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute gradapprox using left side of formula (1). epsilon is small enough, you don't need to worry about the limit.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 5 lines)</span></span><br><span class="line">    thetaplus = theta + epsilon                               <span class="comment"># Step 1</span></span><br><span class="line">    thetaminus = theta - epsilon                              <span class="comment"># Step 2</span></span><br><span class="line">    J_plus = forward_propagation(x,thetaplus)                                  <span class="comment"># Step 3</span></span><br><span class="line">    J_minus = forward_propagation(x,thetaminus)                                 <span class="comment"># Step 4</span></span><br><span class="line">    gradapprox = (J_plus-J_minus)/(<span class="number">2</span>*epsilon)                             <span class="comment"># Step 5</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Check if gradapprox is close enough to the output of backward_propagation()</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    grad = backward_propagation(x, theta)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    numerator = np.linalg.norm(grad - gradapprox)                             <span class="comment"># Step 1'</span></span><br><span class="line">    denominator = np.linalg.norm(grad)+np.linalg.norm(gradapprox)                          <span class="comment"># Step 2'</span></span><br><span class="line">    difference = numerator / denominator                           <span class="comment"># Step 3'</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> difference &lt; <span class="number">1e-7</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"The gradient is correct!"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"The gradient is wrong!"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> difference</span><br></pre></td></tr></table></figure><p>If the difference is smaller that the 1e-7, then we can have high confidence that we’ve correctly computed the gradient in backward_propagation().</p><p><strong>3)N-dimensional gradient checking</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-6dd5258b2f733fd7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如何对多层神经网络进行梯度检测？<br><img src="https://upload-images.jianshu.io/upload_images/8636110-50ee01ca15822334.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-965c5966c012dac0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_check_n</span><span class="params">(parameters, gradients, X, Y, epsilon = <span class="number">1e-7</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":</span></span><br><span class="line"><span class="string">    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters. </span></span><br><span class="line"><span class="string">    x -- input datapoint, of shape (input size, 1)</span></span><br><span class="line"><span class="string">    y -- true "label"</span></span><br><span class="line"><span class="string">    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    difference -- difference (2) between the approximated gradient and the backward propagation gradient</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Set-up variables</span></span><br><span class="line">    parameters_values, _ = dictionary_to_vector(parameters)</span><br><span class="line">    grad = gradients_to_vector(gradients)</span><br><span class="line">    num_parameters = parameters_values.shape[<span class="number">0</span>]</span><br><span class="line">    J_plus = np.zeros((num_parameters, <span class="number">1</span>))</span><br><span class="line">    J_minus = np.zeros((num_parameters, <span class="number">1</span>))</span><br><span class="line">    gradapprox = np.zeros((num_parameters, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute gradapprox</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_parameters):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute J_plus[i]. Inputs: "parameters_values, epsilon". Output = "J_plus[i]".</span></span><br><span class="line">        <span class="comment"># "_" is used because the function you have to outputs two parameters but we only care about the first one</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 3 lines)</span></span><br><span class="line">        thetaplus = np.copy(parameters_values)                                     <span class="comment"># Step 1</span></span><br><span class="line">        thetaplus[i][<span class="number">0</span>] = thetaplus[i][<span class="number">0</span>] + epsilon                           <span class="comment"># Step 2</span></span><br><span class="line">        J_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))                                <span class="comment"># Step 3</span></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute J_minus[i]. Inputs: "parameters_values, epsilon". Output = "J_minus[i]".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 3 lines)</span></span><br><span class="line">        thetaminus = np.copy(parameters_values)                                        <span class="comment"># Step 1</span></span><br><span class="line">        thetaminus[i][<span class="number">0</span>] = thetaplus[i][<span class="number">0</span>] - epsilon                              <span class="comment"># Step 2        </span></span><br><span class="line">        J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus))                                    <span class="comment"># Step 3</span></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute gradapprox[i]</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">        gradapprox[i] = (J_plus[i]-J_minus[i])/(<span class="number">2</span>*epsilon)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compare gradapprox to backward propagation gradients by computing difference.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (approx. 1 line)</span></span><br><span class="line">    numerator = np.linalg.norm(grad - gradapprox)                                           <span class="comment"># Step 1'</span></span><br><span class="line">    denominator = np.linalg.norm(gradapprox) + np.linalg.norm(grad)                                         <span class="comment"># Step 2'</span></span><br><span class="line">    difference = numerator / denominator                                         <span class="comment"># Step 3'</span></span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> difference &gt; <span class="number">1e-7</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"\033[93m"</span> + <span class="string">"There is a mistake in the backward propagation! difference = "</span> + str(difference) + <span class="string">"\033[0m"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">"\033[92m"</span> + <span class="string">"Your backward propagation works perfectly fine! difference = "</span> + str(difference) + <span class="string">"\033[0m"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> difference</span><br></pre></td></tr></table></figure><p>最后运行结果出现difference大于1e-7，说明我们的反向传播函数中出现错误，需要到原函数中进行查找。</p><p><strong>Note</strong></p><ul><li>Gradient Checking is slow. For this reason, we don’t run gradient checking at every iteration during training. Just a few times to check if the gradient is correct.</li><li>Gradient Checking, at least as we’ve presented it, doesn’t work with dropout. You would usually run the gradient check algorithm without dropout to make sure your backprop is correct, then add dropout.</li></ul><p><strong>What we need to rememeber</strong></p><ul><li>Gradient checking verifies closeness between the gradients from backpropagation and the numerical approximation of the gradient (computed using forward propagation).</li><li>Gradient checking is slow, so we don’t run it in every iteration of training. You would usually run it only to make sure your code is correct, then turn it off and use backprop for the actual learning process.</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;训练-验证-测试集&quot;&gt;&lt;a href=&quot;#训练-验证-测试集&quot; class=&quot;headerlink&quot; title=&quot;训练/验证/测试集&quot;&gt;&lt;/a&gt;训练/验证/测试集&lt;/h1&gt;&lt;p&gt;在实际构建和训练深层神经网络的时候，我们往往要确定一些超参数，如下：&lt;br&gt;&lt;img
      
    
    </summary>
    
      <category term="深度学习" scheme="https://github.com/DesmonDay/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://github.com/DesmonDay/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>第4周-深层神经网络</title>
    <link href="https://github.com/DesmonDay/2019/04/10/deep-learningw4/"/>
    <id>https://github.com/DesmonDay/2019/04/10/deep-learningw4/</id>
    <published>2019-04-10T10:59:44.000Z</published>
    <updated>2019-04-17T05:07:07.940Z</updated>
    
    <content type="html"><![CDATA[<p>接下来三周为改善深层神经网络：超参数调试、正则化以及优化。</p><h1 id="深层神经网络介绍"><a href="#深层神经网络介绍" class="headerlink" title="深层神经网络介绍"></a>深层神经网络介绍</h1><p><img src="https://upload-images.jianshu.io/upload_images/8636110-b259b31dc3bdd95a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>一些符号表示：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c0ce4005cfac8297.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="深层网络中的前向传播"><a href="#深层网络中的前向传播" class="headerlink" title="深层网络中的前向传播"></a>深层网络中的前向传播</h1><p>从计算可知，我们需要显式地使用一个for循环来计算每一层的前向传播输出。另外，右侧为向量化的表示方式。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-2a85981119325313.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="核对矩阵中的维数"><a href="#核对矩阵中的维数" class="headerlink" title="核对矩阵中的维数"></a>核对矩阵中的维数</h1><p>这里总结了一下计算过程中我们的参数W和b的维数，以及Z和A的维数。</p><p>首先是参数的维数：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-1a17e18191a86898.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>接着是Z和A的维数，这里指出的是向量化的结果，其中m为样本数目：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-3e2c280ac12ec663.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="为什么要使用深层表示"><a href="#为什么要使用深层表示" class="headerlink" title="为什么要使用深层表示"></a>为什么要使用深层表示</h1><p>吴老师在这里举了三个例子来解释，分别是图像、语音和电路系统。以图像为例，我们通常由简单到复杂， 先识别图像的边缘，再识别图像的局部，最后组成成图像的整体，深层神经网络就是这样一层层地增加识别的难度；同理，对于语音识别，我们从音调的高低，再组合成声音的基本单元：音位，再到单词，最后到词组、句子，一步步地组合成我们需要的，而深层神经网络也是这样对应下来。</p><p>最后举了电路系统的例子，用来解释我们可以用深层神经网络更好地计算一些数学公式。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ee9768b637f1f234.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="搭建深层神经网络块"><a href="#搭建深层神经网络块" class="headerlink" title="搭建深层神经网络块"></a>搭建深层神经网络块</h1><p>输入输出的整个流程图（包括前向传播和后向传播）：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c6f643511f987015.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>注意到一个细节，就是我们会把前向函数计算出来的Z值缓存起来，便于在反向计算时使用。</p><h1 id="参数和超参数"><a href="#参数和超参数" class="headerlink" title="参数和超参数"></a>参数和超参数</h1><p>注意，超参数的选择会影响参数的值。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-aca7792266fb55da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>超参数需要调参，很大程度上要基于经验选择，在一定范围内找到最优值。</p><h1 id="本周作业"><a href="#本周作业" class="headerlink" title="本周作业"></a>本周作业</h1><h2 id="Building-your-Deep-Neural-Network-Step-by-Step"><a href="#Building-your-Deep-Neural-Network-Step-by-Step" class="headerlink" title="Building your Deep Neural Network: Step by Step"></a>Building your Deep Neural Network: Step by Step</h2><h3 id="1-导入包"><a href="#1-导入包" class="headerlink" title="1. 导入包"></a>1. 导入包</h3><p>Import all the packages that we will need during the assignment.</p><ul><li>numpy is the main package for scientific computing with Python.</li><li>matplotlib is a library to plot graphs in Python.</li><li>dnn_utils provides some necessary functions for this notebook.</li><li>testCases provides some test cases to assess the correctness of your functions</li><li>np.random.seed(1) is used to keep all the random function calls consistent. It will help us grade your work. Please don’t change the seed.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> testCases_v2 <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> dnn_utils_v2 <span class="keyword">import</span> sigmoid, sigmoid_backward, relu, relu_backward</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="2-任务大纲"><a href="#2-任务大纲" class="headerlink" title="2. 任务大纲"></a>2. 任务大纲</h3><p>To build your neural network, you will be implementing several “helper functions”. </p><ul><li>Initialize the parameters for a two-layer network and for an  LL -layer neural network.</li><li>Implement the forward propagation module (shown in purple in the figure below).<ul><li>Complete the LINEAR part of a layer’s forward propagation step (resulting in  Z^[l] ).</li><li>We give you the ACTIVATION function (relu/sigmoid).</li><li>Combine the previous two steps into a new [LINEAR-&gt;ACTIVATION] forward function.</li><li>Stack the [LINEAR-&gt;RELU] forward function L-1 time (for layers 1 through L-1) and add a [LINEAR-&gt;SIGMOID] at the end (for the final layer L). This gives you a new L_model_forward function.</li></ul></li><li>Compute the loss.</li><li>Implement the backward propagation module (denoted in red in the figure below).<ul><li>Complete the LINEAR part of a layer’s backward propagation step.</li><li>We give you the gradient of the ACTIVATE function (relu_backward/sigmoid_backward)</li><li>Combine the previous two steps into a new [LINEAR-&gt;ACTIVATION] backward function.</li><li>Stack [LINEAR-&gt;RELU] backward L-1 times and add [LINEAR-&gt;SIGMOID] backward in a new L_model_backward function</li></ul></li><li>Finally update the parameters.</li></ul><p>以图片形式表示：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-8baf676325cae7ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="3-初始化"><a href="#3-初始化" class="headerlink" title="3. 初始化"></a>3. 初始化</h3><p>两层神经网络：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters</span><span class="params">(n_x, n_h, n_y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Argument:</span></span><br><span class="line"><span class="string">    n_x -- size of the input layer</span></span><br><span class="line"><span class="string">    n_h -- size of the hidden layer</span></span><br><span class="line"><span class="string">    n_y -- size of the output layer</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters:</span></span><br><span class="line"><span class="string">                    W1 -- weight matrix of shape (n_h, n_x)</span></span><br><span class="line"><span class="string">                    b1 -- bias vector of shape (n_h, 1)</span></span><br><span class="line"><span class="string">                    W2 -- weight matrix of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">                    b2 -- bias vector of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">    W1 = np.random.randn(n_h,n_x)*<span class="number">0.01</span></span><br><span class="line">    b1 = np.zeros((n_h,<span class="number">1</span>))</span><br><span class="line">    W2 = np.random.randn(n_y,n_h)*<span class="number">0.01</span></span><br><span class="line">    b2 = np.zeros((n_y,<span class="number">1</span>))</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(W1.shape == (n_h, n_x))</span><br><span class="line">    <span class="keyword">assert</span>(b1.shape == (n_h, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">assert</span>(W2.shape == (n_y, n_h))</span><br><span class="line">    <span class="keyword">assert</span>(b2.shape == (n_y, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    parameters = &#123;<span class="string">"W1"</span>: W1,</span><br><span class="line">                  <span class="string">"b1"</span>: b1,</span><br><span class="line">                  <span class="string">"W2"</span>: W2,</span><br><span class="line">                  <span class="string">"b2"</span>: b2&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure></p><p>L层神经网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_parameters_deep</span><span class="params">(layer_dims)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    layer_dims -- python array (list) containing the dimensions of each layer in our network</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":</span></span><br><span class="line"><span class="string">                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])</span></span><br><span class="line"><span class="string">                    bl -- bias vector of shape (layer_dims[l], 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">3</span>) <span class="comment"># 为了作业检查是否正确而设定的种子</span></span><br><span class="line">    parameters = &#123;&#125;</span><br><span class="line">    L = len(layer_dims)            <span class="comment"># number of layers in the network</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        parameters[<span class="string">'W'</span> + str(l)] = np.random.randn(layer_dims[l],layer_dims[l<span class="number">-1</span>])*<span class="number">0.01</span></span><br><span class="line">        parameters[<span class="string">'b'</span> + str(l)] = np.zeros((layer_dims[l], <span class="number">1</span>))</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'W'</span> + str(l)].shape == (layer_dims[l], layer_dims[l<span class="number">-1</span>]))</span><br><span class="line">        <span class="keyword">assert</span>(parameters[<span class="string">'b'</span> + str(l)].shape == (layer_dims[l], <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h3 id="4-前向传播模块"><a href="#4-前向传播模块" class="headerlink" title="4. 前向传播模块"></a>4. 前向传播模块</h3><h4 id="4-1-Linear-Forward"><a href="#4-1-Linear-Forward" class="headerlink" title="4.1 Linear Forward"></a>4.1 Linear Forward</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_forward</span><span class="params">(A, W, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the linear part of a layer's forward propagation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span><br><span class="line"><span class="string">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class="line"><span class="string">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    Z -- the input of the activation function, also called pre-activation parameter </span></span><br><span class="line"><span class="string">    cache -- a python dictionary containing "A", "W" and "b" ; stored for computing the backward pass efficiently</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 line of code)</span></span><br><span class="line">    Z = np.dot(W,A)+b</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span>(Z.shape == (W.shape[<span class="number">0</span>], A.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (A, W, b)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> Z, cache</span><br></pre></td></tr></table></figure><h4 id="4-2-Linear-Activation-Forward"><a href="#4-2-Linear-Activation-Forward" class="headerlink" title="4.2 Linear-Activation Forward"></a>4.2 Linear-Activation Forward</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_forward</span><span class="params">(A_prev, W, b, activation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)</span></span><br><span class="line"><span class="string">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span></span><br><span class="line"><span class="string">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span></span><br><span class="line"><span class="string">    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    A -- the output of the activation function, also called the post-activation value </span></span><br><span class="line"><span class="string">    cache -- a python dictionary containing "linear_cache" and "activation_cache";</span></span><br><span class="line"><span class="string">             stored for computing the backward pass efficiently</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"sigmoid"</span>:</span><br><span class="line">        <span class="comment"># Inputs: "A_prev, W, b". Outputs: "A, activation_cache".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">        A, activation_cache = sigmoid(Z) <span class="comment"># 这里的cache为Z</span></span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">elif</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        <span class="comment"># Inputs: "A_prev, W, b". Outputs: "A, activation_cache".</span></span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        Z, linear_cache = linear_forward(A_prev, W, b)</span><br><span class="line">        A, activation_cache = relu(Z)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">assert</span> (A.shape == (W.shape[<span class="number">0</span>], A_prev.shape[<span class="number">1</span>]))</span><br><span class="line">    cache = (linear_cache, activation_cache) <span class="comment"># linear_cache: A,W,b; activation_cache: Z</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> A, cache</span><br></pre></td></tr></table></figure><h4 id="4-3-L-layer-Model"><a href="#4-3-L-layer-Model" class="headerlink" title="4.3 L-layer Model"></a>4.3 L-layer Model</h4><p><img src="https://upload-images.jianshu.io/upload_images/8636110-60120c093b83d009.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_forward</span><span class="params">(X, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- data, numpy array of shape (input size, number of examples)</span></span><br><span class="line"><span class="string">    parameters -- output of initialize_parameters_deep()</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    AL -- last post-activation value</span></span><br><span class="line"><span class="string">    caches -- list of caches containing:</span></span><br><span class="line"><span class="string">                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)</span></span><br><span class="line"><span class="string">                the cache of linear_sigmoid_forward() (there is one, indexed L-1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    caches = []</span><br><span class="line">    A = X </span><br><span class="line">    L = len(parameters) // <span class="number">2</span>  <span class="comment"># number of layers in the neural network</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Implement [LINEAR -&gt; RELU]*(L-1). Add "cache" to the "caches" list.</span></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(<span class="number">1</span>, L):</span><br><span class="line">        A_prev = A</span><br><span class="line">        <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">        A, cache = linear_activation_forward(A_prev, parameters[<span class="string">"W"</span>+str(l)], parameters[<span class="string">"b"</span>+str(l)], <span class="string">"relu"</span>)</span><br><span class="line">        caches.append(cache)</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Implement LINEAR -&gt; SIGMOID. Add "cache" to the "caches" list.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 2 lines of code)</span></span><br><span class="line">    AL, cache = linear_activation_forward(A, parameters[<span class="string">"W"</span>+str(L)], parameters[<span class="string">"b"</span>+str(L)], <span class="string">"sigmoid"</span>)</span><br><span class="line">    caches.append(cache)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(AL.shape == (<span class="number">1</span>, X.shape[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">return</span> AL, caches</span><br></pre></td></tr></table></figure><h3 id="5-Cost-function"><a href="#5-Cost-function" class="headerlink" title="5. Cost function"></a>5. Cost function</h3><p><img src="https://upload-images.jianshu.io/upload_images/8636110-b7a72fc204f22a2d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_cost</span><span class="params">(AL, Y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the cost function defined by equation (7).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    cost -- cross-entropy cost</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    m = Y.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute loss from AL and y.</span></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 1 lines of code)</span></span><br><span class="line">    cost = -np.sum(Y*np.log(AL) + (<span class="number">1</span>-Y)*np.log(<span class="number">1</span>-AL)) / m</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    cost = np.squeeze(cost) <span class="comment"># To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).</span></span><br><span class="line">    <span class="keyword">assert</span>(cost.shape == ())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cost</span><br></pre></td></tr></table></figure><h3 id="6-反向传播模块"><a href="#6-反向传播模块" class="headerlink" title="6. 反向传播模块"></a>6. 反向传播模块</h3><p><img src="https://upload-images.jianshu.io/upload_images/8636110-435066cf1d671938.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Now, similar to forward propagation, you are going to build the backward propagation in three steps:</p><ul><li>LINEAR backward</li><li>LINEAR -&gt; ACTIVATION backward where ACTIVATION computes the derivative of either the ReLU or sigmoid activation</li><li>[LINEAR -&gt; RELU] X (L-1) -&gt; LINEAR -&gt; SIGMOID backward (whole model)</li></ul><h4 id="6-1-Linear-backward"><a href="#6-1-Linear-backward" class="headerlink" title="6.1 Linear backward"></a>6.1 Linear backward</h4><p><img src="https://upload-images.jianshu.io/upload_images/8636110-0362454b836ff631.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_backward</span><span class="params">(dZ, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the linear portion of backward propagation for a single layer (layer l)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dZ -- Gradient of the cost with respect to the linear output (of current layer l)</span></span><br><span class="line"><span class="string">    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class="line"><span class="string">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class="line"><span class="string">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    A_prev, W, b = cache</span><br><span class="line">    m = A_prev.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ### (≈ 3 lines of code)</span></span><br><span class="line">    dW = np.dot(dZ,A_prev.T)/m</span><br><span class="line">    db = np.sum(dZ,axis=<span class="number">1</span>, keepdims = <span class="keyword">True</span>)/m <span class="comment"># 这里要注意！！！</span></span><br><span class="line">    dA_prev = np.dot(W.T,dZ)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> (dA_prev.shape == A_prev.shape)</span><br><span class="line">    <span class="keyword">assert</span> (dW.shape == W.shape)</span><br><span class="line">    <span class="keyword">assert</span> (db.shape == b.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure><h4 id="6-2-Linear-Activation-backward"><a href="#6-2-Linear-Activation-backward" class="headerlink" title="6.2 Linear-Activation backward"></a>6.2 Linear-Activation backward</h4><p><img src="https://upload-images.jianshu.io/upload_images/8636110-4528f7692864b295.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_activation_backward</span><span class="params">(dA, cache, activation)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    dA -- post-activation gradient for current layer l </span></span><br><span class="line"><span class="string">    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently</span></span><br><span class="line"><span class="string">    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span></span><br><span class="line"><span class="string">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span></span><br><span class="line"><span class="string">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    linear_cache, activation_cache = cache</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> activation == <span class="string">"relu"</span>:</span><br><span class="line">        dZ = relu_backward(dA, activation_cache)</span><br><span class="line">        dA_prev, dW, db = linear_backward(dZ, linear_cache)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dA_prev, dW, db</span><br></pre></td></tr></table></figure><h4 id="6-3-L-Model-Backward"><a href="#6-3-L-Model-Backward" class="headerlink" title="6.3 L-Model Backward"></a>6.3 L-Model Backward</h4><p><img src="https://upload-images.jianshu.io/upload_images/8636110-11097ec0e5799543.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-d23ec336460f48b1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_model_backward</span><span class="params">(AL, Y, caches)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    AL -- probability vector, output of the forward propagation (L_model_forward())</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if non-cat, 1 if cat)</span></span><br><span class="line"><span class="string">    caches -- list of caches containing:</span></span><br><span class="line"><span class="string">                every cache of linear_activation_forward() with "relu" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)</span></span><br><span class="line"><span class="string">                the cache of linear_activation_forward() with "sigmoid" (it's caches[L-1])</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    grads -- A dictionary with the gradients</span></span><br><span class="line"><span class="string">             grads["dA" + str(l)] = ...</span></span><br><span class="line"><span class="string">             grads["dW" + str(l)] = ...</span></span><br><span class="line"><span class="string">             grads["db" + str(l)] = ...</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    L = len(caches) <span class="comment"># the number of layers</span></span><br><span class="line">    m = AL.shape[<span class="number">1</span>]</span><br><span class="line">    Y = Y.reshape(AL.shape) <span class="comment"># after this line, Y is the same shape as AL</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initializing the backpropagation</span></span><br><span class="line">    dAL = - (np.divide(Y, AL) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - AL))</span><br><span class="line">    current_cache = caches[L<span class="number">-1</span>]</span><br><span class="line">    grads[<span class="string">"dA"</span> + str(L)], grads[<span class="string">"dW"</span> + str(L)], grads[<span class="string">"db"</span> + str(L)] = linear_activation_backward(dAL,current_cache,<span class="string">"sigmoid"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> reversed(range(L<span class="number">-1</span>)):</span><br><span class="line">        current_cache = caches[l]</span><br><span class="line">        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[<span class="string">"dA"</span>+str(l+<span class="number">2</span>)], current_cache, <span class="string">"relu"</span>)</span><br><span class="line">        grads[<span class="string">"dA"</span> + str(l+<span class="number">1</span>)] = dA_prev_temp</span><br><span class="line">        grads[<span class="string">"dW"</span> + str(l+<span class="number">1</span>)] = dW_temp</span><br><span class="line">        grads[<span class="string">"db"</span> + str(l+<span class="number">1</span>)] = db_temp</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure><h4 id="6-4-Update-Parameters"><a href="#6-4-Update-Parameters" class="headerlink" title="6.4 Update Parameters"></a>6.4 Update Parameters</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_parameters</span><span class="params">(parameters, grads, learning_rate)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Update parameters using gradient descent</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your parameters </span></span><br><span class="line"><span class="string">    grads -- python dictionary containing your gradients, output of L_model_backward</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing your updated parameters </span></span><br><span class="line"><span class="string">                  parameters["W" + str(l)] = ... </span></span><br><span class="line"><span class="string">                  parameters["b" + str(l)] = ...</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    L = len(parameters) // <span class="number">2</span>  <span class="comment"># number of layers in the neural network</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> range(L):</span><br><span class="line">        parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)] = parameters[<span class="string">"W"</span>+str(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">"dW"</span>+str(l+<span class="number">1</span>)]</span><br><span class="line">        parameters[<span class="string">"b"</span> + str(l+<span class="number">1</span>)] = parameters[<span class="string">"b"</span>+str(l+<span class="number">1</span>)] - learning_rate * grads[<span class="string">"db"</span>+str(l+<span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure><h2 id="Deep-Neural-Network-for-Image-Classification-Application"><a href="#Deep-Neural-Network-for-Image-Classification-Application" class="headerlink" title="Deep Neural Network for Image Classification: Application"></a>Deep Neural Network for Image Classification: Application</h2><h3 id="Packages"><a href="#Packages" class="headerlink" title="Packages"></a>Packages</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> h5py</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> ndimage</span><br><span class="line"><span class="keyword">from</span> dnn_app_utils_v2 <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">5.0</span>, <span class="number">4.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">train_x_orig, train_y, test_x_orig, test_y, classes = load_data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Explore your dataset</span></span><br><span class="line">m_train = train_x_orig.shape[<span class="number">0</span>]</span><br><span class="line">num_px = train_x_orig.shape[<span class="number">1</span>]</span><br><span class="line">m_test = test_x_orig.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of training examples: "</span> + str(m_train))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Number of testing examples: "</span> + str(m_test))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"Each image is of size: ("</span> + str(num_px) + <span class="string">", "</span> + str(num_px) + <span class="string">", 3)"</span>)</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_x_orig shape: "</span> + str(train_x_orig.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_y shape: "</span> + str(train_y.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_x_orig shape: "</span> + str(test_x_orig.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_y shape: "</span> + str(test_y.shape))</span><br></pre></td></tr></table></figure><p><img src="https://upload-images.jianshu.io/upload_images/8636110-bfae12fb41ebccea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Reshape the training and test exmaples</span></span><br><span class="line">train_x_flatten = train_x_orig.reshape(train_x_orig.shape[<span class="number">0</span>],<span class="number">-1</span>).T <span class="comment"># The "-1" makes reshape flatten the remaining dimensions</span></span><br><span class="line">test_x_flatten = test_x_orig.reshape(test_x_orig.shape[<span class="number">0</span>],<span class="number">-1</span>).T</span><br><span class="line"></span><br><span class="line"><span class="comment"># Standardize data to have feature values between 0 and 1.</span></span><br><span class="line">train_x = train_x_flatten/<span class="number">255.</span></span><br><span class="line">test_x = test_x_flatten/<span class="number">255.</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"train_x's shape: "</span> + str(train_x.shape))</span><br><span class="line"><span class="keyword">print</span> (<span class="string">"test_x's shape: "</span> + str(test_x.shape))</span><br></pre></td></tr></table></figure><h3 id="构建模型"><a href="#构建模型" class="headerlink" title="构建模型"></a>构建模型</h3><h4 id="1-2-layer-neural-network"><a href="#1-2-layer-neural-network" class="headerlink" title="1. 2-layer neural network"></a>1. 2-layer neural network</h4><p><img src="https://upload-images.jianshu.io/upload_images/8636110-c612af784c677de0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>将要用到的函数：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-6a1485fb36d28b1c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>模型的构建<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### CONSTANTS DEFINING THE MODEL ####</span></span><br><span class="line">n_x = <span class="number">12288</span>     <span class="comment"># num_px * num_px * 3</span></span><br><span class="line">n_h = <span class="number">7</span></span><br><span class="line">n_y = <span class="number">1</span></span><br><span class="line">layers_dims = (n_x, n_h, n_y)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">two_layer_model</span><span class="params">(X, Y, layers_dims, learning_rate=<span class="number">0.0075</span>, num_iterations=<span class="number">3000</span>, print_cost=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a two-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, of shape (n_x, number of examples)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    layers_dims -- dimensions of the layers (n_x, n_h, n_y)</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">    print_cost -- If set to True, this will print the cost every 100 iterations </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- a dictionary containing W1, W2, b1, and b2</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    np.random=seed(<span class="number">1</span>)</span><br><span class="line">    grads = &#123;&#125;</span><br><span class="line">    costs = []</span><br><span class="line">    m = X.shape[<span class="number">1</span>]</span><br><span class="line">    (n_x, n_h, n_y) = layers_dims</span><br><span class="line"></span><br><span class="line">    parameters = initialize_parameters(n_x, n_h, n_y)</span><br><span class="line"></span><br><span class="line">    W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">    b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">    W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">    b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        A1, cache1 = linear_activation_forward(X, W1, b1, <span class="string">"relu"</span>)</span><br><span class="line">        A2, cache2 = linear_activation_forward(A1, W2, b2, <span class="string">"sigmoid"</span>)</span><br><span class="line"></span><br><span class="line">        cost = compute_cost(A2, Y)</span><br><span class="line"></span><br><span class="line">        dA2 = - (np.divide(Y, A2) - np.divide(<span class="number">1</span> - Y, <span class="number">1</span> - A2))</span><br><span class="line"></span><br><span class="line">        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, <span class="string">"sigmoid"</span>)</span><br><span class="line">        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, <span class="string">"relu"</span>)</span><br><span class="line"></span><br><span class="line">        grads[<span class="string">"dW1"</span>] = dW1</span><br><span class="line">        grads[<span class="string">'db1'</span>] = db1</span><br><span class="line">        grads[<span class="string">'dW2'</span>] = dW2</span><br><span class="line">        grads[<span class="string">'db2'</span>] = db2</span><br><span class="line"></span><br><span class="line">        <span class="comment">## Update parameters</span></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line"></span><br><span class="line">        W1 = parameters[<span class="string">"W1"</span>]</span><br><span class="line">        b1 = parameters[<span class="string">"b1"</span>]</span><br><span class="line">        W2 = parameters[<span class="string">"W2"</span>]</span><br><span class="line">        b2 = parameters[<span class="string">"b2"</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Cost after iteration &#123;&#125;: &#123;&#125;"</span>.format(i, np.squeeze(cost)))</span><br><span class="line">            costs.append(cost)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line"></span><br><span class="line">    plt.plot(costs)</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> parameters</span><br></pre></td></tr></table></figure></p><p>训练模型：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = <span class="number">2500</span>, print_cost=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure></p><p>预测结果：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">predictions_train = predict(train_x, train_y, parameters)</span><br><span class="line"></span><br><span class="line">predictions_test = predict(test_x, test_y, parameters)</span><br></pre></td></tr></table></figure></p><p>Note: You may notice that running the model on fewer iterations (say 1500) gives better accuracy on the test set. This is called “early stopping” and we will talk about it in the next course. Early stopping is a way to prevent overfitting.</p><h4 id="2-L-layer-neural-network"><a href="#2-L-layer-neural-network" class="headerlink" title="2. L-layer neural network"></a>2. L-layer neural network</h4><p><img src="https://upload-images.jianshu.io/upload_images/8636110-b7a1501a32fd62ab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>要用到的函数：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d929d0a3fb804874.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>模型的构建：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### CONSTANTS ###</span></span><br><span class="line">layers_dims = [<span class="number">12288</span>, <span class="number">20</span>, <span class="number">7</span>, <span class="number">5</span>, <span class="number">1</span>] <span class="comment">#  5-layer model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">L_layer_mode</span><span class="params">(X, Y, layers_dims, learning_rate = <span class="number">0.0075</span>, num_iterations=<span class="number">3000</span>, print_cost=False)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)</span></span><br><span class="line"><span class="string">    Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)</span></span><br><span class="line"><span class="string">    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).</span></span><br><span class="line"><span class="string">    learning_rate -- learning rate of the gradient descent update rule</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations of the optimization loop</span></span><br><span class="line"><span class="string">    print_cost -- if True, it prints the cost every 100 steps</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    parameters -- parameters learnt by the model. They can then be used to predict.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line">    costs = []</span><br><span class="line"></span><br><span class="line">    parameters = initialize_parameters_deep(layers_dims)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, num_iterations):</span><br><span class="line">        AL, caches = L_model_forward(X, parameters)</span><br><span class="line"></span><br><span class="line">        cost = compute_cost(AL, Y)</span><br><span class="line"></span><br><span class="line">        grads = L_model_backward(AL, Y, caches)</span><br><span class="line"></span><br><span class="line">        parameters = update_parameters(parameters, grads, learning_rate)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">print</span> (<span class="string">"Cost after iteration %i: %f"</span> %(i, cost))</span><br><span class="line">        <span class="keyword">if</span> print_cost <span class="keyword">and</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            costs.append(cost)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># plot the cost</span></span><br><span class="line">    plt.plot(np.squeeze(costs))</span><br><span class="line">    plt.ylabel(<span class="string">'cost'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'iterations (per tens)'</span>)</span><br><span class="line">    plt.title(<span class="string">"Learning rate ="</span> + str(learning_rate))</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure></p><p>训练模型：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = <span class="number">2500</span>, print_cost = <span class="keyword">True</span>)</span><br></pre></td></tr></table></figure></p><p>预测：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pred_train = predict(train_x, train_y, parameters)</span><br><span class="line">pred_test = predict(test_x, test_y, parameters)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;接下来三周为改善深层神经网络：超参数调试、正则化以及优化。&lt;/p&gt;
&lt;h1 id=&quot;深层神经网络介绍&quot;&gt;&lt;a href=&quot;#深层神经网络介绍&quot; class=&quot;headerlink&quot; title=&quot;深层神经网络介绍&quot;&gt;&lt;/a&gt;深层神经网络介绍&lt;/h1&gt;&lt;p&gt;&lt;img src=
      
    
    </summary>
    
      <category term="深度学习" scheme="https://github.com/DesmonDay/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://github.com/DesmonDay/tags/python/"/>
    
  </entry>
  
</feed>
