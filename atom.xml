<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>DesmonDay&#39;s Blog</title>
  
  <subtitle>一只小辣鸡的自我拯救之路</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://github.com/DesmonDay/"/>
  <updated>2020-03-01T16:30:14.200Z</updated>
  <id>https://github.com/DesmonDay/</id>
  
  <author>
    <name>DesmonDay</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>刷题笔记</title>
    <link href="https://github.com/DesmonDay/2020/03/01/%E5%88%B7%E9%A2%98%E7%AC%94%E8%AE%B0/"/>
    <id>https://github.com/DesmonDay/2020/03/01/刷题笔记/</id>
    <published>2020-03-01T03:35:41.000Z</published>
    <updated>2020-03-01T16:30:14.200Z</updated>
    
    <content type="html"><![CDATA[<p>从现在开始，按照leetcode专题刷题。此博文持续更新。</p><h1 id="栈"><a href="#栈" class="headerlink" title="栈"></a>栈</h1><h2 id="leetcode-225-用队列实现栈"><a href="#leetcode-225-用队列实现栈" class="headerlink" title="leetcode 225. 用队列实现栈"></a>leetcode 225. 用队列实现栈</h2><blockquote><p>只需要一个队列来模拟栈即可。要删除栈顶元素，可以用如下操作，即循环pop和push。获取top元素同理，但记得末尾要添加为temp。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt; <span class="function"><span class="keyword">int</span> <span class="title">pop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">&gt; <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; q.size() - <span class="number">1</span>; i++)&#123;</span><br><span class="line">&gt; q.push(q.front());</span><br><span class="line">&gt; q.pop();</span><br><span class="line">&gt; &#125;</span><br><span class="line">&gt; <span class="keyword">int</span> temp = q.front();</span><br><span class="line">&gt; q.pop();</span><br><span class="line">&gt; <span class="keyword">return</span> temp;</span><br><span class="line">&gt; &#125;</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure></blockquote><h1 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h1><h2 id="leetcode-1-两数之和"><a href="#leetcode-1-两数之和" class="headerlink" title="leetcode 1. 两数之和"></a>leetcode 1. 两数之和</h2><blockquote><p>给定一个整数数组 <code>nums</code> 和一个目标值 <code>target</code>，请你在该数组中找出和为目标值的那 <strong>两个</strong> 整数，并返回他们的数组下标。</p></blockquote><p>O(n)的做法，利用一个map来存储对应的值和下标，贴代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">map</span>&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; hashmap;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.size(); i++)&#123;</span><br><span class="line">    <span class="keyword">int</span> tmp = target - nums[i];</span><br><span class="line">    <span class="keyword">if</span> (hashmap.count(tmp) &gt; <span class="number">0</span>)&#123;</span><br><span class="line">        res.push_back(i);</span><br><span class="line">        res.push_back(hashmap[tmp]);</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    hashmap[nums[i]] = i;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="leetcode-3-找无重复字符的最长子串"><a href="#leetcode-3-找无重复字符的最长子串" class="headerlink" title="leetcode 3. 找无重复字符的最长子串"></a>leetcode 3. 找无重复字符的最长子串</h2><p>不会做，这里要运用<strong>滑动窗口</strong>求解。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">map</span>&lt;<span class="keyword">char</span>, <span class="keyword">int</span>&gt; M;</span><br><span class="line"><span class="keyword">int</span> i = <span class="number">0</span>, maxlen = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; s.size(); j++)&#123;</span><br><span class="line">    <span class="keyword">if</span> (M.count(s[j]))&#123;</span><br><span class="line">        i = max(M[s[j]], i); </span><br><span class="line"><span class="comment">// i是截至j，以j为最后一个元素的最长不重复子串的起始位置</span></span><br><span class="line"><span class="comment">// 即索引范围是[i,j]的子串是以索引j为最后一个元素的最长子串</span></span><br><span class="line">    &#125;</span><br><span class="line">    maxlen = max(maxlen, j - i + <span class="number">1</span>);</span><br><span class="line">    M[s[j]] = j + <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="leetcode-11-盛最多水的容器"><a href="#leetcode-11-盛最多水的容器" class="headerlink" title="leetcode 11. 盛最多水的容器"></a>leetcode 11. 盛最多水的容器</h2><p>思路只差一点就做对了，加油啊！！有时候不要想得太多，先按照一开始的思路写写看。思想有点类似动态规划。</p><h2 id="leetcode-15-三数之和"><a href="#leetcode-15-三数之和" class="headerlink" title="leetcode 15*. 三数之和"></a>leetcode 15*. 三数之和</h2><blockquote><p>给定一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 a，b，c ，使得 a + b + c = 0 ？找出所有满足条件且不重复的三元组。</p><p>注意：答案中不可以包含重复的三元组。</p></blockquote><p>类似于滑动窗口题，需要先进行排序，再利用滑动窗口求解。但注意，由于不允许重复，因此在左右指针更新的时候，需要特别留意！</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; threeSum(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums) &#123;</span><br><span class="line">        <span class="comment">// 同样是滑动窗口题，需要排序后才可以</span></span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; res;</span><br><span class="line">        <span class="keyword">if</span> (nums.size() &lt; <span class="number">3</span>) <span class="keyword">return</span> res;</span><br><span class="line">        sort(nums.begin(), nums.end());</span><br><span class="line">        <span class="keyword">int</span> left, right;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.size(); i++)&#123;</span><br><span class="line">            <span class="keyword">if</span> (nums[i] &gt; <span class="number">0</span>) <span class="keyword">return</span> res; <span class="comment">// 已排序，但第一个数字大于0，则跳过</span></span><br><span class="line">            <span class="keyword">if</span> (i &gt; <span class="number">0</span> &amp;&amp; nums[i] == nums[i - <span class="number">1</span>]) <span class="keyword">continue</span>; <span class="comment">// 遇到重复数字，跳过</span></span><br><span class="line">            left = i + <span class="number">1</span>; </span><br><span class="line">            right = nums.size() - <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">while</span> (left &lt; right)&#123;</span><br><span class="line">                <span class="keyword">if</span> (nums[i] + nums[left] + nums[right] == <span class="number">0</span>)&#123;</span><br><span class="line">                    res.push_back(&#123;nums[i], nums[left], nums[right]&#125;);</span><br><span class="line">                    <span class="comment">// 如果两个while不写，会导致重复结果</span></span><br><span class="line">                    <span class="keyword">while</span> (left &lt; right &amp;&amp; nums[left] == nums[left + <span class="number">1</span>])</span><br><span class="line">                        left = left + <span class="number">1</span>;</span><br><span class="line">                    <span class="keyword">while</span> (left &lt; right &amp;&amp; nums[right] == nums[right - <span class="number">1</span>])</span><br><span class="line">                        right = right - <span class="number">1</span>;</span><br><span class="line">                    left += <span class="number">1</span>; <span class="comment">// 这里还要继续更新，不然无法退出while</span></span><br><span class="line">                    right -= <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (nums[i] + nums[left] + nums[right] &gt; <span class="number">0</span>)&#123;</span><br><span class="line">                    right -= <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">else</span> &#123;</span><br><span class="line">                    left += <span class="number">1</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="leetcode-16-最接近的三数之和"><a href="#leetcode-16-最接近的三数之和" class="headerlink" title="leetcode 16. 最接近的三数之和"></a>leetcode 16. 最接近的三数之和</h2><p>方法与leetcode 15基本相同，同样是排序+滑动窗口。</p><h2 id="leetcode-18-四数之和"><a href="#leetcode-18-四数之和" class="headerlink" title="leetcode 18. 四数之和"></a>leetcode 18. 四数之和</h2><p>方法参考leetcode 15. 先进行排序，随后，我们首先将题目转化为三数之和，计算出新的target，从而可以求解。注意，仍然需要避免重复元素！！在每个循环初始化的时候，都要想清楚为什么。</p><p>还是不太会，这道题。。。希望不会出吧！有时间再做一次。</p><h2 id="leetcode-31-下一个排列"><a href="#leetcode-31-下一个排列" class="headerlink" title="leetcode 31*. 下一个排列"></a>leetcode 31*. 下一个排列</h2><blockquote><p>实现获取下一个排列的函数，算法需要将给定数字序列重新排列成<strong>字典序</strong>中下一个更大的排列。</p><p>如果不存在下一个更大的排列，则将数字重新排列成最小的排列（即升序排列）。</p><p>必须原地修改，只允许使用额外常数空间。</p><p>以下是一些例子，输入位于左侧列，其相应输出位于右侧列。<br>1,2,3 → 1,3,2<br>3,2,1 → 1,2,3<br>1,1,5 → 1,5,1</p></blockquote><p>参考思路：</p><ol><li>判断按照字典序有木有下一个，如果完全降序(包括等号)就没有下一个，则要找第一个</li><li>如何判断有没有下一个呢？只要存在a[i-1]&lt;a[i]的升序结构，则有下一个（我们要从右往左找）</li><li>当发现a[i-1]&lt;a[i]的结构，则从[i, ]中找到最接近a[i-1]又大于a[i-1]的数字，由于降序，从右往左遍历即可得到k</li><li>交换a[i-1]和k，再对[i, ]进行排序。排序只需要首尾不停交换即可，因为已经是降序。</li></ol><p>参考例子：比如[0,5,4,3,2,1]，下一个是[1,0,2,3,4,5]</p><h1 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h1><h2 id="leetcode-5-最长回文子串"><a href="#leetcode-5-最长回文子串" class="headerlink" title="leetcode 5*. 最长回文子串"></a>leetcode 5*. 最长回文子串</h2><p>这道题，百看不会，所以还是要多做很多很多次。</p><blockquote><p>给定一个字符串 <code>s</code>，找到 <code>s</code> 中最长的回文子串。你可以假设 <code>s</code> 的最大长度为 1000。</p></blockquote><p>我记的是O(n^2)的做法，即遍历对称轴来找回文串：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="built_in">string</span> <span class="title">longestPalindrome</span><span class="params">(<span class="built_in">string</span> s)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (s.size() &lt;= <span class="number">1</span>) <span class="keyword">return</span> s;</span><br><span class="line">        <span class="built_in">string</span> ans = <span class="string">""</span>;</span><br><span class="line">        <span class="keyword">int</span> maxlen = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 回文子串为奇数的情况, s[i]为中心轴</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; s.size(); i++)&#123;</span><br><span class="line">            <span class="keyword">int</span> j = <span class="number">0</span>;</span><br><span class="line">            <span class="comment">// while循环</span></span><br><span class="line">            <span class="keyword">while</span> (i - j &gt;= <span class="number">0</span> &amp;&amp; i + j &lt; s.size() &amp;&amp; s[i - j] == s[i + j])&#123;</span><br><span class="line">                <span class="keyword">if</span> (maxlen &lt; <span class="number">1</span> + <span class="number">2</span> * j)&#123;</span><br><span class="line">                    maxlen = <span class="number">1</span> + <span class="number">2</span> * j;</span><br><span class="line">                    ans = s.substr(i - j, maxlen);</span><br><span class="line">                &#125;</span><br><span class="line">                j += <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 回文子串为偶数的情况，s[i],s[i+1]为中心轴</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; s.size(); i++)&#123;</span><br><span class="line">            <span class="keyword">int</span> j = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">while</span> (i - j &gt;= <span class="number">0</span> &amp;&amp; i + j + <span class="number">1</span> &lt; s.size() &amp;&amp; s[i - j] == s[i + j + <span class="number">1</span>])&#123;</span><br><span class="line">                <span class="keyword">if</span> (maxlen &lt; <span class="number">2</span> + <span class="number">2</span> * j)&#123;</span><br><span class="line">                    maxlen = <span class="number">2</span> + <span class="number">2</span> * j;</span><br><span class="line">                    ans = s.substr(i - j, maxlen);</span><br><span class="line">                &#125;</span><br><span class="line">                j += <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ans;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>还有更好的思路，马拉车算法O(N)：<a href="https://blog.csdn.net/csdnnews/article/details/82920678" target="_blank" rel="noopener">https://blog.csdn.net/csdnnews/article/details/82920678</a></p><ol><li>先对字符串进行预处理，两个字符之间加上特殊符号#；</li><li>然后遍历整个字符串，用一个数组来记录以该字符为中心的回文长度，为了方便计算右边界，我在数组中记录长度的一半（向下取整）；</li><li>每一次遍历的时候，如果该字符在已知回文串最右边界的覆盖下，那么就计算其相对最右边界回文串中心对称的位置，得出已知回文串的长度；</li><li>判断该长度和右边界，如果达到了右边界，那么需要进行中心扩展探索。当然，如果第3步该字符没有在最右边界的“羽翼”下，则直接进行中心扩展探索。进行中心扩展探索的时候，同时又更新右边界；</li><li>最后得到最长回文之后，去掉其中的特殊符号即可。</li></ol><h2 id="leetcode-6-Z字形变换"><a href="#leetcode-6-Z字形变换" class="headerlink" title="leetcode 6*. Z字形变换"></a>leetcode 6*. Z字形变换</h2><blockquote><p>将一个给定字符串根据给定的行数，以从上往下、从左到右进行 Z 字形排列。</p><p>比如输入字符串为 “LEETCODEISHIRING” 行数为 3 时，排列如下：</p><p>L    C      I     R<br>E T O E S  I   I   G<br>E    D     H    N</p></blockquote><p>巧妙的思想，用一维数组就可以得到结果，图示如下：</p><p><img src="https://pic.leetcode-cn.com/ebbed8592bd11014e81affb8af6df3e713d88ae0e8003f4f989459d7694e475c-Picture8.png" alt="img" style="zoom: 25%;"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">convert</span><span class="params">(<span class="built_in">string</span> s, <span class="keyword">int</span> numRows)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">string</span> res[numRows];</span><br><span class="line">    <span class="keyword">if</span> (s.empty() || numRows &lt; <span class="number">1</span>) <span class="keyword">return</span> s;</span><br><span class="line">    <span class="keyword">if</span> (numRows == <span class="number">1</span>) <span class="keyword">return</span> s;</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">0</span>, flag = <span class="number">-1</span>; <span class="comment">// 利用flag来改变存储方向</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; s.size(); j++)&#123;</span><br><span class="line">        res[i] += s[j];</span><br><span class="line">        <span class="keyword">if</span> (i == <span class="number">0</span> || i == numRows - <span class="number">1</span>)</span><br><span class="line">            flag = - flag;</span><br><span class="line">        i += flag;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">string</span> ans;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> r : res)&#123;</span><br><span class="line">        ans += r;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="leetcode-8-字符串转换整数-atoi"><a href="#leetcode-8-字符串转换整数-atoi" class="headerlink" title="leetcode 8. 字符串转换整数(atoi)"></a>leetcode 8. 字符串转换整数(atoi)</h2><blockquote><p>请你来实现一个 atoi 函数，使其能将字符串转换成整数。</p><p>首先，该函数会根据需要丢弃无用的开头空格字符，直到寻找到第一个非空格的字符为止。</p><p>当我们寻找到的第一个非空字符为正或者负号时，则将该符号与之后面尽可能多的连续数字组合起来，作为该整数的正负号；假如第一个非空字符是数字，则直接将其与之后连续的数字字符组合起来，形成整数。</p><p>该字符串除了有效的整数部分之后也可能会存在多余的字符，这些字符可以被忽略，它们对于函数不应该造成影响。</p><p>注意：假如该字符串中的第一个非空格字符不是一个有效整数字符、字符串为空或字符串仅包含空白字符时，则你的函数不需要进行转换。</p><p>在任何情况下，若函数不能进行有效的转换时，请返回 0。</p><p>说明：</p><p>假设我们的环境只能存储 32 位大小的有符号整数，那么其数值范围为 [−231,  231 − 1]。如果数值超过这个范围，请返回  INT_MAX (231 − 1) 或 INT_MIN (−231) 。</p><p>示例 1:</p><p>输入: “42”<br>输出: 42</p><p>示例 2:</p><p>输入: “   -42”<br>输出: -42<br>解释: 第一个非空白字符为 ‘-‘, 它是一个负号。<br>     我们尽可能将负号与后面所有连续出现的数字组合起来，最后得到 -42 。</p><p>示例 3:</p><p>输入: “4193 with words”<br>输出: 4193<br>解释: 转换截止于数字 ‘3’ ，因为它的下一个字符不为数字。</p><p>示例 4:</p><p>输入: “words and 987”<br>输出: 0<br>解释: 第一个非空字符是 ‘w’, 但它不是数字或正、负号。<br>     因此无法执行有效的转换。</p><p>示例 5:</p><p>输入: “-91283472332”<br>输出: -2147483648<br>解释: 数字 “-91283472332” 超过 32 位有符号整数范围。<br>     因此返回 INT_MIN (−231) 。</p></blockquote><p>这个问题其实没有过多的技巧，考察的是<strong>细心和耐心</strong>，并且需要不断地调试。在这里我简单罗列几个要点。</p><p>Java 、Python 和 C++ 字符串的设计都是不可变的，即使用 trim() 会产生新的变量，因此我们尽量不使用库函数，使用一个变量 index 去做线性遍历，这样遍历完成以后就得到转换以后的数值。</p><ul><li>根据示例 1，需要去掉前导空格；</li><li>根据示例 2，需要判断第 1 个字符为 + 和 - 的情况，因此，可以设计一个变量 sign，初始化的时候为 1，如果遇到 - ，将 sign 修正为 -1；</li><li>判断是否是数字，可以使用字符的 ASCII 码数值进行比较，即 0 &lt;= c &lt;= ‘9’；</li><li>根据示例 3 和示例 4 ，在遇到第 1 个不是数字的字符的情况下，就得退出循环；</li><li>根据示例 5，如果转换以后的数字超过了 int 类型的范围，需要截取。这里需要将结果 res 变量设计为 long 类型，注意：由于输入的字符串转换以后也有可能超过 long 类型，因此需要在循环内部就判断是否越界，只要越界就退出循环，这样也可以减少不必要的计算；</li><li>因为涉及下标访问，因此全程需要考虑数组下标是否越界的情况。<br>特别注意：由于题目中说“环境只能保存 32 位整数”，因此这里在每一轮循环之前先要检查乘以 1010 以后是否溢出，具体细节请见编码.</li></ul><p>这里贴一下判断函数：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (res &gt; INT_MAX / <span class="number">10</span> || (res == INT_MAX / <span class="number">10</span> &amp;&amp; (currChar - <span class="string">'0'</span>) &gt; INT_MAX % <span class="number">10</span>))&#123;</span><br><span class="line">    <span class="comment">// INT_MAX % 10 = 7</span></span><br><span class="line">    <span class="keyword">return</span> INT_MAX;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> (res &lt; INT_MIN / <span class="number">10</span> || (res == INT_MIN / <span class="number">10</span> &amp;&amp; (currChar - <span class="string">'0'</span>) &gt; - (INT_MIN % <span class="number">10</span>)))&#123;</span><br><span class="line">    <span class="comment">//  -(INT_MIN % 10) = 8</span></span><br><span class="line">    <span class="keyword">return</span> INT_MIN;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="leetcode-10-正则表达式匹配（困难）"><a href="#leetcode-10-正则表达式匹配（困难）" class="headerlink" title="leetcode 10*. 正则表达式匹配（困难）"></a>leetcode 10*. 正则表达式匹配（困难）</h2><p>先放弃。</p><h2 id="leetcode-12-整数转罗马数字"><a href="#leetcode-12-整数转罗马数字" class="headerlink" title="leetcode 12. 整数转罗马数字"></a>leetcode 12. 整数转罗马数字</h2><p>使用<strong>两个数组</strong>，存储对应的string和代表的数字。虽然再对num进行转换即可。</p><h2 id="leetcode-13-罗马数字转整数"><a href="#leetcode-13-罗马数字转整数" class="headerlink" title="leetcode 13. 罗马数字转整数"></a>leetcode 13. 罗马数字转整数</h2><p>使用<char,int>字典存储罗马数字对应的数字。遍历字符串，<strong>如果当前的字符比右边的字符所代表的数字小，则减去当前字符所代表的数字；否则直接加上</strong>。也就是说，一个字符一个字符的处理，而不要想着按照string的方式来找准数字。</char,int></p><h2 id="leetcode-14-最长公共前缀"><a href="#leetcode-14-最长公共前缀" class="headerlink" title="leetcode 14. 最长公共前缀"></a>leetcode 14. 最长公共前缀</h2><p>我的做法：先求出所有字符串中的最短长度，再遍历字符串来判断。时间复杂度相当于：字符串总数 * 最短字符串长度。</p><p>用python的话很简单诶，复杂度只有O(N)，求出最短的和最长的两个字符串，再比较他们的最长公共前缀即可。</p><h1 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h1><p>我的弱点之一。</p><h2 id="leetcode-5-最长回文子串-1"><a href="#leetcode-5-最长回文子串-1" class="headerlink" title="leetcode 5*. 最长回文子串"></a>leetcode 5*. 最长回文子串</h2><p>前面其实出现过这道题，我使用的是中心扩展法，即遍历对称轴来找最长回文子串。其实这道题也可以用动态规划来做。</p><p>但是，我还是只记住中心扩展法好了，再见！</p><h2 id="leetcode-10-正则表达式匹配"><a href="#leetcode-10-正则表达式匹配" class="headerlink" title="leetcode 10*. 正则表达式匹配"></a>leetcode 10*. 正则表达式匹配</h2><p>同样可以用动态规划做，我还是不会。</p><h2 id="leetcode-53-最大子序和"><a href="#leetcode-53-最大子序和" class="headerlink" title="leetcode 53. 最大子序和"></a>leetcode 53. 最大子序和</h2><blockquote><p>给定一个整数数组 nums ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。</p><p>示例:</p><p>输入: [-2,1,-3,4,-1,2,1,-5,4],<br>输出: 6<br>解释: 连续子数组 [4,-1,2,1] 的和最大，为 6。</p></blockquote><p>首先，来理解最标准的动态规划做法。</p><p>初始化一个dp数组，用来存储当前数组位置所对应的最大和。其中，初始状态为dp[0] = nums[0]。状态转移方程为：dp[i] = max(0, dp[i-1]) + nums[i]。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxSubArray</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; dp(nums.size());</span><br><span class="line">    dp[<span class="number">0</span>] = nums[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">int</span> sum = nums[<span class="number">0</span>];</span><br><span class="line">    <span class="comment">// dp[i] = max(dp[i - 1], 0) + nums[i] 状态转移方程</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; nums.size(); i++)&#123;</span><br><span class="line">        dp[i] = max(dp[i - <span class="number">1</span>], <span class="number">0</span>) + nums[i];</span><br><span class="line">        sum = max(sum, dp[i]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从状态转移方程可以看到，dp[i] = max(dp[i - 1], 0) + nums[i]看出，当前状态的值只取决于前一个状态值，所以我们可以用一个变量来代替dp[i-1]：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">maxSubArray</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> temp = nums[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">int</span> sum = nums[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; nums.size(); i++)&#123;</span><br><span class="line">        temp = max(temp, <span class="number">0</span>) + nums[i];</span><br><span class="line">        sum = max(temp, sum);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> sum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="leetcode-62-不同路径"><a href="#leetcode-62-不同路径" class="headerlink" title="leetcode 62. 不同路径"></a>leetcode 62. 不同路径</h2><blockquote><p>一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为“Start” ）。</p><p>机器人每次只能<strong>向下或者向右</strong>移动一步。机器人试图达到网格的右下角（在下图中标记为“Finish”）。</p><p>问总共有多少条不同的路径？</p></blockquote><p>使用二维数组作为状态方程，表示到达该位置的路径数。其中，初始状态，第一行和第一列的值为1。接着需要设计状态转移方程，其实通过观察也可以发现：dp[i][j] = dp[i-1][j] + dp[i][j-1]。（要努力培养这种思维，即如何利用之前已经获得的值呢？）</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">uniquePaths</span><span class="params">(<span class="keyword">int</span> m, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> dp[<span class="number">100</span>][<span class="number">100</span>];</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m; i++)&#123; <span class="comment">//状态初始化</span></span><br><span class="line">        dp[i][<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)&#123; <span class="comment">//状态初始化</span></span><br><span class="line">        dp[<span class="number">0</span>][i] = <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; m; i++)&#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">1</span>; j &lt; n; j++)&#123;</span><br><span class="line">            dp[i][j] = dp[i<span class="number">-1</span>][j] + dp[i][j<span class="number">-1</span>];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> dp[m<span class="number">-1</span>][n<span class="number">-1</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;从现在开始，按照leetcode专题刷题。此博文持续更新。&lt;/p&gt;
&lt;h1 id=&quot;栈&quot;&gt;&lt;a href=&quot;#栈&quot; class=&quot;headerlink&quot; title=&quot;栈&quot;&gt;&lt;/a&gt;栈&lt;/h1&gt;&lt;h2 id=&quot;leetcode-225-用队列实现栈&quot;&gt;&lt;a href=&quot;#l
      
    
    </summary>
    
    
      <category term="C++" scheme="https://github.com/DesmonDay/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>STL容器基本操作</title>
    <link href="https://github.com/DesmonDay/2020/03/01/STL%E5%AE%B9%E5%99%A8%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    <id>https://github.com/DesmonDay/2020/03/01/STL容器基本操作/</id>
    <published>2020-03-01T03:09:36.000Z</published>
    <updated>2020-03-01T14:55:47.891Z</updated>
    
    <content type="html"><![CDATA[<h2 id="队列-queue"><a href="#队列-queue" class="headerlink" title="队列 queue"></a>队列 queue</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义：</span></span><br><span class="line"><span class="built_in">queue</span>&lt;<span class="keyword">int</span>&gt; q;</span><br><span class="line"><span class="comment">// 基本操作</span></span><br><span class="line">q.push(item);</span><br><span class="line">q.pop();</span><br><span class="line">q.front();</span><br><span class="line">q.back();</span><br><span class="line">q.size();</span><br><span class="line">q.empty();</span><br></pre></td></tr></table></figure><h2 id="Vector"><a href="#Vector" class="headerlink" title="Vector"></a>Vector</h2><p><a href="https://blog.csdn.net/weixin_43930512/article/details/91040416" target="_blank" rel="noopener">https://blog.csdn.net/weixin_43930512/article/details/91040416</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// vector初始化</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; vec;</span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; vec(<span class="number">5</span>); <span class="comment">//声明一个初始大小为5的int向量</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; vec(<span class="number">10</span>, <span class="number">1</span>); <span class="comment">//声明一个初始大小为10且值都是1的向量</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; tmp;</span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; vec(tmp); <span class="comment">// 声明并用tmp向量初始化vec向量</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; tmp(vec.begin(), vec.end()); </span><br><span class="line"></span><br><span class="line"><span class="comment">// vector常见操作</span></span><br><span class="line">vec.push_back(v);</span><br><span class="line">vec.insert(vec.begin(), <span class="number">8</span>); <span class="comment">// 在最前面插入新元素8</span></span><br><span class="line">vec.pop_back();</span><br><span class="line">vec.clear(); <span class="comment">//清空</span></span><br><span class="line">vec.erase(iter); <span class="comment">// 迭代器删除</span></span><br><span class="line">vec.at(<span class="number">0</span>);</span><br><span class="line">vec.front();  <span class="comment">//得到头元素</span></span><br><span class="line">vec.back();</span><br><span class="line">vec.begin(); <span class="comment">//返回头元素的指针</span></span><br><span class="line">vec.end();</span><br><span class="line">vec.size();</span><br><span class="line">vec.max_size(); <span class="comment">// 最大可允许的vector元素数量值</span></span><br><span class="line">vec.capacity(); <span class="comment">// vector实际能容纳的大小</span></span><br><span class="line">vec.empty();</span><br><span class="line"><span class="comment">// swap函数，可用于释放过剩的容量</span></span><br></pre></td></tr></table></figure><h2 id="字符串-string"><a href="#字符串-string" class="headerlink" title="字符串 string"></a>字符串 string</h2><h3 id="C风格"><a href="#C风格" class="headerlink" title="C风格"></a>C风格</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 初始化</span></span><br><span class="line"><span class="keyword">char</span> greeting[] = <span class="string">"Hello"</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; greeting &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="comment">// 常见操作</span></span><br><span class="line"><span class="built_in">strcpy</span>(s1, s2); <span class="comment">// 把s2的内容复制到s1</span></span><br><span class="line"><span class="built_in">strcat</span>(s1, s2);</span><br><span class="line"><span class="built_in">strlen</span>(s1); <span class="comment">//返回s1的长度（不加'\0'）sizeof(s1)返回字符串数组大小</span></span><br><span class="line"><span class="built_in">strcmp</span>(s1, s2); <span class="comment">//如果 s1 和 s2 是相同的，则返回 0；如果 s1&lt;s2 则返回值小于 0；如果 s1&gt;s2 则返回值大于 0。</span></span><br><span class="line"><span class="built_in">strchr</span>(s1, ch); <span class="comment">//返回一个指针，指向字符串s1中字符ch第一次出现的位置</span></span><br><span class="line"><span class="built_in">strstr</span>(s1, s2); <span class="comment">//返回一个指针，指向字符串s1中字符串s2第一次出现的位置</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用strchr(s1, ch)</span></span><br><span class="line"><span class="keyword">char</span> first[<span class="number">20</span>] = <span class="string">"Hello"</span>;</span><br><span class="line"><span class="keyword">char</span> *p = <span class="built_in">strchr</span>(first, <span class="string">'l'</span>);</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; p &lt;&lt; <span class="built_in">endl</span>; <span class="comment">// 输出llo</span></span><br></pre></td></tr></table></figure><h3 id="C-风格"><a href="#C-风格" class="headerlink" title="C++风格"></a>C++风格</h3><p>这里查看详细的用法：<a href="https://blog.csdn.net/weixin_43930512/article/details/91041396" target="_blank" rel="noopener">https://blog.csdn.net/weixin_43930512/article/details/91041396</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// string 初始化</span></span><br><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">str1</span><span class="params">(<span class="string">"abcd"</span>)</span></span>;</span><br><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">str2</span><span class="params">(<span class="number">5</span>, <span class="string">'a'</span>)</span></span>;   <span class="comment">// 5个a</span></span><br><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">str3</span><span class="params">(<span class="number">5</span>, <span class="string">"abc"</span>)</span></span>; <span class="comment">// 5个c</span></span><br><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">str4</span><span class="params">(<span class="string">"abcdefg"</span>,<span class="number">3</span>)</span></span>; <span class="comment">//abc</span></span><br><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">str5</span><span class="params">(<span class="string">"abcdefghijk"</span>, <span class="number">3</span>, <span class="number">5</span>)</span></span>; <span class="comment">//defgh, 从第3个位置取5个字符</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// string常见操作</span></span><br><span class="line">增加：+, append, insert, push_back</span><br><span class="line">删除：clear, pop_back, erase</span><br><span class="line">修改：replace, assign, swap</span><br><span class="line">大小：size, length, capacity, max_size, resize, reserve</span><br><span class="line">判断：empty, compare, &gt;=, &lt;=, &gt;, &lt;</span><br><span class="line">其他：getline, <span class="built_in">string</span>转换, substr, find</span><br><span class="line"></span><br><span class="line"><span class="built_in">string</span> str;</span><br><span class="line">getline(<span class="built_in">cin</span>, str);</span><br><span class="line"></span><br><span class="line"><span class="built_in">string</span> a = <span class="string">"1234"</span>;</span><br><span class="line"><span class="keyword">int</span> b = atoi(a.c_str());</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> a = <span class="number">123</span>;</span><br><span class="line"><span class="built_in">string</span> b = to_string(a);</span><br><span class="line"></span><br><span class="line"><span class="built_in">string</span> str = <span class="string">"abc"</span>;</span><br><span class="line">reverse(str.begin(), str.end()); <span class="comment">// cba</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;队列-queue&quot;&gt;&lt;a href=&quot;#队列-queue&quot; class=&quot;headerlink&quot; title=&quot;队列 queue&quot;&gt;&lt;/a&gt;队列 queue&lt;/h2&gt;&lt;figure class=&quot;highlight c++&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td clas
      
    
    </summary>
    
    
      <category term="C++" scheme="https://github.com/DesmonDay/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>对话系统相关知识与论文</title>
    <link href="https://github.com/DesmonDay/2020/02/29/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86%E4%B8%8E%E8%AE%BA%E6%96%87/"/>
    <id>https://github.com/DesmonDay/2020/02/29/对话系统相关知识与论文/</id>
    <published>2020-02-29T04:20:43.000Z</published>
    <updated>2020-02-29T16:44:28.925Z</updated>
    
    <content type="html"><![CDATA[<p>这里简单记录一下对话系统的各种领域，以及对应的代表论文。参考：<a href="https://zhuanlan.zhihu.com/p/83825070?utm_source=zhihu&amp;utm_medium=social&amp;utm_oi=697119379778727936" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/83825070?utm_source=zhihu&amp;utm_medium=social&amp;utm_oi=697119379778727936</a></p><h2 id="生成式对话"><a href="#生成式对话" class="headerlink" title="生成式对话"></a>生成式对话</h2><p>仍然是一个需要探索的领域，主要原因是，我们尽管可以将对话看作是seq2seq问题，但是这种闲聊对话其实其输出空间是非常大的，因此对于建模成生成问题来看的话，其实比较难。最主要的是缺少一个比较客观的自动评估策略。</p><p>不过，这次参加了一个百度的任务驱动型对话，尽管也算是闲聊，但是其闲聊是带着目的性的，即输出空间有限，因此可以使用生成式对话来解决。生成式对话，我们大多数时候可以参考翻译、文本摘要、语言模型等领域的比较典型的论文。</p><h2 id="检索式对话"><a href="#检索式对话" class="headerlink" title="检索式对话"></a>检索式对话</h2><p><img src="https://pic2.zhimg.com/80/v2-f3bc4f128c2b5595c6def4c70e2458ad_720w.jpg" alt="img"></p><p>检索式对话是工业界比较偏爱的方法，可以用于解决闲聊型对话或FAQ问答型对话问题。FAQ通常是限定域的，比较容易解决，但对于闲聊型对话，即开放域，则需要大量的query-response pairs，即语料需要充足。随后，主要经历两个步骤。</p><h3 id="召回"><a href="#召回" class="headerlink" title="召回"></a>召回</h3><p>首先，在用神经网络深度匹配合适的回复之前，一般要先经过一个“粗筛”的模块召回若干相关的回复，减少q-r匹配的工作量。这个模块一般将用户当前轮的query与语料库里query进行快速匹配（当然你也可以加更多feature提高合适回复的召回率），得到几十上百个候选回复，完成第一轮的匹配。</p><p>因此这就要求q-q粗召模块比较轻量级，且匹配相关度比较好。因此我们可能需要检索模型BM25和一些轻量级文本特征表示模型如BOW。在开源文本匹配方面，可以使用百度的AnyQ开源工具。</p><h3 id="精排"><a href="#精排" class="headerlink" title="精排"></a>精排</h3><p>有了若干候选回复后，我们就需要一个精排模块来挑选合适的回复。这里主要的代表性工作，按照时间先后顺序有：</p><p>Multi-view: Multi-view response selection for human-computer conversation.</p><p>SMN: </p><p>DAM: Multi-turn response selection for chatbots with deep attention matching network</p><p>DGU: <a href="https://link.zhihu.com/?target=https%3A//github.com/baidu/Dialogue/tree/master/DGU" target="_blank" rel="noopener">https://link.zhihu.com/?target=https%3A//github.com/baidu/Dialogue/tree/master/DGU</a></p><h2 id="任务完成型对话"><a href="#任务完成型对话" class="headerlink" title="任务完成型对话"></a>任务完成型对话</h2><p>任务完成型对话的最终目标是完成任务，即需要在每一轮对话都采取合适的决策。这相当于一个多步决策求取reward（对话目标完成情况）最大化的问题，也就是强化学习的问题。（可惜，我没有学RL！！）但是，貌似RL在对话领域里还是有一定地位的，有机会再学习吧。</p><h3 id="对话动作"><a href="#对话动作" class="headerlink" title="对话动作"></a>对话动作</h3><p>用户发出的这个蕴含在自然语言中的命令就称为用户动作user action，我们可以将用户动作看做是用户输入的语义表示。因此，将用户动作从用户的自然语言文本甚至语音信号中解析出来的过程就称为自然语音理解（NLU）或口语理解（SLU）。</p><p>简单的想法是将每个action表示为全局唯一的id，但是action和action之间经常存在很深的耦合关系。比如”预定附近的椰子鸡“与”预定椰子鸡“之间是上下位关系，”预定西二旗附近的椰子鸡“与”预定西三旗附近的椰子鸡“有一个共同的”父action“——预定椰子鸡，我们采取的折中方式为“意图+槽位”，即用意图来表示一个模糊的目标，使用该意图预定义的一系列槽位来限制这个模糊目标，使得目标具体化。</p><p>例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"> 意图: 订餐</span><br><span class="line"> 槽位: &#123;</span><br><span class="line">       地点: 西二旗,</span><br><span class="line">       餐厅名: 椰子鸡</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>完成这个自然语言输入到用户动作这种结构化语义表示(frame)的过程称为自然语言理解（NLU）。实际中，意图和槽位设计可能要复杂的多，比如有的槽位是不可枚举的（比如时间），槽位有冲突，甚至槽位内要嵌套槽位等，这些就要具体情况具体分析了。</p><h3 id="理解用户输入"><a href="#理解用户输入" class="headerlink" title="理解用户输入"></a>理解用户输入</h3><p>通常，我们默认一句话最多包含一个意图，因此可以将NLU任务中的意图识别看成简单的<strong>文本分类任务</strong>。</p><blockquote><p>通常，我们在意图识别前，需要加一级领域分类，避免意图识别模型无法cover其他的领域。</p></blockquote><p>而一个意图往往包含多个槽位，因此我们可以自然地将槽位解析任务建模为<strong>序列标注任务</strong>或者简化为<strong>文本多标签分类任务</strong>。由于意图识别和槽位解析任务息息相关，因此经常有将这两个任务进行join training的模型。</p><p>如今，意图识别与槽位解析的SOTA方法是百度对话团队的DGU，基于ERNIE2.0+处理多轮对话的精巧tricks刷爆了绝大多数对话任务。</p><h3 id="记录对话状态"><a href="#记录对话状态" class="headerlink" title="记录对话状态"></a>记录对话状态</h3><p>为了弄清楚用户的具体意图（把该填的槽填上，该解决的取值冲突解决掉），往往需要记下对话的关键信息，以供<strong>对话策略模块</strong>使用，帮助进行每一轮的决策。这里成为<strong>对话状态(dialogue state)</strong>，完成其更新的过程称为<strong>对话状态追踪</strong>（<em>dialogue state tracking，DST，又称 belief tracking</em>）。</p><p>显然，对话状态的描述需要通过frame的方式来描述。而这种结构化的表示并不是在对话记录中显式存在的，很难通过<strong>大规模数据驱动</strong>的方法来学习记录对话状态的DST模型。</p><ol><li>规则方法：最直接的一种策略是直接将NLU的输出结果（意图、槽位概率分布）离散化，直接取概率最高的意图、槽位作为本轮的用户动作，然后更新到DST模块中，这种方法适合DST的冷启动。显然，直接离散化最高概率的规则方法实现的DST会高度依赖NLU的准确性，且只能处理简单情况，并且在DST更新时会完全忽略已经积累的对话状态。所以，使用统计方法来建模ASR（语音识别）和NLU输出的不确定性是非常有必要的。</li><li>统计方法：构建DST数据集，使用有监督学习操作进行学习。比如利用分类器，或者建模成有监督的序列标注问题。然而，显然DST学习到的函数映射是基于NLU输出的概率分布的，一旦更新了NLU，则DST所熟悉的输入分布发生巨大改变，导致性能大打折扣。因此，自然的想法是让NLU和DST<strong>从pipeline结构变成端到端结构</strong>，即让用户自然语言输入直接连接到对话状态上，因此就可以将DST问题建模成“多轮分类”问题。<strong>DGU</strong>解决DST问题就会根据这种多轮分类的思路来做。</li></ol><h3 id="多轮决策完成对话目标"><a href="#多轮决策完成对话目标" class="headerlink" title="多轮决策完成对话目标"></a>多轮决策完成对话目标</h3><p>接下来，系统可以根据当前轮NLU模块解析出来的用户动作和积累的对话状态来完成相应的“推理”（称为对话策略模块），决定下一步是去澄清意图，say goodbye还是其他什么动作，并且后续NLG模块（<strong>自然语言生成</strong>）也会根据DP模块输出的对话决策（系统动作）来决定回复内容（即结构-&gt;文本）。</p><p>要完成对话目标，有监督学习对话策略是不靠谱的，所以对话策略的学习离不开RL。（又是我不会的，唉）经典代表作：A network-based end-to-end trainable task-oriented dialogue system</p><p><img src="https://pic1.zhimg.com/80/v2-b8dc5e4fb0b68a94e3ab8c753c12c9a4_720w.jpg" alt="img"></p><p>Belief Tracker:</p><p><img src="https://pic3.zhimg.com/80/v2-51969c97cec2df472bb9358f65eb6072_720w.jpg" alt="img"></p><h3 id="NLG"><a href="#NLG" class="headerlink" title="NLG"></a>NLG</h3><p>假如我们的pipeline系统终于可以作出合理决策（action）了。比如用户说，“好的，谢谢”，那么我们的系统经过语义理解、对话状态查询和作出决策，得出了“说再见”的系统动作，于是就要有一个模块将系统动作（即结构化语义表示）来翻译成自然语言输出“不客气哦，下次再见啦～”，完成这个结构-&gt;文本的模块就是自然语言生成（NLG）模块。</p><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ol><li>多轮对话决策和单轮的意图识别的区别是什么？是因为要考虑上下文做一个更优决策吗？</li></ol><blockquote><p>最主要的是多轮对话涉及到意图的继承以及是否为开启一个新的意图</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这里简单记录一下对话系统的各种领域，以及对应的代表论文。参考：&lt;a href=&quot;https://zhuanlan.zhihu.com/p/83825070?utm_source=zhihu&amp;amp;utm_medium=social&amp;amp;utm_oi=697119379
      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/DesmonDay/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>预训练语言模型</title>
    <link href="https://github.com/DesmonDay/2020/02/25/%E9%A2%84%E8%AE%AD%E7%BB%83%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    <id>https://github.com/DesmonDay/2020/02/25/预训练语言模型/</id>
    <published>2020-02-25T07:46:28.000Z</published>
    <updated>2020-02-29T04:02:37.485Z</updated>
    
    <content type="html"><![CDATA[<p>本文记录一些关于预训练语言模型讲解比较好的中文博客，有时间再补充自己的理解。</p><h2 id="预训练方式"><a href="#预训练方式" class="headerlink" title="预训练方式"></a>预训练方式</h2><p>传统的模型预训练手段就是语言模型，比如<a href="https://arxiv.org/abs/1802.05365" target="_blank" rel="noopener">ELMo</a>模型就是以BiLSTM为基础架构、用两个方向的语言模型分别预训练两个方向的LSTM的；后面的OpenAI的GPT、<a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" target="_blank" rel="noopener">GPT-2</a>也是用标准的、单向的语言模型来预训练。</p><p>之后还有更多的预训练方式，比如BERT使用了称为”掩码语言模型(Masked Language Model)”的方式来预训练；而XLNet提出的为更彻底的”Permutation Language Modeling”，称为“乱序语言模型”；还有UNILM模型，直接用当个BERT架构做Seq2seq等等。</p><h2 id="ELMO"><a href="#ELMO" class="headerlink" title="ELMO"></a>ELMO</h2><p><a href="https://zhuanlan.zhihu.com/p/88993965" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/88993965</a> 最大的优点是动态词向量，同一个词，出现在不同的语境中，会有不同的词向量。</p><p>分开训练两个语言模型，即正向语言模型和反向语言模型，将对应字通过正向与反向语言模型得到的vector拼接。</p><p><img src="https://pic3.zhimg.com/80/v2-ac881c5eee80e89020e2e001dfdf9722_720w.jpg" alt="img"></p><h2 id="OpenAI-GPT"><a href="#OpenAI-GPT" class="headerlink" title="OpenAI GPT"></a>OpenAI GPT</h2><p>OpenAI Transformer是一类可迁移到多种NLP任务的，基于Transformer的语言模型。它的基本思想同ULMFiT相同，都是在尽量不改变模型结构的情况下将预训练的语言模型应用到各种任务。不同的是，OpenAI Transformer主张用Transformer结构，而ULMFiT中使用的是基于RNN的语言模型。文中所用的网络结构如下：</p><p><img src="https://pic3.zhimg.com/80/v2-8a4cbead57525234c6a33b23c96d18c6_720w.jpg" alt="img"></p><p>模型的训练过程分为两步：</p><ol><li><p>Unsupervised pre-training ：第一阶段的目标是预训练语言模型，给定tokens的语料，目标函数为最大化似然函数：</p><p><img src="https://www.zhihu.com/equation?tex=L_%7B1%7D%28U%29+%3D+%5Csum_%7Bi%7DlogP%28u_%7Bi%7D%7Cu_%7Bi-k%7D%2C+...%2C+u_%7Bi-1%7D%3B%5Ctheta%29+%5C%5C" alt="[公式]"></p><p>该模型中应用multi-head self-attention，并在之后增加position-wise的前向传播层，最后输出一个分布：</p><p><img src="https://www.zhihu.com/equation?tex=h_%7B0%7D%3DUW_%7Be%7D%2BW_%7Bp%7D+%5C%5C" alt="[公式]"></p><p><img src="https://www.zhihu.com/equation?tex=h_%7Bl%7D+%3D+transformer_block%28h_%7Bl-1%7D%29++%5C%5C" alt="[公式]"></p><p><img src="https://www.zhihu.com/equation?tex=P%28u%29+%3D+softmax%28h_%7Bn%7DW_%7Be%7D%5E%7BT%7D%29+%5C%5C" alt="[公式]"></p></li><li><p>Supervised fine-tuning ：有了预训练的语言模型之后，对于有标签的训练集，给定输入序列$x^1,x^2,…,x^m$和标签$y$，可以通过语言模型得到$h_l^m$，经过输出层后对$y$进行预测：</p><p><img src="https://www.zhihu.com/equation?tex=P%28y%7Cx%5E%7B1%7D%2C...%2Cx%5E%7Bm%7D%29%3Dsoftmax%28h_%7Bl%7D%5E%7Bm%7DW_%7By%7D%29++%5C%5C" alt="[公式]"></p><p>目标函数为：</p><p><img src="https://www.zhihu.com/equation?tex=L_%7B2%7D%28C%29%3D+%5Csum_%7B%28x%2Cy%29%7DlogP%28y%7Cx%5E%7B1%7D...%2Cx%5E%7Bm%7D%29+%5C%5C" alt="[公式]"></p><p>则整个任务的目标函数为：</p><p><img src="https://www.zhihu.com/equation?tex=L_%7B3%7D%28C%29%3D+L_%7B2%7D%28C%29%2B%5Clambda+%2A+L_%7B1%7D%28C%29+%5C%5C" alt="[公式]"></p></li></ol><h2 id="fasttext"><a href="#fasttext" class="headerlink" title="fasttext"></a>fasttext</h2><p>与word2vec的区别：</p><ul><li>相似处：</li></ul><ol><li>图模型结构很像，都是采用embedding向量的形式，得到word的隐向量表达。</li><li>都采用很多相似的优化方法，比如使用Hierarchical softmax优化训练和预测中的打分速度。</li></ol><ul><li>不同处：</li></ul><ol><li>模型的输出层：word2vec的输出层，对应的是每一个term，计算某term的概率最大；而fasttext的输出层对应的是分类的label。不过不管输出层对应的是什么内容，其对应的vector都不会被保留和使用。</li><li>模型的输入层：word2vec的输出层，是 context window 内的term；而fasttext 对应的整个sentence的内容，包括term，也包括 n-gram的内容。</li></ol><ul><li>两者本质的不同，体现在 h-softmax的使用：</li></ul><ol><li>Word2vec的目的是得到词向量，该词向量最终是在输入层得到，输出层对应的 h-softmax也会生成一系列的向量，但最终都被抛弃，不会使用。</li><li>fastText则充分利用了h-softmax的分类功能，遍历分类树的所有叶节点，找到概率最大的label（一个或者N个）</li></ol><h2 id="ULMFiT-源码值得学习"><a href="#ULMFiT-源码值得学习" class="headerlink" title="ULMFiT 源码值得学习"></a>ULMFiT 源码值得学习</h2><p>ULMFiT是一种有效的NLP迁移学习方法，核心思想是通过精调预训练的语言模型完成其他NLP任务。文中所用的语言模型参考了<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1708.02182" target="_blank" rel="noopener">Merity et al. 2017a</a>的<strong>AWD-LSTM</strong>模型，即没有attention或shortcut的<strong>三层LSTM模型</strong>。</p><p>ULMFit的过程分三步：</p><p><img src="https://pic3.zhimg.com/80/v2-757fc68dcb577afacc6d8a7f635c12ee_720w.jpg" alt="img"></p><ol><li><p>General-domain LM pre-train</p><ul><li><p>在Wikitext-103上进行语言模型的预训练。</p></li><li><p>预训练的语料要求：large &amp; capture general properties of language</p></li><li>预训练对小数据集十分有效，之后仅有少量样本就可以使模型泛化。</li></ul></li><li><p>Target task LM fine-tuning</p><ul><li><p>Discriminative fine-tuning: 因为网络中不同层可以捕获不同类型的信息，因此在精调时也应该使用不同的learning rate。作者为每一层赋予一个学习率$\delta^l$，实验后发现，首先通过精调模型的最后一层L确定学习率$\delta^L$，递推地选择上一层学习率进行精调的效果最好，递推公式为$\delta^{l-1}=\delta^l / 2.6$</p></li><li><p>Slanted triangular learning rates(STLR)</p><p>为了针对特定任务选择参数，理想情况下需要在训练开始时让参数快速收敛到一个合适的区域，之后进行精调。为了达到这种效果，作者提出STLR方法，即让LR在训练初期短暂递增，在之后下降。</p><p><img src="https://img-blog.csdnimg.cn/20190424144737769.png" alt="img"></p><p><img src="https://img-blog.csdnimg.cn/20190424144811923.png" alt="img"></p></li><li><p><strong>Target task classifier fine-tuning</strong></p><p>为了完成分类任务的精调，作者在最后一层添加了两个线性block，每个都有batch-norm和dropout，使用ReLU作为中间层激活函数，最后经过softmax输出分类的概率分布。</p></li></ul><p>ULMFit适用领域：分类</p></li></ol><h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><p><a href="https://zhuanlan.zhihu.com/p/46652512" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/46652512</a></p><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p><img src="https://pic1.zhimg.com/80/v2-d942b566bde7c44704b7d03a1b596c0c_720w.jpg" alt="img"></p><p>对比OpenAI GPT(Generative pre-trained transformer)，BERT是双向的Transformer block连接；就像单向rnn和双向rnn的区别，直觉上来讲效果会好一些。</p><p>对比ELMo，虽然都是“双向”，但目标函数其实是不同的。ELMO是实际上是以$P(w_i|w1,…,w_{i-1})$和$P(w_i|w_{i+1},…w_n)$作为目标函数，独立训练出两个representation然后拼接，而BERT则是以$P(w_i|w_1,…,w_{i-1},w_{i+1},…,w_n)$作为目标函数训练LM。</p><h3 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h3><p>embedding是由三种embedding求和而成：</p><p><img src="https://pic2.zhimg.com/80/v2-11505b394299037e999d12997e9d1789_720w.jpg" alt="img"></p><p>其中：</p><ul><li>Token Embeddings是词向量，第一个单词是<strong>CLS标志</strong>，可以用于之后的分类任务</li><li><strong>Segment Embeddings</strong>用来区别两种句子，因为预训练不光做LM还要做以两个句子为输入的分类任务</li><li><strong>Position Embeddings</strong>和之前文章中的Transformer不一样，不是三角函数而是<strong>学习出来</strong>的</li></ul><h3 id="预训练任务1-Masked-LM"><a href="#预训练任务1-Masked-LM" class="headerlink" title="预训练任务1-Masked LM"></a>预训练任务1-Masked LM</h3><p>第一步预训练的目标就是做语言模型，从上文模型结构中看到了这个模型的不同，即bidirectional。</p><blockquote><p>为什么要这样做bidirectional，作者的解释如下：</p><p>如果使用预训练模型处理其他任务，那人们想要的肯定不止某个词左边的信息，而是左右两边的信息。而考虑到这点的模型ELMo只是将left-to-right和right-to-left分别训练拼接起来。直觉上来讲我们其实想要一个deeply bidirectional的模型，但是普通的LM又无法做到，因为在训练时可能会“穿越”。所以作者用了一个加mask的trick。</p></blockquote><p>在训练过程中作者<strong>随机mask 15%的token</strong>，而不是把像cbow一样把每个词都预测一遍。<strong>最终的损失函数只计算被mask掉那个token。</strong>Mask如何做也是有技巧的，如果一直用标记[MASK]代替（在实际预测时是碰不到这个标记的）会影响模型，所以随机mask的时候10%的单词会被替代成其他单词，10%的单词不替换，剩下80%才被替换为[MASK]。要注意的是Masked LM预训练阶段模型是不知道真正被mask的是哪个词，所以<strong>模型每个词都要关注</strong>。</p><h3 id="预训练任务2-Next-Sentence-Prediction"><a href="#预训练任务2-Next-Sentence-Prediction" class="headerlink" title="预训练任务2-Next Sentence Prediction"></a>预训练任务2-Next Sentence Prediction</h3><p>因为涉及到QA和NLI之类的任务，增加了第二个预训练任务，目的是让模型理解两个句子之间的联系。训练的输入是句子A和B，B有一半的几率是A的下一句，输入这两个句子，<strong>模型预测B是不是A的下一句</strong>。</p><blockquote><p><strong>作者特意说了语料的选取很关键，要选用document-level的而不是sentence-level的，这样可以具备抽象连续长序列特征的能力。</strong></p></blockquote><h3 id="Fine-tunning"><a href="#Fine-tunning" class="headerlink" title="Fine-tunning"></a>Fine-tunning</h3><p>分类：对于sequence-level的分类任务，BERT直接取第一个[CLS]token的final hidden state $C\in \mathbb{R}H$，加一层权重$W$后softmax预测label proba:</p><script type="math/tex; mode=display">P = softmax(CW^T)</script><p>与文本中已有的其它字/词相比，这个无明显语义信息的符号会<strong>更“公平”地融合文本中各个字/词的语义信息</strong>。</p><p><img src="https://pic2.zhimg.com/80/v2-b054e303cdafa0ce41ad761d5d0314e1_720w.jpg" alt="img"></p><h3 id="BERT缺点"><a href="#BERT缺点" class="headerlink" title="BERT缺点"></a>BERT缺点</h3><ol><li>预训练阶段因为采取引入[Mask]标记来Mask掉部分单词的训练模式，而Fine-tuning阶段是看不到这种被强行加入的Mask标记的，所以两个阶段存在使用模式不一致的情形，这可能会带来一定的性能损失；</li><li>Bert在第一个预训练阶段，假设句子中多个单词被Mask掉，这些被Mask掉的单词之间没有任何关系，是条件独立的，而有时候这些单词之间是有关系的，XLNet则考虑了这种关系。</li></ol><h2 id="XLNet"><a href="#XLNet" class="headerlink" title="XLNet"></a>XLNet</h2><p>参考：<a href="https://zhuanlan.zhihu.com/p/70257427" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/70257427</a></p><p>针对GPT/ELMO进行对比：使用到的是自回归语言模型（<strong>Autoregressive LM</strong>），即通常所讲的根据上文内容预测下一个可能跟随的单词。GPT 就是典型的自回归语言模型。ELMO尽管看上去利用了上文，也利用了下文，但是本质上仍然是自回归LM。</p><p>针对BERT进行对比：Bert通过在输入X中随机Mask掉一部分单词，然后预训练过程的主要任务之一是根据上下文单词来预测这些被Mask掉的单词，类似这种预训练模式被称为<strong>DAE LM</strong>。其缺点是在输入侧引入[Mask]标记，导致预训练阶段和Fine-tuning阶段不一致的问题，因为Fine-tuning阶段是看不到[Mask]标记的。</p><p>XLNet的出发点就是：<strong>能否融合自回归LM和DAE LM两者的优点</strong>。就是说如果站在自回归LM的角度，如何引入和双向语言模型等价的效果；如果站在DAE LM的角度看，它本身是融入双向语言模型的，如何抛掉表面的那个[Mask]标记，让预训练和Fine-tuning保持一致。</p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>XLNet仍然遵循两阶段的过程，第一个阶段是语言模型预训练阶段；第二阶段是任务数据Fine-tuning阶段。</p><p>它主要希望改动第一个阶段，就是说不像Bert那种带Mask符号的Denoising-autoencoder的模式，而是采用自回归LM的模式。也就是，<strong>能够比较充分地在自回归语言模型中，引入双向语言模型</strong>。即看上去输入句子$X$仍然是自左向右的输入，看到$T_i$单词的上文context_before，来预测$T_i$这个单词；但又希望在context_before里，不仅仅看到上文单词，也能看到$T_i$单词后面的下文context_after中的下文单词。</p><h3 id="Permutation-Language-Model"><a href="#Permutation-Language-Model" class="headerlink" title="Permutation Language Model"></a>Permutation Language Model</h3><p>这个部分是XLNet的主要理论创新。</p><p>如何解决的思路：XLNet在预训练阶段，引入Permutation Language Model的训练目标。即，比如包含单词$T_i$的当前输入句子$X$，由顺序的几个单词构成，比如$x_1,x_2,x_3,x_4$四个单词顺序构成。我们假设，要预测的单词$T_i$是$x_3$，位置在position 3，我们希望在context_before中也能看到position 4的单词$x_4$。因此可以这么做：<strong>假设我们固定住$x_3$所在位置，然后随机排列组合句子中的4个单词，在随机排列组合后的各种可能里再选择一部分作为模型预训练的输入$X$。</strong> 比如，随机排列组合后，抽取出$x_4,x_2,x_3,x_1$这一个排列组合作为模型的输入$X$。这就是XLNet的基本思想。</p><p>难点是如何实现上述思想：首先要强调，尽管上面讲的是把句子X的单词排列组合后，再随机抽取例子作为输入，但是，实际上你是不能这么做的，因为<strong>Fine-tuning阶段你不可能也去排列组合原始输入</strong>。所以，必须让预训练阶段的输入部分，看上去仍然是x1,x2,x3,x4这个输入顺序，但是可以在Transformer部分做些工作，来达成希望的目标。其实，就是利用了Transformer内部的<strong>Attention mask</strong>掩码。通过Attention Mask，把其它没有被选到的单词Mask掉，不让它们在预测单词Ti的时候发生作用，如此而已。一个简单图示：</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-4cbf76012c776bc9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>假设随机的一种生成顺序为“\<s>-&gt;迎-&gt;京-&gt;你-&gt;欢-&gt;北-&gt;\<e>“，那么可以使用上图第二个子图中的方式去Mask掉Attention矩阵。这个训练方案源于纯Attention的模型本质上是一个无序的模型，它里边的词序实际上是通过position embedding加上去的。因此实际上，我们输入的不仅只有token本身，还包括token所在位置id。</e></s></p><p><img src="https://pic1.zhimg.com/80/v2-2bb1a60af4fe2fa751647fdce48e337c_720w.jpg" alt="img"></p><h3 id="评论"><a href="#评论" class="headerlink" title="评论"></a>评论</h3><blockquote><p>XLNet开启了自回归语言模型如何引入下文的一个思路，相信对于后续工作会有启发。当然，XLNet不仅仅做了这些，它还引入了其它的因素，也算是一个当前有效技术的集成体。</p><p>感觉XLNet就是Bert、GPT 2.0和Transformer XL的综合体变身，首先，它通过PLM预训练目标，吸收了Bert的双向语言模型；然后，GPT2.0的核心其实是更多更高质量的预训练数据，这个明显也被XLNet吸收进来了；再然后，Transformer XL的主要思想也被吸收进来，它的主要目标是解决Transformer对于长文档NLP应用不够友好的问题。</p></blockquote><h3 id="与BERT的预训练过程的异同"><a href="#与BERT的预训练过程的异同" class="headerlink" title="与BERT的预训练过程的异同"></a>与BERT的预训练过程的异同</h3><blockquote><p> 区别主要在于：Bert是直接在<strong>输入端显示</strong>地通过引入Mask标记，在输入侧隐藏掉一部分单词，让这些单词在预测的时候不发挥作用，要求利用上下文中其它单词去预测某个被Mask掉的单词；而XLNet则抛弃掉输入侧的Mask标记，通过Attention Mask机制，在<strong>Transformer内部</strong>随机Mask掉一部分单词（这个被Mask掉的单词比例跟当前单词在句子中的位置有关系，位置越靠前，被Mask掉的比例越高，位置越靠后，被Mask掉的比例越低），让这些被Mask掉的单词在预测某个单词的时候不发生作用。本质上两者并没什么太大的不同，只是Mask的位置，<strong>Bert更表面化一些，XLNet则把这个过程隐藏在了Transformer内部而已</strong>。这样，就可以抛掉表面的[Mask]标记，解决它所说的<strong>预训练里带有[Mask]标记导致的和Fine-tuning过程不一致的问题</strong>。</p><p>XLNet这种改造，维持了表面看上去的自回归语言模型的从左向右的模式，这个Bert做不到，这个有明显的好处，就是对于<strong>生成类</strong>的任务，能够在维持表面从左向右的生成过程前提下，模型里隐含了上下文的信息。所以看上去，XLNet貌似应该对于生成类型的NLP任务，会比Bert有明显优势。另外，因为XLNet还引入了Transformer XL的机制，所以对于长文档输入类型的NLP任务，也会比Bert有明显优势。</p></blockquote><h3 id="XLNet起作用的三个因素"><a href="#XLNet起作用的三个因素" class="headerlink" title="XLNet起作用的三个因素"></a>XLNet起作用的三个因素</h3><ol><li>新的预训练目标：Permutation Language Model。这个可以理解为在自回归LM模式下，如何采取具体手段，来融入双向语言模型。这个是XLNet在模型角度比较大的贡献，确实也打开了NLP中两阶段模式潮流的一个新思路。</li><li>引入了Transformer-XL的主要思路：相对位置编码以及分段RNN机制。实践已经证明这两点对于长文档任务是很有帮助的；</li><li>加大增加了预训练阶段使用的数据规模；</li></ol><p>接下来摘抄一下张俊林老师的结论：</p><blockquote><p>XLNet综合而言，效果是优于Bert的，尤其是在长文档类型任务，效果提升明显。Bert生成做不好根本还是预训练的mask模式同时看到上文和下文，decoder没法达到这一点，原因应该还是在mask这种模式上.</p></blockquote><h2 id="ALBERT"><a href="#ALBERT" class="headerlink" title="ALBERT"></a>ALBERT</h2><p>参考：<a href="https://blog.csdn.net/u012526436/article/details/101924049" target="_blank" rel="noopener">https://blog.csdn.net/u012526436/article/details/101924049</a></p><h3 id="相对于BERT的改进"><a href="#相对于BERT的改进" class="headerlink" title="相对于BERT的改进"></a>相对于BERT的改进</h3><p>ALBERT也是采用和BERT一样的Transformer的encoder结果，激活函数使用的也是GELU。我们规定几个参数，词的embedding我们设置为E，encoder的层数我们设置为L，hidden size即encoder的输出值的维度我们设置为H，前馈神经网络的节点数设置为4H，attention的head个数设置为H/64。</p><p>在ALBERT中主要有三个改进方向。</p><h4 id="对Embedding因式分解"><a href="#对Embedding因式分解" class="headerlink" title="对Embedding因式分解"></a>对Embedding因式分解</h4><p>在BERT中，词embedding与encoder输出的embedding维度是一样的都是768。但是ALBERT认为，词级别的embedding是没有上下文依赖的表述，而隐藏层的输出值不仅包括了词本生的意思还包括一些上下文信息，<strong>理论上来说隐藏层的表述包含的信息应该更多一些，因此应该让H&gt;&gt;E</strong>，所以ALBERT的词向量的维度是小于encoder输出值维度的。</p><p>在NLP任务中，通常词典都会很大，embedding matrix的大小是E×V，如果和BERT一样让H=E，那么embedding matrix的参数量会很大，并且反向传播的过程中，更新的内容也比较稀疏。</p><p>结合上述说的两个点，ALBERT采用了一种因式分解的方法来降低参数量。首先把one-hot向量映射到一个低维度的空间，大小为E，然后再映射到一个高维度的空间，说白了就是<strong>先经过一个维度很低的embedding matrix，然后再经过一个高维度matrix把维度变到隐藏层的空间内</strong>，从而把参数量从O(V×H)降低到了O(V×E+E×H)，当E&lt;&lt;H时参数量减少的很明显。</p><h4 id="跨层的参数共享（Cross-layer-parameters）"><a href="#跨层的参数共享（Cross-layer-parameters）" class="headerlink" title="跨层的参数共享（Cross-layer parameters）"></a>跨层的参数共享（Cross-layer parameters）</h4><p>在ALBERT还提出了一种参数共享的方法，Transformer中共享参数有多种方案，只共享全连接层，只共享attention层，ALBERT结合了上述两种方案，全连接层与attention层都进行参数共享，也就是说<strong>共享encoder内的所有参数</strong>，同样量级下的Transformer采用该方案后实际上效果是有下降的，但是参数量减少了很多，训练速度也提升了很多。</p><h4 id="句间连贯（Inter-sentence-coherence-loss）"><a href="#句间连贯（Inter-sentence-coherence-loss）" class="headerlink" title="句间连贯（Inter-sentence coherence loss）"></a>句间连贯（Inter-sentence coherence loss）</h4><p>BERT的NSP任务实际上是一个<strong>二分类</strong>，训练数据的正样本是通过采样同一个文档中的两个连续的句子，而负样本是通过采用两个不同的文档的句子。该任务主要是希望能提高下游任务的效果，例如NLI自然语言推理任务。但是后续的研究发现该任务效果并不好，主要原因是因为其任务过于简单。NSP其实包含了两个子任务，主题预测与关系一致性预测，但是主题预测相比于关系一致性预测简单太多了，并且在MLM任务中其实也有类型的效果。</p><p>在ALBERT中，为了只保留一致性任务去除主题识别的影响，提出了一个新的任务 sentence-order prediction（SOP）。SOP因为是在同一个文档中选的，其只关注句子的顺序并没有主题方面的影响。并且SOP能解决NSP的任务，但是NSP并不能解决SOP的任务，该任务的添加给最终的结果提升了一个点。</p><h4 id="移除dropout"><a href="#移除dropout" class="headerlink" title="移除dropout"></a>移除dropout</h4><p>ALBERT在训练了100w步之后，模型依旧没有过拟合，于是乎作者果断移除了dropout，没想到对下游任务的效果竟然有一定的提升。这也是业界第一次发现dropout对大规模的预训练模型会造成负面影响。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>ALBERT实际上是通过参数共享的方式降低了内存，预测阶段还是需要和BERT一样的时间，如果采用了xxlarge版本的ALBERT，那实际上预测速度会更慢。</p><p>ALBERT解决的是<strong>训练时候的速度提升</strong>，如果要真的做到总体运算量的减少，的确是一个复杂且艰巨的任务，毕竟鱼与熊掌不可兼得。不过话说回来，ALBERT也更加适合采用feature base或者模型蒸馏等方式来提升最终效果。</p><h2 id="RoBERTa"><a href="#RoBERTa" class="headerlink" title="RoBERTa"></a>RoBERTa</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文记录一些关于预训练语言模型讲解比较好的中文博客，有时间再补充自己的理解。&lt;/p&gt;
&lt;h2 id=&quot;预训练方式&quot;&gt;&lt;a href=&quot;#预训练方式&quot; class=&quot;headerlink&quot; title=&quot;预训练方式&quot;&gt;&lt;/a&gt;预训练方式&lt;/h2&gt;&lt;p&gt;传统的模型预训练手段就是
      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/DesmonDay/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>一些重要知识</title>
    <link href="https://github.com/DesmonDay/2020/02/22/%E4%B8%80%E4%BA%9B%E9%87%8D%E8%A6%81%E7%9F%A5%E8%AF%86/"/>
    <id>https://github.com/DesmonDay/2020/02/22/一些重要知识/</id>
    <published>2020-02-22T15:21:21.000Z</published>
    <updated>2020-02-29T04:00:50.986Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1x1卷积核"><a href="#1x1卷积核" class="headerlink" title="1x1卷积核"></a>1x1卷积核</h2><p><a href="https://zhuanlan.zhihu.com/p/40050371" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/40050371</a></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-f48e097dd1a9f711.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-1fc976e41c8264ea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="1x1卷积核作用"><a href="#1x1卷积核作用" class="headerlink" title="1x1卷积核作用"></a><strong>1x1卷积核作用</strong></h3><ul><li><p><strong>降维/升维</strong>：改变通道大小</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-c1ab7da0836e4108.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p></li><li><p><strong>增加非线性</strong></p><p>1*1卷积核，可以在保持feature map尺度不变的（即不损失分辨率）的前提下大幅增加非线性特性（利用后接的非线性激活函数），把网络做的很deep。</p><p>备注：一个filter对应卷积后得到一个feature map，不同的filter(不同的weight和bias)，卷积以后得到不同的feature map，提取不同的特征，得到对应的specialized neuron</p></li><li><p><strong>跨通道信息交互（channal 的变换）</strong></p><p>例子：使用1x1卷积核，实现降维和升维的操作其实就是channel间信息的线性组合变化，3x3，64channels的卷积核后面添加一个1x1，28channels的卷积核，就变成了3x3，28channels的卷积核，原来的64个channels就可以理解为跨通道线性组合变成了28channels，这就是通道间的信息交互[7]。</p></li></ul><h2 id="beam-search"><a href="#beam-search" class="headerlink" title="beam search"></a>beam search</h2><p>对于seq2seq，我们实际上是在建模</p><p>$p(Y|X)=p(Y_1|X)p(Y_2|X,Y_1)p(Y_3|X,Y_1,Y_2)p(Y_4|X,Y_1,Y_2,Y_3)…$</p><p>在解码时，我们希望能找到最大概率的$Y$。</p><ul><li>贪心：如果我们在第一步p(Y_1|X)时，直接选择最大概率的（期望目标是$P$），然后代入第二步$p(Y_2|X,Y_1)$，再次选择最大概率$Y_2$，依次类推，每一步选择当前最大概率的输出，则称为<strong>贪心搜索</strong>，是一种最低成本的解码方案。但这种方案得到的结果未必是最优的，而假如第一步我们选择了概率不是最大的$Y_1$，代入第二步时也许会得到非常大的条件概率$p(Y_2|X,Y_1)$，从而两者乘积会超过逐位取最大的算法。</li><li>beam search: 如果要枚举所有路径最优，其计算量是无法接受的。因此seq2seq使用了一种折中的方法：beam search。该方法的思想是：<strong>在每步计算时，只保留当前最优的topk个候选结果</strong>。比如取$topk=3$，则第一步时，只保留使得$p(Y_1|X)$最大的前三个$Y_1$，然后分别代入$p(Y_2|X,Y_1)$，然后各取前三个$Y_2$，这样就有了9个组合。这时候计算每一种组合的总概率，仍然只保留前三个，依次递归，直到出现第一个\<end>。普通贪心搜索相当于$topk=1$。</end></li></ul><h2 id="SVD分解原理"><a href="#SVD分解原理" class="headerlink" title="SVD分解原理"></a>SVD分解原理</h2><p>参考资料：<a href="https://www.cnblogs.com/pinard/p/6251584.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6251584.html</a></p><h2 id="文本主题模型-潜在语义索引（LSI）"><a href="#文本主题模型-潜在语义索引（LSI）" class="headerlink" title="文本主题模型-潜在语义索引（LSI）"></a>文本主题模型-潜在语义索引（LSI）</h2><p>参考资料：<a href="https://www.cnblogs.com/pinard/p/6805861.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6805861.html</a></p><h2 id="非负矩阵分解"><a href="#非负矩阵分解" class="headerlink" title="非负矩阵分解"></a>非负矩阵分解</h2><p><a href="https://www.cnblogs.com/pinard/p/6812011.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6812011.html</a></p><h2 id="LDA模型"><a href="#LDA模型" class="headerlink" title="LDA模型"></a>LDA模型</h2><p><a href="https://www.cnblogs.com/pinard/p/6831308.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6831308.html</a></p><h2 id="RNN-LSTM"><a href="#RNN-LSTM" class="headerlink" title="RNN, LSTM"></a>RNN, LSTM</h2><p><a href="https://blog.csdn.net/zhaojc1995/article/details/80572098" target="_blank" rel="noopener">https://blog.csdn.net/zhaojc1995/article/details/80572098</a></p><h2 id="TextCNN"><a href="#TextCNN" class="headerlink" title="TextCNN"></a>TextCNN</h2><p><a href="https://www.cnblogs.com/bymo/p/9675654.html" target="_blank" rel="noopener">https://www.cnblogs.com/bymo/p/9675654.html</a></p><p><img src="https://img2018.cnblogs.com/blog/1182656/201809/1182656-20180919171920103-1233770993.png" alt="img"></p><p><img src="https://img2018.cnblogs.com/blog/1182656/201809/1182656-20180920144901341-139977476.png" alt="img"></p><p>通道：</p><ul><li>图像中可以利用（R,G,B）作为不同channel;</li><li>文本的输入的channel通常是<strong>不同方式的embedding方式</strong>（比如 word2vec或Glove），实践中也有利用静态词向量和fine-tunning词向量作为不同channel的做法</li></ul><p>一维卷积（conv-1d）：</p><ul><li>图像是二维数据；</li><li>文本是一维数据，因此<strong>在TextCNN卷积用的是一维卷积</strong>（在<strong>word-level</strong>上是一维卷积；虽然文本经过词向量表达后是二维数据，但是在embedding-level上的二维卷积没有意义）一维卷积带来的问题是需要<strong>通过设计不同 kernel_size 的 filter 获取不同宽度的视野</strong>。</li></ul><p>Pooling层：</p><p><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1404.2188.pdf" target="_blank" rel="noopener">A Convolutional Neural Network for Modelling Sentences</a> 中将pooling层改成了<strong>(dynamic) k-max pooling</strong>，pooling阶段保留 k 个最大的信息，保留了全局的序列信息。</p><h2 id="Word2vec"><a href="#Word2vec" class="headerlink" title="Word2vec"></a>Word2vec</h2><p><a href="https://www.cnblogs.com/pinard/p/7160330.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/7160330.html</a></p><p><a href="https://www.jianshu.com/p/471d9bfbd72f" target="_blank" rel="noopener">https://www.jianshu.com/p/471d9bfbd72f</a> </p><p><a href="https://zhuanlan.zhihu.com/p/35500923" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/35500923</a> (susht师姐讲得更好)</p><h2 id="其他资料"><a href="#其他资料" class="headerlink" title="其他资料"></a>其他资料</h2><ol><li><p><a href="https://spaces.ac.cn/archives/5861" target="_blank" rel="noopener">玩转Keras之seq2seq自动生成标题</a> - 特别的一点，这个任务具有先验知识：<strong>标题中的大部分字词都在文章中出现过</strong>（注：仅仅是出现过，并不一定是连续出现，更不能说标题包含在文章中，不然就成为一个普通的序列标注问题了）。因此，可以将文章中的词集作为一个先验分布，加到解码过程的分类模型中，使得模型在解码输出时更倾向选用文章中已有的字词。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-6ebc35177d0a0a14.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p></li></ol><ol><li><a href="https://zhuanlan.zhihu.com/p/35586145" target="_blank" rel="noopener">文本多分类踩过的坑</a>: 又是susht师姐的文章。很详细的记录了一些文本多分类时的问题，值得深入思考。</li><li>贝叶斯估计、极大似然估计、最大后验估计：</li></ol><p><img src="https://mmbiz.qpic.cn/mmbiz_jpg/UnIETViboUMffyV0Q2kDawtBdqGtVezicjd2UPZ7Y3gaEZ04dV4VeERXgdUImvXztPxB0YuCwVs6bMIomiaIzhUbA/640?wx_fmt=jpeg&amp;tp=webp&amp;wxfrom=5&amp;wx_lazy=1&amp;wx_co=1" alt="img"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1x1卷积核&quot;&gt;&lt;a href=&quot;#1x1卷积核&quot; class=&quot;headerlink&quot; title=&quot;1x1卷积核&quot;&gt;&lt;/a&gt;1x1卷积核&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/40050371&quot; target
      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/DesmonDay/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>Attention机制-NLP领域小结</title>
    <link href="https://github.com/DesmonDay/2020/02/20/Attention%E6%9C%BA%E5%88%B6-NLP%E9%A2%86%E5%9F%9F%E5%B0%8F%E7%BB%93/"/>
    <id>https://github.com/DesmonDay/2020/02/20/Attention机制-NLP领域小结/</id>
    <published>2020-02-20T15:42:25.000Z</published>
    <updated>2020-02-23T16:09:27.872Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Attention用于NLP的小结"><a href="#Attention用于NLP的小结" class="headerlink" title="Attention用于NLP的小结"></a>Attention用于NLP的小结</h1><p>基本转载susht师姐的知乎博文，真是优秀的师姐啊，向她学习！</p><p><a href="https://zhuanlan.zhihu.com/p/35739040" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/35739040</a></p><p><img src="https://pic1.zhimg.com/80/v2-ef2e241114ab58a033e31f76b598a898_hd.jpg" alt="img"></p><p>定义：参考<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Attention is All You Need</a>中的说法，假设当前时刻t下，我们有一个query向量，和一段key向量，这里query可以理解为一个包含比较多信息的全局向量，我们利用这个query对所有key向量进行加权求和，学习到一个更合适的新向量去做分类或者预测等任务。</p><p>假设$q_t$是时刻$t$下的query，$K$是key，$k_s$是其中一个key向量，$v$是value矩阵。我们先对$q_t$$和每个key进行相似度计算得到一个非归一化的score分数：</p><script type="math/tex; mode=display">s(q_t,k_s)=\frac{<q_t,k_s>}{\sqrt{d_k}}</script><p>这里用到了点乘，分母则是为了调整内积结果，使得内积不那么大，避免softmax结果过于专一。然后对score进行softmax归一化，作为attention概率权重：</p><script type="math/tex; mode=display">a(q_t,k_s) = \frac{exp(s(q_t,k_s))}{\sum_{i=1}^N exp(s(q_t,k_i))}​</script><p>最后我们对每个位置key所对应的权重和value进行加权求和，得到最终的输出向量：</p><script type="math/tex; mode=display">Attention(q_t,K,V)=\sum_{s=1}^ma(q_t,k_s)v_s</script><p>对应到具体任务上可以理解更加清晰：</p><ol><li>在机器翻译任务中，query可以定义成decoder中某一步的hidden state，key是encoder中每一步的hidden state，我们用每一个query对所有key都做一个对齐，decoder每一步都会得到一个不一样的对齐向量。</li><li>在文本分类任务中，query可以定义成一个可学的随机变量（参数），key就是输入文本每一步的hidden state，通过加权求和得到句向量，再进行分类。Attention机制用在文本分类中，我们可以看做是对句子进行加权，提高重要词语的注意力，减小其它词语的关注度。</li></ol><h2 id="根据Attention的计算区域"><a href="#根据Attention的计算区域" class="headerlink" title="根据Attention的计算区域"></a>根据Attention的计算区域</h2><ol><li>Soft Attention: 这是比较常见的Attention方式，对所有key求权重概率，每个key都有一个对应的权重，是一种全局的计算方式（也可以叫Global Attention）</li><li>Hard Attention: 这种方式是直接精准定位到某个key，其余key就都不管了，相当于这个key的概率是1，其余key的概率全部是0。因此这种对齐方式要求很高，要求一步到位，如果没有正确对齐，会带来很大的影响。另一方面，因为不可导，一般需要用强化学习的方法进行训练。（或者使用gumbel softmax之类的）</li><li><strong>Local</strong> Attention，这种方式其实是以上两种方式的一个折中，对一个窗口区域进行计算。先用Hard方式定位到某个地方，以这个点为中心可以得到一个窗口区域，在这个小区域内用Soft方式来算Attention。</li></ol><h2 id="根据Attention的所用信息"><a href="#根据Attention的所用信息" class="headerlink" title="根据Attention的所用信息"></a>根据Attention的所用信息</h2><p>假设我们要对一段原文计算Attention，这里原文指的是我们要做attention的文本，那么所用信息包括内部信息和外部信息，内部信息指的是原文本身的信息，而外部信息指的是除原文以外的额外信息：</p><ol><li><strong>General</strong> Attention：这种方式利用到了外部信息，常用于需要构建两段文本关系的任务，query一般包含了额外信息，根据外部query对原文进行对齐。</li><li><strong>Local</strong> Attention：这种方式只使用内部信息，key和value以及query只和输入原文有关，在<strong>self attention</strong>中，key=value=query。既然没有外部信息，那么在原文中的每个词可以跟该句子中的所有词进行Attention计算，相当于寻找原文内部的关系。</li></ol><h2 id="根据Attention的结构层次"><a href="#根据Attention的结构层次" class="headerlink" title="根据Attention的结构层次"></a>根据Attention的结构层次</h2><p>分为单层attention，多层attention和多头attention:</p><ol><li>单层Attention，这是比较普遍的做法，用一个query对一段原文进行一次attention。</li><li>多层Attention，一般用于<strong>文本具有层次关系(比如包括多个句子的文档)</strong>的模型，假设我们把一个document划分成多个句子，在第一层，我们分别对每个句子使用attention计算出一个句向量（也就是单层attention）；在第二层，我们对所有句向量再做attention计算出一个文档向量（也是一个单层attention），最后再用这个文档向量去做任务。</li><li>多头Attention，这是Attention is All You Need中提到的multi-head attention，用到了多个query对一段原文进行了多次attention，每个query都关注到原文的不同部分，相当于<strong>重复做多次单层attention</strong>。</li></ol><h2 id="从模型方面看"><a href="#从模型方面看" class="headerlink" title="从模型方面看"></a>从模型方面看</h2><p>Attention一般用在CNN和LSTM上，也可以直接进行纯Attention计算:</p><ol><li><p>CNN+Attention: </p><p>CNN的卷积操作可以提取重要特征，也算是Attention的思想，但是CNN的卷积感受视野是局部的，需要通过<strong>叠加多层卷积区</strong>去扩大视野。另外，Max Pooling直接提取数值最大的特征，也像是<strong>hard attention</strong>的思想，直接选中某个特征。</p><p>可以应用这几个方面：</p><ul><li>在卷积操作前做attention，比如Attention-Based BCNN-1，这个任务是文本蕴含任务需要处理两段文本，同时对两段输入的序列向量进行attention，计算出特征向量，再拼接到原始向量中，作为卷积层的输入。</li><li>在卷积操作后做attention，比如Attention-Based BCNN-2，对两段文本的卷积层的输出做attention，作为pooling层的输入。</li><li>在pooling层做attention，代替max pooling。比如<strong>Attention pooling</strong>，首先我们用LSTM学到一个比较好的句向量，作为query，然后用CNN先学习到一个特征矩阵作为key，再用query对key产生权重，进行attention，得到最后的句向量。</li></ul></li><li><p>LSTM+Attention</p><p>LSTM内部有Gate机制，其中input gate选择哪些当前信息进行输入，forget gate选择遗忘哪些过去信息，这算是一定程度的Attention了，而且号称可以解决长期依赖问题，实际上LSTM需要一步一步去捕捉序列信息，在长文本上的表现是会随着step增加而慢慢衰减，难以保留全部的有用信息。</p><p>LSTM通常需要得到一个向量，再去做任务，常用的方式有：</p><ul><li>直接使用最后的hidden state(可能会损失一定的前文信息，难以表达全文)</li><li>对所有step下的hidden state进行等权平均（对所有step一视同仁）</li><li>Attention机制，对所有step的hidden state进行加权，把注意力集中到整段文本中比较重要的hidden state信息。性能比前面两种要好一点，而方便可视化观察哪些step是重要的，但是要小心过拟合，而且也增加了计算量。</li></ul></li><li><p><strong>纯Attention</strong>: Attention is all you need，没有用到CNN/RNN，乍一听也是一股清流了，但是仔细一看，本质上还是一堆向量去计算attention。  </p></li></ol><h2 id="相似度计算方式"><a href="#相似度计算方式" class="headerlink" title="相似度计算方式"></a>相似度计算方式</h2><p>做attention时，我们需要计算query和某个key的相似度（分数），常用的方法有：</p><ol><li>点乘：最简单的方法，$s(q,k) = q^Tk$</li><li>矩阵相乘：$s(q,k)=q^TWk$</li><li>cos相似度：$s(q,k)=\frac{q^Tk}{||q||\cdot||k||}$</li><li>串联方式：把q,k拼接起来，$s(q,k)=W[q;k]$</li><li>用多层感知机：$s(q,k)=v^T_atanh(Wq+Uk)$</li></ol><h2 id="Attention适合的任务"><a href="#Attention适合的任务" class="headerlink" title="Attention适合的任务"></a>Attention适合的任务</h2><ol><li>长文本任务：document级别，因为长文本本身所携带的信息量比较大，可能会带来信息过载问题，很多任务可能只需要用到其中一些<strong>关键信息</strong>（比如文本分类），所以Attention机制用在这里正适合capture这些关键信息。</li><li><strong>涉及到两段的相关文本</strong>，可能会需要对两段内容进行对齐，找到这两段文本之间的一些相关关系。比如机器翻译，将英文翻译成中文，英文和中文明显是有对齐关系的，Attention机制可以找出，在翻译到某个中文字的时候，需要对齐到哪个英文单词。又比如阅读理解，给出问题和文章，其实问题中也可以对齐到文章相关的描述，比如“什么时候”可以对齐到文章中相关的时间部分。</li><li>任务很大部分取决于<strong>某些特征</strong>。我举个例子，比如在AI+法律领域，根据初步判决文书来预测所触犯的法律条款，在文书中可能会有一些罪名判定，而这种特征对任务是非常重要的，所以用Attention来capture到这种特征就比较有用。（CNN也可以）</li></ol><p>常见的task:</p><ol><li>机器翻译：encoder用于对原文建模，decoder用于生成译文，attention用于连接原文和译文，在每一步翻译的时候关注不同的原文信息。</li><li>摘要生成：encoder用于对原文建模，decoder用于生成新文本，从形式上和机器翻译都是seq2seq任务，但是从任务特点上看，机器翻译可以具体对齐到某几个词，但这里是由长文本生成短文本，decoder可能需要capture到encoder更多的内容，进行总结。（对话系统也可）</li><li>图文互搜：encoder对图片建模，decoder生成相关文本，在decoder生成每个词的时候，用attention机制来关注图片的不同部分。</li><li>文本蕴含：判断前提和假设是否相关，attention机制用来对前提和假设进行对齐。</li><li>阅读理解：可以对文本进行self attention，也可以对文章和问题进行对齐。</li><li>文本分类：一般是对一段句子进行attention，得到一个句向量去做分类。</li><li>序列标注：<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1712.01586" target="_blank" rel="noopener">Deep Semantic Role Labeling with Self-Attention</a>，这篇论文在softmax前用到了self attention，学习句子结构信息，和利用到标签依赖关系的CRF进行pk。</li><li>关系抽取：也可以用到self attention</li></ol><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p>我这次参加了百度常规赛：知识驱动对话，也是利用了Attention作为decoder的一部分，比赛效果还可以。之后再写一篇文章作为总结。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Attention用于NLP的小结&quot;&gt;&lt;a href=&quot;#Attention用于NLP的小结&quot; class=&quot;headerlink&quot; title=&quot;Attention用于NLP的小结&quot;&gt;&lt;/a&gt;Attention用于NLP的小结&lt;/h1&gt;&lt;p&gt;基本转载susht师
      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/DesmonDay/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>特征工程</title>
    <link href="https://github.com/DesmonDay/2020/02/19/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    <id>https://github.com/DesmonDay/2020/02/19/特征工程/</id>
    <published>2020-02-19T13:20:53.000Z</published>
    <updated>2020-02-19T13:55:09.773Z</updated>
    
    <content type="html"><![CDATA[<p>本笔记参考二水马的rebirth。</p><h1 id="1-特征选择"><a href="#1-特征选择" class="headerlink" title="1. 特征选择"></a>1. 特征选择</h1><h2 id="1-1-特征来源"><a href="#1-1-特征来源" class="headerlink" title="1.1 特征来源"></a>1.1 特征来源</h2><ol><li>业务已整理好的数据，需要找出适合问题的特征</li><li>高级特征，需要从业务特征中去寻找⾼级数据特征</li></ol><h2 id="1-2-选择合适的特征"><a href="#1-2-选择合适的特征" class="headerlink" title="1.2 选择合适的特征"></a>1.2 选择合适的特征</h2><p>第⼀步是找到该领域懂业务的<strong>专家</strong>，让他们给⼀些建议，这些特征就是我们的特征的第⼀候选集。</p><p>在尝试降维之前，有必要用特征工程的方法选择出较重要的特征结合，这些方法不会用到领域知识，而仅是统计学的方法。</p><p>特征选择方法一般分为三类：</p><ul><li><p>过滤法选择特征</p><ol><li><strong>方差筛选</strong>：方差越大的特征，则认为是比较有用；若方差较小，如小于1，则该特征对算法作用没那么大。若某特征方差为0，则其所有样本的特征取值相同，应当舍弃该特征。实际应用中，应指定方差阈值，当方差小于该阈值特征则筛选掉。</li><li><strong>相关系数</strong>：主要⽤于输出连续值的监督学习算法中。我们分别计算所有训练集中各个特征与输出值之间的相关系数，设定⼀个阈值，选择相关系数较⼤的部分特征。</li><li>假设检验：如卡方检验。卡方检验可以检验某个特征分布和输出值分布之间的相关性。除此之外，还可以用F检验和t检验。</li><li>互信息：从信息熵角度分析各个特征和输出值之间的关系评分。互信息值越大，说明该特征和输出值之间的相关性越⼤，越需要保留。</li></ol></li><li><p>包装法选择特征</p><p>思想是选择⼀个⽬标函数来⼀步步的筛选特征。最常⽤的包装法是递归消除特征法(recursive feature elimination, 以下简称RFE)。递归消除特征法使⽤⼀个机器学习模型来进⾏多轮训练，每轮训练后，消除若干权值系数的对应的特征，再基于新的特征集进⾏下⼀轮训练。</p><p>经典方法：SVM-RFE方法。以⽀持向量机来做RFE 的机器学习模型选择特征，假设有n 个特征。</p><ol><li>它在第⼀轮训练的时候，会选择所有的特征来训练，得到分类的超平⾯$wx+b$；</li><li>选择出$w$中分量的平⽅值$w_i^2$最小的那个序号$i$对应的特征，将其排除；</li><li>对剩下的$n-1$个特征和输出值，重新训练SVM，直到剩下的特征数满足需求。</li></ol></li><li><p>嵌入法选择特征</p><ol><li>使⽤全部特征，使⽤L1 正则化和L2 正则化来选择特征。正则化惩罚项越⼤，那么模型的系数就会越⼩。</li><li>当正则化惩罚项⼤到⼀定的程度的时候，部分特征系数会变成0，当正则化惩罚项继续增⼤到⼀定程度时，所有的特征系数都会趋于0。⼀部分特征系数会更容易先变成0，这部分系数就是可以筛掉的。即选择特征系数较⼤的特征。</li><li>⽤的L1 正则化和L2 正则化来选择特征的基学习器是逻辑回归，此外也可以使⽤决策树或者GBDT。</li><li>可以得到特征系数coef或者得到特征重要度(feature importances)的算法才可以作为嵌入法的基学习器。</li></ol></li></ul><h2 id="1-3-寻找高级特征（交叉特征）"><a href="#1-3-寻找高级特征（交叉特征）" class="headerlink" title="1.3 寻找高级特征（交叉特征）"></a>1.3 寻找高级特征（交叉特征）</h2><p>⽐如有车的路程特征和时间间隔特征，我们就可以得到车的平均速度这个⼆级特征。根据车的速度特征，我们就可以得到车的加速度这个三级特征，根据车的加速度特征，我们就可以得到车的加加<br>速度这个四级特征。也就是说，⾼级特征可以⼀直寻找下去。</p><ol><li>特征加和：计算累积</li><li>特征之差：计算增量</li><li>特征乘积：价格与销量得到营业额</li><li>特征除商：销售额除以商品数量得到均价</li></ol><h2 id="1-4-特征选择小结"><a href="#1-4-特征选择小结" class="headerlink" title="1.4 特征选择小结"></a>1.4 特征选择小结</h2><p>特征选择是特征⼯程的第⼀步，它关系到机器学习算法的上限。因此原则是<strong>尽量不错过⼀个可能有⽤的特征，但是也不滥⽤太多的特征</strong>。</p><h1 id="2-特征表达"><a href="#2-特征表达" class="headerlink" title="2. 特征表达"></a>2. 特征表达</h1><h2 id="2-1-缺失值处理"><a href="#2-1-缺失值处理" class="headerlink" title="2.1 缺失值处理"></a>2.1 缺失值处理</h2><p>连续值：均值填充、中位数填充</p><p>离散值：选择所有有该特征值的样本中出现最频繁的类别值，填充缺失值。</p><h2 id="2-2-特殊特征处理"><a href="#2-2-特殊特征处理" class="headerlink" title="2.2 特殊特征处理"></a>2.2 特殊特征处理</h2><ol><li>日期时间<ul><li><strong>时间差值法</strong>：计算出所有样本的时间到某⼀个未来时间（或过去时间）之间的数值差距，转换成<strong>连续值</strong>；</li><li><strong>细分特征</strong>：年、⽉、⽇、周、时，将⼀个时间特征转化为若⼲个<strong>离散特征</strong>，这种⽅法在分析具有明显时间趋势的问题⽐较好⽤。</li><li><strong>权重法</strong>：即根据时间的新旧得到⼀个权重值。⽐如对于商品，三个⽉前购买的设置⼀个较低的权重，最近三天购买的设置⼀个中等的权重，在三个⽉内但是三天前的设置⼀个较⼤的权重。还有其他的设置权重的⽅法，这个要根据要解决的问题来灵活确定。</li></ul></li><li>地理位置<ul><li>细分特征：城市、区县、街道</li><li>经纬度：如果需要判断用户分布区域，一般处理成连续值比较好，这时可以将地址处理成经度和纬度的连续特征。</li></ul></li></ol><h2 id="2-3-离散特征的连续化处理"><a href="#2-3-离散特征的连续化处理" class="headerlink" title="2.3 离散特征的连续化处理"></a>2.3 离散特征的连续化处理</h2><ol><li>one-hot向量</li><li>embedding</li></ol><h2 id="2-4-离散特征的离散化处理"><a href="#2-4-离散特征的离散化处理" class="headerlink" title="2.4 离散特征的离散化处理"></a>2.4 离散特征的离散化处理</h2><ol><li>one-hot向量</li><li>dummy coding</li><li>根据背景知识划分：⽐如，商品的销量对应的特征，原始特征是季节春夏秋冬。可以将其转化为淡季和旺季这样的⼆值特征，⽅便建模。</li></ol><h2 id="2-5-连续特征的离散化处理"><a href="#2-5-连续特征的离散化处理" class="headerlink" title="2.5 连续特征的离散化处理"></a>2.5 连续特征的离散化处理</h2><ol><li>等频分桶</li><li>等值分桶</li><li>GBDT+LR</li></ol><h1 id="3-数据归一化"><a href="#3-数据归一化" class="headerlink" title="3. 数据归一化"></a>3. 数据归一化</h1><h2 id="3-1-为什么需要数据归一化"><a href="#3-1-为什么需要数据归一化" class="headerlink" title="3.1 为什么需要数据归一化"></a>3.1 为什么需要数据归一化</h2><ol><li>归⼀化后加快了梯度下降求最优解的速度</li><li>归⼀化有可能提⾼精度</li></ol><h2 id="3-2-常见数据归一化方法"><a href="#3-2-常见数据归一化方法" class="headerlink" title="3.2 常见数据归一化方法"></a>3.2 常见数据归一化方法</h2><ol><li><p>线性归一化：</p><script type="math/tex; mode=display">x' = \frac{x-min}{max-min}</script><p>缺点：一旦测试集有特征小于min或大于max的数据，则会导致max和min发生变化，需要重新计算。</p></li><li><p>标准化：z-score标准化。</p><script type="math/tex; mode=display">x' = \frac{x - mean}{std}</script></li></ol><ol><li><p>非线性归一化：取指数、对数、正切、等频分桶、等值分桶</p></li><li><p>L1/L2范数标准化：如果只是为了统⼀量纲，那么可以通过L2 范数整体标准化。</p></li><li><p>中心化：PCA 降维时候的预处理，均值变为零，⽅差不变</p><script type="math/tex; mode=display">x' = x-mean</script></li></ol><h3 id="3-3-PCA-主成分分析"><a href="#3-3-PCA-主成分分析" class="headerlink" title="3.3 PCA(主成分分析)"></a>3.3 PCA(主成分分析)</h3><p>两种推导：</p><ol><li>样本点到这个超平面的距离足够近</li><li>样本点在这个超平面的投影能尽可能地分开</li></ol><p>PCA算法流程：</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-110fb4b24415592a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png" style="zoom:67%;"></p><p><strong>问题</strong>：能否用PCA 代替隐含层的降维操作</p><p>假设我们有⼀个如下图所⽰的隐藏层。隐藏层在这个⽹络中起到了⼀定的降维作⽤。假如现在我们⽤另⼀种维度下降的⽅法，⽐如说主成分分析法(PCA) 来替代这个隐藏层。那么，这两者的输出效果是⼀样的吗？</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-7f81a305f78c2195.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>答：不一样。因为PCA 提取的是<strong>数据分布⽅差⽐较⼤</strong>的⽅向，隐藏层可以提取有预测能⼒的特征。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本笔记参考二水马的rebirth。&lt;/p&gt;
&lt;h1 id=&quot;1-特征选择&quot;&gt;&lt;a href=&quot;#1-特征选择&quot; class=&quot;headerlink&quot; title=&quot;1. 特征选择&quot;&gt;&lt;/a&gt;1. 特征选择&lt;/h1&gt;&lt;h2 id=&quot;1-1-特征来源&quot;&gt;&lt;a href=&quot;#1-
      
    
    </summary>
    
      <category term="找工作" scheme="https://github.com/DesmonDay/categories/%E6%89%BE%E5%B7%A5%E4%BD%9C/"/>
    
    
      <category term="work" scheme="https://github.com/DesmonDay/tags/work/"/>
    
  </entry>
  
  <entry>
    <title>实习复习过程</title>
    <link href="https://github.com/DesmonDay/2020/02/19/%E5%AE%9E%E4%B9%A0%E5%A4%8D%E4%B9%A0%E8%BF%87%E7%A8%8B/"/>
    <id>https://github.com/DesmonDay/2020/02/19/实习复习过程/</id>
    <published>2020-02-19T02:11:46.000Z</published>
    <updated>2020-02-19T13:22:45.498Z</updated>
    
    <content type="html"><![CDATA[<p>目前的复习进度：</p><ol><li>剑指offer已刷完</li><li>机器学习复习中</li><li>项目继续准备中</li><li>需要开始刷leetcode</li></ol><p>机器学习所用复习资料：</p><ol><li>西瓜书</li><li>二水马的笔记rebirth</li><li>susht师姐的知乎专栏</li><li>各种博客：刘建平Pinard</li></ol><p>深度学习复习：</p><ol><li>二水马的笔记rebirth</li><li>之前看吴恩达课程所做的笔记</li><li>还需自己再熟悉pytorch和tensorflow常用函数</li></ol><p>项目：</p><ol><li>驱动型对话数据集</li><li>文本对抗比赛</li><li>本科毕设——推荐相关</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;目前的复习进度：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;剑指offer已刷完&lt;/li&gt;
&lt;li&gt;机器学习复习中&lt;/li&gt;
&lt;li&gt;项目继续准备中&lt;/li&gt;
&lt;li&gt;需要开始刷leetcode&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;机器学习所用复习资料：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;西瓜书&lt;/li&gt;
      
    
    </summary>
    
      <category term="找工作" scheme="https://github.com/DesmonDay/categories/%E6%89%BE%E5%B7%A5%E4%BD%9C/"/>
    
    
      <category term="work" scheme="https://github.com/DesmonDay/tags/work/"/>
    
  </entry>
  
  <entry>
    <title>PointerNet and CopyNet</title>
    <link href="https://github.com/DesmonDay/2020/02/15/PointerNet-and-CopyNet/"/>
    <id>https://github.com/DesmonDay/2020/02/15/PointerNet-and-CopyNet/</id>
    <published>2020-02-15T15:04:55.000Z</published>
    <updated>2020-02-27T16:20:18.532Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PointerNet和CopyNet"><a href="#PointerNet和CopyNet" class="headerlink" title="PointerNet和CopyNet"></a>PointerNet和CopyNet</h1><p>参考：<a href="https://cloud.tencent.com/developer/article/1537657" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1537657</a></p><p>由于时间问题，这里只是简单做个记录，有机会再回来补充。这两个算法是比较经典的解决OOV问题的方法，之后也有很多基于此的工作。应对OOV的三个主要方法：</p><ol><li>扩大词表：扩大词表后，可以将部分低频词纳入了词表，但是这些低频词由于缺乏足够数量的语料，训练出来的词向量往往效果不佳，所以扩大词表在提升模型的效果方面，存在一定的瓶颈。</li><li>指针拷贝网络</li><li>字向量/n-gram：中文任务使用字向量（例如BERT），英文任务使用n-gram。</li></ol><p>这两个算法的思想基本都是利用了Attention机制里的对齐向量a。</p><h2 id="Pointer-Networks"><a href="#Pointer-Networks" class="headerlink" title="Pointer Networks"></a>Pointer Networks</h2><p>论文提出了当前文本序列生成模型的三个问题，这三个问题都可以通过使用PtrNet解决：</p><p>（1）目标序列的词表，和源序列的词语内容是<strong>强相关</strong>的。面对不同语言、不同应用场景的任务，往往需要重新构造词表。</p><p>（2）词表的长度需要在模型构建之前作为超参数进行配置。如果需要变更词表的长度，就需要重新训练模型。</p><p>（3）在部分任务场景里，不允许有OOV的存在</p><p>本论文的思路，就是让decoder输出一个由目标序列指向源序列的指针序列。Pointer Networks利用了Attention模型的一个中间变量：对齐系数a。因为对齐系数a表示目标序列当前step和源序列所有step之间的相关性大小，所以我们可以<strong>通过选取对齐系数向量a（向量a长度为源序列的step长度）中数值最大的那个维度（每个维度指向源序列的一个step）</strong>，实现一个从目标序列step到源序列step的指针。最后，根据这个指针指向的源序列的词，直接提取这个词进行输出。</p><h2 id="Pointing-the-Unknown-Words"><a href="#Pointing-the-Unknown-Words" class="headerlink" title="Pointing the Unknown Words"></a>Pointing the Unknown Words</h2><p>这篇论文将pointer networks在NLP领域的文本摘要任务和翻译任务上进行了落地实践（上一篇并没有）。</p><p>论文提出的Pointer Softmax Network模型包含三个主要模块。用通俗的语言解释，如果传统模型效果好，就选择传统模型的输出值进行输出，如果PtrNet模型效果好，就选择PtrNet模型的输出值进行输出，然后需要有一个开关网络来判断，传统模型效果好还是PtrNet模型效果好。这三个模块的描述如下：</p><ol><li>Shortlist Softmax：这个模块由传统的Attention Mechanism实现，输出一个在词表里的词的条件概率。这个模块需要维护一个<strong>固定大小的词表</strong>。</li><li>Location Softmax：这个模块利用了Attention Mechanism的对齐系数a。对齐系数a的向量长度是源序列的step长度，并且对齐系数a每个维度的值，表示decoder当前step输出源序列每个step的概率大小。我们就可以在对齐系数a的各个维度中，取出数值最大的那个维度，作为decoder当前step的指针，这个维度上的值就是其概率大小。</li><li>Switching Network：前面两个模块的公式里的<em>p(z)</em>项，就是由SwitchingNetwork模块生成输出的。Switching Network模型通过<em>p(z)</em>选择是采纳ShortlistSoftmax输出的预测词，还是采纳Location Softmax输出的预测词。Switching Network由一个多层感知机MLP实现，并在最后接一个sigmoid激活函数。</li></ol><p><img src="https://ask.qcloudimg.com/http-save/6552462/5jvu97d12o.png?imageView2/2/w/1620" alt="img"></p><p>额外说明的点：</p><ol><li>在decoder的公式中，每个step有且仅有一个softmax会生效并输出预测值。这个比较理想化，因为开关网络最后一层接的是sigmoid，不能完全保证输出的是0或者1。所以工程实践估计是采用0.5作为判决门限，来选择使用哪个softmax。</li><li>虽然引入PtrNet机制会扩大网络规模，增加网络的参数，但是在模型训练环节，反而会让模型收敛得更快。这一点深有体会。</li><li>要将PtrNet用于翻译任务，需要做一些额外的工作：遇到OOV词时，在使用Location Softmax模块前，会进行两个判定，一个是对OOV词进行查表（法语-英语字典）<strong>判断相应的词是否在target和source中同时出现</strong>，另一个是查找OOV词是否直接在target和source中同时出现，如果其中一个判定成功，则模型可以使用Location Softmax（逻辑上很麻烦对不对，特别是还要额外引入一个词典）。</li></ol><h2 id="Incorporating-Copying-Mechanism-in-Seq2Seq-Learning"><a href="#Incorporating-Copying-Mechanism-in-Seq2Seq-Learning" class="headerlink" title="Incorporating Copying Mechanism in Seq2Seq Learning"></a>Incorporating Copying Mechanism in Seq2Seq <strong>Learning</strong></h2><p>这篇论文提出的CopyNet模型包含两个具有创新点的模块（encoder模块不算在内）:</p><ol><li><p>Generate-Mode &amp; Copy-Mode</p><p>Generate-Mode&amp; Copy-Mode模块会维护两个词表，一个是传统的词表（但是这个词表不包含UNK），一个是源序列中的词构成的词表。</p></li></ol><ul><li>对于传统词表中的词和UNK，模型采用Generate-Mode计算词语输出概率：</li></ul><p><img src="https://ask.qcloudimg.com/http-save/6552462/bqra6ml3ih.png?imageView2/2/w/1620" alt="img"></p><p><img src="https://ask.qcloudimg.com/http-save/6552462/vjbnouaunr.png?imageView2/2/w/1620" alt="img"></p><p>​    其中v是词的onehot表示，W是Generate-Mode的词向量表，s是decoder的状态。公式的意思也就是拿词向量乘以状态s，得到一个分数，再进行归一化，获得概率值。</p><ul><li>对于源序列词表中的词，模型采用Copy-Mode计算词语输出概率：</li></ul><p><img src="https://ask.qcloudimg.com/http-save/6552462/2mpmuazlud.png?imageView2/2/w/1620" alt="img"></p><p><img src="C:\Users\90866\Desktop\1620" alt="img"></p><p>​    其中h是encoder输出的output，w是待训练矩阵，s是decoder的状态。</p><p>​    第一点是：这里词表的长度是源序列中的词<strong>去重</strong>后的数量，和[2]中源序列的长度不一样。</p><p>​    第二点是：如果目标序列中的词y有在源序列词表中，那么Copy-Mode输出的概率就不为0。y在源序列的各个step中每出现一次，就要根据公式计算一次概率值，最后Copy-Mode输出的概率，等于源序列的所有step中有出现y的概率值之和。</p><ul><li>最后，模型会将Generate-Mode和Copy-Mode输出的词语概率进行相加汇总，得到最终的词语概率分布。</li></ul><p><img src="https://ask.qcloudimg.com/http-save/6552462/4fnpg6jyyz.png?imageView2/2/w/1620" alt="img"></p><ol><li>State Update</li></ol><p><img src="https://ask.qcloudimg.com/http-save/6552462/fhd0jn1hzh.png?imageView2/2/w/1620" alt="img"></p><p><img src="https://ask.qcloudimg.com/http-save/6552462/ma1wsuo7zo.png?imageView2/2/w/1620" alt="img"></p><p>t-1时刻的单词被用来更新t时刻的状态，但是copynet不仅用t-1时刻单词的word_embedding,也用t-1时刻的单词在M中隐藏层状态的具体位置有关。</p><p>思路：</p><blockquote><p>（1）CopyNet模型融合了生成式（abstractive）摘要任务和抽取型（extractive）摘要任务的思想。decoder输出的大部分关键词来源于Copy-Mode，这体现了abstractive summarization。然后再由Generate-Mode把语句撸通顺，这体现了extractive summarization。</p><p>（2）拷贝机制的本质是提取关键词，这个输出可以作为上游模块，和其它任务相结合，例如文本分类任务。</p></blockquote><h2 id="Get-To-The-Point-Summarization-with-Pointer-Generator-Networks"><a href="#Get-To-The-Point-Summarization-with-Pointer-Generator-Networks" class="headerlink" title="Get To The Point: Summarization with Pointer-Generator Networks"></a><strong>Get To The Point: Summarization with Pointer-Generator Networks</strong></h2><p><img src="https://img-blog.csdnimg.cn/20190215101435790.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQyMjEyNjY=,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><p>最好的一篇工作。<a href="https://blog.csdn.net/appleml/article/details/87105488" target="_blank" rel="noopener">https://blog.csdn.net/appleml/article/details/87105488</a></p><p>模型结构包含以下几个部分：</p><ol><li><p>传统的带Attention机制的Generator Network:</p><script type="math/tex; mode=display">e_i^t = v^Ttanh(W_hh_i+W_ss_t+b_{attn}) \\a^t = softmax(e^t) \\h^*_t = \sum_i a^t_ih_i \\P_{vocab}=softmax(V'(V[s_t,h_t^*]+b)+b')</script><p>计算公式和Bahdanau Attention基本一致，差异是：输出层P_vocab用了个两层的MLP。</p></li><li><p>用于从源序列拷贝词语的Pointer Network：</p></li></ol><script type="math/tex; mode=display">\sum_{}</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;PointerNet和CopyNet&quot;&gt;&lt;a href=&quot;#PointerNet和CopyNet&quot; class=&quot;headerlink&quot; title=&quot;PointerNet和CopyNet&quot;&gt;&lt;/a&gt;PointerNet和CopyNet&lt;/h1&gt;&lt;p&gt;参考：&lt;a 
      
    
    </summary>
    
    
      <category term="NLP" scheme="https://github.com/DesmonDay/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>XGBoosts和GBDT简单对比</title>
    <link href="https://github.com/DesmonDay/2020/02/10/xgboost%E5%92%8CGBDT%E7%AE%80%E5%8D%95%E5%AF%B9%E6%AF%94/"/>
    <id>https://github.com/DesmonDay/2020/02/10/xgboost和GBDT简单对比/</id>
    <published>2020-02-10T07:08:49.000Z</published>
    <updated>2020-02-26T07:52:31.051Z</updated>
    
    <content type="html"><![CDATA[<p>参考<a href="https://www.cnblogs.com/pinard/p/11114748.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/11114748.html</a></p><h2 id="XGBoots和GBDT简单对比"><a href="#XGBoots和GBDT简单对比" class="headerlink" title="XGBoots和GBDT简单对比"></a>XGBoots和GBDT简单对比</h2><p>XGBoost作为GBDT的高效实现，在算法竞赛中很受欢迎。对比GBDT，XGBoost主要在三个方面做了优化：</p><ol><li><p>算法本身的优化</p><ul><li>在算法的弱学习器模型选择上，对比GBDT只支持决策树，还可以支持很多其他的弱学习器；</li><li>在算法的损失函数上，除了本身的损失，还加上了正则化的部分（$L_t=\sum_{i=1}^m L(y_i,f_{t-1}(x_i))+\gamma J+\frac{\lambda}{2}\sum_{j=1}^Jw^2_{tj}$）</li><li>在算法的优化方式上，GBDT的损失函数只对误差部分做负梯度（一阶泰勒）展开，而XGBoost损失函数对误差部分做二阶泰勒展开，更加准确。</li></ul></li><li><p>算法运行效率的优化</p><p>对每个弱学习器，比如决策树建立的过程做并行选择，找到合适的子树分裂特征和特征值。在并行选择之前，先对所有的特征的值进行排序分组，方便进行并行选择；对分组的特征，选择合适的分组大小，使用CPU缓存进行读取加速。将各个分组保存到多个硬盘以提高IO速度。</p></li><li><p>算法健壮性的优化</p><p>对于缺失值的特征，通过枚举所有缺失值在当前节点是否进入左子树还是右子树来决定缺失值的处理方式。算法本身加入了L1和L2正则化项，可以防止过拟合，泛化能力更强。</p></li></ol><h2 id="XGBoost类库参数"><a href="#XGBoost类库参数" class="headerlink" title="XGBoost类库参数"></a>XGBoost类库参数</h2><h3 id="XGBoost框架参数"><a href="#XGBoost框架参数" class="headerlink" title="XGBoost框架参数"></a>XGBoost框架参数</h3><ol><li><strong>booster</strong>决定了XGBoost使用的弱学习器类型。可以是默认的gbtree, 也就是CART决策树，还可以是线性弱学习器gblinear以及DART。一般来说，我们使用gbtree就可以了，不需要调参。</li><li><strong>n_estimators</strong>则是非常重要的要调的参数，它关系到我们XGBoost模型的复杂度，因为它代表了我们决策树弱学习器的个数。这个参数对应sklearn GBDT的n_estimators。n_estimators太小，容易欠拟合，n_estimators太大，模型会过于复杂，一般需要调参选择一个适中的数值。</li><li><strong>objective</strong>代表了我们要解决的问题是分类还是回归，或其他问题，以及对应的损失函数。具体可以取的值很多，一般我们只关心在分类和回归的时候使用的参数。在回归问题objective一般使用reg:squarederror ，即MSE均方误差。二分类问题一般使用binary:logistic, 多分类问题一般使用multi:softmax。</li></ol><h3 id="XGBoost弱学习器参数"><a href="#XGBoost弱学习器参数" class="headerlink" title="XGBoost弱学习器参数"></a>XGBoost弱学习器参数</h3><p>使用gbtree默认弱学习器的参数。</p><ol><li><p>max_depth：使用网格搜索调参。</p></li><li><p>min_child_weight: 最小的子节点权重阈值. 使用网格搜索调参。</p></li><li><p>gamma: XGBoost的决策树分裂所带来的损失减小阈值。也就会我们在尝试树结构分裂时，会尝试最大化下式：</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-2f948b702cd06344.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这个最大化后的值需要大于我们的gamma，才能继续分裂子树。这个值需要使用网格搜索调参。</p></li><li><p>subsample: 子采样参数，不放回抽样。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。初期可以取值1，如果发现过拟合后可以网格搜索调参找一个相对小一些的值。</p></li><li><p>colsample_bytree/colsample_bylevel/colsample_bynode: 这三个参数都是用于特征采样的，默认都是不做采样，即使用所有的特征建立决策树。colsample_bytree控制整棵树的特征采样比例，colsample_bylevel控制某一层的特征采样比例，而colsample_bynode控制某一个树节点的特征采样比例。</p></li><li><p>reg_alpha/reg_lambda: XGBoost的正则化参数。</p></li></ol><p>上面这些参数都是需要调参的，不过一般先调max_depth，min_child_weight和gamma。如果发现有过拟合的情况下，再尝试调后面几个参数。</p><h3 id="XGBoost其他参数"><a href="#XGBoost其他参数" class="headerlink" title="XGBoost其他参数"></a>XGBoost其他参数</h3><ol><li>learning_rate: 控制每个弱学习器的权重缩减系数，较小的learning_rate意味着我们需要更多的弱学习器的迭代次数。所以这两个参数n_estimators和learning_rate要一起调参才有效果。</li><li>n_jobs: 控制算法的并发线程数</li><li>scale_pos_weight: 用于类别不平衡的时候，负例和正例的比例</li><li>importance_type: 可以查询各个特征的重要性程度。可以选择“gain”, “weight”, “cover”, “total_gain” 或者 “total_cover”。最后可以通过调用booster的get_score方法获取对应的特征权重。“weight”通过特征被选中作为分裂特征的计数来计算重要性，“gain”和“total_gain”则通过分别计算特征被选中做分裂特征时带来的平均增益和总增益来计算重要性。“cover”和 “total_cover”通过计算特征被选中做分裂时的平均样本覆盖度和总体样本覆盖度来来计算重要性。</li></ol><p>我们可以通过验证集的准确率来判断我们前面网格搜索调参是否起到了效果。实际处理的时候需要反复搜索参数并验证。</p><h2 id="XGBoost的缺点"><a href="#XGBoost的缺点" class="headerlink" title="XGBoost的缺点"></a>XGBoost的缺点</h2><p>XGBoost是基于预排序方法的决策树算法。这种构建决策树的算法基本思想是：首先，对所有特征都按照特征的数值进行预排序。其次，在遍历分割点的时候用O(#data)的代价找到一个特征上的最好分割点。最后，在找到一个特征的最好分割点后，将数据分裂成左右子节点。</p><p>这种预排序算法的优点是能精确地找到分割点。</p><p>缺点也很明显：</p><ol><li><strong>空间</strong>消耗大。这样的算法需要保存数据的特征值，还保存了特征排序的结果（例如，为了后续快速的计算分割点，保存了排序后的索引），这就需要消耗训练数据两倍的内存。</li><li><strong>时间</strong>上也有较大的开销，在遍历每一个分割点的时候，都需要进行分裂增益的计算，消耗的代价大。</li><li>对<strong>cache</strong>优化不友好。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的<strong>cache miss</strong>。</li></ol><h2 id="LightGBM原理"><a href="#LightGBM原理" class="headerlink" title="LightGBM原理"></a>LightGBM原理</h2><p>参考推文：<a href="https://mp.weixin.qq.com/s?__biz=MzIwNzc2NTk0NQ==&amp;mid=2247485750&amp;idx=2&amp;sn=b9d024592a38daac5a8f88a5ce17b24f&amp;chksm=970c21e0a07ba8f6e0c4a8702d564adbca8ccc0580f3adec49e81292f85b8c80d3b6a7eb5885&amp;mpshare=1&amp;scene=1&amp;srcid=&amp;sharer_sharetime=1582702883906&amp;sharer_shareid=92ce7d436f5cb488a5c13e20e344e867&amp;key=5dcdf35eccaf36f7ecc5f253915dd66ef16aa5ef38be519e584990c97051929d494c82d05710ef1a4eab0b4d972e08a222289103576eb06ee12a69c849fb83d638912b4ed44e6454be8c3ced9e6de5c8&amp;ascene=1&amp;uin=MjQ2OTEwMDI0Mg%3D%3D&amp;devicetype=Windows+10&amp;version=62080079&amp;lang=zh_CN&amp;exportkey=AX3TDJHX%2B5g%2F%2F1hH6qMIgNU%3D&amp;pass_ticket=YOQR6c8AlwkTgL3wewLa8JhPutxWfxJQLmw1zPNKYnF07R8u5kuJEld%2BSBEBd3rq" target="_blank" rel="noopener">夕小瑶的卖萌屋</a></p><h3 id="LightGBM的优化"><a href="#LightGBM的优化" class="headerlink" title="LightGBM的优化"></a>LightGBM的优化</h3><ul><li>基于Histogram的决策树算法。</li><li>单边梯度采样 Gradient-based One-Side Sampling(GOSS)：使用GOSS可以减少大量只具有小梯度的数据实例，这样在计算信息增益的时候只利用剩下的具有高梯度的数据就可以了，相比XGBoost遍历所有特征值节省了不少时间和空间上的开销。</li><li>互斥特征捆绑 Exclusive Feature Bundling(EFB)：使用EFB可以将许多互斥的特征绑定为一个特征，这样达到了降维的目的。</li><li>带深度限制的Leaf-wise的叶子生长策略：大多数GBDT工具使用低效的按层生长 (level-wise) 的决策树生长策略，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销。实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。LightGBM使用了带有深度限制的按叶子生长 (leaf-wise) 算法。</li><li>直接支持类别特征(Categorical Feature)</li><li>支持高效并行</li><li>Cache命中率优化</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;参考&lt;a href=&quot;https://www.cnblogs.com/pinard/p/11114748.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.cnblogs.com/pinard/p/11114748.html&lt;
      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/DesmonDay/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>Tensorflow的记忆</title>
    <link href="https://github.com/DesmonDay/2019/12/25/Tensorflow%E7%9A%84%E8%AE%B0%E5%BF%86/"/>
    <id>https://github.com/DesmonDay/2019/12/25/Tensorflow的记忆/</id>
    <published>2019-12-25T06:40:52.000Z</published>
    <updated>2020-02-28T04:18:44.420Z</updated>
    
    <content type="html"><![CDATA[<p>一般，我们在参加某个比赛或者实现某种算法的时候，需要经历以下几个过程：</p><ol><li>数据集收集、清洗</li><li>特征构建</li><li>模型实现</li><li>得到结果，再根据反馈在2-4进行多次想法的迭代</li></ol><p>其中，在模型实现上，比如我主要方向是NLP，而NLP如今基本是深度学习时代（特别是预训练模型BERT出来后），因此多需要对深度学习框架有比较深的了解和使用经验。而Tensorflow和Pytorch两家经常被用来做对比。Tensorflow其实更适合用在工业界（静态图，TensorFlow的运行机制属于”定义”与”运行“相分离。），而Pytorch由于其方便性（动态图）而广泛流行于学术界。所以往往，实际操作时，我们通常会使用Pytorch快速实现模型，在对结果进行多次尝试找到最优参数后，再将使用Tensorflow进行复现。也就是说，Tensorflow很重要！！</p><p>Tensorflow的概念：</p><ol><li>张量（tensor）：TensorFlow程序使用tensor数据结构来代表所有的数据，计算图中，操作间传递的数据都是tensor，你可以把TensorFlow tensor看做一个n维的数组或者列表。</li><li>变量（variable）：常用于定义模型中的参数，是通过不断训练得到的值。比如权重和偏置。</li><li>占位符（placeholder）：输入变量的载体。也可以理解成定义函数时的参数。</li><li>图中的节点操作（op）：一个op获得0个或者多个Tensor，执行计算，产生0个或者多个Tensor<strong>。op是描述张量中的运算关系，是网络中真正结构。</strong></li></ol><p>一个TensorFlow图描述了计算的过程，为了进行计算，图必须在会话里启动，会话将图的op分发到诸如CPU或者GPU的设备上，同时提供执行op的方法，这些方法执行后，将产生的tensor返回，在python语言中，返回的tensor是numpy array对象，在C或者C++语言中，返回的tensor是tensorflow:Tensor实例。</p><p>session与图的交互过程中定义了以下两种数据的流向机制：</p><ul><li>注入机制（feed）：通过占位符向模式中传入数据</li><li>取回机制（fetch）：从模式中取得结果</li></ul><p>所以，这里就是记录一些，Tensorflow常用的函数之类的，也当做我的小小笔记本好了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># NLP类Tensorflow常用导入包</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.seq2seq <span class="keyword">as</span> seq2seq</span><br></pre></td></tr></table></figure><p>关于tensowflow中的会话相关操作，参考博客：<a href="https://www.cnblogs.com/zyly/p/8869763.html" target="_blank" rel="noopener">https://www.cnblogs.com/zyly/p/8869763.html</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set GPU fraction</span></span><br><span class="line">gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_fraction)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># build model or load model, 以我参加的一个比赛为例</span></span><br><span class="line">    model = Seq2SeqModel(args, word2id, embeddings)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 如果有训练好的模型，则导入</span></span><br><span class="line">    <span class="comment"># load checkpoint if exists</span></span><br><span class="line">    ckpt = tf.train.get_checkpoint_state(args.model_dir)</span><br><span class="line">    <span class="keyword">if</span> ckpt <span class="keyword">and</span> tf.train.checkpoint_exists(ckpt.model_checkpoint_path):</span><br><span class="line">        model.saver.restore(sess, ckpt.model_checkpoint_path)</span><br><span class="line">    <span class="keyword">else</span>: <span class="comment"># 如果没有模型，则初始化参数，进行训练</span></span><br><span class="line">        sess.run(tf.global_variables_initializer())</span><br></pre></td></tr></table></figure><p>Tensorflow的summary用法参考博客：<a href="https://www.cnblogs.com/lyc-seu/p/8647792.html" target="_blank" rel="noopener">https://www.cnblogs.com/lyc-seu/p/8647792.html</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定一个文件用来保存图，之后可以使用tensorboard查看训练过程</span></span><br><span class="line">summary_writer = tf.summary.FileWriter(args.model_dir, sess=sess.graph)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 训练过程中</span></span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> tqdm(batches, desc=<span class="string">'Training'</span>):</span><br><span class="line">    <span class="comment"># 如果我们需要记录某个step的图，则调用summary_writer</span></span><br><span class="line">    summary_writer.add_summary(summary, cur_step)</span><br></pre></td></tr></table></figure><p>构建多层RNN</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_create_rnn_cell</span><span class="params">(self, hidden_size, rnn_type=<span class="string">'lstm'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Create multi-layer rnn with dropout layer, called by decoder</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">a_cell</span><span class="params">(hidden_size, rnn_type)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> rnn_type == <span class="string">'lstm'</span>:</span><br><span class="line">            single_cell = tf.contrib.rnn.LSTMCell(hidden_size)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            single_cell = tf.contrib.rnn.GRUCell(hidden_size)</span><br><span class="line">        cell = tf.contrib.rnn.DropoutWrapper(single_cell, output_keep_prob=self.keep_prob)</span><br><span class="line">                <span class="keyword">return</span> cell</span><br><span class="line"></span><br><span class="line">    rnn_cell = tf.contrib.rnn.MultiRNNCell([a_cell(hidden_size, rnn_type) <span class="keyword">for</span> _ <span class="keyword">in</span> range(self.num_layers)])</span><br><span class="line">    <span class="keyword">return</span> rnn_cell</span><br></pre></td></tr></table></figure><p>创建模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2SeqModel</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, args)</span>:</span></span><br><span class="line">        self.num_layers = args.num_layers</span><br><span class="line">        <span class="comment"># 初始化参数...</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_model</span><span class="params">(self, embedding)</span>:</span></span><br><span class="line">        <span class="comment">## 设置占位符（</span></span><br><span class="line">        self.keep_prob = tf.placeholder(tf.float32, [], name=<span class="string">'keep_prob'</span>)</span><br><span class="line">        self.src_enc_input = tf.placeholder(tf.int32, [<span class="keyword">None</span>, <span class="keyword">None</span>], name=<span class="string">'src_enc_input'</span>)</span><br><span class="line">        <span class="keyword">if</span> self.train_wordvec:</span><br><span class="line">            self.word_embed = tf.get_variable(<span class="string">'embedding'</span>, [self.vocab_size, self.emb_size], dtype=tf.float32)</span><br><span class="line">            self.word_embed.assign(embedding)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.word_embed = tf.get_variable(<span class="string">'embedding'</span>, [self.vocab_size, self.emb_size], trainable=<span class="keyword">False</span>, dtype=tf.float32)</span><br><span class="line">            self.word_embed.assign(embedding)</span><br><span class="line">            </span><br><span class="line">        <span class="comment">## embedding_lookup</span></span><br><span class="line">        self.src_enc_emb = tf.nn.embedding_lookup(self.word_embed, self.src_enc_input)</span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">##### encoder</span></span><br><span class="line">        src_enc_output, src_enc_state = self.cudnn_rnn(self.num_dirs, self.src_input_emb, self.src_enc_length, self.hidden_size, <span class="string">'src_enc'</span>, self.rnn_type)</span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        self.tgt_dec_input = tf.placeholder(tf.int32, [<span class="keyword">None</span>, <span class="keyword">None</span>], name=<span class="string">'tgt_dec_input'</span>)</span><br><span class="line">        self.tgt_dec_length = tf.placeholder(tf.int32, [<span class="keyword">None</span>], name=<span class="string">'tgt_dec_length'</span>)</span><br><span class="line">        self.max_target_length = tf.reduce_max(self.tgt_dec_length, name=<span class="string">'max_target_length'</span>)</span><br><span class="line">        <span class="comment"># 序列掩码，用于decoder部分</span></span><br><span class="line">        self.mask = tf.sequence_mask(self.tgt_dec_length, self.max_target_length, dtype=tf.float32, name=<span class="string">'mask'</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">##### decoder</span></span><br></pre></td></tr></table></figure> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 优化器</span></span><br><span class="line">optimizer = tf.train.AdamOptimizer(self.learning_rate)</span><br><span class="line">trainable_params = tf.trainable_variables()</span><br><span class="line">gradients = tf.gradients(self.loss, trainable_params)</span><br><span class="line">clip_gradients, _ = tf.clip_by_global_norm(gradients, self.max_grad_norm)</span><br><span class="line">self.train_op = optimizer.apply_gradients(zip(clip_gradients, trainable_params))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一般，我们在参加某个比赛或者实现某种算法的时候，需要经历以下几个过程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;数据集收集、清洗&lt;/li&gt;
&lt;li&gt;特征构建&lt;/li&gt;
&lt;li&gt;模型实现&lt;/li&gt;
&lt;li&gt;得到结果，再根据反馈在2-4进行多次想法的迭代&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;其中，在
      
    
    </summary>
    
    
      <category term="NLP" scheme="https://github.com/DesmonDay/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>对话系统中的推荐-论文调研</title>
    <link href="https://github.com/DesmonDay/2019/09/29/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E6%8E%A8%E8%8D%90-%E8%AE%BA%E6%96%87%E8%B0%83%E7%A0%94/"/>
    <id>https://github.com/DesmonDay/2019/09/29/对话系统中的推荐-论文调研/</id>
    <published>2019-09-29T04:07:59.000Z</published>
    <updated>2020-02-29T04:12:08.718Z</updated>
    
    <content type="html"><![CDATA[<h1 id="对话系统-推荐-论文调研"><a href="#对话系统-推荐-论文调研" class="headerlink" title="对话系统+推荐 论文调研"></a>对话系统+推荐 论文调研</h1><h2 id="相关数据集"><a href="#相关数据集" class="headerlink" title="相关数据集"></a>相关数据集</h2><ul><li><p>REDIAL(REcommendations through DIALog)[1]: 电影对话推荐数据集，由作者通过Amazon Mechanical Turk(AMT)构建，共包含10004个对话。举例如下：</p><hr><p><strong>SEEKER</strong>: hi ! i ’m looking for an action filled movie similar to jurassic park</p><p><em>HUMAN</em>: ok , have you seen jurassic world ?</p><p><strong>SEEKER</strong>: yes i loved that movie as well . are there any good movies without dinosaurs ?</p><p><em>HUMAN</em>: have you seen king kong ?</p><p><strong>SEEKER</strong>: yes ! that was another great giant monster movie : )</p><p><em>HUMAN</em>: what about jaws &amp; amp ; jaws 2 ? oldies but goodies scary action ? suspense gives me the rumblies</p><p><strong>SEEKER</strong>: i absolutely loved the first one . one of my favorite movies ever : ) honestly i ca n’t say i remember much about the second one . jaws 3-d was quite funny and jaws : the revenge was just weird . i do love suspense too …</p><p><em>HUMAN</em>: i like suspense but sometimes i can barely handle it ! it gives me anxiety ! lol</p><p><strong>SEEKER</strong>: that ’s understandable . as long as it ’s not too gory i do n’t generally have a problem with those things .</p><p><em>HUMAN</em>: well , it was great chatting with you ! have a great one !</p><p><strong>SEEKER</strong>: you too! thanks!</p><hr></li><li><p>REDIAL + DBpedia[2]: 增加了电影的相关实体，如导演、电影风格等。(实际上是利用REDIAL对话数据集中提及的实体，然后自己到DBpedia中对应查找相关联实体)</p></li><li><p>QA+Recommendation dataset[3]: 同样是电影领域，但是对应的回复只有电影名称，不适用于对话领域。</p></li><li><p>Reddit dataset[3]: 相比之下对话更加流畅，但对话的目的并不是推荐电影，不适用。</p></li><li><p>target-guided dataset[4]: 对话的目的是引导对话中出现提前设置的目标词或相关词，但对话本身为开放领域。</p></li><li><p>E-commerce Conversational Search and Recommendation Dataset[5]: 其领域是电子商务中的产品推荐，为半合成数据集。包括Electronics, CDs &amp; Vinyl, Kindle Store和Cell Phones。但目前数据下载链接无法访问，下图为论文中的样例。对应论文提出的模型是针对对话中的问题生成。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-d8cea23942297ff0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p></li></ul><p>相对而言，目前只有REDIAL是比较完整的涉及对话加推荐的数据集，但是其<strong>仅限于电影领域</strong>。</p><p>而另一个Amazon的数据集则针对电子产品，但是作者设计出数据集主要用于<strong>提出问题</strong>(System Ask, user respond，具体论文还没有细看)。同时，与我们所想的用在“售前机器人”这样的场景差别比较大，更加适合用在<strong>日常对话中的推荐</strong>，即缺乏“售前对话”的数据集。</p><h2 id="论文方法"><a href="#论文方法" class="headerlink" title="论文方法"></a>论文方法</h2><ul><li><p>REDIAL baseline[1]: 第一篇考虑多轮推荐对话的论文，相当于baseline。包含一个基于<strong>HRED</strong>的对话生成系统以及一个基于autoencoder的推荐系统，通过对话中的mentioned items将两个系统连接起来。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-ece6d62054b62f7a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p></li></ul><ul><li><p>Transformer: [2]论文实现的一个baseline模型，与REDIAL类似，但是其对话生成系统是基于Transformer结构完成的，其余部分不变。</p></li><li><p>KBRD[2]: 端到端框架，通过知识图谱将对话系统和推荐系统连接起来。在生成推荐时，不仅仅考虑在对话中提及的items，还将与这些items相关的实体一并链接到知识图谱中，通过entity attention来得到推荐系统对于所有items的推荐概率；通过推荐系统对应用户的hidden representation可以计算得到一个vocabulary bias($b_u$)，随后对话系统基于Transformer框架且结合$b_u$，从而生成对话中词的概率，从而生成response。</p></li></ul><p><img src="https://upload-images.jianshu.io/upload_images/8636110-30fa7937a71647a2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>推荐系统+强化学习的相关数据集均没有涉及对话方面。</p><h2 id="论文列表"><a href="#论文列表" class="headerlink" title="论文列表"></a>论文列表</h2><p>[1] Towards deep conversational recommendations, NeurlPS2018</p><p>[2] Towards Knowledge-Based Recommender Dialog System, emnlp2019</p><p>[3] Evaluating Prerequisite Qualities for Learning End-to-End Dialog Systems, ICLR2016</p><p>[4] Target-Guided Open-Domain Conversation, ACL2019</p><p>[5] Towards conversational search and recommendation: System Ask, user respond， CIKM2018</p><p>其他相关的未列举，大多数是比较久远的年份了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;对话系统-推荐-论文调研&quot;&gt;&lt;a href=&quot;#对话系统-推荐-论文调研&quot; class=&quot;headerlink&quot; title=&quot;对话系统+推荐 论文调研&quot;&gt;&lt;/a&gt;对话系统+推荐 论文调研&lt;/h1&gt;&lt;h2 id=&quot;相关数据集&quot;&gt;&lt;a href=&quot;#相关数据集&quot; c
      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/DesmonDay/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>Transformer笔记</title>
    <link href="https://github.com/DesmonDay/2019/09/21/Transformer%E7%AC%94%E8%AE%B0/"/>
    <id>https://github.com/DesmonDay/2019/09/21/Transformer笔记/</id>
    <published>2019-09-21T07:52:02.000Z</published>
    <updated>2020-02-22T14:21:16.169Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Transformer笔记"><a href="#Transformer笔记" class="headerlink" title="Transformer笔记"></a>Transformer笔记</h1><p>本笔记主要供自己复习，只记录一些关键的点。参考链接：<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html#prelims" target="_blank" rel="noopener">http://nlp.seas.harvard.edu/2018/04/03/attention.html#prelims</a></p><h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p>一般的神经序列模型都包含encoder-decoder架构。其中，encoder将输入序列$(x_1,x_2,..,x_n)$的符号表示(symbol representations)映射到连续表示序列$z=(z_1,z_2,…z_n)$。给定$z$，decoder随后一次一个元素地生成输出序列$(y_1,y_2,…,y_m)$。在每个步骤中，模型是自动回归(auto-regressive)，在生成下一个时，把先前生成的符号作为附加输入。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A standard Encoder-Decoder architecture. Base for this and many </span></span><br><span class="line"><span class="string">    other models.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder, src_embed, tgt_embed, generator)</span>:</span></span><br><span class="line">        super(EncoderDecoder, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        self.src_embed = src_embed</span><br><span class="line">        self.tgt_embed = tgt_embed</span><br><span class="line">        self.generator = generator</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, src, tgt, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Take in and process masked src and target sequences."</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask,</span><br><span class="line">                            tgt, tgt_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(self, src, src_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span><span class="params">(self, memory, src_mask, tgt, tgt_mask)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Define standard linear + softmax generation step."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.proj(x), dim=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure><p>Transformer的模型架构如下：</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-79468beb5ae88542.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="Encoder-and-Decoder-Stack"><a href="#Encoder-and-Decoder-Stack" class="headerlink" title="Encoder and Decoder Stack"></a>Encoder and Decoder Stack</h2><h3 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h3><p>Transformer的encoder由$N=6$个独立块组成，可看模型框架图得知，位于图的左半部分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clones</span><span class="params">(module, N)</span>:</span></span><br><span class="line">    <span class="string">"Produce N identical layers."</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> range(N)])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Core encoder is a stack of N layers"</span></span><br><span class="line">    <span class="comment"># 此处是Encoder部分，是由6个layers组成的stack</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="string">"Pass the input (and mask) through each layer in turn."</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure><p>另外，每个layer(块)包括了两个sub-layers. 其中，第一个layer是多头注意力机制，第二个layer是简单的全连接层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Encoder is made up of self-attn and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(EncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line">        self.size = size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, mask)</span>:</span></span><br><span class="line">        <span class="string">"Follow Figure 1 (left) for connections."</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure><p>另外，从模型图中可以看到，每个子layer中都有残差连接部分，随后跟随着layer norm层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SublayerConnection</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, dropout)</span>:</span></span><br><span class="line">        super(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, sublayer)</span>:</span></span><br><span class="line">        <span class="string">"Apply residual connection to any sublayer with the same size."</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LayerNorm</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Construct a layernorm module (See citation for details)."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, features, eps=<span class="number">1e-6</span>)</span>:</span></span><br><span class="line">        super(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        mean = x.mean(<span class="number">-1</span>, keepdim=<span class="keyword">True</span>)</span><br><span class="line">        std = x.std(<span class="number">-1</span>, keepdim=<span class="keyword">True</span>)</span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br></pre></td></tr></table></figure><h3 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h3><p>decoder也是由6个块构成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Generic N layer decoder with masking."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, layer, N)</span>:</span></span><br><span class="line">        super(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure><p>除了每个encoder层中的两个子层之外，decoder还插入第三子层，其对堆叠encoder的输出执行多头注意（multi-head attention）。与编码器类似，我们在每个子层周围使用残差连接（residual connections），然后进行层规范化（layer normalization）。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Decoder is made of self-attn, src-attn, and feed forward (defined below)"</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, self_attn, src_attn, feed_forward, dropout)</span>:</span></span><br><span class="line">        super(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, memory, src_mask, tgt_mask)</span>:</span></span><br><span class="line">        <span class="string">"Follow Figure 1 (right) for connections."</span></span><br><span class="line">        m = memory</span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure><p>另外，需要修改解码器中的自注意子层（self-attention sub-layer）以防止位置出现在后续位置（subsequent positions）。这种掩蔽与输出嵌入偏移一个位置的事实相结合，<strong>确保了位置i的预测仅依赖于小于i的位置处的已知输出</strong>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subsequent_mask</span><span class="params">(size)</span>:</span></span><br><span class="line">    <span class="string">"Mask out subsequent positions."</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">'uint8'</span>)</span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(subsequent_mask) == <span class="number">0</span></span><br></pre></td></tr></table></figure><p>我们可以通过图示来展示这个掩码的作用（其中，显示了每个tgt单词（行）允许查看的位置（列））：</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-a7ebf9dec279cb0f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h2><p>An attention function can be described as <strong>mapping a query and a set of key-value pairs to an output</strong>, where the query, keys, values, and output are all vectors. The output is computed as <strong>a weighted sum of the values</strong>, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p><h3 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h3><p><img src="https://upload-images.jianshu.io/upload_images/8636110-b8e83271fed25135.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这个Attention机制称为”Scaled Dot-Product Attention”。对应公式如下：</p><script type="math/tex; mode=display">Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</script><p>实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(query, key, value, mask=None, dropout=None)</span>:</span></span><br><span class="line">    <span class="string">"Compute 'Scaled Dot Product Attention'"</span></span><br><span class="line">    d_k = query.size(<span class="number">-1</span>)</span><br><span class="line">    scores = torch.matmul(query, key.transpose(<span class="number">-2</span>, <span class="number">-1</span>)) \</span><br><span class="line">             / math.sqrt(d_k)</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, <span class="number">-1e9</span>)</span><br><span class="line">    p_attn = F.softmax(scores, dim = <span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure><p>截止参考博客发出来的时候，比较常用的两种attention机制分别是 additive attention（使用具有单隐层的神经网络来计算compatibility function）和dot-product (multiplicative) attention（这种attention和scaled dot-product attention基本一致，除了多了分母部分）。两者相比，后者的运行速度和空间存储利用更好。</p><p>之所以除$\sqrt{d_k}$，解释如下，主要是为了消除点乘的值可能会太大，导致softmax函数进行低梯度空间的情况：</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-c15390dfbff52bea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="Multi-head-Attention"><a href="#Multi-head-Attention" class="headerlink" title="Multi-head Attention"></a>Multi-head Attention</h3><p><img src="https://upload-images.jianshu.io/upload_images/8636110-77539b2534ce68d4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png" style="zoom: 50%;"></p><script type="math/tex; mode=display">MultiHead(Q,K,V) = Concat(head_1,...,head_h)W^O \\where\ head_i = Attention(QW_i^Q, KW_i^K,VW_i^V)</script><p>其中，投影矩阵为$W_i^Q \in \mathbb{R}^{d_{model} \times d_k}$,  $W_i^K \in \mathbb{R}^{d_{model} \times d_k}$, $W_i^V \in \mathbb{R}^{d_{model} \times d_v}$。在Transformer中，一共用了8个头，其中$d_k=d_v=d_{model}/h$。由于每个头部的维数减少，总的计算成本与全维度的单头部注意的计算成本相似。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiHeadedAttention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, h, d_model, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        <span class="string">"Take in model size and number of heads."</span></span><br><span class="line">        super(MultiHeadedAttention, self).__init__()</span><br><span class="line">        <span class="keyword">assert</span> d_model % h == <span class="number">0</span></span><br><span class="line">        <span class="comment"># We assume d_v always equals d_k</span></span><br><span class="line">        self.d_k = d_model // h</span><br><span class="line">        self.h = h</span><br><span class="line">        self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>)</span><br><span class="line">        self.attn = <span class="keyword">None</span></span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, query, key, value, mask=None)</span>:</span></span><br><span class="line">        <span class="string">"Implements Figure 2"</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="comment"># Same mask applied to all h heads.</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        nbatches = query.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k </span></span><br><span class="line">        query, key, value = \</span><br><span class="line">            [l(x).view(nbatches, <span class="number">-1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">             <span class="keyword">for</span> l, x <span class="keyword">in</span> zip(self.linears, (query, key, value))]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class="line">        x, self.attn = attention(query, key, value, mask=mask, </span><br><span class="line">                                 dropout=self.dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3) "Concat" using a view and apply a final linear. </span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous() \</span><br><span class="line">             .view(nbatches, <span class="number">-1</span>, self.h * self.d_k)</span><br><span class="line">        <span class="keyword">return</span> self.linears[<span class="number">-1</span>](x)</span><br></pre></td></tr></table></figure><h3 id="模型中Attention的应用"><a href="#模型中Attention的应用" class="headerlink" title="模型中Attention的应用"></a>模型中Attention的应用</h3><ol><li><p>在encoder-decoder层中，queries来自先前时刻的decoder输出，而keys和values来自encoder层的输出。这样可以允许每个位置的decoder输出能够注意到输入序列的全部位置，这模仿了典型的encoder-decoder注意机制模型。</p></li><li><p>encoder层使用了自注意力机制，其中，query,key,value的值都相同，即encoder的输出。这样的话，每个位置的encoder都可以与encoder之前的所有时刻有关联。而<strong>self-attention</strong>的实际意义是：在序列内部做Attention，寻找序列内部的联系。</p></li><li><p>在decoder层也使用了自注意力机制，即允许解码器中的每个时刻可以关注在该时刻之前的所有时刻。为了保持decoder层的自回归特性，需要防止解码器中的信息向左流动（因为是并行训练，防止看到后面的信息），因此要通过mask掉输入中所有非法连接的值（设置为负无穷大）。具体可以参考下图，感觉讲的应该有点道理，之后再补充苏剑林老师的理解。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-cd697e7bf806f27f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p></li></ol><h3 id="Attention机制的好处"><a href="#Attention机制的好处" class="headerlink" title="Attention机制的好处"></a>Attention机制的好处</h3><p>Attention层的好处是能够一步到位捕捉到全局的联系，因为它直接把序列两两比较（代价是计算量变为$O(n^2)$，当然由于是纯矩阵运算，这个计算量也不是很严重）；相比之下，RNN需要一步步递推才能捕捉到，而CNN则需要通过层叠来扩大感受野，这是Attention层的明显优势。</p><h2 id="Position-wise-Feed-Forward-Network"><a href="#Position-wise-Feed-Forward-Network" class="headerlink" title="Position-wise Feed-Forward Network"></a>Position-wise Feed-Forward Network</h2><p>每个子层都包含了全连接FFN，分别独立的应用于每个position中。其实它的作用有点类似卷积核大小为1的情况。输入和输出的维度都是$d_{model}=512$，中间隐层维度为$d_{ff}=2048$。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionwiseFeedForward</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implements FFN equation."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, d_ff, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">        super(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br></pre></td></tr></table></figure><h2 id="Embeddings-and-Softmax"><a href="#Embeddings-and-Softmax" class="headerlink" title="Embeddings and Softmax"></a>Embeddings and Softmax</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Embeddings</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, vocab)</span>:</span></span><br><span class="line">        super(Embeddings, self).__init__()</span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure><h2 id="Postional-Encoding"><a href="#Postional-Encoding" class="headerlink" title="Postional Encoding"></a>Postional Encoding</h2><p>由于模型中即没有recurrence和convolution，为了可以利用语句中的词序，我们必须将位置信息想办法加进去。因此，模型中对在encoder和decoder的输入处加入了positional encoding。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-627bc132cf18fd28.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>对应公式如下:</p><script type="math/tex; mode=display">PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}}) \\PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}})</script><p>其中，$pos$是position，$i$是维度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PositionalEncoding</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implement the PE function."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d_model, dropout, max_len=<span class="number">5000</span>)</span>:</span></span><br><span class="line">        super(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the positional encodings once in log space.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        self.register_buffer(<span class="string">'pe'</span>, pe)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], </span><br><span class="line">                         requires_grad=<span class="keyword">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure><h2 id="完整模型"><a href="#完整模型" class="headerlink" title="完整模型"></a>完整模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_model</span><span class="params">(src_vocab, tgt_vocab, N=<span class="number">6</span>, </span></span></span><br><span class="line"><span class="function"><span class="params">               d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="string">"Helper: Construct a model from hyperparameters."</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    attn = MultiHeadedAttention(h, d_model)</span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), </span><br><span class="line">                             c(ff), dropout), N),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">        Generator(d_model, tgt_vocab))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># This was important from their code. </span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line">tmp_model = make_model(<span class="number">10</span>, <span class="number">10</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure><p>接下里就是模型的训练部分了，不进行讲解。</p><h2 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h2><p>要学会这种写法。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-775d4829b9fb130b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NoamOpt</span>:</span></span><br><span class="line">    <span class="string">"Optim wrapper that implements rate."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, model_size, factor, warmup, optimizer)</span>:</span></span><br><span class="line">        self.optimizer = optimizer</span><br><span class="line">        self._step = <span class="number">0</span></span><br><span class="line">        self.warmup = warmup</span><br><span class="line">        self.factor = factor</span><br><span class="line">        self.model_size = model_size</span><br><span class="line">        self._rate = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">step</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"Update parameters and rate"</span></span><br><span class="line">        self._step += <span class="number">1</span></span><br><span class="line">        rate = self.rate()</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> self.optimizer.param_groups:</span><br><span class="line">            p[<span class="string">'lr'</span>] = rate</span><br><span class="line">        self._rate = rate</span><br><span class="line">        self.optimizer.step()</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rate</span><span class="params">(self, step = None)</span>:</span></span><br><span class="line">        <span class="string">"Implement `lrate` above"</span></span><br><span class="line">        <span class="keyword">if</span> step <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            step = self._step</span><br><span class="line">        <span class="keyword">return</span> self.factor * \</span><br><span class="line">            (self.model_size ** (<span class="number">-0.5</span>) *</span><br><span class="line">            min(step ** (<span class="number">-0.5</span>), step * self.warmup ** (<span class="number">-1.5</span>)))</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_std_opt</span><span class="params">(model)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> NoamOpt(model.src_embed[<span class="number">0</span>].d_model, <span class="number">2</span>, <span class="number">4000</span>,</span><br><span class="line">            torch.optim.Adam(model.parameters(), lr=<span class="number">0</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>))</span><br><span class="line"></span><br><span class="line">opts = [NoamOpt(<span class="number">512</span>, <span class="number">1</span>, <span class="number">4000</span>, <span class="keyword">None</span>), </span><br><span class="line">        NoamOpt(<span class="number">512</span>, <span class="number">1</span>, <span class="number">8000</span>, <span class="keyword">None</span>),</span><br><span class="line">        NoamOpt(<span class="number">256</span>, <span class="number">1</span>, <span class="number">4000</span>, <span class="keyword">None</span>)]</span><br><span class="line">plt.plot(np.arange(<span class="number">1</span>, <span class="number">20000</span>), [[opt.rate(i) <span class="keyword">for</span> opt <span class="keyword">in</span> opts] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">20000</span>)])</span><br><span class="line">plt.legend([<span class="string">"512:4000"</span>, <span class="string">"512:8000"</span>, <span class="string">"256:4000"</span>])</span><br></pre></td></tr></table></figure><p><img src="https://upload-images.jianshu.io/upload_images/8636110-cf0ab027799b58a2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><h3 id="Label-Smoothing"><a href="#Label-Smoothing" class="headerlink" title="Label Smoothing"></a>Label Smoothing</h3><p>在训练中，使用了label smoothing。这种做法会使perplexity增大，as the model learns to be more unsure, but improves accuracy and BLEU score. （中文好差，不知道怎么翻译得好一些）。标签平滑的优势是能够防止模型追求确切概率而不影响模型学习正确分类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LabelSmoothing</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"Implement label smoothing."</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size, padding_idx, smoothing=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        super(LabelSmoothing, self).__init__()</span><br><span class="line">        self.criterion = nn.KLDivLoss(size_average=<span class="keyword">False</span>)</span><br><span class="line">        self.padding_idx = padding_idx</span><br><span class="line">        self.confidence = <span class="number">1.0</span> - smoothing</span><br><span class="line">        self.smoothing = smoothing</span><br><span class="line">        self.size = size</span><br><span class="line">        self.true_dist = <span class="keyword">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, target)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> x.size(<span class="number">1</span>) == self.size</span><br><span class="line">        true_dist = x.data.clone()</span><br><span class="line">        true_dist.fill_(self.smoothing / (self.size - <span class="number">2</span>)) <span class="comment"># 将该tensor用指定的数值填充</span></span><br><span class="line">        true_dist.scatter_(<span class="number">1</span>, target.data.unsqueeze(<span class="number">1</span>), self.confidence)</span><br><span class="line">        true_dist[:, self.padding_idx] = <span class="number">0</span></span><br><span class="line">        mask = torch.nonzero(target.data == self.padding_idx)</span><br><span class="line">        <span class="keyword">if</span> mask.dim() &gt; <span class="number">0</span>:</span><br><span class="line">            true_dist.index_fill_(<span class="number">0</span>, mask.squeeze(), <span class="number">0.0</span>)</span><br><span class="line">        self.true_dist = true_dist</span><br><span class="line">        <span class="keyword">return</span> self.criterion(x, Variable(true_dist, requires_grad=<span class="keyword">False</span>))</span><br></pre></td></tr></table></figure><ul><li><p>scatter_函数理解举例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">x = torch.rand(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line"> <span class="comment">#0.4319  0.6500  0.4080  0.8760  0.2355</span></span><br><span class="line"> <span class="comment">#0.2609  0.4711  0.8486  0.8573  0.1029</span></span><br><span class="line"><span class="comment"># LongTensor的shape刚好与x的shape对应，也就是LongTensor每个index指定x中一个数据的填充位置。</span></span><br><span class="line"><span class="comment"># dim=0，表示按行填充，主要理解按行填充。</span></span><br><span class="line">torch.zeros(<span class="number">3</span>, <span class="number">5</span>).scatter_(<span class="number">0</span>, torch.LongTensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">2</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]]), x)</span><br><span class="line"><span class="comment"># 举例LongTensor中的第0行第2列index=2，表示在第2行（从0开始）进行填充填充，对应到zeros(3, 5)中就是位置（2,2）。 所以此处要求zeros(3, 5)的列数要与x列数相同，而LongTensor中的index最大值应与zeros(3, 5)行数相一致。</span></span><br><span class="line"><span class="number">0.4319</span>  <span class="number">0.4711</span>  <span class="number">0.8486</span>  <span class="number">0.8760</span>  <span class="number">0.2355</span></span><br><span class="line"><span class="number">0.0000</span>  <span class="number">0.6500</span>  <span class="number">0.0000</span>  <span class="number">0.8573</span>  <span class="number">0.0000</span></span><br><span class="line"><span class="number">0.2609</span>  <span class="number">0.0000</span>  <span class="number">0.4080</span>  <span class="number">0.0000</span>  <span class="number">0.1029</span></span><br><span class="line">参考链接：https://www.cnblogs.com/dogecheng/p/<span class="number">11938009.</span>html，有公式说明</span><br></pre></td></tr></table></figure><p>scatter()一般可以用来对标签进行<strong>one-hot编码</strong>，举例如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">class_num = <span class="number">10</span></span><br><span class="line">batch_size = <span class="number">4</span></span><br><span class="line">label = torch.LongTensor(batch_size, <span class="number">1</span>).random_() % class_num</span><br><span class="line"><span class="comment">#tensor([[6],</span></span><br><span class="line"><span class="comment">#        [0],</span></span><br><span class="line"><span class="comment">#        [3],</span></span><br><span class="line"><span class="comment">#        [2]])</span></span><br><span class="line">torch.zeros(batch_size, class_num).scatter_(<span class="number">1</span>, label, <span class="number">1</span>)</span><br><span class="line"><span class="comment">#tensor([[0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],</span></span><br><span class="line"><span class="comment">#        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])</span></span><br></pre></td></tr></table></figure></li></ul><p>普通的label smoothing写法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">new_labels = (<span class="number">1.0</span> - label_smoothing) * one_hot_labels + label_smoothing / num_classes</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Transformer笔记&quot;&gt;&lt;a href=&quot;#Transformer笔记&quot; class=&quot;headerlink&quot; title=&quot;Transformer笔记&quot;&gt;&lt;/a&gt;Transformer笔记&lt;/h1&gt;&lt;p&gt;本笔记主要供自己复习，只记录一些关键的点。参考链接：
      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/DesmonDay/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>HMM-隐马尔可夫模型</title>
    <link href="https://github.com/DesmonDay/2019/09/05/HMM-%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/"/>
    <id>https://github.com/DesmonDay/2019/09/05/HMM-隐马尔可夫模型/</id>
    <published>2019-09-05T06:45:10.000Z</published>
    <updated>2020-02-22T14:15:08.863Z</updated>
    
    <content type="html"><![CDATA[<p>参考：<a href="https://www.cnblogs.com/pinard/p/6945257.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6945257.html</a></p><h1 id="HMM"><a href="#HMM" class="headerlink" title="HMM"></a>HMM</h1><h2 id="1-什么问题需要HMM模型"><a href="#1-什么问题需要HMM模型" class="headerlink" title="1. 什么问题需要HMM模型"></a>1. 什么问题需要HMM模型</h2><p>使⽤HMM 模型时我们的问题⼀般有这两个特征：</p><ul><li>问题是基于序列的，⽐如时间序列，或者状态序列。</li><li>问题中有两类数据<ul><li>⼀类序列数据是可以观测到的，即观测序列；</li><li>另⼀类数据是不能观察到的，即隐藏状态序列，简称状态序列。</li></ul></li></ul><h2 id="2-HMM定义"><a href="#2-HMM定义" class="headerlink" title="2. HMM定义"></a>2. HMM定义</h2><ul><li><p>隐藏状态集合：$Q={q_1,q_2,…,q_N}$， 观察状态集合：$V={v_1,v_2,…,v_M}$</p></li><li><p>对于长度为$T$的序列，对应状态序列和观测序列为：</p><script type="math/tex; mode=display">I = {i_1,i_2,...,i_T} i_t \in Q\\O = {o_1,o_2,...,o_T}, o_t \in V</script></li><li><p>两个重要假设</p><ul><li><p>齐次马尔科夫链假设：任意时刻的隐藏状态只依赖于前一个隐藏状态，表示为：</p><script type="math/tex; mode=display">a_{ij} = P(i_{t+1} = q_j | i_t = q_i)</script><p>对应状态转移矩阵：$A=[a_{ij}]_{N\times N}$</p></li><li><p>观测独立性假设：任意时刻的观察状态只依赖于当前时刻的隐藏状态，观测状态生成概率为：</p><script type="math/tex; mode=display">b_j(k) = P(o_t=v_k|i_t=q_j)</script></li></ul></li><li><p>一个HMM可表示为三元组：$\lambda = (A,B,\prod)$</p></li></ul><h2 id="3-HMM三个基本问题"><a href="#3-HMM三个基本问题" class="headerlink" title="3. HMM三个基本问题"></a>3. HMM三个基本问题</h2><ul><li><p>估计观测序列概率（似然问题）：</p><ul><li><p>给定模型和观测序列，计算观测序列在该模型下出现的概率</p></li><li><p>求解方法为<strong>前向后向算法</strong>(均为动态规划)：可参考<a href="https://www.cnblogs.com/pinard/p/6955871.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6955871.html</a></p></li><li><p>前向算法总结：</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-9b33930cdbc358da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p></li><li><p>后向算法总结：</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-3414527f99d702bb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p></li></ul></li><li><p>模型参数学习问题（学习问题）：</p><ul><li><p>给定观测序列，估计模型的参数，使得在该模型下观测序列出现的概率最大。</p></li><li><p>求解方法为<strong>基于EM算法的鲍姆-韦尔奇算法</strong>。</p></li><li><p>鲍姆-韦尔奇算法总结：</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-3f288ea5c331c5b8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p></li></ul></li><li><p>预测问题（解码问题）：</p><ul><li><p>给定模型和观测序列，求在给定观测序列的条件下，最可能出现的状态序列。</p></li><li><p>求解方法为<strong>基于动态规划的维特比算法</strong>。</p></li><li><p>维特比算法总结：</p><p><img src="C:\Users\90866\AppData\Roaming\Typora\typora-user-images\1582182663650.png" alt="1582182663650" style="zoom:80%;"></p></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;参考：&lt;a href=&quot;https://www.cnblogs.com/pinard/p/6945257.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.cnblogs.com/pinard/p/6945257.html&lt;/
      
    
    </summary>
    
      <category term="NLP" scheme="https://github.com/DesmonDay/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>Optimizer的选择</title>
    <link href="https://github.com/DesmonDay/2019/08/23/Optimizer%E7%9A%84%E9%80%89%E6%8B%A9/"/>
    <id>https://github.com/DesmonDay/2019/08/23/Optimizer的选择/</id>
    <published>2019-08-23T06:31:42.000Z</published>
    <updated>2020-02-19T07:23:09.242Z</updated>
    
    <content type="html"><![CDATA[<p>在深度学习中，选择合适的优化器不仅会加速整个神经网络训练过程，并且会避免在训练的过程中碰到鞍点。⽂中会结合自己的使⽤情况，对使⽤过的优化器提出⼀些自己的理解。参考二水马的笔记rebirth。</p><h1 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h1><p>SGD是非常常见的优化方法，每次迭代计算Mini-batch的梯度，再对参数进行更新。公式：</p><script type="math/tex; mode=display">v_t = \mu \nabla_theta J(\theta) \\\theta = \theta - v_t</script><p>缺点：对于损失方程有比较严重的振荡，并且容易收敛到局部最小值。</p><h1 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h1><p>为了克服SGD振荡比较严重的问题，Momentum将物理中的动量概念引入SGD当中，通过积累之前的动量来替代梯度。</p><script type="math/tex; mode=display">v_t = \gamma v_{t-1} + \mu \nabla_{\theta} J(\theta) \\\theta = \theta - v_t</script><p>其中，$v$是初始速度，$\mu$是学习率，$\gamma$是动量参数。</p><p>相较于SGD，Momentum 就相当于在从⼭坡上不停的向下⾛，当没有阻⼒的话，它的动量会越来越⼤，但是如果遇到了阻⼒，速度就会变小。也就是说，在训练的时候，在梯度⽅向不变的维度上，训练速度变快；梯度⽅向有所改变的维度上，更新速度变慢，这样就可以加快收敛并减小振荡。</p><h1 id="Nesterov"><a href="#Nesterov" class="headerlink" title="Nesterov"></a>Nesterov</h1><p>加速梯度更新。但在随机梯度的情况下，Nesterov动量没有改进收敛率。</p><script type="math/tex; mode=display">v_t = \gamma * v_{t-1} + \mu * \nabla_{theta}[\frac{1}{m}L(f(x^{(i)};\theta+\gamma v),y^{(i)})] \\\theta = \theta - v_t</script><h1 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h1><p>Adagrad算法，独立地适应所有模型参数的学习率。相较于SGD，Adagrad相当于<strong>对学习率多加了一个约束</strong>，即</p><script type="math/tex; mode=display">计算梯度：g = \frac{1}{m}\nabla_{\theta}\sum_iL(f(x^{(i)};\theta),y^{(i)})\\累积平方梯度：r_{t+1} = r_{t} + g \odot g \\计算参数更新：\nabla \theta = -\frac{\epsilon}{\sqrt{\delta+r}} \odot g \\应用更新：\theta = \theta + \nabla\theta</script><p>在训练开始时积累梯度平方会导致有效学习率过早和过量的减小。</p><h1 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h1><p>和AdaGrad相比，使用移动平均引入了一个新的超参数$\rho$，用来控制移动平均的长度范围。</p><script type="math/tex; mode=display">计算梯度：g = \frac{1}{m}\nabla_{\theta}\sum_iL(f(x^{(i)};\theta),y^{(i)})\\累积平方梯度：r_{t+1} = \rho r_{t} + (1-\rho)g \odot g \\计算参数更新：\nabla \theta = -\frac{\epsilon}{\sqrt{\delta+r}} \odot g \\应用更新：\theta = \theta + \nabla\theta</script><h1 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h1><p>Adam 是⼀个结合了Momentum 与Adagrad 的产物，它既考虑到了利⽤动量项来加速训练过程，又考虑到对于学习率的约束。利⽤梯度的⼀阶矩估计和⼆阶矩估计动态调整每个参数的学习率。Adam 的优点主要在于经过偏置校正后，每⼀次迭代学习率都有个确定范围，使得参数⽐较平稳。其公式为：</p><script type="math/tex; mode=display">计算梯度：g = \frac{1}{m}\nabla_{\theta}\sum_iL(f(x^{(i)};\theta),y^{(i)})\\更新有偏一阶矩估计：s = \rho_1 s + (1 - \rho_1)g \\更新有偏二阶矩估计：r = \rho_2 r + (1 - \rho_2)g\odot g\\修正一阶矩的偏差：\hat{s} = \frac{s}{1 - \rho_1^t}\\修正二阶矩的偏差：\hat{r} = \frac{r}{1 - \rho_2^t} \\计算更新：\nabla \theta = - \epsilon \frac{\hat{s}}{\sqrt{\hat{r}}+\delta}\\应用更新：\theta = \theta + \nabla\theta</script><p>其中，$\rho_1$和$\rho_2$为矩估计的指数衰减速率。</p><p>Adam通常被认为对超参数的选择相当鲁棒，尽管学习率有时需要从建议的默认修改。</p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>通过实践证明，Adam 结合了Adagrad 善于处理稀疏梯度和Momentum 善于处理⾮平稳⽬标的优点，相较于其他⼏种优化器效果更好。同时，我们也注意到很多论⽂中都会引⽤SGD，Adagrad作为优化函数。但相较于其他⽅法，在实践中，SGD 需要更多的训练时间以及可能会被困到鞍点的<br>缺点，都制约了它在很多真实数据上的表现。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在深度学习中，选择合适的优化器不仅会加速整个神经网络训练过程，并且会避免在训练的过程中碰到鞍点。⽂中会结合自己的使⽤情况，对使⽤过的优化器提出⼀些自己的理解。参考二水马的笔记rebirth。&lt;/p&gt;
&lt;h1 id=&quot;SGD&quot;&gt;&lt;a href=&quot;#SGD&quot; class=&quot;he
      
    
    </summary>
    
      <category term="深度学习" scheme="https://github.com/DesmonDay/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://github.com/DesmonDay/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Batch Normalization</title>
    <link href="https://github.com/DesmonDay/2019/08/19/Batch-Normalization/"/>
    <id>https://github.com/DesmonDay/2019/08/19/Batch-Normalization/</id>
    <published>2019-08-19T03:54:39.000Z</published>
    <updated>2020-02-19T06:38:35.314Z</updated>
    
    <content type="html"><![CDATA[<h2 id="使用BN的原因和BN的原理"><a href="#使用BN的原因和BN的原理" class="headerlink" title="使用BN的原因和BN的原理"></a>使用BN的原因和BN的原理</h2><p>批归一化实际上是为了解决“Internal Covariate Shift”问题，即在深度神经网络在训练过程中使得每一层神经网络的输入保持相同分布。</p><p>其基本思想为：因为深度神经网络在做非线性变换前的激活输入值，在随着网络深度加深或者训练过程中，其分布逐渐发生偏移或者变动，这就导致”梯度消失“问题，从而训练收敛慢。</p><p>为什么会造成“梯度消失”问题？这是因为，变化的整体分布逐渐往非线性函数的取值区间的上下限两端靠近，我们可以观察sigmoid函数和tanh函数的图像，其上下限两端的梯度均接近0. 而接近上下限则会导致梯度接近0， 从而发生“梯度消失”现象。</p><p>因此，BN所做的事就是通过一定的规范化手段，把每层神经网络任意神经元的输入值分布强行拉回到均值为0，方差为1的标准正态分布。其中，$\epsilon$是为了控制分母为正。</p><script type="math/tex; mode=display">x_i' = \frac{x_i-\mu}{\sqrt{\sigma^2+\epsilon}}</script><p>然而，如果强行对数据进行缩放，可能会导致一些问题（毕竟强行改变了人家的分布），因此，BN增加了<em>scale</em>和<em>shift</em>的操作。其核心思想是找到一个线性和非线性的较好平衡点，既能享受非线性的较强表达能力的好处，又避免太靠非线性区两头使得网络收敛速度太慢。</p><script type="math/tex; mode=display">y_i = scale * x_i' +shift</script><p>既然BN的名称为batch normalization，那其表现就会受到batch size的影响：</p><ol><li>size太小，算出的$\mu$和$\sigma$不准确，影响归一化，导致性能下降；</li><li>size太小，则内存放不下。</li></ol><h2 id="BN放置的位置"><a href="#BN放置的位置" class="headerlink" title="BN放置的位置"></a>BN放置的位置</h2><p>许多人会纠结Batch Normalization操作放置的位置，这里做一个确定性的结论：通常位于X=WU+B激活输入值获得之后，非线性函数变换（激活函数）之前，图示如下：</p><p><img src="https://images2018.cnblogs.com/blog/1192699/201804/1192699-20180405213955224-1791925244.png" alt="img"></p><h2 id="其他归一化手段"><a href="#其他归一化手段" class="headerlink" title="其他归一化手段"></a>其他归一化手段</h2><p>除了BN，还有layer normalization、Group Normalization和Instance Normalization。</p><p>我们参考susht师姐整理的内容进行总结：</p><p><img src="https://pic4.zhimg.com/v2-91307a71718b2ce805678a612625676f_r.jpg" alt="preview"></p><p>其实这些方法只是归一化的方向不一样，不再沿batch方向归一化，他们的不同点就在于归一化的方向不一样。</p><p>BN：批量归一化，往batch方向做归一化，归一化维度是[N，H，W]</p><p>LN：层次归一化，往channel方向做归一化，归一化维度为[C，H，W]</p><p>IN：实例归一化，只在一个channel内做归一化，归一化维度为[H，W]</p><p>GN：介于LN和IN之间，在channel方向分group来做归一化，归一化的维度为[C//G , H, W]</p><h2 id="深度学习框架中使用BN"><a href="#深度学习框架中使用BN" class="headerlink" title="深度学习框架中使用BN"></a>深度学习框架中使用BN</h2><p>这里贴一下一点注意事项。</p><p>在pytorch中使用BN很简单，但注意，对应模型在训练和验证（测试）两种情况，需要区分model.train()和model.eval()。另外贴一下使用方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nn.Conv2d(in_channel, out_channel, <span class="number">3</span>, stride, <span class="number">1</span>, bias=<span class="keyword">False</span>)</span><br><span class="line">nn.BatchNorm2d(out_channel)<span class="comment">#BatchNorm2d最常用于卷积网络中(防止梯度消失或爆炸)，设置的参数就是卷积的输出通道数</span></span><br><span class="line">nn.ReLU(inplace=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>在tensorflow中BN的实现主要有三个：</p><ol><li>tf.nn.batch_normalization</li><li>tf.layers.batch_normalization</li><li>tf.contrib.layers.batch_norm</li></ol><p>参考资料：</p><ol><li><a href="https://www.cnblogs.com/guoyaohua/p/8724433.html" target="_blank" rel="noopener">https://www.cnblogs.com/guoyaohua/p/8724433.html</a></li><li><a href="https://zhuanlan.zhihu.com/p/36101196" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/36101196</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;使用BN的原因和BN的原理&quot;&gt;&lt;a href=&quot;#使用BN的原因和BN的原理&quot; class=&quot;headerlink&quot; title=&quot;使用BN的原因和BN的原理&quot;&gt;&lt;/a&gt;使用BN的原因和BN的原理&lt;/h2&gt;&lt;p&gt;批归一化实际上是为了解决“Internal Cova
      
    
    </summary>
    
    
      <category term="python" scheme="https://github.com/DesmonDay/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>第14周-序列模型和注意力机制</title>
    <link href="https://github.com/DesmonDay/2019/04/28/deep-learningw14/"/>
    <id>https://github.com/DesmonDay/2019/04/28/deep-learningw14/</id>
    <published>2019-04-28T06:12:48.000Z</published>
    <updated>2019-05-01T16:41:06.622Z</updated>
    
    <content type="html"><![CDATA[<h1 id="基础模型"><a href="#基础模型" class="headerlink" title="基础模型"></a>基础模型</h1><h2 id="Sequence-to-sequence-model-encoder-decoder-network"><a href="#Sequence-to-sequence-model-encoder-decoder-network" class="headerlink" title="Sequence to sequence model(encoder-decoder network)"></a>Sequence to sequence model(encoder-decoder network)</h2><p>论文标题：</p><ol><li>Sequence to sequence learning with neural networks, 2014</li><li>Learning phrase representations using RNN encoder-decoder for statistical machine translation, 2014</li></ol><p>假设我们要翻译以下句子（法语-&gt;英语）：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ba79ab26ed9d15e5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>首先，我们需要建立一个网络，该网络称为<strong>encoder network</strong>(编码网络)，它是一个RNN网络，其中RNN的单元可以是GRU，也可以是LSTM。每次只向该网络中输入一个法语单词，将整个句子输入完毕后，RNN会输出一个向量来代表这个输入序列。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-260763f4c22287ce.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在之后，我们需要建立一个<strong>decoder network</strong>(解码网络)，它以编码网络的输出作为输入，然后可以被训练为每次输出一个翻译后的单词。一直到它输出序列的结尾或者句子结尾标记，那么这个解码网络的工作就结束了。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-5b82df19496a649c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在给出足够的法语和英语文本的情况下，如果我们训练这个模型，通过输入一个法语句子来输出对应的英语翻译，这个模型将会非常有效。</p><h2 id="Image-captioning"><a href="#Image-captioning" class="headerlink" title="Image captioning"></a>Image captioning</h2><p>论文标题：</p><ol><li>Deep captioning with multimodal recurrent neural network, 2014</li><li>Show and tell: Neural image caption generator, 2014</li><li>Deep visual-semantic alignments for generating image descriptions, 2015</li></ol><p>还有一个非常类似的结构被用来做图像描述。</p><p>比如给定一张图片，如这张猫的图片，我们希望网络能够自动地输出该图片的描述：一只猫坐在椅子上。方法如下：在之前的图像课程中我们知道了如何将图片输入到卷积神经网络中，比如一个预训练的AlexNet结构，然后让其学习图片的编码或者学习图片的一系列特征。如下，如果我们去掉最后的Softmax层，那么这个预训练的AlexNet结构会给我们一个4096维的特征向量，而向量表示的就是这只猫的图片。。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e11143623397661a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>因此，这个预训练网络可以是图像的编码网络(encoder network)，接着我们可以把输出的向量输入到RNN中，而RNN要做的就是生成图像的描述，这与我们之前所讲的英语法语翻译的结构很类似。事实证明这个方法在图像描述的领域中很有效，特别是当我们想生成的描述不是很长时。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e966cf9e737be645.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>现在我们知道了Seq2Seq模型和图像描述模型是怎样运作的，不过这两个模型的运作方式有一些不同，主要体现在如何用语言模型合成新的文本，并生成对应序列的方面。</p><h1 id="Picking-the-most-likely-sentence"><a href="#Picking-the-most-likely-sentence" class="headerlink" title="Picking the most likely sentence"></a>Picking the most likely sentence</h1><p>在seq2seq机器翻译模型和我们在第一周课程里所用的语言模型之间有很多相似的地方，也有很多重要的区别。</p><p>我们可以把机器翻译想成是建立一个条件语言模型。下图先给出我们在学习序列模型第一周语言模型时所给出的网络结构：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-028837dd1aeb5362.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>这个模型可以让我们估计句子的可能性，我们也可以用其生成一个新的句子。</p><p>而机器翻译模型长的是下面这样，其中绿色表示encoder网络，而紫色表示decoder网络。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-6eaff6c73ec5c1c8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>所以机器翻译模型与语言模型很相似，不同在于语言模型总是以零向量开始，而encoder网络会计算出一系列句子并将其输入到decoder网络中。因此我们称其为条件语言模型。相比语言模型是输出任意句子的概率，翻译模型会输出句子的英文翻译，这取决于输入的法语句子。也就是说，我们估计这句英文翻译的概率，比如”Jane is visiting Africa in September”，这句翻译取决于法语句子”Jane visited I’Afrique en septembre”，这就是英语句子相对于输入的法语句子的可能性，所以它是条件语言模型。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-dc75067c047e5cb0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>下面我们希望能够利用上面的模型实现法译英。通过输入的法语句子，模型会告诉我们各种英文翻译所对应的可能性（概率）。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-41c54e23bd9b6c05.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>其中，x在这里是法语句子，而y这里对应不同英语句子翻译的概率。显然我们不希望它随机地进行输出，如果我们从这个分布中进行取样，我们可能会得到各种不同的翻译，可能有的糟糕，有的还可以：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-47f3012dd9af9f24.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>所以当我们使用这个模型来进行翻译时，我们并不是从得到的分布中进行随机采样，而是要找到一个英语句子y，使得条件概率最大化。所以在开发机器翻译系统时，我们需要做的一件事就是想出一个算法，用来找到合适的y值，使得该项最大化。而解决这个问题的算法称为Beam Search(束搜索)。</p><h2 id="Why-not-a-greedy-search"><a href="#Why-not-a-greedy-search" class="headerlink" title="Why not a greedy search?"></a>Why not a greedy search?</h2><p>在下一节介绍束搜索前，我们需要了解为什么不使用贪心搜索。这是一种来自计算机科学的算法，在生成第一个词的分布之后，它会根据我们的条件语言模型挑选出最有可能的第一个词，放入机器翻译模型中，接着它会继续挑选出最有可能的其他词。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ef67140fe31da49b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>但是我们真正需要的是一次性挑选出整个单词序列，从y<1>、y<2>到y<ty>，来使得整体的概率最大化。所以这种贪心算法是先挑出最好的第一个词，在这之后再挑最好的第二个词，这样一直挑选下去，而这种方法其实并不管用。</ty></2></1></p><p>为了证明整个观点，我们考虑下面两种翻译：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-a1897897502347e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>显然第一个翻译要好于第二个，所以我们希望机器翻译模型可以说第一个句子的p(y|x)比第二个句子高。但如果贪心算法挑选出了”Jane is”这两个词，而由于”is going”在英语中更加常见，所以对于法语句子来说”Jane is going”要比”Jane is visiting”会有更高的概率，所以很有可能如果我们仅仅根据前两个词来估计第三个词的可能性，我们最后得到的会是第二个句子，但这个句子显然比第一个差。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-1f24d333c8447e43.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>当然，在英语中各种词汇的组合数量还有许多，所以如果我们的字典中有10,000个单词，并且我们的翻译可能有10个词长，那么可能的组合就是10,000^10那么多。所以可能的句子数量非常巨大，我们不可能计算每一种组合的可能性，所以此时最常用的办法是用一个近似的搜索算法，这个近似的搜索算法做的就是尽力挑选句子使得条件概率最大化。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-93646929bb4c91fc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>尽管这不可能保证找到的句子是条件概率最大化，但这也足够了。</p><p>因此接下来，我们需要一个合适的搜索算法。</p><h1 id="集束搜索：Beam-Search"><a href="#集束搜索：Beam-Search" class="headerlink" title="集束搜索：Beam Search"></a>集束搜索：Beam Search</h1><h2 id="基本的Beam-Search"><a href="#基本的Beam-Search" class="headerlink" title="基本的Beam Search"></a>基本的Beam Search</h2><p>下面用我们之前所举的法语句子为例讲述beam search算法。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d6364f761df9c972.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>首先我们用这个网络结构来评估第一个单词的概率值，而贪婪算法只会挑出最有可能的一个单词然后继续，而集束搜索则会考虑多个选择。因此集束搜索会有一个参数B，全称为<strong>Beam width</strong>，在本例中<strong>设置为3</strong>。这就意味着集束搜索会一次性考虑3个可能的选择。比如对于第一个单词有不同选择的可能，最后找到in、jane、september是最有可能的三个选项，那么集束搜索算法会把结果存到计算机内存中，以便后面尝试用这三个词。</p><p>因此为了执行beam search的<strong>第一步</strong>，我们需要输入法语句子到编码网络，然后第一步，也就是解码网络的softmax层会输出10,000个概率值，在这10,000个输出的概率值取出前三个存起来。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-36f761c6dcd6b259.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>在将in、jane、september作为第一个单词的三个可能选择后，beam search所做的<strong>第二步</strong>是针对每个选择，考虑第二个单词是什么。为了评估第二个词的概率值，我们会用下面这个神经网络。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-a47eb4c0d493f4da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>将y<1>（假设为in）的输出作为输入，从而可以评估第二个单词的概率。需要注意的是，在第二步里我们更关心的是找到最可能第一个和第二个单词对，而不是仅仅第二个单词具有最大概率。而具体的计算公式如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-88c2c2b2e3947490.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>因此对于第一个单词的三个候选选择，我们可以保存它们对应的概率值，然后再乘以第二个概率值，从而得到两个单词对的概率值。</1></p><p>对应预测单词jane后的第二个单词，相似网络如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c757813680bb6dfc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>同样的，对于september:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c1d4f41783d8b5dc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>那么我们通过上述的第二步，最后可以得到10,000*3种结果，也就是30,000个可能的结果，即集束宽乘以词汇表大小。而我们要做的就是评估这30,000个选择，然后<strong>选出前三个选择</strong>，假设选出来的是in september、jane is和jane visits。那么我们注意到，第一个单词的september已经没有可能了，但现在我们还是有3个选择。另外，由于我们的集束宽为3，因此我们会有三个网络副本，每个网络的第一个单词不同。</p><p>接下来简要讲第三步，类似上面的网络：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-540574efd8bb8f9b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>再继续同样的步骤，直到达到EOS标记。注意到，如果B=1，那么集束搜索就相当于前面所讲的贪婪搜索。</p><h2 id="改进的集束搜索：Refinements-to-beam-search"><a href="#改进的集束搜索：Refinements-to-beam-search" class="headerlink" title="改进的集束搜索：Refinements to beam search"></a>改进的集束搜索：Refinements to beam search</h2><h3 id="Length-normalization"><a href="#Length-normalization" class="headerlink" title="Length normalization"></a>Length normalization</h3><p>我们知道束搜索所做的是最大化下面的乘积概率：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f40580d0e5bf2929.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>如果计算这些概率值，而这些概率值通常是远小于1，将它们相乘最后会造成数值下溢。因此在实际操作时我们不会真的计算乘积，而是取log值。而最大化这个log求和的概率值，也相当于最大化上面的乘积，得到相同的结果。因此通过取log，我们会得到一个数值上更稳定的算法，不容易出现数值的舍入误差。因为对数函数是严格单调递增的函数，而我们要最大化P(y|x)，因此最大化logP(y|x)和最大化P(y|x)的结果一样。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-6e1b1849b35f0bc5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>而这个目标函数有一个潜在的缺点，就是它会更偏向简短的翻译结果，因为短句子的概率是由更少的小于1的数字乘积得到的，对取log后的目标函数也是一样。因此我们对目标函数做另一个改变，即对这些数值做归一化，从而消除长短句的影响：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ea1cede76ed7b9dc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>但通常上，我们不会直接除以Ty，而是会采取更加柔和的方法，即在Ty上加上指数alpha，alpha=0.7。如果取alpha=1，就是直接除以Ty；而如果alpha=0，那么便没有归一化。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-633005637a01cdda.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>需要明确的是，这个超参数alpha（可调整）的设置并没有理论验证，而是大家都发现在实践中效果很好，所以很多人会这么做。</p><p>总结，在集束搜索中，我们会得到许多不同长度的句子，从1到可能最大设置为30之类的。然后我们针对这些所有的可能的输出句子，用上式对它们打分，取出概率最大的几个句子，再对这些束搜索得到的句子计算这个目标函数，最后从经过评估的这些句子中，挑选出在归一化的log概率目标函数上得分最高的一个。（A normalized log likelihood objective)</p><h3 id="Beam-search-discusstion"><a href="#Beam-search-discusstion" class="headerlink" title="Beam search discusstion"></a>Beam search discusstion</h3><p>如何选择B呢？如果B选择大，那么我们的计算代价也会很大，因为我们要把更多的可能选择保存起来。因此这里总结一下关于如何选择beam width的想法。</p><p>首先，我们知道B的大小会有以下影响：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e800f8190cd7616a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>对一般的产品系统而言，B一般选择为10；而对于科研而言，人们想压榨出全部的性能，也经常能够看到大家用束宽为1000或者3000。因此在实现我们的应用时，尝试不同的束宽值，当B很大的时候，性能提高会越来越小。因此对一般应用而言，我们可以从1到3到10这样调整。</p><p>另外，与BFS和DFS进行对比：Unlike exact search algorithms like BFS(Breadth First Search) and DFS(Depth First Search), Beam Search runs faster but is not guaranteed to find exact maximum for argmax_y(y|x)。</p><h2 id="集束搜索的误差分析：Error-analysis-on-beam-search"><a href="#集束搜索的误差分析：Error-analysis-on-beam-search" class="headerlink" title="集束搜索的误差分析：Error analysis on beam search"></a>集束搜索的误差分析：Error analysis on beam search</h2><p>集束搜索是一种近似搜索算法，也被称作启发式搜索算法。它不总是输出可能性最大的句子，而仅记录着B为前3或者10或是100种可能。如果束搜索算法出现错误，我们就可以用误差分析来发现问题的来源，看看是束搜索算法出现问题，还是我们的RNN模型出现问题。</p><p>假设给定以下例子，其中y*为最标准答案，而y^则是我们算法预测的结果，很明显是糟糕的翻译，其改变了句子的原意。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-728349452f845fdf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>而我们的模型有两部分，一个部分是RNN模型（即encoder-decoder模型），另一部分是束搜索算法。我们需要知道是哪一部分出现了问题。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-057605270bee8438.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>为了知道是哪个部分出现问题，我们需要使用RNN模型计算P(y*|X)和yP(y^|x)，需要注意我们要比较的是归一化后的最优化目标函数值。我们可以有以下结论:</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-d5b5fd3c92e66e87.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>整个误差分析过程如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-792e8d1399886acb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>经过这样的误差分析过程，我们就可以对验证集(dev set)中每一个错误例子，即算法输出了比人工翻译更差的结果的情况，我们可以尝试确定这些错误是搜索算法出了问题，还是生成目标函数的RNN模型出了问题。只有当我们发现束搜索算法造成了大部分错误时，才值得花费努力增大集束宽度；如果我们发现是RNN出错，我们可以考虑是增加正则化还是获取更多训练数据，亦或是尝试其他的网络结构。</p><p>这就是束搜索中的误差分析。这个特定的误差分析过程是十分有用的，它可以用于分析近似最佳算法（如束搜索算法），这些算法被用来优化学习算法（例如序列到序列模型/RNN）输出的目标函数。</p><h1 id="BLEU分数"><a href="#BLEU分数" class="headerlink" title="BLEU分数"></a>BLEU分数</h1><p>论文标题：A method for automatic evaluation of machine translation, 2002</p><h2 id="Evaluating-machine-translation"><a href="#Evaluating-machine-translation" class="headerlink" title="Evaluating machine translation"></a>Evaluating machine translation</h2><p>机器翻译的一大难题是比如给定一个法语句子，可以有多种英文翻译，而且都同样好。那么当具有多种同样好的翻译结果时，应该怎样评估一个机器翻译系统呢？常见的办法是通过BLEU得分来解决。</p><p>比如下面的一个法语句子，两个翻译结果都很好：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-87d9fd7fedc737fc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>而BLEU得分做的就是给定一个机器生成的翻译，它能够自动地计算一个分数来衡量机器翻译的好坏。而只要我们的翻译和人工翻译的结果足够接近，那么它就会得到一个高的BLEU分数。另外，BLEU的全称是bilingual evaluation understudy(双语评估替补)。BLEU分数背后的理念是观察机器生成的翻译，然后看<strong>生成的词是否出现在至少一个人工翻译参考之中</strong>。因此这些人工翻译的参考会出现包含在验证集或是测试集中。</p><p>下面看一个极端的例子。我们假设机器翻译系统(MT)输出了这样一个结果，显然是很糟糕的翻译。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c2f353e76c000a49.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>衡量机器翻译输出质量的方法之一是观察输出结果的每一个词看是否出现在人工翻译参考之中，这被称为机器翻译的精确度(a precision of the machine translation output)。这个情况下，机器翻译输出了7个词，并且每个词都出现在了参考1或参考2中，因此看上去每个词都很合理，所以这个输出的精确度为7/7。然而这显然不正确，因此这就是为什么把出现在参考中的词在MT输出的所有词中所占的比例作为精确度评估标准并不是很有用的原因。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ef76a7acc543f250.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>因此，我们取而代之的是用<strong>改良后的精确度评估方法</strong>(modified precision)，我们把每一个单词的记分上限定为它在参考句子中出现的最多次数。因此在本例中，the在参考1中出现两次，在参考2中出现1次，因此其得分上限为2。因此我们说，这个输出句子的得分为2/7，其中分母就是7个词中单词the总共出现的次数，而分子就是单词the出现的计数，<strong>达到上限(在参考句子中的最大出现次数)时就截断计数</strong>。</p><h2 id="Bleu-score-on-bigrams"><a href="#Bleu-score-on-bigrams" class="headerlink" title="Bleu score on bigrams"></a>Bleu score on bigrams</h2><p>在前面我们都只考虑了单个的单词，但我们也需要考虑成对的单词。接下来我们定义二元词组上的(bigram:指相邻的两个单词)的BLEU得分。（也有可能为三元词组trigram）</p><p>还是上面的例子，但是我们假设机器翻译出了较之前好一些的结果。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-849fb66f9bfba93c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>在该翻译上的二元词组有如下几个：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c469b3db983836f3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>分别计算二元词组的出现次数以及他们的截断值，以及最后计算得到的精确度为：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-edb69916491fd3f4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>更加具体的公式如下，评估的是一元词组乃至n元词组，从而知道机器翻译结果与人工参考翻译相似重复的程度。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-bb36541a937dd52e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>另外，我们可以确信，如果机器翻译结果与人工翻译结果一致，那么P1等等都会等于1。因此为了达到1.0的改良精确度，我们只需要与人工翻译结果其中的一个完全一致就好了。</p><h2 id="Bleu-details"><a href="#Bleu-details" class="headerlink" title="Bleu details"></a>Bleu details</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-b4d68bb849c47616.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>我们会加一个BP = “brevity penalty”(简单惩罚)。事实表明如果我们输出了一个非常短的翻译，那么它更容易得到了一个非常短的翻译，那么它会更容易得到一个高精确度，因为输出的大部分词都可能出现的人工参考中。因此BP就是一个调整因子，来惩罚输出了太短输出结果的翻译系统。</p><p>在之前的讲解中，我们知道拥有单一实数评估指标(a single real number evaluation metric)的重要性，它使得我们可以尝试不同想法，然后选取最好的。因此，BLEU分数对于机器翻译来说，具有革命性的原因是因为它有一个相当不错的（尽管不是完美的）但是非常好的单一实数评估指标，从而<strong>加快了整个机器翻译领域的进程</strong>。实践中，很少人会从零实现一个BLEU分数，如今有很多开源的实现结果。不过今天，BLEU分数被用来评估许多生成文本(generate text)的系统，比如机器翻译系统，也有图像描述系统，即我们用神经网络生成图像描述，然后用BLEU分数看看是否与参考描述相符。</p><p>不过，BLEU没有用于语音识别，因为在语音识别中通常只有一个答案，评估语音识别结果我们可以看是否十分相近或是字字正确(exactly word for word correct)。不过，在图像描述应用中，对于同一图片的不同描述，可能是同样好的。总之，BLEU很重要，能够自动评估翻译结果，从而帮助加快算法开发进程。</p><h1 id="注意力模型"><a href="#注意力模型" class="headerlink" title="注意力模型"></a>注意力模型</h1><p>论文标题：</p><ol><li>Neural Machine Translation by Jointly Learning to Align and Translate, 2014（首次提出attention）</li><li>Show attention and tell: neural image caption generation with visual attention, 2015</li></ol><p>注意力模型已经是深度学习领域中最重要的思想之一。下面开始解释。</p><p>·<br>假设给定一个很长的法语句子。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-a09bbaee23505414.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>在我们的神经网络中，这个绿色的编码器要做的就是读整个句子并且记忆，然后在感知机中传递，而对于紫色的解码网络则将生成英文翻译。其实，人工翻译并不会通过读整个法语句子，再记忆里面的东西，然后从零开始机械式地翻译成一个英语句子。人工翻译首先可能会先翻译出句子的部分，再看下一部分，并翻译，再一直这样下去。我们可以通过一点点地翻译，因为记忆整个长句子是非常困难的。</p><p>因此对于上面的网络，它对于短句子翻译的效果很好，能够有较高的BLEU分数，但对相对长的句子（比如大于30或40个单词的句子），它的表现就会变差。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-19e88609b7b49855.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>而有了注意力模型，我们会发现它的翻译过程和人类很像，即一次只翻译句子的一部分，因此我们也不会看到BLEU分数巨大的下倾(huge dip)。而这个下倾实际上衡量了神经网络记忆一个长句子的能力，这是我们不希望看到的。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-68ad521726cf8790.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="Attention-model-intuition"><a href="#Attention-model-intuition" class="headerlink" title="Attention model intuition"></a>Attention model intuition</h2><p>我们举一个短句来理解注意力模型。我们用双向RNN来对句子中的单词特征集进行计算，然后用另一个RNN来进行翻译。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-1d17f2792463f5d1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>当尝试生成句子的第一个单词翻译时，我们的输入包括S<0>，以及对句子中上下文单词的注意力。直观来想，RNN向前进，一次生成一个词，直到最后生成<code>&lt;EOS&gt;</code>，而alpha&lt;t,t’&gt;告诉我们，当我们在翻译第t个英文词时，应该将多少注意力放置在t’个法语单词上，这允许它在每个时间步会去看周围词距内的法语词要花多少注意力。</0></p><h2 id="Attention-model"><a href="#Attention-model" class="headerlink" title="Attention model"></a>Attention model</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-e56e95d532c2d1d6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这个alpha参数告诉我们上下文有多少取决于我们得到的特征或者我们从不同时间步得到的激活值。所以我们定义上下文的方式实际上来源于被注意力权重加权的不同时间步中的特征值（激活值）。因此更加公式化的注意力权重将会满足非负的条件，它们加起来等于1。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-33d87cf4e97e1f09.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>另外，我们也会输入上下文，对应的公式为：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-486d469a8d6d4a25.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>其中，a&lt;t’&gt;等于前向和后向的激活值：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-5e1da806cf47270a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>因此，alpha<t,t'>就表示y<t>应该在t'时花在a<t'>上的注意力(amount of "attention" y<t> should pay to a<t'>)。换句话说，当我们在t处生成输出词，我们应该花多少注意力在第t’个输入词上面，这是生成输出的其中一步。随后下一个时间步，我们会生成第二个输出，相似的，我们有了一个新的注意力权重集，再找到一个新的方式将他们相加，这就产生了一个新的上下文，允许我们生成第二个词。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c87c2396dbff39fe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></t'></t></t'></t></t,t'></p><p>为了更加清楚，这里采用原论文的图片进行理解：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-25c60a522ee34b85.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>接下来需要了解如何计算注意力权重集。</p><h2 id="计算注意力权重"><a href="#计算注意力权重" class="headerlink" title="计算注意力权重"></a>计算注意力权重</h2><p>从上一节，我们知道：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-bd3f4d025fbe2dfe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>其中，y<t>表示第t个输出词，a&lt;t’&gt;表示第t’时间步的激活值。</t></p><p>我们用下面的式子来计算alpha&lt;t,t’&gt;。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f2e3fc1c0afdd9e3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>因此在此之前，我们需要计算e<t,t'>，然后用softmax计算出上述注意力权重值，保证和为1。如何计算e呢，我们用下面的这个小型神经网络：![image.png](https://upload-images.jianshu.io/upload_images/8636110-2161008f31e0f7e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)其中，s<t-1>就是神经网络在上个时间步的隐藏状态，然后a<t'>则为另一个输入。直观来想就是，如果我们想要决定要花多少注意力在t’的激活值上，那么它在很大程度上取决于我们上一个时间步的隐藏状态的激活值。但我们还没有当前状态的激活值，所以我们会看看生成上一个翻译的RNN的隐藏状态，然后对于每一个位置都看看他们的特征值，因此a<t,t'>和e<t,t'>取决于这两个量。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-68990f07ce95f08d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></t,t'></t,t'></t'></t-1></t,t'></p><p>这个算法的缺点是它要花费三次方的时间，即时间复杂度为O(n^3)。因此如果我们有Tx个输入单词和Ty个输出单词，那么注意力参数的总数会是Tx * Ty。但是在机器翻译的应用中，输入和输出的句子一般不会太长，不过现在也有很多研究在尝试减少这样的消耗。另外也有一篇类似的image captioning论文运用了类似的思想。</p><h2 id="Attention-examples"><a href="#Attention-examples" class="headerlink" title="Attention examples"></a>Attention examples</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-0c7b833ad052eb00.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="语音识别：Speech-recognition"><a href="#语音识别：Speech-recognition" class="headerlink" title="语音识别：Speech recognition"></a>语音识别：Speech recognition</h1><p><img src="https://upload-images.jianshu.io/upload_images/8636110-c4a16133c6266c81.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>首先简单介绍语音识别的流程。假设我们说一个音频片段为”the quick brown fox”，<br><img src="https://upload-images.jianshu.io/upload_images/8636110-54c9846e5b68a13f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>我们希望一个语音识别算法(speech recognition algorithm)，通过输入这段音频，然后输出音频的文本内容。考虑到人的耳朵并不会处理声音的原始波形，而是通过一种特殊的物理结构来测量这些不同频率和强度的声波。因此音频数据的常见预处理步骤，就是运行这个原始的音频片段，然后生成一个声谱图(spectrogram)。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-127a36d36e11c98d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>横轴是时间，纵轴是声音的频率，而图中的不同颜色显示了声波能量的大小(the amount of energy)，也就是不同的时间和频率上这些声音有多大。</p><p>以前的语音学家都是通过基本的音位单元(basic units of sound called phonemes)来表示音频(audio)，然后在end-to-end模型中，我们发现这种音位表示法已经不再必要了，而是可以构建一个系统，通过向系统中输入音频片段(audio clip)，然后直接输出音频的文本。使这种方法成为可能的一件事是使用一个很大的数据集，所以语音研究的数据集一般会超过300个小时，而商业系统已经训练出了上万个小时的数据。在文本音频数据集(transribe audio data sets)同时包含x和y，通过深度学习算法大大推进了语音识别的进程。接下来讲解如何建立一个语音识别系统。</p><h2 id="Attention-model-for-speech-recognition"><a href="#Attention-model-for-speech-recognition" class="headerlink" title="Attention model for speech recognition"></a>Attention model for speech recognition</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-d311db6470d6cfcf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们可以这样做：在横轴上，也就是在输入音频的不同时间帧上，我们可以用注意力模型来输出文本描述。</p><h2 id="CTC-cost-for-speech-recognition"><a href="#CTC-cost-for-speech-recognition" class="headerlink" title="CTC cost for speech recognition"></a>CTC cost for speech recognition</h2><p>论文标题：Connectionist Temporal Classification: Labeling unsegmented sequence data with recurrent neural networks, 2006.</p><p>CTC的全称为Connectionist Temporal Classification。它的算法思想如下：</p><p>假如语音片段内容是某人说”the quick brown fox”，这时候我们使用一个新的网络，结构如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-493bffae94c71bd6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>其中，其输入的x和输出的y数量是一样的，另外，尽管这里画的只是一个简单的单向RNN结构，但实际上它有可能是双向的LSTM结构或者双向的GRU结构，并且通常是很深的模型。需要注意，在语音识别中，通常输入的时间步数量(the number of input time steps)要比输出的时间步数量(the number of output time steps)多出很多。举例：假设有一段10秒的音频，并且特征是100赫兹的，即每秒有100个样本，于是这段10秒的音频片段就有1000个输入。但我们的输出就没有1000个字母或字符，这时候<strong>CTC损失函数</strong>允许RNN生成这样的输出：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-9dbfe3efc6dd3cfa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>这句输出对应的是”the q”。<strong>CTC损失函数的一个基本规则是将空白符之间的重复字符折叠起来</strong>，注意空白符和空格符是不一样的。比如，上图中我们发现空格符是短下划线两边还有两个竖杠，从而用于区分空白符和空格符。</p><p>因此，尽管我们的文本只有”the quick brown fox”包括空格一共有19个字符，但在这样的情况下，通常允许神经网络有重复的字符和插入空白符来使其强制输出1000个字符。</p><h1 id="触发字检测：Trigger-word-detection"><a href="#触发字检测：Trigger-word-detection" class="headerlink" title="触发字检测：Trigger word detection"></a>触发字检测：Trigger word detection</h1><p><img src="https://upload-images.jianshu.io/upload_images/8636110-c4af0063d7252e6b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>下面简单介绍一个能够实现的算法：</p><p>如果刚探测到一个触发字，将对应时间的目标标签设为1，之后的设为0；如果再遇到触发词，也设置为1。很明显这是一个非平衡数据集，0的数量比1多很多。</p><p>解决方法：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-74dfb154c9462061.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>实际上没听懂这里，看看编程能不能懂。</p><h1 id="本周作业"><a href="#本周作业" class="headerlink" title="本周作业"></a>本周作业</h1><h1 id="完成，向NLP继续进军。"><a href="#完成，向NLP继续进军。" class="headerlink" title="完成，向NLP继续进军。"></a>完成，向NLP继续进军。</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;基础模型&quot;&gt;&lt;a href=&quot;#基础模型&quot; class=&quot;headerlink&quot; title=&quot;基础模型&quot;&gt;&lt;/a&gt;基础模型&lt;/h1&gt;&lt;h2 id=&quot;Sequence-to-sequence-model-encoder-decoder-network&quot;&gt;&lt;a hr
      
    
    </summary>
    
      <category term="深度学习" scheme="https://github.com/DesmonDay/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://github.com/DesmonDay/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>第13周-自然语言处理与词嵌入</title>
    <link href="https://github.com/DesmonDay/2019/04/27/deep-learningw13/"/>
    <id>https://github.com/DesmonDay/2019/04/27/deep-learningw13/</id>
    <published>2019-04-27T12:47:25.000Z</published>
    <updated>2019-05-01T15:46:05.977Z</updated>
    
    <content type="html"><![CDATA[<h1 id="词嵌入-Word-Embedding"><a href="#词嵌入-Word-Embedding" class="headerlink" title="词嵌入: Word Embedding"></a>词嵌入: Word Embedding</h1><p>我们之前用的词向量表示法为one-hot向量，但这种表示方法存在很大的缺陷，我们用o_3455表示该向量。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b69751a957a9523b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>比如，苹果和梨具有相似性，但用one-hot向量表示的话，神经网络无法捕捉他们之间的相似性。这是因为两个不同one-hot向量的内积为0，即不同单词之间的距离相同。而很明显，苹果和梨的距离，是要小于苹果和国家的距离的。</p><p>因此我们考虑用特征化后的向量来表示词，举个简单例子，假设有很多不同的特征，使我们得到新的词嵌入表示。我们用e_5391来表示特征化后的向量，此时使用这种向量表示，苹果和橙子之间的距离就很接近了，且算法会发现苹果和橙子要比苹果和国家更相似。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-dad888a611f8f1e8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>当然，实际上我们学习的特征向量是比较复杂的，而不是一个个具体的特征。如果我们学习到了300维的词嵌入，我们通常会把这300维向量嵌入到一个二维空间，从而实现可视化。通常我们用t-SNE算法来实现。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-72b76b6b8ad9cad1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="使用词嵌入：Using-word-embeddings"><a href="#使用词嵌入：Using-word-embeddings" class="headerlink" title="使用词嵌入：Using word embeddings"></a>使用词嵌入：Using word embeddings</h2><p>先举一个命名实体识别的例子。假设我们的训练集很小，甚至都不知道durian(榴莲)这个词。那么我们可以用事先训练出来的词向量。<br>学习词向量的算法会从大量的文本中学习，所以即使我们的训练集很小，如果我们使用已训练出来的词向量，那么结果也不会差。这也算是迁移学习的一种，即我们将从大量文本中学习到的词向量，应用到自己的任务上。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-3fca17c45498af4c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>在本例中，我们可以通过后文的farmer来意识到前面的单词是人名，因此最好使用双向RNN模型。</p><p>下面总结用词嵌入做迁移学习的步骤。</p><ol><li>Learn word embeddings from large text corpus.(1-100B words), or download pre-trained embedding online.</li><li>Transfer embedding to new task with smaller training set.(say, 100k words.) 比如用一个300维的词嵌入来表示单词，这样的好处是可以用更低维度的特征向量来代替原来的10,000维的one-hot向量，相比之下低维向量会更加紧凑。</li><li>Optional: Continue to finetune the word embeddings with new data. 这一步一般在我们的数据集比较大的时候才做。</li></ol><p>当我们的训练集相对较小时，词嵌入的作用最明显，因此其广泛用于NLP领域。比如它已经用在了命名实体识别、文本摘要、文本解析、指代消解里，这些都是很标准的NLP任务；词嵌入在语言模型、机器翻译领域用得少一些，尤其是我们做语言模型或者机器翻译任务时，这些任务我们有大量的数据。在其他的迁移学习场景下也一样，如果我们从某一任务A迁移到某一个任务B，只有A中有大量数据，B中数据少时，迁移才有用。</p><p>这里还举了与人脸识别的不同。对于人脸识别，我们训练一个Siamese网络来学习不同人脸的128维表示，然后比较编码结果来判断两个图片是否为同一个人脸。只要输入一个人脸，就返回一个编码结果。两者对比，人脸编码可能涉及海量的图片，而自然语言处理有一个固定的词汇表，像没有出现的就标记为”\<unk\>“。</unk\></p><h2 id="词嵌入的特性"><a href="#词嵌入的特性" class="headerlink" title="词嵌入的特性"></a>词嵌入的特性</h2><p>词嵌入的一个特性是可以帮助实现类比推理。我们希望词嵌入可以捕捉单词的特征表示，假如我们提出一个问题，man对应woman，那么king对应什么？用词嵌入可以实现这种推导。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ce068fbd6d430d76.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>如图，通过e_man-e_woman和e_king-e_queen，我们发现他们的主要差异，通过向量表示，可以发现是gender(性别)上的差异。所以得出这种类比推理的结果方法是，当算法被问及man对应woman，那king对应什么时，算法所做的就是计算e_man-e_woman，然后找出一个向量，使得e_man-e_woman约等于e_king-e_?。也就是说，当这个新词为queen时，式子近似相等。这种思想帮助很多研究者对词嵌入领域建立了更深刻的理解。</p><p>论文标题：Linguistic regularities in continuous space word representations, 2013.</p><p>实现类比推理的方法即找到相似度最大的单词：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-4f6bfafe877960c8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>另外，注意到我们用t-SNE算法映射到的二维空间，不一定能够像左图那样呈现出一个平行四边形，因为这种映射是使用了一种很复杂的非线性映射。</p><p>下面列举常用的相似度函数：</p><ol><li>余弦相似度（常用来衡量词嵌入之间的相似度）：<br> <img src="https://upload-images.jianshu.io/upload_images/8636110-676805ea293c9357.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></li><li>欧式距离：<br> <img src="https://upload-images.jianshu.io/upload_images/8636110-dd85369ffbc4f0ba.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></li></ol><p>通过在大量的语料库上学习，词嵌入算法可以发现像下面这样的类比推理(analogy reasoning)模式：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-4de132349dfb7b1b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="词嵌入矩阵"><a href="#词嵌入矩阵" class="headerlink" title="词嵌入矩阵"></a>词嵌入矩阵</h2><p>将嵌入矩阵E与单词对应的one-hot矩阵相乘，我们可以得到对应单词的词向量，即E·O_j = E_j。但一般我们实际上是用专门的函数，即找出对应的列，来找到对应单词的词向量，这样更高效，比如Keras有个函数叫keras.layers.Embedding可以实现。我们在学习的时候，会随机初始化E矩阵，然后通过梯度下降方法来求出E。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ada21eecc44bdd3f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="具体算法学习词嵌入"><a href="#具体算法学习词嵌入" class="headerlink" title="具体算法学习词嵌入"></a>具体算法学习词嵌入</h2><p>论文标题：A neural probabilistic language model, 2003.</p><p>介绍一个早期最成功的用于学习嵌入矩阵E的NLP模型，比如假定给出四个单词，预测下一个单词会是什么。：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b3958ee014798af9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>Other context/target pairs</strong>: </p><p>在前面我们知道了算法预测出了某个单词juice，我们将其称为target words，它是通过前面的context(last 4 words)学到的。所以如果我们的目标是学习一个嵌入向量，研究人员已经尝试过很多不同类型的上下文，<strong>如果我们要构建一个语言模型，那么一般选取目标词之前的几个词作为上下文；但如果我们的目标不是学习语言模型本身，而是学习词嵌入，那么我们可以选择其他上下文。</strong></p><p>比如，我们可以提出一个学习问题，而它的上下文是左边和右边的各四个词，即我们可以把target word左右两个的词作为上下文。因此我们的算法获得了a glass of orange和to go along with，然后要求预测出中间这个词。提出这样一个问题，这个问题需要将左边4个词和右边4个词的嵌入向量提供给神经网络，来预测中间的单词是什么。或者上下文只有前一个词的嵌入向量，然后用来预测下一个词。或者上下文可以是附近的一个词，比如glass，利用它来推导juice。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-cef310702e888287.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>而这种利用上下文前后两个单词的思想与Word2Vec中的skip gram模型一致，下文会介绍Word2Vec.</p><h2 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h2><p>论文标题：Efficient estimation of word representations in vector space, 2013.</p><p>假设给定一个句子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-bebd48a7b239949a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>在skip gram模型中，我们要做的是抽取上下文和目标词配对，来构造一个监督学习问题。而我们的上下文不一定总是在目标词之前离得最近的n个单词。我们会随机选一个词作为context word，如orange，然后我们要做的是随机在一定词距内(比如context word前后5个词或10个词内）选择目标词target word，比如juice。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-bccd935a4e58cc20.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>因此我们构造一个监督学习问题，它给定context word，要求你预测在这个词正负10个词距或者5个词距内随机选择的某个目标词。而构造这个监督学习问题的目标并不是解决这个监督学习问题本身，而是我们想要用这个学习问题来学到一个好的词嵌入模型。</p><h3 id="Skip-gram模型"><a href="#Skip-gram模型" class="headerlink" title="Skip gram模型"></a>Skip gram模型</h3><p>假设我们的单词总数量为10,000.那么给定一个context word作为输入，我们要求预测出target word。（这里之所以叫skip-gram，就是因为我们预测的是context word从左数或者右数的某个target word。）<br><img src="https://upload-images.jianshu.io/upload_images/8636110-54695b88b17d7d4b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们的网络结构是这样的：输入context word的one-hot向量，然后经过嵌入矩阵的相乘得到e_c，再通过一个Softmax单元得到目标词的one-hot表示。因此这里的参数有两个部分，一个是嵌入矩阵本身，一个是softmax单元本身的参数。所以我们的网络结构和具体Softmax函数如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-6fa505f529777475.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>而Softmax函数的损失函数如下，注意到y和y_hat都是one-hot向量：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ee88a2a23705f4a9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>而Skip-gram模型实际上存在一些问题，尤其是在softmax模型中，每次我们想要计算概率时，我们需要对词汇表的所有词做求和运算，而这个词汇表可能会很大，那么分母的求和操作就会相当慢。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-9b88697bbc563b43.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>解决方法有Hierachical softmax（分级的Softmax分类器），简单来说就是不是一开始就确定到底是属于哪一类，而是先告诉我们该词是属于哪一级别，相当于利用一个树形结构。我们使用的是霍夫曼树，相当于使用了二元分类，即二元逻辑回归的方法。在实践中，分级的Softmax分类器会被构造成常用词在顶部，而不常用词则在树的深部，即不对称的二叉树（不同的经验法则）。更具体的解释：<a href="https://www.cnblogs.com/pinard/p/7243513.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/7243513.html</a><br><img src="https://upload-images.jianshu.io/upload_images/8636110-0e29831514e81939.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>接下来我们需要理解，怎么对上下文C进行采样？一旦我们对上下文进行采样，那么目标词t就会在上下文前后词距比如10的单词中进行采样。一种选择是我们可以在语料库中随机均匀地采样，这样做我们会发现有一些词像the/a/of/and会频繁出现，而其他的apple/durian的词则很少出现，这是我们不希望的情况（很多时间更新频繁词）；因此我们通常采用一些启发式方法来平衡频繁词和普通词的采样。</p><h3 id="CBOW模型-Continuous-Bag-of-Words"><a href="#CBOW模型-Continuous-Bag-of-Words" class="headerlink" title="CBOW模型(Continuous Bag-of-Words)"></a>CBOW模型(Continuous Bag-of-Words)</h3><p>CBOW模型通过获得中间词的上下文，然后用这些周围的词去预测中间的词。</p><h3 id="负采样：Negative-Sampling"><a href="#负采样：Negative-Sampling" class="headerlink" title="负采样：Negative Sampling"></a>负采样：Negative Sampling</h3><p><img src="https://upload-images.jianshu.io/upload_images/8636110-bebd48a7b239949a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>负采样可以比较好的解决Skip gram模型的计算问题。我们在这个算法中要做的是构造一个新的监督学习问题，比如给定一对单词，如orange和juice，我们要去预测这是否是context-target pair？在这个例子中，orange和juice就是一个正样本，即为1。而比如orange和king，我们将其视为负样本，即为0。所以我们要做的是采样得到一个context word和target word，也就是表中的第一样，给出了一个正样本；接着我们再使用相同的上下文词，在词典中随机选取几个词（如果我们的词在上下文词的词距中也没关系），作为负样本。接着我们构造的监督问题就是输入这对词，然后去预测目标的标签，即预测输出y。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-1d0c7b574085f460.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>我们的目的就是区分这两个词是通过对靠近的两个词采样，还是随机采样的。我们的目的就是区分两种不同的采样方式。</p><p>所以这就是如何生成训练集的方法。而如何选择K呢？如果是小数据集，那么K从5到20比较好，但如果是大数据集，那么K为2-5。在本例中，我们选择K为4.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b7fdd6568e488ecc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>对应的模型：原本我们使用的是Softmax分类器，但是计算成本过高。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b0b75a550cdb0e21.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>因此我们采用负采样的方法来进行训练。此时我们的负采样输入和输出分别为：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-708c9138ab4bf471.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>所以我们会使用<strong>二元逻辑回归分类器</strong>来判断是否为正样本还是负样本。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-81e6db3c34f1e23c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>因此整个的Skip gram模型优化后如下。（给定上下文词orange，通过与嵌入矩阵E相乘得到嵌入向量，然后我们会得到10,000个可能的logistic回归问题，其中一个将会是用来判断目标词是否是juice的分类器，而其他的可能用来预测king是否是目标词之类的。把他们看成10,000个二元分类器，但并不是每次迭代都训练全部的10,000个，即<strong>每次迭代我们只训练其他的K+1个分类器</strong>。这样每次迭代的成本要比Softmax小很多。）<br><img src="https://upload-images.jianshu.io/upload_images/8636110-0ecb9a6fe27f1b58.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这个算法还有一个细节，即如何选择负采样的样本？我们可以对候选的目标词进行采样，可以根据其在语料中的经验频率进行采样（会导致the,of,and等多次被采样），而另一个极端就是用1除以词汇表总词数，均匀且随机地抽取负样本。而论文的作者发现的一个经验方法是既不用经验频率，也不是均匀采样，而可以用介于他们之间的方法。他们做的是对词频的3/4次方除以整体的值进行采样。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d6754994b242345a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="GloVe词向量"><a href="#GloVe词向量" class="headerlink" title="GloVe词向量"></a>GloVe词向量</h2><p>论文标题：Global vector for word representation, 2014.</p><p>在之前，我们曾通过挑选语料库中位置相近的两个词，列举出词对，即上下文和目标词，而GloVe算法做的是使其关系明确化。假设X_ij是单词i在单词j上下文中出现的次数，那么这里i和j的功能就和t和c的功能一样，所以我们可以认为X_ij等同于X_tc。根据上下文和目标词的定义，我们可以得出X_ij等于X_ji的结论。事实上，如果我们将上下文和目标词的范围定义为出现于左右各1词以内的话，就会有对称关系，但如果上下文总是目标词前一个词的话，那就不对称了。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-aab282e606d3b427.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>对于GloVe算法，我们可以定义上下文和目标词Wie任意两个位置相近的单词，假设是左右各10个词额距离，那么X_ij就是一个能够获取单词i和单词j出现位置相近时或者彼此接近的频率的计算器。</p><p>Glove模型做的就是最小化他们之间的差距：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-dd3925abbecc8c53.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>而公式中的点乘就是要告诉我们这两个单词之间有多少联系，t和c之间有多紧密，i和j之间联系程度如何，换句话说他们同时出现的频率是多少，这是由X_ij影响的。接着我们需要解决参数theta和e的问题，然后用梯度下降法来最小化上面的公式。需要补充的细节是，如果X_ij=0，那么log0是未定义的，所以我们添加了一个额外的加权项f(X_ij)(weighting term)。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ebbe984ad95ececd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>如果X_ij等于0，我们会约定0log0=0。因此这个求和公式表明，这个和仅是一个上下文和目标词关系里连续出现至少一次的词对的和。f(X_ij)的另一个作用是，有些词在英语里出现十分频繁，比如this,is,of,a等等，这些词称为”停止词”，在频繁词和不常用词之间也会有一个连续体(continumm: 相邻两者相似但起首与末尾截然不同的)。另外也有一些不常用词，比如durion，但我们还是想考虑在内，但又不像常用词那么频繁。因此，这个加权项f(X_ij)就可以是一个函数，给予这些出现频率不同的词不同的权重。（具体可以看GloVe算法的论文）</p><p>最后一个关于此算法有趣的事是theta和e是完全堆成的。因此有一种训练算法的方法是一致地初始化theta和e，然后使用梯度下降来最小化输出。当每个词都处理完之后去平均值，所以给定一个词w，我们有：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ebd4600cd0ec5c0a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="A-note-on-the-featurization-view-of-embeddings"><a href="#A-note-on-the-featurization-view-of-embeddings" class="headerlink" title="A note on the featurization view of embeddings"></a>A note on the featurization view of embeddings</h2><p>在前面讲词嵌入的时候，我们是用下面这样一个简单的例子来解释的。但是，实际上我们训练出来的词向量很难对每个维度有这样清楚的理解，即很难知道哪个轴代表gender之类的。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ef901ebc6cf6b990.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>例如，对于我们在前面学到的GloVe算法：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-70c70df5fa9db494.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>我们可以乘积项利用线性代数的知识表示为：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-105665c3bc9f3fd4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>我们就知道，我们不能保证这些用来表示特征的轴能够等同于人类可能简单理解的轴。具体而言，第一个特征可能是gender/roya/age/foot/等的组合，它也许是名词或是一个行为动词和其他所有特征的组合，所以很难看出独立组成部分。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-4e8e1a43202079f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>尽管有这种类型的线性变换，但是这个平行四边形映射也说明我们解决了这个问题。因此尽管存在特征量潜在的任意线性变换，我们最终还是能学习出解决类似问题的平行四边形映射。</p><h1 id="情感分类：Sentiment-classification"><a href="#情感分类：Sentiment-classification" class="headerlink" title="情感分类：Sentiment classification"></a>情感分类：Sentiment classification</h1><p>问题阐述，一般是对评论进行分类：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d77d210646ecbd19.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>情感分类的一个最大挑战是可能标记的训练集没有那么多。对于情感分类任务来说，训练集大小从10,000到100,000个单词都很常见，也可能小于10,000个单词，而使用词嵌入能够带来更好的效果，尤其是只有很小的训练集时。</p><p>一个简单的情感分类的模型如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f71bba9251809c51.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>注意到取平均的操作使得我们的模型可以适用于任意长短的评论。</p><p>而这个算法的一个问题是没有考虑词序。尤其是，对于这样一个负面的评价”Completely lacking in good taste, good service, and good ambience.”，但由于good出现了很多次，那么仅仅平均或求和得到的嵌入向量可能会多次出现good的含义，因此我们的分类器可能会认为这是一个正面的评价。所以我们可以用一个RNN模型来做情感分类。</p><h2 id="RNN-for-sentiment-classification"><a href="#RNN-for-sentiment-classification" class="headerlink" title="RNN for sentiment classification"></a>RNN for sentiment classification</h2><p>我们可以使用如下的RNN模型：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-a347f55b5b01ff85.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>这个模型就是我们之前所介绍过的many-to-one结构。有了这样的模型，考虑词的顺序，这样就会有更好的效果了。</p><h1 id="词嵌入除偏：Debias-word-embedding"><a href="#词嵌入除偏：Debias-word-embedding" class="headerlink" title="词嵌入除偏：Debias word embedding"></a>词嵌入除偏：Debias word embedding</h1><p>论文标题：Man is computer programmer as woman is homemaker? Debiasing word embeddings 2016</p><p>现在机器学习和人工智能算法正渐渐地被信任用以辅助或者指定极其重要的决策，因此我们想尽可能地确保它们不受<strong>非预期形式偏见</strong>影响，比如性别歧视(gender bias)、种族歧视(ethnic bias)等等。本节会展示词嵌入中一些有关减少或是消除这些形式的偏见的方法。</p><p>常见词嵌入bias如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-055bea92b4d8f957.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>具体来说，word embeddings can reflect gender, ethnicity, age, sexual orientation, and other biases of the text used to train the model. 通常，这些偏见都和社会经济状态相关。</p><p>假设下面这些词的嵌入画在平面图如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-90897e106c8e4e4b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>第一步我们要做的是identify bias direction（定义步），比如采用取平均的方法：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d90327669786993e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>（实际上取平均的算法过于简单，原论文里不是这样做的，而是做了奇异值分解，从而确定了bias direction，也就是偏见的方向。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b1729f55eb91a9bc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>第二步是Neutralize（中和步）: For every word that is not definitional（定义不明确）, project to get rid of bias，比如图片上的babysitter、docter。对于这样的词，我们可以减小将它们在bias direction上进行处理，来减少或消除它们性别歧视趋势的成分。</p><p>第三步是Equalize pairs(均衡步)。比如我们有这样的词对grandmother和grandfather，girl和boy，我们只希望这些词对的不同体现在性别上，确保和babysitter和docter之类的词有相似的距离。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-158cd369401bcab3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>所以论文作者所做的就是尝试训练一个分类器，来尝试解决哪些词是明确定义的，哪些词不是。结果表明大部分英文单词在性别方面是没有明确定义的，而只有一部分词不是性别中立的。</p><h1 id="本周作业"><a href="#本周作业" class="headerlink" title="本周作业"></a>本周作业</h1><h2 id="Operations-on-word-vectors"><a href="#Operations-on-word-vectors" class="headerlink" title="Operations on word vectors"></a>Operations on word vectors</h2><h3 id="1-Cosine-similarity"><a href="#1-Cosine-similarity" class="headerlink" title="1- Cosine similarity"></a>1- Cosine similarity</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cosine_similarity</span><span class="params">(u, v)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Cosine similarity reflects the degree of similariy between u and v</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        u -- a word vector of shape (n,)          </span></span><br><span class="line"><span class="string">        v -- a word vector of shape (n,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        cosine_similarity -- the cosine similarity between u and v defined by the formula above.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    distance = <span class="number">0.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Compute the dot product between u and v (≈1 line)</span></span><br><span class="line">    dot = np.dot(u,v)</span><br><span class="line">    <span class="comment"># Compute the L2 norm of u (≈1 line)</span></span><br><span class="line">    norm_u = np.sqrt(np.sum(u*u))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute the L2 norm of v (≈1 line)</span></span><br><span class="line">    norm_v = np.sqrt(np.sum(v*v))</span><br><span class="line">    <span class="comment"># Compute the cosine similarity defined by formula (1) (≈1 line)</span></span><br><span class="line">    cosine_similarity = dot / (norm_u * norm_v)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cosine_similarity</span><br></pre></td></tr></table></figure><h3 id="2-Word-analogy-task"><a href="#2-Word-analogy-task" class="headerlink" title="2- Word analogy task"></a>2- Word analogy task</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">complete_analogy</span><span class="params">(word_a, word_b, word_c, word_to_vec_map)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Performs the word analogy task as explained above: a is to b as c is to ____. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    word_a -- a word, string</span></span><br><span class="line"><span class="string">    word_b -- a word, string</span></span><br><span class="line"><span class="string">    word_c -- a word, string</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary that maps words to their corresponding vectors. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># convert words to lower case</span></span><br><span class="line">    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Get the word embeddings v_a, v_b and v_c (≈1-3 lines)</span></span><br><span class="line">    e_a, e_b, e_c = word_to_vec_map[word_a],word_to_vec_map[word_b],word_to_vec_map[word_c]</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    words = word_to_vec_map.keys()</span><br><span class="line">    max_cosine_sim = <span class="number">-100</span>              <span class="comment"># Initialize max_cosine_sim to a large negative number</span></span><br><span class="line">    best_word = <span class="keyword">None</span>                   <span class="comment"># Initialize best_word with None, it will help keep track of the word to output</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># loop over the whole word vector set</span></span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> words: <span class="comment"># w is string</span></span><br><span class="line">        <span class="comment"># to avoid best_word being one of the input words, pass on them.</span></span><br><span class="line">        <span class="keyword">if</span> w <span class="keyword">in</span> [word_a, word_b, word_c] :</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">### START CODE HERE ###</span></span><br><span class="line">        <span class="comment"># Compute cosine similarity between the combined_vector and the current word (≈1 line)</span></span><br><span class="line">        cosine_sim = cosine_similarity(e_b-e_a, word_to_vec_map[w]-e_c)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># If the cosine_sim is more than the max_cosine_sim seen so far,</span></span><br><span class="line">            <span class="comment"># then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (≈3 lines)</span></span><br><span class="line">        <span class="keyword">if</span> cosine_sim &gt; max_cosine_sim:</span><br><span class="line">            max_cosine_sim = cosine_sim</span><br><span class="line">            best_word = w</span><br><span class="line">        <span class="comment">### END CODE HERE ###</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> best_word</span><br></pre></td></tr></table></figure><h3 id="3-Debiasing-word-vectors-OPTIONAL-UNGRADED"><a href="#3-Debiasing-word-vectors-OPTIONAL-UNGRADED" class="headerlink" title="3- Debiasing word vectors (OPTIONAL/UNGRADED)"></a>3- Debiasing word vectors (OPTIONAL/UNGRADED)</h3><h4 id="3-1-Neutralize-bias-for-non-gender-specific-words"><a href="#3-1-Neutralize-bias-for-non-gender-specific-words" class="headerlink" title="3.1- Neutralize bias for non-gender specific words"></a>3.1- Neutralize bias for non-gender specific words</h4><p><img src="https://upload-images.jianshu.io/upload_images/8636110-d7339d5c8b5fe9e5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-ac20abc1877f2d6e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neutralize</span><span class="params">(word, g, word_to_vec_map)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Removes the bias of "word" by projecting it on the space orthogonal to the bias axis. </span></span><br><span class="line"><span class="string">    This function ensures that gender neutral words are zero in the gender subspace.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        word -- string indicating the word to debias</span></span><br><span class="line"><span class="string">        g -- numpy-array of shape (50,), corresponding to the bias axis (such as gender)</span></span><br><span class="line"><span class="string">        word_to_vec_map -- dictionary mapping words to their corresponding vectors.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        e_debiased -- neutralized word vector representation of the input "word"</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Select word vector representation of "word". Use word_to_vec_map. (≈ 1 line)</span></span><br><span class="line">    e = word_to_vec_map[word]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Compute e_biascomponent using the formula give above. (≈ 1 line)</span></span><br><span class="line">    </span><br><span class="line">    e_biascomponent = np.dot(e,g) / np.square(np.sqrt(np.sum(g**<span class="number">2</span>))) * g</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#e_biascomponent = np.dot(e, g) / np.square(np.linalg.norm(g)) * g</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># Neutralize e by substracting e_biascomponent from it </span></span><br><span class="line">    <span class="comment"># e_debiased should be equal to its orthogonal projection. (≈ 1 line)</span></span><br><span class="line">    e_debiased = e - e_biascomponent</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> e_debiased</span><br></pre></td></tr></table></figure><h4 id="3-2-Equalization-algorithm-for-gender-specific-words"><a href="#3-2-Equalization-algorithm-for-gender-specific-words" class="headerlink" title="3.2- Equalization algorithm for gender-specific words"></a>3.2- Equalization algorithm for gender-specific words</h4><p>Next, lets see how debiasing can also be applied to word pairs such as “actress” and “actor.” Equalization is applied to pairs of words that you might want to have differ only through the gender property. As a concrete example, suppose that “actress” is closer to “babysit” than “actor.” By applying neutralizing to “babysit” we can reduce the gender-stereotype associated with babysitting. But this still does not guarantee that “actor” and “actress” are equidistant from “babysit.” The equalization algorithm takes care of this.</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-681c53121600f01a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>The derivation of the linear algebra to do this is a bit more complex. (See Bolukbasi et al., 2016 for details.) But the key equations are:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ac96125437c58c03.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">equalize</span><span class="params">(pair, bias_axis, word_to_vec_map)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Debias gender specific words by following the equalize method described in the figure above.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    pair -- pair of strings of gender specific words to debias, e.g. ("actress", "actor") </span></span><br><span class="line"><span class="string">    bias_axis -- numpy-array of shape (50,), vector corresponding to the bias axis, e.g. gender</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping words to their corresponding vectors</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    e_1 -- word vector corresponding to the first word</span></span><br><span class="line"><span class="string">    e_2 -- word vector corresponding to the second word</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Step 1: Select word vector representation of "word". Use word_to_vec_map. (≈ 2 lines)</span></span><br><span class="line">    w1, w2 = pair</span><br><span class="line">    e_w1, e_w2 = word_to_vec_map[w1], word_to_vec_map[w2]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: Compute the mean of e_w1 and e_w2 (≈ 1 line)</span></span><br><span class="line">    mu = (e_w1 + e_w2) / <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 3: Compute the projections of mu over the bias axis and the orthogonal axis (≈ 2 lines)</span></span><br><span class="line">    mu_B = np.dot(mu, bias_axis) / np.sum(bias_axis**<span class="number">2</span>) * bias_axis</span><br><span class="line">    mu_orth = mu - mu_B</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Step 4: Set e1_orth and e2_orth to be equal to mu_orth (≈2 lines)</span></span><br><span class="line">    e_w1B = np.dot(e_w1, bias_axis) / np.sum(bias_axis**<span class="number">2</span>) * bias_axis</span><br><span class="line">    e_w2B = np.dot(e_w2, bias_axis) / np.sum(bias_axis**<span class="number">2</span>) * bias_axis</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># Step 5: Adjust the Bias part of u1 and u2 using the formulas given in the figure above (≈2 lines)</span></span><br><span class="line">    corrected_e_w1B = np.sqrt(np.abs(<span class="number">1</span>-np.sum(mu_orth**<span class="number">2</span>))) * ((e_w1B-mu_B)/np.linalg.norm(e_w1-mu_orth-mu_B))</span><br><span class="line">    corrected_e_w2B = np.sqrt(np.abs(<span class="number">1</span>-np.sum(mu_orth**<span class="number">2</span>))) * ((e_w2B-mu_B)/np.linalg.norm(e_w2-mu_orth-mu_B))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 6: Debias by equalizing u1 and u2 to the sum of their projections (≈2 lines)</span></span><br><span class="line">    e1 = corrected_e_w1B + mu_orth</span><br><span class="line">    e2 = corrected_e_w2B + mu_orth</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> e1, e2</span><br></pre></td></tr></table></figure><p>Please feel free to play with the input words in the cell above, to apply equalization to other pairs of words.</p><p>These debiasing algorithms are very helpful for reducing bias, but are not perfect and do not eliminate all traces of bias. For example, one weakness of this implementation was that the bias direction g was defined using only the pair of words woman and man. As discussed earlier, if g were defined by computing g1=e_woman−e_man; g2=em_other−e_father; g3=e_girl−e_boy; and so on and averaging over them, you would obtain a better estimate of the “gender” dimension in the 50 dimensional word embedding space. Feel free to play with such variants as well.</p><h2 id="Emojify"><a href="#Emojify" class="headerlink" title="Emojify!"></a>Emojify!</h2><h3 id="1-Baseline-model-Emojifier-V1"><a href="#1-Baseline-model-Emojifier-V1" class="headerlink" title="1- Baseline model: Emojifier-V1"></a>1- Baseline model: Emojifier-V1</h3><h4 id="1-1-Dataset-EMOJISET"><a href="#1-1-Dataset-EMOJISET" class="headerlink" title="1.1- Dataset EMOJISET"></a>1.1- Dataset EMOJISET</h4><p>Let’s start by building a simple baseline classifier. </p><p>You have a tiny dataset (X, Y) where:</p><ul><li>X contains 127 sentences (strings)</li><li>Y contains a integer label between 0 and 4 corresponding to an emoji for each sentence</li></ul><p><img src="https://upload-images.jianshu.io/upload_images/8636110-5988832092fefe2d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="1-2-Overview-of-the-Emojifier-V1"><a href="#1-2-Overview-of-the-Emojifier-V1" class="headerlink" title="1.2- Overview of the Emojifier-V1"></a>1.2- Overview of the Emojifier-V1</h4><p>In this part, you are going to implement a baseline model called “Emojifier-v1”.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-477d8bda868724b8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>To get our labels into a format suitable for training a softmax classifier, lets convert  YY  from its current shape current shape (m,1) into a “one-hot representation” (m,5), where each row is a one-hot vector giving the label of one example, You can do so using this next code snipper. Here, Y_oh stands for “Y-one-hot” in the variable names Y_oh_train and Y_oh_test:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Y_oh_train = convert_to_one_hot(Y_train, C = <span class="number">5</span>)</span><br><span class="line">Y_oh_test = convert_to_one_hot(Y_test, C = <span class="number">5</span>)</span><br></pre></td></tr></table></figure><h4 id="1-3-Implementing-Emojifier-V1"><a href="#1-3-Implementing-Emojifier-V1" class="headerlink" title="1.3- Implementing Emojifier-V1"></a>1.3- Implementing Emojifier-V1</h4><p>As shown in Figure (2), the first step is to convert an input sentence into the word vector representation, which then get averaged together. Similar to the previous exercise, we will use pretrained 50-dimensional GloVe embeddings. Run the following cell to load the word_to_vec_map, which contains all the vector representations.<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">word_to_index, index_to_word, word_to_vec_map = read_glove_vecs(<span class="string">'data/glove.6B.50d.txt'</span>)</span><br></pre></td></tr></table></figure></p><p>对词向量取平均：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentence_to_avg</span><span class="params">(sentence, word_to_vec_map)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Converts a sentence (string) into a list of words (strings). Extracts the GloVe representation of each word</span></span><br><span class="line"><span class="string">    and averages its value into a single vector encoding the meaning of the sentence.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    sentence -- string, one training example from X</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    avg -- average vector encoding information about the sentence, numpy-array of shape (50,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Step 1: Split sentence into list of lower case words (≈ 1 line) 小写</span></span><br><span class="line">    words = list(sentence.lower().split())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize the average word vector, should have the same shape as your word vectors.</span></span><br><span class="line">    avg = np.zeros((<span class="number">50</span>,))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: average the word vectors. You can loop over the words in the list "words".</span></span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> words:</span><br><span class="line">        avg += word_to_vec_map[w]</span><br><span class="line">    avg = avg / len(words)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> avg</span><br></pre></td></tr></table></figure></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-74b3e105e731b29c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>模型：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model</span><span class="params">(X, Y, word_to_vec_map, learning_rate = <span class="number">0.01</span>, num_iterations = <span class="number">400</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Model to train word vector representations in numpy.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- input data, numpy array of sentences as strings, of shape (m, 1)</span></span><br><span class="line"><span class="string">    Y -- labels, numpy array of integers between 0 and 7, numpy-array of shape (m, 1)</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation</span></span><br><span class="line"><span class="string">    learning_rate -- learning_rate for the stochastic gradient descent algorithm</span></span><br><span class="line"><span class="string">    num_iterations -- number of iterations</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    pred -- vector of predictions, numpy-array of shape (m, 1)</span></span><br><span class="line"><span class="string">    W -- weight matrix of the softmax layer, of shape (n_y, n_h)</span></span><br><span class="line"><span class="string">    b -- bias of the softmax layer, of shape (n_y,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    np.random.seed(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define number of training examples</span></span><br><span class="line">    m = Y.shape[<span class="number">0</span>]                          <span class="comment"># number of training examples</span></span><br><span class="line">    n_y = <span class="number">5</span>                                 <span class="comment"># number of classes  </span></span><br><span class="line">    n_h = <span class="number">50</span>                                <span class="comment"># dimensions of the GloVe vectors </span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize parameters using Xavier initialization</span></span><br><span class="line">    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)</span><br><span class="line">    b = np.zeros((n_y,))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Convert Y to Y_onehot with n_y classes</span></span><br><span class="line">    Y_oh = convert_to_one_hot(Y, C = n_y) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Optimization loop</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(num_iterations):                       <span class="comment"># Loop over the number of iterations</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                                <span class="comment"># Loop over the training examples</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment">### START CODE HERE ### (≈ 4 lines of code)</span></span><br><span class="line">            <span class="comment"># Average the word vectors of the words from the i'th training example</span></span><br><span class="line">            avg = sentence_to_avg(X[i], word_to_vec_map)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Forward propagate the avg through the softmax layer</span></span><br><span class="line">            z = np.dot(W,avg)+b</span><br><span class="line">            a = softmax(z)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute cost using the j'th training label's one hot representation and "A" (the output of the softmax)</span></span><br><span class="line">            cost = -np.sum(Y_oh[i]*np.log(a))</span><br><span class="line">            <span class="comment">### END CODE HERE ###</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Compute gradients </span></span><br><span class="line">            dz = a - Y_oh[i]</span><br><span class="line">            dW = np.dot(dz.reshape(n_y,<span class="number">1</span>), avg.reshape(<span class="number">1</span>, n_h))</span><br><span class="line">            db = dz</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Update parameters with Stochastic Gradient Descent</span></span><br><span class="line">            W = W - learning_rate * dW</span><br><span class="line">            b = b - learning_rate * db</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> t % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"Epoch: "</span> + str(t) + <span class="string">" --- cost = "</span> + str(cost))</span><br><span class="line">            pred = predict(X, Y, W, b, word_to_vec_map)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pred, W, b</span><br></pre></td></tr></table></figure></p><h4 id="1-4-Examining-test-set-performance"><a href="#1-4-Examining-test-set-performance" class="headerlink" title="1.4- Examining test set performance"></a>1.4- Examining test set performance</h4><p><img src="https://upload-images.jianshu.io/upload_images/8636110-22d4157616527d4b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-820057ddf5210855.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Printing the confusion matrix can also help understand which classes are more difficult for your model. A confusion matrix shows how often an example whose label is one class (“actual” class) is mislabeled by the algorithm with a different class (“predicted” class).<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f62b12a091ddf0cf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>What you should remember from this part:</p><ul><li>Even with a 127 training examples, you can get a reasonably good model for Emojifying. This is due to the generalization power word vectors gives you.</li><li>Emojify-V1 will perform poorly on sentences such as “This movie is not good and not enjoyable” because it doesn’t understand combinations of words—it just averages all the words’ embedding vectors together, without paying attention to the ordering of words. You will build a better algorithm in the next part.</li></ul><h3 id="2-Emojifier-V2-Using-LSTMs-in-Keras"><a href="#2-Emojifier-V2-Using-LSTMs-in-Keras" class="headerlink" title="2- Emojifier-V2: Using LSTMs in Keras:"></a>2- Emojifier-V2: Using LSTMs in Keras:</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Input, Dropout, LSTM, Activation</span><br><span class="line"><span class="keyword">from</span> keras.layers.embeddings <span class="keyword">import</span> Embedding</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> sequence</span><br><span class="line"><span class="keyword">from</span> keras.initializers <span class="keyword">import</span> glorot_uniform</span><br><span class="line">np.random.seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h4 id="2-1-Overview-of-the-model"><a href="#2-1-Overview-of-the-model" class="headerlink" title="2.1- Overview of the model"></a>2.1- Overview of the model</h4><p><img src="https://upload-images.jianshu.io/upload_images/8636110-9259a2dd421de22a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h4 id="2-2-Keras-and-mini-batching"><a href="#2-2-Keras-and-mini-batching" class="headerlink" title="2.2- Keras and mini-batching"></a>2.2- Keras and mini-batching</h4><p>In this exercise, we want to train Keras using mini-batches. However, most deep learning frameworks require that all sequences in the same mini-batch have the same length. This is what allows vectorization to work: If you had a 3-word sentence and a 4-word sentence, then the computations needed for them are different (one takes 3 steps of an LSTM, one takes 4 steps) so it’s just not possible to do them both at the same time.</p><p>The common solution to this is to use <strong>padding</strong>. Specifically, set a maximum sequence length, and pad all sequences to the same length. For example, of the maximum sequence length is 20, we could pad every sentence with “0”s so that each input sentence is of length 20. Thus, a sentence “i love you” would be represented as (e_i,e_love,e_you,0⃗ ,0⃗ ,…,0⃗ ). In this example, any sentences longer than 20 words would have to be truncated. One simple way to choose the maximum sequence length is to just pick the length of the longest sentence in the training set.</p><h4 id="2-3-The-Embedding-layer"><a href="#2-3-The-Embedding-layer" class="headerlink" title="2.3- The Embedding layer"></a>2.3- The Embedding layer</h4><p>In Keras, the embedding matrix is represented as a “layer”, and maps positive integers (indices corresponding to words) into dense vectors of fixed size (the embedding vectors). It can be trained or initialized with a pretrained embedding. In this part, you will learn how to create an Embedding() layer in Keras, initialize it with the GloVe 50-dimensional vectors loaded earlier in the notebook. Because our training set is quite small, we will not update the word embeddings but will instead leave their values fixed. But in the code below, we’ll show you how Keras allows you to either train or leave fixed this layer.</p><p>The Embedding() layer takes an integer matrix of size (batch size, max input length) as input. This corresponds to sentences converted into lists of indices (integers), as shown in the figure below.</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-dcaf6e24664d4d8d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>The largest integer (i.e. word index) in the input should be no larger than the vocabulary size. The layer outputs an array of shape (batch size, max input length, dimension of word vectors).</p><p>The first step is to convert all your training sentences into lists of indices, and then zero-pad all these lists so that their length is the length of the longest sentence.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentences_to_indices</span><span class="params">(X, word_to_index, max_len)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.</span></span><br><span class="line"><span class="string">    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    X -- array of sentences (strings), of shape (m, 1)</span></span><br><span class="line"><span class="string">    word_to_index -- a dictionary containing the each word mapped to its index</span></span><br><span class="line"><span class="string">    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. </span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    m = X.shape[<span class="number">0</span>]                                   <span class="comment"># number of training examples</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Initialize X_indices as a numpy matrix of zeros and the correct shape</span></span><br><span class="line">    X_indices = np.zeros((m,max_len))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):                               <span class="comment"># loop over training examples</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert the ith training sentence in lower case and split is into words. You should get a list of words.</span></span><br><span class="line">        sentence_words = list(X[i].lower().split())</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Initialize j to 0</span></span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Loop over the words of sentence_words</span></span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> sentence_words:</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Set the (i,j)th entry of X_indices to the index of the correct word.</span></span><br><span class="line">            X_indices[i, j] = word_to_index[w]</span><br><span class="line">            <span class="comment"># Increment j to j + 1</span></span><br><span class="line">            j = j + <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X_indices</span><br></pre></td></tr></table></figure><p>Build the Embedding() layer in Keras, using pre-trained word vectors.<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pretrained_embedding_layer</span><span class="params">(word_to_vec_map,word_to_index)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.</span></span><br><span class="line"><span class="string">    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    embedding_layer -- pretrained layer Keras instance</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    vocab_len = len(word_to_index)+<span class="number">1</span></span><br><span class="line">    emb_dim = word_to_vec_map[<span class="string">"cucumber"</span>].shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)</span></span><br><span class="line">    emb_matrix = np.zeros((vocab_len, emb_dim)) <span class="comment">#所有单词的嵌入矩阵</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set each row "index" of the embedding matrix to be the word vector representation of the "index"th word of the vocabulary</span></span><br><span class="line">    <span class="keyword">for</span> word, index <span class="keyword">in</span> word_to_index.items():</span><br><span class="line">        emb_matrix[index, :] = word_to_vec_map[word]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define Keras embedding layer with the correct output/input sizes, make it trainable.</span></span><br><span class="line">    <span class="comment"># Use Embedding(...). Make sure to set trainable=False.</span></span><br><span class="line">    embedding_layer = Embedding(input_dim=vocab_len,output_dim=emb_dim,trainable=<span class="keyword">False</span>)</span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the "None".</span></span><br><span class="line">    embedding_layer.build((<span class="keyword">None</span>,))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.</span></span><br><span class="line">    embedding_layer.set_weights([emb_matrix])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> embedding_layer</span><br></pre></td></tr></table></figure></p><h3 id="2-3-Building-the-Emojifier-V2"><a href="#2-3-Building-the-Emojifier-V2" class="headerlink" title="2.3- Building the Emojifier-V2"></a>2.3- Building the Emojifier-V2</h3><p><img src="https://upload-images.jianshu.io/upload_images/8636110-aee79f1c430ef952.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>注意LSTM的参数设值，比如return_sequences。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Emojify_V2</span><span class="params">(input_shape,word_to_index,word_to_index)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Function creating the Emojify-v2 model's graph.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    input_shape -- shape of the input, usually (max_len,)</span></span><br><span class="line"><span class="string">    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation</span></span><br><span class="line"><span class="string">    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    model -- a model instance in Keras</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).</span></span><br><span class="line">    sentence_indices = Input(input_shape, dtype=<span class="string">'int32'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create the embedding layer pretrained with GloVe Vectors (≈1 line)</span></span><br><span class="line">    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Propagate sentence_indices through your embedding layer, you get back the embeddings</span></span><br><span class="line">    embeddings = embedding_layer(sentence_indices) <span class="comment">#注意这里！</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Propagate the embeddings through an LSTM layer with 128-dimensional hidden state</span></span><br><span class="line">    <span class="comment"># Be careful, the returned output should be a batch of sequences.</span></span><br><span class="line">    X = LSTM(units=<span class="number">128</span>,return_sequences=<span class="keyword">True</span>)(embeddings) <span class="comment"># 注意参数设置，要一个个认真检查</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add dropout with a probability of 0.5</span></span><br><span class="line">    X = Dropout(rate=<span class="number">0.5</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Propagate X trough another LSTM layer with 128-dimensional hidden state</span></span><br><span class="line">    <span class="comment"># Be careful, the returned output should be a single hidden state, not a batch of sequences.</span></span><br><span class="line">    X = LSTM(units=<span class="number">128</span>,return_sequences=<span class="keyword">False</span>)(X)</span><br><span class="line">    <span class="comment"># Add dropout with a probability of 0.5</span></span><br><span class="line">    X = Dropout(rate=<span class="number">0.5</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.</span></span><br><span class="line">    X = Dense(units=<span class="number">5</span>)(X)</span><br><span class="line">    <span class="comment"># Add a softmax activation</span></span><br><span class="line">    X = Activation(<span class="string">'softmax'</span>)(X)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create Model instance which converts sentence_indices into X.</span></span><br><span class="line">    model = Model(inputs=sentence_indices,outputs=X)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-b4818af62aae12b2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>接下来的步骤：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>, optimizer=<span class="string">'adam'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br><span class="line">X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)</span><br><span class="line">Y_train_oh = convert_to_one_hot(Y_train, C = <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">model.fit(X_train_indices, Y_train_oh, epochs = <span class="number">50</span>, batch_size = <span class="number">32</span>, shuffle=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)</span><br><span class="line">Y_test_oh = convert_to_one_hot(Y_test, C = <span class="number">5</span>)</span><br><span class="line">loss, acc = model.evaluate(X_test_indices, Y_test_oh)</span><br><span class="line">print()</span><br><span class="line">print(<span class="string">"Test accuracy = "</span>, acc)</span><br></pre></td></tr></table></figure></p><p><strong>What we should remember</strong>:</p><ul><li>If you have an NLP task where the training set is small, using word embeddings can help your algorithm significantly. Word embeddings allow your model to work on words in the test set that may not even have appeared in your training set.</li><li>Training sequence models in Keras (and in most other deep learning frameworks) requires a few important details:<ul><li>To use mini-batches, the sequences need to be <strong>padded</strong> so that all the examples in a mini-batch have the same length.</li><li>An Embedding() layer can be initialized with pretrained values. These values can be either fixed or trained further on your dataset. If however your labeled dataset is small, it’s usually not worth trying to train a large pre-trained set of embeddings.</li><li>LSTM() has a flag called return_sequences to decide if you would like to return every hidden states or only the last one.</li><li>You can use Dropout() right after LSTM() to regularize your network.</li></ul></li></ul><p>总结：Keras使用还是不太熟练啊。另外，感觉自己缺乏编程能力，需要多练习，特别是针对深度学习框架。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;词嵌入-Word-Embedding&quot;&gt;&lt;a href=&quot;#词嵌入-Word-Embedding&quot; class=&quot;headerlink&quot; title=&quot;词嵌入: Word Embedding&quot;&gt;&lt;/a&gt;词嵌入: Word Embedding&lt;/h1&gt;&lt;p&gt;我们之前
      
    
    </summary>
    
      <category term="深度学习" scheme="https://github.com/DesmonDay/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://github.com/DesmonDay/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>第12周-循环神经网络(RNN)</title>
    <link href="https://github.com/DesmonDay/2019/04/23/deep-learningw12/"/>
    <id>https://github.com/DesmonDay/2019/04/23/deep-learningw12/</id>
    <published>2019-04-23T09:21:00.000Z</published>
    <updated>2019-05-01T15:47:53.333Z</updated>
    
    <content type="html"><![CDATA[<h1 id="为什么用序列模型：Why-sequence-models"><a href="#为什么用序列模型：Why-sequence-models" class="headerlink" title="为什么用序列模型：Why sequence models?"></a>为什么用序列模型：Why sequence models?</h1><p>首先，我们看几个序列数据的例子:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d932f29b3f3fbe30.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>所有的这些问题都可以作为使用标签数据(X,Y)作为训练集的监督学习。不过从上图，我们也可以知道这里的序列数据是非常不同的。有些输入输出都是序列，但长度不同；有的只有输入或输出才是序列，等等。</p><h1 id="数学符号：Notation"><a href="#数学符号：Notation" class="headerlink" title="数学符号：Notation"></a>数学符号：Notation</h1><h2 id="Motivation-Example"><a href="#Motivation-Example" class="headerlink" title="Motivation Example"></a>Motivation Example</h2><p>假设我们想要建立一个能够自动识别句中人名位置的序列模型。所以这就是一个命名实体识别问题，常用于搜索引擎。比如，索引过去24小时内所有新闻报道提及的人名。命名实体识别系统可以用来查找不同类型的文本中的人名、公司名、时间、地点、国家名、货币名等等。</p><p>假设我们的输入输出设置如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-37849034ae083b14.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-df6c867d4977f8d3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>其中，y用一个向量对应句子中的每个单词，如果该单词为人名，那么为1，否则为0。从技术层面角度，这并不是最好的输出形式，还有其他更加复杂的输出形式。它不仅能够表明输入词是否是人名的一部分，还能够告诉你这个人名从句子哪里开始和结束。</p><p>我们将用x<1>,x<2>,…,x<9>来索引句子中单词的位置，用x&lt;\t&gt;表示序列的中间位置。这里的t意味着它们是时序序列，输出同样的用y<1>,y<2>,…y<9>来索引位置。另外，我们用T_y表示句子的长度。为了表示训练样本i的序列中第t个单词(元素)，我们用X(i)&lt;\t&gt;来表示，再用Tx(i)表示第i个训练样本的输入序列长度，这种表示方法对输出序列也成立。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-12426e1eb0b70a16.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></9></2></1></9></2></1></p><h2 id="Representing-words"><a href="#Representing-words" class="headerlink" title="Representing words"></a>Representing words</h2><p>接下来探讨怎样表示一个句子里单个的词。首先，我们可能会做一张词汇表，将我们要表示的词按字典顺序放入。比如，这里有个词汇量为10000的词汇表，这对自然处理语言应用来说是非常小规模的。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-53233244b3a9e6c2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>接下来，我们可以用<strong>one-hot</strong>表示每个单词，这个每个单词用one-hot向量来表示：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-8fe77d39fca4e914.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们的目标是，用这样的表示方式表示X，用序列模型在X和目标输出Y之间学习建立一个映射，我们会把它当做监督学习的问题来做。</p><p>另一个需要注意的问题是，如果我们遇到了一个不在单词表中的单词，我们就创建一个新的标记<code>&quot;&lt;UNK&gt;&quot;</code>，来表示这个单词不在词汇表中。</p><h1 id="循环神经网络模型"><a href="#循环神经网络模型" class="headerlink" title="循环神经网络模型"></a>循环神经网络模型</h1><h2 id="Why-not-a-standard-network"><a href="#Why-not-a-standard-network" class="headerlink" title="Why not a standard network?"></a>Why not a standard network?</h2><p>A standard network:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-2260330da984017d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Problems:</p><ul><li>Inputs, outputs can be different lengths in different examples.</li><li>Doesn’t share features learned across different positions of text.类似卷积神经网络中所学到的，我们希望将部分图片里学到的内容快速推广到图片的其他部分，而我们希望对序列数据也有相似的效果。</li></ul><p>另外，用一个更好的representation能够让我们减少模型中的参数数量。</p><h2 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h2><p>RNN的基本结构如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-aa877c8fdbe61164.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>首先顺序读取序列中的第一个单词，并且尝试预测一个输出值；再读取第二个单词的时候，我们不仅仅利用这个单词进行预测，而且会用到前一个隐藏层的输出作为当前层的输入来预测。就这样按照顺序进行下去。另外，我们通常为设置一个伪激活值a<0>作为RNN的最初始输入，通常为0向量。另外在本例中，Tx=Ty，如果不等，则需要对网络结构进行调整。</0></p><p>循环神经网络是从左向右扫描数据，同时每个时间步的参数也是共享的。我们用W_ax来表示从x<1>到隐藏层的连接的一系列参数，并且每个时间步所使用的都是相同的参数W_ax。而激活值，也就是水平联系，是由参数W_aa决定的，同时每一个时间步都使用相同的参数W_aa。同样的，输出都由参数W_ya决定。（对这里的参数名称做一个解释，比如W_ax，表示这个参数会乘以X来得到a，类似这样感知的理解就好，看图就能够明白了）<br><img src="https://upload-images.jianshu.io/upload_images/8636110-65dafc9207622144.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>注意：最右侧循环图可以理解为左边图的简化版本，但难以理解。</1></p><p>在这种RNN中，当我们预测y<3>时，不仅要使用x<3>的信息，还要使用来自x<1>和x<2>的信息。但这个RNN的一个缺点是它只使用了这个序列中<strong>之前的信息</strong>来做出预测。即当预测y<3>时，它没有用到如x<4>、x<5>、x<6>等等的信息。这就造成了一个问题：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f8a89d0b56d93c18.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>为了判断Teddy是否是人名的一部分，只使用前面的两个单词来预测是远远不够的，即之给定前面的三个单词，不可能确切知道Teddy是否是人名的一部分。</6></5></4></3></2></1></3></3></p><p>因此这个特定结构神经网络的缺点是，它在某一时刻的预测仅使用了从序列中之前时刻的输入信息，并没有使用序列之后地信息。这个问题我们会在Bidirectional RNN(双向RNN)中得到解答。</p><h2 id="Forward-Propagation"><a href="#Forward-Propagation" class="headerlink" title="Forward Propagation"></a>Forward Propagation</h2><p>我们将网络结构更清晰的表示如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-243c146d3b45b0b8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>用等式表示为：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-3628eebbc758f041.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们注意到，式子可以在最后很有规律地表示出其结果。另外，激活函数可以是不同的，比如在RNN中，g1往往选择为tanh，也可以为relu，而g2需要根据我们y的输出值来确定（如果是二分类问题，则选择为sigmoid，如本例的命名实体识别问题；如果是多分类，可以选择softmax等等）。</p><h2 id="Simplified-RNN-notation"><a href="#Simplified-RNN-notation" class="headerlink" title="Simplified RNN notation"></a>Simplified RNN notation</h2><p>在上一小节，我们得到了以下公式：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-dbc63f1e76c12e00.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们将a&lt;\t&gt;的式子简写作：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-a0fe2e3720d0fd9e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>其中，具体的解释如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b37e5421a68fa78a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>将y&lt;\t&gt;写作：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ff02aec2c8ab0538.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>此时，W_y和b_y的下标只有y，这表示会输出什么类型的量，所以W_y是计算y类型的量的权重矩阵；而上面的W_a和b_a表示这些参数是用来计算a类型输出的。</p><h1 id="Back-propagation-through-time"><a href="#Back-propagation-through-time" class="headerlink" title="Back propagation through time"></a>Back propagation through time</h1><p>我们已经学习了循环神经网络的基础结构，在本节，我们将了解反向传播是怎样在循环神经网络中运行的。</p><p>我们已知的前向传播过程：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ea9b7b9a8f14b011.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>接下来，为了计算反向传播，我们还需要一个损失函数。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-cd02087594a1250d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>由于类似二元分类，因此我们选取的损失函数为交叉熵损失函数。</p><p>将损失函数和反向传播表示到网络中：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-123e68b7a83222d7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>其中最为重要的递归操作为从右往左的梯度计算。</p><h1 id="不同类型的RNN"><a href="#不同类型的RNN" class="headerlink" title="不同类型的RNN"></a>不同类型的RNN</h1><p>本文参考Andrej Karpathy的博客: The Unreasonable Effectiveness of Recurrent Neural Networks（对应翻译版：<a href="https://blog.csdn.net/menc15/article/details/78775010" target="_blank" rel="noopener">https://blog.csdn.net/menc15/article/details/78775010</a> ）</p><p>在上一节，我们介绍的RNN结构里Tx和Ty是相等的。但是在实际应用中，Tx和Ty不一定相等。本节会进行介绍。</p><p>常见序列数据：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b447ca3ca31d8374.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="Examples-of-RNN-architectures"><a href="#Examples-of-RNN-architectures" class="headerlink" title="Examples of RNN architectures"></a>Examples of RNN architectures</h2><p>many-to-many(输入输出长度相同), many-to-one(例如情感分类), one-to-one(普通神经网络):<br><img src="https://upload-images.jianshu.io/upload_images/8636110-8bcd5703f21d5141.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>one-to-many(音乐生成), many-to-many(输入输出长度不同，如翻译，用的是encoder-decoder模型):<br><img src="https://upload-images.jianshu.io/upload_images/8636110-453e6eac0e9de534.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>另外还有一个attention模型，之后会讲解。</p><h2 id="Summary-of-RNN"><a href="#Summary-of-RNN" class="headerlink" title="Summary of RNN"></a>Summary of RNN</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-62a3311d3bc8e1bc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="语言模型和序列生成"><a href="#语言模型和序列生成" class="headerlink" title="语言模型和序列生成"></a>语言模型和序列生成</h1><p>一个语言模型能够计算出句子的可能性，如语言识别中，举一个简单的例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d5bcb1c69363f9dc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>因此，语言模型会计算出某个特定句子出现的概率是多少。这个模型是两种系统的基本组成部分，即语音识别系统和机器翻译系统，它能正确输出最接近的句子。而语言模型做的基本工作就是输入一个句子，准确地说是一个文本序列，然后会估计该序列中各个单词出现的可能性。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f3547621968f6bec.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="Language-modeling-with-an-RNN"><a href="#Language-modeling-with-an-RNN" class="headerlink" title="Language modeling with an RNN"></a>Language modeling with an RNN</h2><p>首先我们需要一个训练集，这个训练集可能是一个大型的英文语料库，也可能是其他我们需要的语言的语料库。</p><ol><li>第一件事需要进行tokenize，分解文本流为词，或将其转化为序列（比如用one-hot向量表示每一个词）。</li><li>另一件可能要做的事是定义句子的结尾，一般的做法是增加一个额外的标记，叫做<code>&quot;&lt;EOS&gt;&quot;</code>。这样能够让我们清楚一个句子什么时候结束。因此<code>EOS</code>标记可以添加到训练集中每一个句子的结尾。（注意句号可以去掉，或者当做一个单词也计入词典）</li><li>另一件是如果我们的训练集里有一些词并不在词典里（这个词典可能是10,000个常见的英文单词），那么将不在里面的单词用<code>&quot;&lt;UNK&gt;&quot;</code>取代，即用<code>&quot;&lt;UNK&gt;&quot;</code>代替未知词。我们只针对<code>&quot;&lt;UNK&gt;&quot;</code>来建立概率模型。</li><li>下一步，我们要构建一个RNN来构建这些序列的概率模型。</li></ol><h2 id="RNN-model"><a href="#RNN-model" class="headerlink" title="RNN model"></a>RNN model</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-a55c72fd35631fbe.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-9266f437645d63ea.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>由图上的RNN模型可知，我们首先输入x<1>和a<0>均为零向量，然后开始尝试输出第一个词y_hat<1>。随后，我们将y_hat<1>和a<1>作为计算a<2>的输入，接着通过softmax函数计算出第二个预测词y_hat<2>（这时候，我们所求的是使P(?|Cats)最大的单词，这样依次计算下去…</2></2></1></1></1></0></1></p><p>我们使用的是Softmax函数作为输出层的激活函数，因此选用的loss function为交叉熵损失函数。其中，y_i&lt;\t&gt;为真实的输出，而y_hat_i&lt;\t&gt;则为预测的单词输出，然后再将所有时刻的loss相加，得到最后总的loss function。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ae929432c67ac294.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>假设现在有一个新句子，为了简单起见，假设其含有是三个单词。那么第一层Softmax会计算P(y<1>)，第二层Softmax计算P(y<2>|y<1>)，第三层Softmax计算P(y<3>|y<1>,y<2>)。从而整个句子的概率为：<br>    <img src="https://upload-images.jianshu.io/upload_images/8636110-c191e422a19f7b6c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></2></1></3></1></2></1></p><h1 id="对新序列采样：sampling-novel-sequences"><a href="#对新序列采样：sampling-novel-sequences" class="headerlink" title="对新序列采样：sampling novel sequences"></a>对新序列采样：sampling novel sequences</h1><p>在训练完一个序列模型之后，要想了解这个模型学到了什么，一种非正式的方法就是进行一次新序列采样。</p><h2 id="Sampling-a-sequence-from-a-trained-RNN"><a href="#Sampling-a-sequence-from-a-trained-RNN" class="headerlink" title="Sampling a sequence from a trained RNN"></a>Sampling a sequence from a trained RNN</h2><p>我们记得一个序列模型模拟了任意特定单词序列的概率，而我们要做的是对这个概率分布进行采样，来生成一个新的单词序列。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-54e8c57ca8ba962a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>已知我们训练时所用的RNN模型如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-e7571e716d844f83.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>然而为了采样，我们会做一些不同的事情。第一步要做的是对我们想要模型生成的第一个词进行采样，于是我们输入x<1>=0,a<0>=0，因此现在我们的第一个时间步得到的是所有可能的输出，即经过Softmax层后得到的概率。然后根据这个Softmax的分布进行随机采样，使用numpy命令如np.random.choice来对第一个词进行采样；接下来我们进入第二个时间步，把刚刚采样的y<1>传递到下一个位置作为输入，接着Softmax层会预测第二个词；这样依次进行…</1></0></1></p><p>什么时候一个句子算结束呢？一个方法是如果代表句子结尾的标识<code>&quot;&lt;EOS&gt;&quot;</code>在词典中，那么我们可以一直进行采样直到<strong>得到<code>&quot;&lt;EOS&gt;&quot;</code></strong>，这代表我们已经抵达结尾，可以停止随机采样；另一个方法是词典中没有这个标识，那么我们可以决定从20个或100个或其他词中进行采样，直到达到所<strong>设定的时间步</strong>。</p><p>这种方法可能会出现预测出<code>&quot;&lt;UNK&gt;&quot;</code>的情况。如果我们想要避免这种情况，那么可以在<code>&quot;&lt;UNK&gt;&quot;</code>出现时就继续在剩下的词中进行重采样，直到得到一个不是<code>&quot;&lt;UNK&gt;&quot;</code>的单词；当然，如果我们不介意有未知标识的产生，也可以不理会。</p><p>以上就是我们从RNN语言模型中生成一个随机选择的句子。</p><h2 id="Character-level-language-model"><a href="#Character-level-language-model" class="headerlink" title="Character-level language model"></a>Character-level language model</h2><p>在之前我们所建立的都是基于词汇的RNN模型，也就是说，字典中的词都是英语单词。根据我们的实际应用，我们还可以构建一个基于字符的RNN模型。这时，我们的字典中不再是单词，而是常见字符。</p><p>此时输入输出都是单个字符，而不再是单独的词汇。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-78b59a7d11dc5caa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>基于字符的语言模型的优点是我们不必担心会出现未知的标识，比如基于字符的语言模型会把’Mao’视为概率非0的序列，而基于词汇的语言模型，如果’Mao’不在字典中，那么我们只能把它当做未知标识。</p><p>然后基于字符的语言模型的一个明显缺点是我们最后会得到太长的序列。所以基于字符的语言模型在捕捉句子中的依赖关系，也就是句子较前部分如何影响较后部分，不如基于词汇的语言模型，并且它的计算成本也会很大。</p><p>因此在自然语言领域中，大多数应用是使用基于词汇的语言模型。在随着计算能力的提高，在一些特殊情况下，人们也会开始使用基于字符的模型，但这也需要更昂贵的计算成本。</p><p>下面展示一些采样后的结果：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-1884db2f163dac1e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="Vanishing-gradients-with-RNNs"><a href="#Vanishing-gradients-with-RNNs" class="headerlink" title="Vanishing gradients with RNNs"></a>Vanishing gradients with RNNs</h1><p>在前面的学习我们已经了解了RNN是如何工作的，并且知道如何将RNN应用到具体问题上，比如命名实体识别(name entity recognition)、语言模型等等。然后基本的RNN模型有一个很大的问题，也就是<strong>梯度消失</strong>的问题。</p><p>下面解释梯度消失。首先给出两个句子，这两个句子有着长期的依赖，也就是很前面的单词对句子很后面的单词有影响。(cat对应was，cats对应were)</p><ul><li>The cat, which already ate…, was full.</li><li>The cats, which already ate…, were full.<br>但是目前我们见到的<strong>基本RNN模型不擅长捕获这种长期依赖效应</strong>。在之前的讨论，我们知道在训练很深的网络时，我们讨论了梯度消失的问题。如果深度很深，那么从输出y得到的梯度将很难传播回去，很难影响到前面层的权重。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-265dd247b2b9312b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></li></ul><p>而RNN也有着同样的问题，所以其反向传播也比较困难。因为同样的梯度消失的问题，后面层的输出误差很难影响前面层的计算。这就意味着，实际上能难让一个神经网络能够意识到它要记住看到的是单数名词还是复数名词，然后在序列后面生成依赖单复数形式。也就是说，基本的RNN模型里，一个单词只能被其前面的几个单词所影响。这就是RNN的一个缺点。</p><p>如果出现了<strong>梯度爆炸</strong>的情况，一般很容易发现，因为梯度可能会出现如NaN(数值溢出之类的现象。一个解决方法就是进行<strong>梯度修剪</strong>(gradient clipping)，也就是观察我们的梯度向量，如果它大于某个阈值，那么我们缩放梯度向量，保证它不会太大。</p><p>但梯度消失是更难解决的。</p><p>另外复习:</p><ul><li>ReLu激活函数的主要贡献是：<ul><li>解决了梯度消失、爆炸的问题</li><li>计算方便、计算速度快</li><li>加速了网络的训练</li></ul></li><li>BatchNorm: 本质上是解决反向传播过程中的梯度问题。反向传播式子中有w的存在，所以w的大小影响了梯度的消失和爆炸。BatchNorm就是通过对每一层的输出规范化为均值和方差一致的方法，消除了w带来的放大缩小的影响，进而解决梯度消失和爆炸的问题。</li><li>残差结构：解决了梯度消失过快的问题，因此即使构建的网络很深层也不必担心。</li></ul><p>参考博客：<a href="https://blog.csdn.net/qq_25737169/article/details/78847691" target="_blank" rel="noopener">https://blog.csdn.net/qq_25737169/article/details/78847691</a></p><h1 id="GRU-Gated-Recurrent-Unit"><a href="#GRU-Gated-Recurrent-Unit" class="headerlink" title="GRU: Gated Recurrent Unit"></a>GRU: Gated Recurrent Unit</h1><p>参考论文：</p><ol><li>On the properties of neural machine translation: Encoder-decoder approaches, 2014.</li><li>Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling, 2014.</li></ol><p>我们已经学习了基础的RNN模型的运行机制，在本节，我们会学习门控循环单元GPU。它改变了RNN的隐藏层，使其更好地捕捉深层连接，并改善了梯度消失问题。</p><h2 id="RNN-unit"><a href="#RNN-unit" class="headerlink" title="RNN unit"></a>RNN unit</h2><p><img src="https://upload-images.jianshu.io/upload_images/8636110-33ab1bcbf06b3cb3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h2 id="GRU-simplified"><a href="#GRU-simplified" class="headerlink" title="GRU(simplified)"></a>GRU(simplified)</h2><p>GRU设置了一个新的变量C，为记忆细胞(memory cell)，记忆细胞的作用是提供了记忆的能力，比如一只猫是单数还是复数。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-bfb4d77c79ad247d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>在GRU中，c&lt;\t&gt;等于激活值a&lt;\t&gt;。（因为在LSTM中，两个值并不相等，因此为避免混淆，这里采用两种不同的名称表示，即便他们的值是一样的。）</p><p>接下来写出GPU(简化版本)中的关键公式：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-51fc765c5b4d2f63.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>其中，第一个式子表示记忆细胞的候选值，我们用tanh计算出来。</p><p>而GPU的关键思想就是第二个式子，也就是门（用”Γ”代替，另外小写u表示”update”)。为了思考GRU的工作机制，我们思考门的原理。可以看到它是用一个sigmoid函数来计算，对大多数输入来说，sigmoid值在大多数情况下都接近0或者1。</p><p>接下来看第三个式子。如果当Γ_u=1时，说明我们将c&lt;\t&gt;设置为计算的候选值c_tilda&lt;\t&gt;。而如果Γ_u=0，说明我们将c&lt;\t&gt;设置为c&lt;\t-1&gt;，这也说明，Γ_u越小，则前一时刻的状态信息带入越多。而针对我们之前所说的猫吃饱没的例子，我们应该将cat和was之间的所有Γ_u都设置为0，即不进行更新，只设置为旧的值。这样到了was的时候，神经网络还能记得cats的信息。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c121835dcc48c5b7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们将GRU图示化，以便更好理解：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-abe4597abfc4bf63.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>而GRU的优点是，通过门来决定，当我们从左到右扫描一个句子的时候，这个时机是应该更新还是不更新记忆细胞。由于sigmoid值很可能取0，因此我们可以通过门的设定来维持记忆细胞的值，即c&lt;\t&gt;=c&lt;\t-1&gt;，而因为Γ_u很接近0，从而不会有梯度消失的问题了。因为Γ_u很接近0，所以c&lt;\t&gt;几乎等于c&lt;\t-1&gt;，而且c&lt;\t&gt;的值也很好地被维持了，即使经过很多很多的时间步。所以这就是缓解梯度消失问题的关键，因此允许神经网络运行在非常庞大的依赖词上，比如cat和was单词，即便它们被很多单词分隔，也仍然能够运行。</p><p>下面补充一点实现的细节。在上面有关记忆细胞候选值的式子里，c&lt;\t&gt;可以是一个向量。如果我们有一个100维的隐藏的激活值，那么c&lt;\t&gt;也是100维的，c_tilda&lt;\t&gt;也是相同的维度，从而Γ_u也是相同的维度。因此说明c&lt;\t&gt;的式子实际上是元素对应的乘积。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b9d520811feecc7e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>而这100维的记忆细胞里，我们只更新需要更新的比特。</p><p>当然，在实际应用中Γ_u不会真的等于0或1，有时候它是0到1的中间值，但这对于直观思考是很方便地。而元素对应的成绩做的是告诉GRU单元哪个记忆细胞的向量维度在每个时间步要做更新，因此我们可以选择保持一些比特不变，而去更新其他的比特。</p><h2 id="Full-GRU"><a href="#Full-GRU" class="headerlink" title="Full GRU"></a>Full GRU</h2><p>针对完整版本的GRU，我们添加了一个变量Γ_r，这个Γ_r告诉我们计算出的c&lt;\t-1&gt;与c&lt;\t&gt;的候选值的相关性有多大。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-b4800a75c12cf672.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>为什么需要Γ_r呢？这是因为多年来研究者们试验过很多不同可能的方法来设计这些单元，尝试让神经网络有更深层的连接，尝试产生更大范围的影响，并且解决梯度消失的问题。</p><h2 id="参考其他博客的解释"><a href="#参考其他博客的解释" class="headerlink" title="参考其他博客的解释"></a>参考其他博客的解释</h2><p>为了让自己更加深刻的理解GRU，接下来还参考了以下博文：<a href="https://www.cnblogs.com/jiangxinyang/p/9376021.html" target="_blank" rel="noopener">https://www.cnblogs.com/jiangxinyang/p/9376021.html</a></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-7d87ba39f2e6cd12.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>图中的zt和rt分别表示更新门和重置门。更新门用于控制前一时刻的状态信息被带入到当前状态中的程度，更新门的值越大说明前一时刻的状态信息带入越多。重置门控制前一状态有多少信息被写入到当前的候选集 h_tilda_t 上，重置门越小，前一状态的信息被写入的越少。</p><ol><li><p>GRU的前向传播：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7be385f8b161c998.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p></li><li><p>GRU的训练过程：从前向传播过程中的公式可以看出要学习的参数有Wr、Wz、Wh、Wo。其中前三个参数都是拼接的（因为后先的向量也是拼接的），所以在训练的过程中需要将他们分割出来：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-59fce5507d3623da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>输出层的输入和输出层的输出分别为（注意到输出的激活函数是sigmoid）：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-1fc6bc02fac1c769.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>损失函数（这里用的是平方损失函数）：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-131ec80bce316acc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>接下来求偏导（我没有进行推导）：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f4be19597aa9375b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>其中各中间参数为：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-f926c844c1a38601.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>在算出了对各参数的偏导之后，就可以更新参数，依次迭代直到损失收敛。</p></li></ol><h1 id="长短时记忆网络：LSTM"><a href="#长短时记忆网络：LSTM" class="headerlink" title="长短时记忆网络：LSTM"></a>长短时记忆网络：LSTM</h1><p>论文标题：Long shot-term memory, 1997.</p><p>在上一节，我们学习了GRU，它能够让我们在序列中学习非常深的连接(long range connection)。LSTM单元也能做到这一点。</p><p>下面是GRU和LSTM的公式对照。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-25df2c3b3314c65e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>我们注意到，LSTM不再有a&lt;\t&gt;=c&lt;\t&gt;。和GRU一样，有一个更新门，而LSTM的新特性是不只有一个更新门Γ_u控制，而新增加了一个遗忘门Γ_f。然后增加了一个输出门来用于输出a&lt;\t&gt;。</p><p>接下来将LSTM表示为图形：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-557568c6fe786add.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>接下来我们将多个LSTM单元按顺序连接起来，可以发现一件有趣的事：我们会发现在连接的图中有一条线（即所画的红线），这条线显示了只要我们正确地设置了遗忘门和更新门，LSTM是能够很容易地将c<0>的值一直传递到右边，比如c<3>=c<0>。这就是为什么LSTM和GRU非常擅长于长时间记忆某个值，即使经过很长的时间步，依旧能够保持住存在于记忆细胞中的某个值。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-fddaead310ac055e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></0></3></0></p><p>然而，这里介绍的和一般使用的版本不太一样。最常用的版本(LSTM的变体：Recurrent Nets that Time and Count, 2000)是我们的门值不仅取决于a&lt;\t-1&gt;和x&lt;\t&gt;，有时候人们也会偷窥一下c&lt;\t-1&gt;的值，这叫做peephole connection(窥视孔连接)。它的意思是，门值不仅取决于a&lt;\t-1&gt;和x&lt;\t&gt;，也取决于上一个记忆细胞的值。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d09e6eeb0e3993da.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>那么我们应该什么时候用GRU，什么时候用LSTM？这里没有统一的准则。尽管我们先介绍了GRU，但在深度学习历史上，LSTM是更早出现的，而GRU是近几年才发明出来的，它可能源于在更加复杂的LSTM模型中做出的简化。研究者们在很多不同问题上尝试了这两种模型，看看在不同问题不同算法中哪个模型更好。</p><p>GRU的优点是更加简单，所以更容易创建一个更大的网络，而且它只有两个门，在计算性上也运行得更快，可以扩大模型的规模；而LSTM更加强大和灵活，因为它有三个门而不是两个。如果我们必须选择一个来使用，那么LSTM应该会作为默认选择来尝试。</p><h2 id="LSTM反向传播推导-未验证"><a href="#LSTM反向传播推导-未验证" class="headerlink" title="LSTM反向传播推导(未验证)"></a>LSTM反向传播推导(未验证)</h2><p>门求偏导：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c4272320bb39158d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-b3d7de5f4728a3a6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>参数求偏导：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-bd0c79d5a5cf075d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br><img src="https://upload-images.jianshu.io/upload_images/8636110-0ad7eaf74f28955d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>最后，计算隐藏状态、记忆状态和输入的偏导数：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-5bc620d0876deeb7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h1 id="双向RNN：Bidirectional-RNN-BRNN"><a href="#双向RNN：Bidirectional-RNN-BRNN" class="headerlink" title="双向RNN：Bidirectional RNN(BRNN)"></a>双向RNN：Bidirectional RNN(BRNN)</h1><p>到目前为止我们已经学习了RNN模型的关键构件，但还有两个方法能够让我们构建更好的模型。其中一个方法就是双向RNN模型，这个模型能够让我们在序列的某点处不仅可以获取之前的信息，还可以获取之后的信息；第二个方法是深层RNN，我们将在下一节讲解。</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-94465d05da44841f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>为了了解RNN的动机，我们先看一下之前在命名实体识别中见过多次的神经网络。这个网络的问题是，在判断Teddy是否是人名的一部分时，光看句子的前面部分是不够的。为了判断y<3>是0还是1，除了前三个单词，我们还需要更多的信息。所以这是一个单向的RNN。无论里面的单元是GRU、LSTM，这个结论总是成立的，即这是一个单向的RNN神经网络。</3></p><p>下面我们来解释BRNN的工作原理。为了简单，我们用四个输入或者说只有四个单词的句子，这样输入只有x<1>到x<4>。然后我们有四个前向的循环单元，这四个循环单元都有一个当前输入x&lt;\t&gt;，进而得到预测的y_hat&lt;\t&gt;。这是最初的结构：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-207909d44ae745ae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></4></1></p><p>接下来，我们要增加一个反向循环层。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-920edd50c3d7362e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>这样，网络就形成了一个无环图(acyclic graph)。给定一个输入序列x<1>到x<4>，这个序列首先计算前向的a<1>，然后计算前向的a<2>，接着a<3>、a<4>。而反向序列从计算a’<4>开始反向进行，计算反向的a’<3>，直到计算完成。再把所有的激活值都计算完成后，就可以得到预测结果了。（这里我们用a<1>表示前向激活值，a’<1>表示后向激活值）</1></1></3></4></4></3></2></1></4></1></p><p>而预测结果的计算公式如下式子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-ae40aa5b003d9629.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>通过这种方式，我们的预测不仅利用了过去的信息，也利用了未来的信息。而神经网络里的基础单元不仅仅可以是标准RNN单元，也可以是GRU单元或者LSTM单元。事实上，很多的NLP问题，有LSTM单元的双向RNN模型是用得最多的。所以如果有NLP问题，并且文本句子都是完整的，首先需要标定这些句子，那么一个有LSTM单元的双向RNN模型，既有前向过程也有反向过程，将是不错的首选。</p><p>BPNN不仅能用于基本的RNN结构，也能用于GRU和LSTM。通过这些改变，我们可以用一个RNN或GRU或LSTM构建的模型，并且能够预测任意位置，即使在句子的中间位置，因为模型能够考虑整个句子的信息。而这个<strong>双向RNN网络模型的缺点是我们需要完整的数据序列，才能够预测任意位置</strong>。比如我们需要构建一个语音识别系统，那么BRNN需要考虑整个语音表达。也就是说，我们需要等某个人说完，获取整个语音表达才能处理这段语音，并进一步做语音识别。因此，对于实际的语音识别的应用，通常会有更加复杂的模块，而不仅仅用我们见过的标准BRNN模型。当然，对于很多自然语言处理的应用，如果我们总是可以获取整个句子，那么标准的BRNN模型实际上会很有效。</p><h1 id="深层循环神经网络-Deep-RNNs"><a href="#深层循环神经网络-Deep-RNNs" class="headerlink" title="深层循环神经网络: Deep RNNs"></a>深层循环神经网络: Deep RNNs</h1><p>目前我们学到的不同RNN的版本，每个都可以独当一面。但是要学习非常复杂的函数时，通常我们会把RNN的多个层堆叠在一起构建更深的模型。</p><p>我们知道标准的神经网络结构如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-3639d261d113dffc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>下面给出一个具有三个隐层的RNN网络：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-fd40e2ab6147b4a7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>我们下面来讲解一下a[2]<3>应该怎么计算。激活值a[2]<3>有两个输入，一个从左边传来， 一个从下边传来，那么计算的时候，用激活函数作用于权重矩阵如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7b6fcea20ef8253a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>另外，Wa^[2]和ba^[2]也用于同一层的计算，同理第一层有参数Wa^[1]和ba^[1]参与计算。对于RNN来说，能够有三层已经很多了，一般我们设置为1或者2。（注意，time step为深度，这个为层数num_layers）</3></3></p><p>当然，有一种结构也会比较常见。即我们将输出去掉，替换成一些深层，而这些层并没有水平连接，只是一个深层网络，然后用来预测y<1>，同样也加深层网络来预测y<2>，等等。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-afb408d18ac35097.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></2></1></p><p>而这些单元既可以是标准的RNN单元，也可以是GRU或者LSTM单元。通常，深层的RNN训练需要很多计算资源，需要很长的时间。</p><h1 id="本周作业"><a href="#本周作业" class="headerlink" title="本周作业"></a>本周作业</h1><p>第一个作业里有关基本RNN和LSTM的反向传播的部分，没有写完，就先放着了，以后有机会再重新填坑。</p><h2 id="Building-your-Recurrent-Neural-Network-Step-by-Step"><a href="#Building-your-Recurrent-Neural-Network-Step-by-Step" class="headerlink" title="Building your Recurrent Neural Network - Step by Step"></a>Building your Recurrent Neural Network - Step by Step</h2><p>Recurrent Neural Networks (RNN) are very effective for Natural Language Processing and other sequence tasks because they have “memory”. They can read inputs x⟨t⟩ (such as words) one at a time, and remember some information/context through the hidden layer activations that get passed from one time-step to the next. This allows a uni-directional RNN to take information from the past to process later inputs. A bidirection RNN can take context from both the past and the future.</p><p>一些符号标记：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-7b7f7cdfe6c3e3e2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><h3 id="1-Forward-propagation-for-the-basic-Recurrent-Neural-Network"><a href="#1-Forward-propagation-for-the-basic-Recurrent-Neural-Network" class="headerlink" title="1-Forward propagation for the basic Recurrent Neural Network"></a>1-Forward propagation for the basic Recurrent Neural Network</h3><p>Later this week, you will generate music using an RNN. The basic RNN that you will implement has the structure below. In this example, Tx = Ty.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-560316ebb79f05e5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Here’s how you can implement an RNN:</p><p><strong>Steps</strong>:</p><ol><li>Implement the calculations needed for one time-step of the RNN.</li><li>Implement a loop over $T_x$ time-steps in order to process all the inputs, one at a time. </li></ol><h4 id="1-1-RNN-cell"><a href="#1-1-RNN-cell" class="headerlink" title="1.1- RNN cell"></a>1.1- RNN cell</h4><p>A Recurrent neural network can be seen as the repetition of a single cell. You are first going to implement the computations for a single time-step. The following figure describes the operations for a single time-step of an RNN cell.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-969c576b3584b1ae.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-cc03d74c1fe1ab89.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>一个RNN cell处理一个词，而下面代码将其向量化，所以一个RNN cell可以同时处理m个样本中的词。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell_forward</span><span class="params">(xt, a_prev, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements a single forward step of the RNN-cell as described in Figure (2)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    xt -- your input data at timestep "t", numpy array of shape (n_x, m). n_x为词向量的长度，m为样本个数</span></span><br><span class="line"><span class="string">    a_prev -- Hidden state at timestep "t-1", numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        ba --  Bias, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a_next -- next hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    yt_pred -- prediction at timestep "t", numpy array of shape (n_y, m)</span></span><br><span class="line"><span class="string">    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Retrieve parameters from "parameters"</span></span><br><span class="line">    Wax = parameters[<span class="string">"Wax"</span>]</span><br><span class="line">    Waa = parameters[<span class="string">"Waa"</span>]</span><br><span class="line">    Wya = parameters[<span class="string">"Wya"</span>]</span><br><span class="line">    ba = parameters[<span class="string">"ba"</span>]</span><br><span class="line">    by = parameters[<span class="string">"by"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute next activation state using the formula given above</span></span><br><span class="line">    a_next = np.tanh(np.dot(Waa,a_prev)+np.dot(Wax,xt)+ba)</span><br><span class="line">    <span class="comment"># compute output of the current cell using the formula given above</span></span><br><span class="line">    yt_pred = softmax(np.dot(Wya,a_next)+by)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># store values you need for backward propagation in cache</span></span><br><span class="line">    cache = (a_next, a_prev, xt, parameters)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a_next, yt_pred, cache</span><br></pre></td></tr></table></figure></p><p>测试：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">1</span>)</span><br><span class="line">xt = np.random.randn(<span class="number">3</span>,<span class="number">10</span>)</span><br><span class="line">a_prev = np.random.randn(<span class="number">5</span>,<span class="number">10</span>)</span><br><span class="line">Waa = np.random.randn(<span class="number">5</span>,<span class="number">5</span>)</span><br><span class="line">Wax = np.random.randn(<span class="number">5</span>,<span class="number">3</span>)</span><br><span class="line">Wya = np.random.randn(<span class="number">2</span>,<span class="number">5</span>)</span><br><span class="line">ba = np.random.randn(<span class="number">5</span>,<span class="number">1</span>)</span><br><span class="line">by = np.random.randn(<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">parameters = &#123;<span class="string">"Waa"</span>:Waa, <span class="string">"Wax"</span>: Wax, <span class="string">"Wya"</span>: Wya, <span class="string">"ba"</span>: ba, <span class="string">"by"</span>: by&#125;</span><br><span class="line"></span><br><span class="line">a_next, yt_pred, cache = rnn_cell_forward(xt, a_prev, parameters)</span><br></pre></td></tr></table></figure></p><h4 id="1-2-RNN-forward-pass"><a href="#1-2-RNN-forward-pass" class="headerlink" title="1.2- RNN forward pass"></a>1.2- RNN forward pass</h4><p>You can see an RNN as the repetition of the cell you’ve just built. If your input sequence or data is carried over 10 time steps, then you will copy the RNN cell 10 times. Each cell takes as input the hidden state from the previous cell(a&lt;\t-1&gt;) and the current time-step’s input data(x&lt;\t&gt;). It outputs a hidden state(a&lt;\t&gt;) and a prediciton(y&lt;\t&gt;) for this time-step.</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-ec9cdb2b5803abb7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p><strong>Exercise</strong>: Code the forward propagation of the RNN described in Figure(3).</p><p><strong>Instructions</strong>:</p><ol><li>Create a vector of zeros (a) that will store all the hidden states computed by the RNN.</li><li>Initialize the “next” hidden state as a0 (initial hidden state).</li><li>Start looping over each time step, your incremental index is t:<ul><li>Update the “next” hidden state and the cache by running rnn_step_forward</li><li>Store the “next” hidden state in a(t^th position)</li><li>Store the prediction in y</li><li>Add the cache to the list of caches</li></ul></li><li>Return a, y and caches<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_forward</span><span class="params">(x, a0, parameters)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    Implement the forward propagation of the recurrent neural network described in Figure(3).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">        x -- Input data for every time-step, of shape(n_x, m, T_x).</span></span><br><span class="line"><span class="string">        a0 -- Initial hidden state, of shape (n_a, m).</span></span><br><span class="line"><span class="string">        parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                ba --  Bias numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- tuple of values needed for the backward pass, contains (list of caches, x)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Initialze "caches" which will contain the list of all caches</span></span><br><span class="line">    caches = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of x and Wy</span></span><br><span class="line">    n_x, m, T_x = x.shape</span><br><span class="line">    n_y, n_a = parameters[<span class="string">"Wya"</span>].shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># initialize "a" and "y" with zeros</span></span><br><span class="line">    a = np.zeros((n_a, m, T_x))</span><br><span class="line">    y_pred = np.zeros((n_y, m, T_x))</span><br><span class="line"></span><br><span class="line">    a_next = a0</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T_x):</span><br><span class="line">        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t],a_next, parameters)</span><br><span class="line">        a[:,:,t] = a_next</span><br><span class="line">        y_pred[:,:,t] = yt_pred</span><br><span class="line">        caches.append(cache)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    caches = (caches, x)</span><br><span class="line">    <span class="keyword">return</span> a, y_pred, caches</span><br></pre></td></tr></table></figure></li></ol><p>You’ve successfully built the forward propagation of a recurrent neural network from scratch. This will work well enough for some applications, but it suffers from vanishing gradient problems. So it works best when each output y&lt;\t&gt; can be estimated using mainly “local” context(meaning information from inputs x&lt;\t’&gt;) where t’ is not too far from t.</p><h3 id="2-backward-propagation-for-the-basic-Recurrent-Neural-Network"><a href="#2-backward-propagation-for-the-basic-Recurrent-Neural-Network" class="headerlink" title="2-backward propagation for the basic Recurrent Neural Network"></a>2-backward propagation for the basic Recurrent Neural Network</h3><h4 id="2-1-Basic-RNN-backward-pass"><a href="#2-1-Basic-RNN-backward-pass" class="headerlink" title="2.1- Basic RNN backward pass"></a>2.1- Basic RNN backward pass</h4><p>We start by computing the backward pass for the basic RNN-cell.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-72f75c51fdc14580.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>注意，上图中右边求导数的公式只计算了相对于a_next的梯度，完整的公式需要乘上a_next相对于cost function J的梯度。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_cell_backward</span><span class="params">(da_next, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implements the backward pass for the RNN-cell (single time-step).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    da_next -- Gradient of loss with respect to next hidden state</span></span><br><span class="line"><span class="string">    cache -- python dictionary containing useful values (output of rnn_step_forward())</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dx -- Gradients of input data, of shape (n_x, m)</span></span><br><span class="line"><span class="string">                        da_prev -- Gradients of previous hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        dba -- Gradients of bias vector, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Retrieve values from cache</span></span><br><span class="line">    (a_next, a_prev, xt, parameters) = cache</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve values from parameters</span></span><br><span class="line">    Wax = parameters[<span class="string">"Wax"</span>]</span><br><span class="line">    Waa = parameters[<span class="string">"Waa"</span>]</span><br><span class="line">    Wya = parameters[<span class="string">"Wya"</span>]</span><br><span class="line">    ba = parameters[<span class="string">"ba"</span>]</span><br><span class="line">    by = parameters[<span class="string">"by"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the gradient of tanh with respect to a_next</span></span><br><span class="line">    dtanh = (<span class="number">1</span>-a_next*a_next)*da_next</span><br><span class="line">    dxt = np.dot(Wax.T, dtanh)</span><br><span class="line">    dWax = np.dot(dtanh, xt.T)</span><br><span class="line">    da_prev = np.dot(Waa.T, dtanh)</span><br><span class="line">    dWaa = np.dot(dtanh, a_prev.T)</span><br><span class="line">    dba = np.sum(dtanh, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>)</span><br><span class="line">    gradients = &#123;<span class="string">"dxt"</span>: dxt, <span class="string">"da_prev"</span>: da_prev, <span class="string">"dWax"</span>: dWax, <span class="string">"dWaa"</span>: dWaa, <span class="string">"dba"</span>: dba&#125;</span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure><p><strong>Backward pass through the RNN</strong></p><p>好难，先占坑。</p><p>Computing the gradients of the cost with respect to a&lt;\t&gt; at every time-step t is useful because it is what helps the gradient backpropagate to the previous RNN-cell To do so, you need to iterate through all the time steps starting at the end, and at each step, you increment the overall dba, dWaa, dWax and you store dx.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rnn_backward</span><span class="params">(da, caches)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the backward pass for a RNN over an entire sequence of input data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    da -- Upstream gradients of all hidden states, of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- tuple containing information from the forward pass (rnn_forward)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    gradients -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        dx -- Gradient w.r.t. the input data, numpy-array of shape (n_x, m, T_x)</span></span><br><span class="line"><span class="string">                        da0 -- Gradient w.r.t the initial hidden state, numpy-array of shape (n_a, m)</span></span><br><span class="line"><span class="string">                        dWax -- Gradient w.r.t the input's weight matrix, numpy-array of shape (n_a, n_x)</span></span><br><span class="line"><span class="string">                        dWaa -- Gradient w.r.t the hidden state's weight matrix, numpy-arrayof shape (n_a, n_a)</span></span><br><span class="line"><span class="string">                        dba -- Gradient w.r.t the bias, of shape (n_a, 1)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Retrieve values from the first cache (t=1) of caches (≈2 lines)</span></span><br><span class="line">   </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from da's and x1's shapes (≈2 lines)</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize the gradients with the right sizes (≈6 lines)</span></span><br></pre></td></tr></table></figure><h3 id="3-Forward-propagation-of-Long-Short-Term-Memory-LSTM-network"><a href="#3-Forward-propagation-of-Long-Short-Term-Memory-LSTM-network" class="headerlink" title="3- Forward propagation of Long Short-Term Memory(LSTM) network"></a>3- Forward propagation of Long Short-Term Memory(LSTM) network</h3><p>This following figure shows the operations of an LSTM-cell.<br><img src="https://upload-images.jianshu.io/upload_images/8636110-faa16612756bff66.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>Similar to the RNN example above, you will start by implementing the LSTM cell for a single time-step. Then you can iteratively call it from inside a for-loop to have it process an input with T_x time-steps.</p><p><strong>About the gates</strong></p><ul><li><p>Forget gate: For the sake of this illustration, lets assume we are reading words in a piece of text, and want use an LSTM to keep track of grammatical structures, such as whether the subject is singular or plural. If the subject changes from a singular word to a plural word, we need to find a way to get rid of our previously stored memory value of the singular/plural state. In an LSTM, the forget gate lets us do this:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-d5af764c972ec343.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>Here,  Wf are weights that govern the forget gate’s behavior. We concatenate [a⟨t−1⟩,x⟨t⟩] and multiply by Wf. The equation above results in a vector  Γ⟨t⟩f with values between 0 and 1. This forget gate vector will be multiplied element-wise by the previous cell state c⟨t−1⟩. So if one of the values of  Γ⟨t⟩fΓf⟨t⟩  is 0 (or close to 0) then it means that the LSTM should remove that piece of information (e.g. the singular subject) in the corresponding component of  c⟨t−1⟩. If one of the values is 1, then it will keep the information.</p></li><li><p>Update gate: Once we forget that the subject being discussed is singular, we need to find a way to update it to reflect that the new subject is now plural. Here is the formula for the update gate:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-58b5ff406da60509.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p></li><li><p>Updating the cell: To update the new subject we need to create a new vector of numbers that we can add to our previous cell state. The equation we use is:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-5eeabc165db7889c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p></li><li><p>Output gate: To decide which outputs we will use, we will use the following two formulas:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-3ae9380d4a619dbc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p></li></ul><h4 id="3-1-LSTM-cell"><a href="#3-1-LSTM-cell" class="headerlink" title="3.1- LSTM cell"></a>3.1- LSTM cell</h4><p>Exercise: Implement the LSTM cell described in the Figure (3).</p><p>Instructions:<br><img src="https://upload-images.jianshu.io/upload_images/8636110-002ba438ea40c34b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>直接按照公式计算即可，注意concat（矩阵拼接）的写法。可以用np.vstack((a,b))，表示竖直拼接。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_cell_forward</span><span class="params">(xt, a_prev, c_prev, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement a single forward step of the LSTM-cell as described in Figure (4)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    xt -- your input data at timestep "t", numpy array of shape (n_x, m).</span></span><br><span class="line"><span class="string">    a_prev -- Hidden state at timestep "t-1", numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">    c_prev -- Memory state at timestep "t-1", numpy array of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wi -- Weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bi -- Bias of the save gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wc -- Weight matrix of the first "tanh", numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bc --  Bias of the first "tanh", numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wo -- Weight matrix of the focus gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bo --  Bias of the focus gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">                        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a_next -- next hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    c_next -- next memory state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    yt_pred -- prediction at timestep "t", numpy array of shape (n_y, m)</span></span><br><span class="line"><span class="string">    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilda),</span></span><br><span class="line"><span class="string">          c stands for the memory value</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Retrieve parameters from "parameters"</span></span><br><span class="line">    Wf = parameters[<span class="string">"Wf"</span>]</span><br><span class="line">    bf = parameters[<span class="string">"bf"</span>]</span><br><span class="line">    Wi = parameters[<span class="string">"Wi"</span>]</span><br><span class="line">    bi = parameters[<span class="string">"bi"</span>]</span><br><span class="line">    Wc = parameters[<span class="string">"Wc"</span>]</span><br><span class="line">    bc = parameters[<span class="string">"bc"</span>]</span><br><span class="line">    Wo = parameters[<span class="string">"Wo"</span>]</span><br><span class="line">    bo = parameters[<span class="string">"bo"</span>]</span><br><span class="line">    Wy = parameters[<span class="string">"Wy"</span>]</span><br><span class="line">    by = parameters[<span class="string">"by"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of xt and Wy</span></span><br><span class="line">    n_x, m = xt.shape</span><br><span class="line">    n_y, n_a = Wy.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Concatenate a_prev and xt (≈3 lines)</span></span><br><span class="line">    <span class="comment"># concat = np.zeros([n_x+n_a, m])</span></span><br><span class="line">    <span class="comment"># concat[:n_a, :] = a_prev</span></span><br><span class="line">    <span class="comment"># concat[n_a:, :] = xt</span></span><br><span class="line">    concat = np.vstack((a_prev, xt))</span><br><span class="line">    <span class="comment"># Compute values for ft, it, cct, c_next, ot, a_next using the formulas given figure (4) (≈6 lines)</span></span><br><span class="line">    ft = sigmoid(np.dot(Wf, concat) + bf)</span><br><span class="line">    it = sigmoid(np.dot(Wi, concat) + bi)</span><br><span class="line">    cct = np.tanh(np.dot(Wc, concat) + bc)</span><br><span class="line">    c_next = ft*c_prev + it*cct</span><br><span class="line">    ot = sigmoid(np.dot(Wo, concat) + bo)</span><br><span class="line">    a_next = ot * np.tanh(c_next)</span><br><span class="line">    <span class="comment"># Compute prediction of the LSTM cell (≈1 line)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a_next, c_next, yt_pred, cache</span><br></pre></td></tr></table></figure><h4 id="3-2-Forward-pass-for-LSTM"><a href="#3-2-Forward-pass-for-LSTM" class="headerlink" title="3.2- Forward pass for LSTM"></a>3.2- Forward pass for LSTM</h4><p>Now that you have implemented one step of an LSTM, you can now iterate this over this using a for-loop to process a sequence of Tx inputs.</p><p><img src="https://upload-images.jianshu.io/upload_images/8636110-53b7e99a279cda10.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm_forward</span><span class="params">(x, a0, parameters)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Implement the forward propagation of the recurrent neural network using an LSTM-cell described in Figure (3).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Arguments:</span></span><br><span class="line"><span class="string">    x -- Input data for every time-step, of shape (n_x, m, T_x).</span></span><br><span class="line"><span class="string">    a0 -- Initial hidden state, of shape (n_a, m)</span></span><br><span class="line"><span class="string">    parameters -- python dictionary containing:</span></span><br><span class="line"><span class="string">                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wi -- Weight matrix of the save gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bi -- Bias of the save gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wc -- Weight matrix of the first "tanh", numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bc -- Bias of the first "tanh", numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wo -- Weight matrix of the focus gate, numpy array of shape (n_a, n_a + n_x)</span></span><br><span class="line"><span class="string">                        bo -- Bias of the focus gate, numpy array of shape (n_a, 1)</span></span><br><span class="line"><span class="string">                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)</span></span><br><span class="line"><span class="string">                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)</span></span><br><span class="line"><span class="string">                        </span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)</span></span><br><span class="line"><span class="string">    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)</span></span><br><span class="line"><span class="string">    caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Initialize "caches", which will track the list of all the caches</span></span><br><span class="line">    caches = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment">### START CODE HERE ###</span></span><br><span class="line">    <span class="comment"># Retrieve dimensions from shapes of xt and Wy (≈2 lines)</span></span><br><span class="line">    n_x, m, T_x = x.shape</span><br><span class="line">    n_y, n_a = parameters[<span class="string">"Wy"</span>].shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># initialize "a", "c" and "y" with zeros (≈3 lines)</span></span><br><span class="line">    a = np.zeros((n_a, m, T_x))</span><br><span class="line">    c = np.zeros((n_a, m, T_x))</span><br><span class="line">    y = np.zeros((n_y, m, T_x))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Initialize a_next and c_next (≈2 lines)</span></span><br><span class="line">    a_next = a0</span><br><span class="line">    c_next = np.zeros((n_a, m))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># loop over all time-steps</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T_x):</span><br><span class="line">        <span class="comment"># Update next hidden state, next memory state, compute the prediction, get the cache (≈1 line)</span></span><br><span class="line">        a_next, c_next, yt_pred, cache = lstm_cell_forward(x[:,:,t],a_next,c_next,parameters)</span><br><span class="line">        <span class="comment"># Save the value of the new "next" hidden state in a (≈1 line)</span></span><br><span class="line">        a[:,:,t] = a_next</span><br><span class="line">        <span class="comment"># Save the value of the prediction in y (≈1 line)</span></span><br><span class="line">        y[:,:,t] = yt_pred</span><br><span class="line">        <span class="comment"># Save the value of the next cell state (≈1 line)</span></span><br><span class="line">        c[:,:,t] = c_next</span><br><span class="line">        <span class="comment"># Append the cache into caches (≈1 line)</span></span><br><span class="line">        caches.append(cache)</span><br><span class="line">        </span><br><span class="line">    <span class="comment">### END CODE HERE ###</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># store values needed for backward propagation in cache</span></span><br><span class="line">    caches = (caches, x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> a, y, c, caches</span><br></pre></td></tr></table></figure><h3 id="4-Backward-propagation-for-LSTM"><a href="#4-Backward-propagation-for-LSTM" class="headerlink" title="4- Backward propagation for LSTM"></a>4- Backward propagation for LSTM</h3><p>占坑。</p><h2 id="Character-level-language-model-Dinosaurus-land"><a href="#Character-level-language-model-Dinosaurus-land" class="headerlink" title="Character level language model - Dinosaurus land"></a>Character level language model - Dinosaurus land</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;为什么用序列模型：Why-sequence-models&quot;&gt;&lt;a href=&quot;#为什么用序列模型：Why-sequence-models&quot; class=&quot;headerlink&quot; title=&quot;为什么用序列模型：Why sequence models?&quot;&gt;&lt;/a&gt;为
      
    
    </summary>
    
      <category term="深度学习" scheme="https://github.com/DesmonDay/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://github.com/DesmonDay/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>占坑-目标检测</title>
    <link href="https://github.com/DesmonDay/2019/04/23/deep-learningwx/"/>
    <id>https://github.com/DesmonDay/2019/04/23/deep-learningwx/</id>
    <published>2019-04-23T08:54:42.000Z</published>
    <updated>2019-04-23T09:20:22.710Z</updated>
    
    <content type="html"><![CDATA[<p>由于我主要是要了解CV，而不是深入学习。因此为了节省时间，不会再对幻灯片内容做详细的解释。未看完，占坑。</p><h1 id="目标定位：Object-Localization"><a href="#目标定位：Object-Localization" class="headerlink" title="目标定位：Object Localization"></a>目标定位：Object Localization</h1><p><img src="https://upload-images.jianshu.io/upload_images/8636110-450af11aa403610a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>下面对分类定位做详细解释：如果我们单纯的对图片分类，判断是否有行人、车之类的，可以直接通过Softmax层来输出结果；但如果我们还想要定位，比如定位车辆，那么可以让神经网络多输出几个单元，输出一个边框界(bx,by,bh,bw:被检测对象的边框化表示)。<br><img src="https://upload-images.jianshu.io/upload_images/8636110-c80a4fba678ec142.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p><p>下面讲解如何确定目标标签y。如下：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-fa12871a6a4d60a8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>其中，Pc表示图片中是否有我们要检测的对象，而bx/by/bh/bw指明对象位置，而C1/C2/C3告知我们对象的类型。举两个例子：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-be113d141fe62a9a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>可以看到，左图车存在，所以Pc=1；而右图不存在车，因此Pc=0，剩下的数字皆不必理会。</p><p>接下来定义训练神经网络的损失函数，我们用的是平方损失函数：<br><img src="https://upload-images.jianshu.io/upload_images/8636110-1fff519549149653.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>我们对Pc=y1进行分类讨论。另外，这里用平方误差简化了描述过程，在实际应用中，我们可以对c1/c2/c3和softmax激活函数应用对数损失函数并输出其中一个元素值，通常做法是对边界框坐标应用平方差，对Pc应用逻辑回归函数，甚至采用平方预测误差函数也可以。</p><h1 id="特征点检测"><a href="#特征点检测" class="headerlink" title="特征点检测"></a>特征点检测</h1><h1 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h1><h1 id="卷积的滑动窗口实现"><a href="#卷积的滑动窗口实现" class="headerlink" title="卷积的滑动窗口实现"></a>卷积的滑动窗口实现</h1><h1 id="Bounding-Box预测"><a href="#Bounding-Box预测" class="headerlink" title="Bounding Box预测"></a>Bounding Box预测</h1><h1 id="交并比"><a href="#交并比" class="headerlink" title="交并比"></a>交并比</h1><h1 id="非极大值抑制"><a href="#非极大值抑制" class="headerlink" title="非极大值抑制"></a>非极大值抑制</h1><h1 id="Anchor-Boxes"><a href="#Anchor-Boxes" class="headerlink" title="Anchor Boxes"></a>Anchor Boxes</h1><h1 id="YOLO算法"><a href="#YOLO算法" class="headerlink" title="YOLO算法"></a>YOLO算法</h1><h1 id="RPN网络"><a href="#RPN网络" class="headerlink" title="RPN网络"></a>RPN网络</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;由于我主要是要了解CV，而不是深入学习。因此为了节省时间，不会再对幻灯片内容做详细的解释。未看完，占坑。&lt;/p&gt;
&lt;h1 id=&quot;目标定位：Object-Localization&quot;&gt;&lt;a href=&quot;#目标定位：Object-Localization&quot; class=&quot;hea
      
    
    </summary>
    
      <category term="深度学习" scheme="https://github.com/DesmonDay/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="python" scheme="https://github.com/DesmonDay/tags/python/"/>
    
  </entry>
  
</feed>
